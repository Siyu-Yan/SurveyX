{
    "from": "google",
    "scholar_id": "_v8YKyWbvf8J",
    "detail_id": null,
    "title": "Zero-shot text-to-image generation",
    "abstract": "Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",
    "bib_name": "ramesh2021zero",
    "md_text": "# Zero-Shot Text-to-Image Generation\nAditya Ramesh 1 Mikhail Pavlov 1 Gabriel Goh 1 Scott Gray 1 Chelsea Voss 1 Alec Radford 1 Mark Chen 1 Ilya Sutskever 1\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5229/5229ac48-3bef-42e2-a874-ea792340dfd0.png\" style=\"width: 50%;\"></div>\n# Abstract\nText-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.\n# 1. Introduction\nModern machine learning approaches to text to image synthesis started with the work of Mansimov et al. (2015), who showed that the DRAW Gregor et al. (2015) generative model, when extended to condition on image captions, could also generate novel visual scenes. Reed et al. (2016b) later demonstrated that using a generative adversarial network (Goodfellow et al., 2014), rather than a recurrent variational auto-encoder, improved image fidelity. Reed et al. (2016b) showed that this system could not only generate objects with recognizable properties, but also could zero-shot generalize to held-out categories. Over the next few years, progress continued using a combination of methods. These include improving the generative model architecture with modifications like multi-scale generators (Zhang et al., 2017; 2018), integrating attention and auxiliary losses (Xu et al., 2018), and leveraging additional sources of conditioning information beyond just text (Reed et al., 2016a; Li et al., 2019; Koh et al., 2021). Separately, Nguyen et al. (2017) propose an energy-based framework for conditional image generation that obtained a large improvement in sample quality relative to contem-\nModern machine learning approaches to text to image synthesis started with the work of Mansimov et al. (2015), who showed that the DRAW Gregor et al. (2015) generative model, when extended to condition on image captions, could also generate novel visual scenes. Reed et al. (2016b) later demonstrated that using a generative adversarial network (Goodfellow et al., 2014), rather than a recurrent variational auto-encoder, improved image fidelity. Reed et al. (2016b) showed that this system could not only generate objects with recognizable properties, but also could zero-shot generalize to held-out categories.\nFigure 1. Comparison of original images (top) and reconstructions from the discrete VAE (bottom). The encoder downsamples the spatial resolution by a factor of 8. While details (e.g., the texture of the cat\u2019s fur, the writing on the storefront, and the thin lines in the illustration) are sometimes lost or distorted, the main features of the image are still typically recognizable. We use a large vocabulary size of 8192 to mitigate the loss of information.\nporary methods. Their approach can incorporate pretrained discriminative models, and they show that it is capable of performing text-to-image generation when applied to a captioning model pretrained on MS-COCO. More recently, Cho et al. (2020) also propose a method that involves optimizing the input to a pretrained cross-modal masked language model. While significant increases in visual fidelity have occurred as a result of the work since Mansimov et al. (2015), samples can still suffer from severe artifacts such as object distortion, illogical object placement, or unnatural blending of foreground and background elements. Recent advances fueled by large-scale generative models suggest a possible route for further improvements. Specifically, when compute, model size, and data are scaled carefully, autoregressive transformers (Vaswani et al., 2017) have achieved impressive results in several domains such as text (Radford et al., 2019), images (Chen et al., 2020), and audio (Dhariwal et al., 2020). By comparison, text-to-image generation has typically been evaluated on relatively small datasets such as MS-COCO and CUB-200 (Welinder et al., 2010). Could dataset size and\nBy comparison, text-to-image generation has typically been evaluated on relatively small datasets such as MS-COCO and CUB-200 (Welinder et al., 2010). Could dataset size and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/456d/456d073d-26ad-4583-8054-f0eca0c21e4d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. With varying degrees of reliability, our model appears to be able to combine distinct concepts in plausible ways, creat anthropomorphized versions of animals, render text, and perform some types of image-to-image translation.</div>\nmodel size be the limiting factor of current approaches? In this work, we demonstrate that training a 12-billion parameter autoregressive transformer on 250 million image-text pairs collected from the internet results in a flexible, high fidelity generative model of images controllable through natural language. The resulting system achieves high quality image generation on the popular MS-COCO dataset zero-shot, without using any of the training labels. It is preferred over prior work trained on the dataset by human evaluators 90% of the time. We also find that it is able to perform complex tasks such as image-to-image translation at a rudimentary level. This previously required custom approaches (Isola et al., 2017), rather emerging as a capability of a single, large generative model.\n# 2. Method\nOur goal is to train a transformer (Vaswani et al., 2017) to autoregressively model the text and image tokens as a single stream of data. However, using pixels directly as image tokens would require an inordinate amount of memory for high-resolution images. Likelihood objectives tend to prioritize modeling short-range dependencies between pixels (Salimans et al., 2017), so much of the modeling capacity would be spent capturing high-frequency details instead of the low-frequency structure that makes objects visually recognizable to us.\n# We address these issues by using a two-stage training procedure, similar to (Oord et al., 2017; Razavi et al., 2019):\n\u2022 Stage 1. We train a discrete variational autoencoder (dVAE)1 to compress each 256\u00d7256 RGB image into a 32 \u00d7 32 grid of image tokens, each element of\nwhich can assume 8192 possible values. This reduces the context size of the transformer by a factor of 192 without a large degradation in visual quality (see Figure 1). \u2022 Stage 2. We concatenate up to 256 BPE-encoded text tokens with the 32 \u00d7 32 = 1024 image tokens, and train an autoregressive transformer to model the joint distribution over the text and image tokens.\nThe overall procedure can be viewed as maximizing the evidence lower bound (ELB) (Kingma & Welling, 2013; Rezende et al., 2014) on the joint likelihood of the model distribution over images x, captions y, and the tokens z for the encoded RGB image. We model this distribution using the factorization p\u03b8,\u03c8(x, y, z) = p\u03b8(x | y, z)p\u03c8(y, z), which yields the lower bound\nln p\u03b8,\u03c8(x, y) \u2a7eE z\u223cq\u03c6(z | x) \ufffd ln p\u03b8(x | y, z) \u2212\n(1)\n# where:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3fd3/3fd367bf-a820-4c0f-8930-71a16de3f3aa.png\" style=\"width: 50%;\"></div>\n# 2.1. Stage One: Learning the Visual Codebook\nIn the first stage of training, we maximize the ELB with respect to \u03c6 and \u03b8, which corresponds to training a dVAE on the images alone. We set the initial prior p\u03c8 to the uniform categorical distribution over the K = 8192 codebook vectors, and q\u03c6 to be categorical distributions parameterized by the 8192 logits at the same spatial position in the 32\u00d732 grid output by the encoder. The ELB now becomes difficult to optimize: as q\u03c8 is a discrete distribution, and we cannot use the reparameterization gradient to maximize it. Oord et al. (2017); Razavi et al. (2019) address this using an online cluster assignment procedure coupled with the straight-through estimator (Bengio et al., 2013). We instead use the gumbel-softmax relaxation (Jang et al., 2016; Maddison et al., 2016), replacing the expectation over q\u03c6 with one over q\u03c4 \u03c6, where the relaxation becomes tight as the temperature \u03c4 \u21920. The likelihood for p\u03b8 is evaluated using the log-laplace distribution (see\nIn the first stage of training, we maximize the ELB with respect to \u03c6 and \u03b8, which corresponds to training a dVAE on the images alone. We set the initial prior p\u03c8 to the uniform categorical distribution over the K = 8192 codebook vectors, and q\u03c6 to be categorical distributions parameterized by the 8192 logits at the same spatial position in the 32\u00d732 grid output by the encoder.\nThe ELB now becomes difficult to optimize: as q\u03c8 is a discrete distribution, and we cannot use the reparameterization gradient to maximize it. Oord et al. (2017); Razavi et al. (2019) address this using an online cluster assignment procedure coupled with the straight-through estimator (Bengio et al., 2013). We instead use the gumbel-softmax relaxation (Jang et al., 2016; Maddison et al., 2016), replacing the expectation over q\u03c6 with one over q\u03c4 \u03c6, where the relaxation becomes tight as the temperature \u03c4 \u21920. The likelihood for p\u03b8 is evaluated using the log-laplace distribution (see\nAppendix A.3 for a derivation). The relaxed ELB is maximized using Adam (Kingma & Ba, 2014) with exponentially weighted iterate averaging. Appendix A.2 gives a complete description of the hyperparameters, but we found the following to be especially important for stable training: \u2022 Specific annealing schedules for the relaxation temperature and step size. We found that annealing \u03c4 to 1/16 was sufficient to close the gap between the relaxed validation ELB and the true validation ELB with q\u03c6 intsead of q\u03c4 \u03c6. \u2022 The use of 1 \u00d7 1 convolutions at the end of the encoder and the beginning of the decoder. We found that reducing the receptive field size for the convolutions around the relaxation led to it generalizing better to the true ELB. \u2022 Multiplication of the outgoing activations from the encoder and decoder resblocks by a small constant, to ensure stable training at initialization. We also found that increasing the KL weight to \u03b2 = 6.6\npromotes better codebook usage and ultimately leads to a smaller reconstruction error at the end of training.4\n# 2.2. Stage Two: Learning the Prior\nIn the second stage, we fix \u03c6 and \u03b8, and learn the prior distribution over the text and image tokens by maximizing the ELB with respect to \u03c8. Here, p\u03c8 is represented by a 12-billion parameter sparse transformer (Child et al., 2019). Given a text-image pair, we BPE-encode (Sennrich et al., 2015) the lowercased caption using at most 256 tokens5 with vocabulary size 16,384, and encode the image using 32 \u00d7 32 = 1024 tokens with vocabulary size 8192. The image tokens are obtained using argmax sampling from the dVAE encoder logits, without adding any gumbel noise.6 Finally, the text and image tokens are concatenated and modeled autoregressively as a single stream of data. The transformer is a decoder-only model in which each image token can attend to all text tokens in any one of its 64 self-attention layers. The full architecture is described in Appendix B.1. There are three different kinds of self-attention masks used in the model. The part of the attention masks corresponding to the text-to-text attention is the standard causal mask, and the part for the image-to-image attention uses either a row, column, or convolutional attention mask.7 We limit the length of a text caption to 256 tokens, though it is not totally clear what to do for the \u201cpadding\u201d positions in between the last text token and the start-of-image token. One option is to set the logits for these tokens to \u2212\u221ein the self-attention operations. Instead, we opt to learn a special padding token separately for each of the 256 text positions. This token is used only when no text token is available. In preliminary experiments on Conceptual Captions (Sharma et al., 2018), we found that this resulted in higher validation loss, but better performance on out-of-distribution captions. 4This is contrary to the usual tradeoff between the two terms. We speculate that for smaller values of \u03b2, the noise from the relaxation causes the optimizer to reduce codebook usage toward the beginning of training, resulting in worse ELB at convergence. 5During training, we apply 10% BPE dropout (Provilkov et al., 2019), whose use is common in the neural machine translation literature. 6Strictly speaking, Equation 1 requires us to sample from the categorical distribution specified by the dVAE encoder logits, rather than taking the argmax. In preliminary experiments on ImageNet, we found that this was a useful regularizer in the overparameterized regime, and allows the transformer to be trained using soft targets for the cross-entropy loss. We decided against this\n4This is contrary to the usual tradeoff between the two terms. We speculate that for smaller values of \u03b2, the noise from the relaxation causes the optimizer to reduce codebook usage toward the beginning of training, resulting in worse ELB at convergence. 5During training, we apply 10% BPE dropout (Provilkov et al., 2019), whose use is common in the neural machine translation literature. 6Strictly speaking, Equation 1 requires us to sample from the categorical distribution specified by the dVAE encoder logits, rather than taking the argmax. In preliminary experiments on ImageNet, we found that this was a useful regularizer in the overparameterized regime, and allows the transformer to be trained using soft targets for the cross-entropy loss. We decided against this here since the model in consideration is in the underparameterized regime. 7We found using a single attention operation for all three interactions \u2013 \u201ctext attends to text\u201d, \u201cimage attends to text\u201d, and \u201cimage attends to image\u201d \u2013 to perform better than using separate attention operations that are independently normalized.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/299c/299c2968-463f-4067-8936-fc13811f49b4.png\" style=\"width: 50%;\"></div>\nFigure 4. Illustration of per-resblock gradient scaling for a transformer resblock. The solid line indicates the sequence of operations for forward propagation, and the dashed line the sequence of operations for backpropagation. We scale the incoming gradient for each resblock by its gradient scale, and unscale the outgoing gradient before it is added to the sum of the gradients from the successive resblocks. The activations and gradients along the identity path are stored in 32-bit precision. The \u201cfilter\u201d operation sets all Inf and NaN values in the activation gradient to zero. Without this, a nonfinite event in the current resblock would cause the gradient scales for all preceding resblocks to unnecessarily drop, thereby resulting in underflow.\nWe normalize the cross-entropy losses for the text and image tokens by the total number of each kind in a batch of data. Since we are primarily interested in image modeling, we multiply the cross-entropy loss for the text by 1/8 and the cross-entropy loss for the image by 7/8. The objective is optimized using Adam with exponentially weighted iterate averaging; Appendix B.2 describes the training procedure in more detail. We reserved about 606,000 images for validation, and found no signs of overfitting at convergence.\n# 2.3. Data Collection\nOur preliminary experiments for models up to 1.2 billion parameters were carried out on Conceptual Captions, a dataset of 3.3 million text-image pairs that was developed as an extension to MS-COCO (Lin et al., 2014).\nTo scale up to 12-billion parameters, we created a dataset of a similar scale to JFT-300M (Sun et al., 2017) by collecting 250 million text-images pairs from the internet. This dataset does not include MS-COCO, but does include Conceptual Captions and a filtered subset of YFCC100M (Thomee et al., 2016). As MS-COCO was created from the latter, our training data includes a fraction of the MS-COCO validation images (but none of the captions). We control for this in the quantitative results presented in Section 3 and find that it has\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f81/6f812976-54af-4952-b214-abb5fee93941.png\" style=\"width: 50%;\"></div>\nFigure 5. Communication patterns used for distributed training. Each parameter array in the model is sharded among the eight GPUs on each machine. During forward propagation, we prefetch the parameter shards for the next resblock (using all-gather) while computing the activations for the current resblock. To conserve memory, the parameter shards from the other GPUs are immediately discarded. Similarly, during backpropagation, we prefetch the parameter shards for the previous resblock while computing the activations and gradients for the current resblock. After all GPUs have computed the gradient with respect to an all-gathered parameter, the reduce-scatter operation leaves each GPU with only one slice \u2013 i.e., the gradient for its parameter shard, averaged over the eight GPUs.\n# no appreciable bearing on the results. We provide further details about the data collection process in Appendix C.\n# 2.4. Mixed-Precision Training\nTo save GPU memory and increase throughput, most parameters, Adam moments, and activations are stored in 16-bit precision. We also use activation checkpointing and recompute the activations within the resblocks during the backward pass. Getting the model to train in 16-bit precision past one billion parameters, without diverging, was the most challenging part of this project.\nWe believe the root cause of this instability to be underflow in the 16-bit gradients. Appendix D presents a set of guidelines we developed to avoid underflow when training large-scale generative models. Here, we describe one of these guidelines: per-resblock gradient scaling.\nSimilar to prior work (Liu et al., 2020), we found that the norms of the activation gradients from the resblocks decrease monotonically as we move from the earlier resblocks\nEffective Parameter Count\nCompression Rank\nCompression Rate\n2.8 \u00b7 109 (dmodel = 1920)\n512\n\u224883%\n5.6 \u00b7 109 (dmodel = 2688)\n640\n\u224885%\n12.0 \u00b7 109 (dmodel = 3968)\n896\n\u224886%\nTable 1. We show the relationship between model size and the minimum compression rank for the gradients (up to a multiple of 128) necessary to avoid a gap in the training loss during the first 10% of training. These results suggest that in our setting, we can achieve a compression rate of about 85%, independent of model size.\nto the later ones.8 As the model is made deeper and wider, the true exponents of the activation gradients for later resblocks can fall below the minimum exponent of the 16-bit format. Consequently, they get rounded to zero, a phenomenon called underflow. We found that eliminating underflow allowed for stable training to convergence. Standard loss scaling (Micikevicius et al., 2017) is able to avoid underflow when the range spanned by the smallest and largest activation gradients (in absolute value) fits within the exponent range of the 16-bit format. On NVIDIA V100 GPUs, this exponent range is specified by five bits. While this is sufficient for training vanilla language models of the same size, we found the range to be too small for the text-to-image model. Our fix, which is shown in Figure 4, involves using a separate \u201cgradient scale\u201d for each resblock in the model. This can be seen as a practical alternative to a more general framework for mixed-precision training called Flexpoint (K\u00f6ster et al., 2017), with the advantage that specialized GPU kernels are not required. We found that Sun et al. (2020) had independently developed similar procedure for training convolutional networks in 4-bit precision.\nto the later ones.8 As the model is made deeper and wider, the true exponents of the activation gradients for later resblocks can fall below the minimum exponent of the 16-bit format. Consequently, they get rounded to zero, a phenomenon called underflow. We found that eliminating underflow allowed for stable training to convergence.\n# 2.5. Distributed Optimization\nOur 12-billion parameter model consumes about 24 GB of memory when stored in 16-bit precision, which exceeds the memory of a 16 GB NVIDIA V100 GPU. We address this using parameter sharding (Rajbhandari et al., 2019). As shown in Figure 5, parameter sharding allows us to almost completely hide the latency of the intra-machine communication by overlapping it with compute-intensive operations.\nOn the cluster used to train the model, the bandwidth between machines is much lower than the bandwidth among GPUs on the same machine. This makes the cost of the operation used to average the gradient among the machines (all-reduce) the main bottleneck during training. We were\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7e3/d7e36da0-9851-44d1-a51a-8d7253be7a1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6. Effect of increasing the number of images for the contrastive reranking procedure on MS-COCO captions.</div>\nable to drastically reduce this cost by compressing the gradients using PowerSGD (Vogels et al., 2019).\ndients using PowerSGD (Vogels et al., 2019). In our implementation, each GPU in a machine computes the low-rank factors for its parameter shard gradients independently of its neighboring GPUs.9 Once the low-rank factors are computed, each machine sets its error buffer to the residual between the uncompressed gradient averaged over its eight GPUs (obtained from reduce-scatter), and the decompressed gradient obtained from the low-rank factors. PowerSGD replaces the large communication operation for an uncompressed parameter gradient with two, much smaller communication operations for its low-rank factors. For a given compression rank r and transformer activation size dmodel, the compression rate is given by 1 \u2212 5r/(8dmodel) (see Appendix E.1). Table 1 shows that we can achieve a compression rate of about 85%, independent of model size.\nIn Appendix E.2, we describe various details that were necessary to get PowerSGD to perform well at scale. These include:\n\u2022 Saving memory by accumulating the gradient into the error buffers during backpropagation, rather than allocating separate buffers.\n9There is still intra-machine communication for other operations; what we mean is that the low-rank factors across the shards, when concatenated, are not regarded as collectively approximating the gradient for the full parameter matrix.\n\u2022 Minimizing instances in which we zero out the error buffers (e.g., due to nonfinite values encountered during mixed-precision backpropagation, or when resuming training from a checkpoint). \u2022 Improving numerical stability by using Householder orthogonalization instead of Gram-Schmidt, together with the addition of a small multiple of the identity matrix to the input. \u2022 Avoiding underflow by using a custom 16-bit floating point format for the error buffers, their low-rank factors, and the all-reduce communication operations involving them.\nWe also found the warm-start procedure for the Q matrix described in Vogels et al. (2019) to be unnecessary: we were able to get equivalent results by fixing Q to a random gaussian matrix at the start of training, and never updating it.10\n# 2.6. Sample Generation\nSimilar to Razavi et al. (2019), we rerank the samples drawn from the transformer using a pretrained contrastive model (Radford et al., 2021). Given a caption and a candidate image, the contrastive model assigns a score based on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d69/1d69576e-e57d-4d35-ae08-5737d43cf309.png\" style=\"width: 50%;\"></div>\nFigure 7. Human evaluation of our model (evaluated zero-shot without temperature reduction) vs prior work (DF-GAN) on captions from MS-COCO. In a best-of-five vote, our model\u2019s sample was chosen as the most realistic 90.0% of the time, and was chosen as the image best matching a shared caption 93.3% of the time.\n<div style=\"text-align: center;\">Figure 7. Human evaluation of our model (evaluated zero-shot without temperature reduction) vs prior work (DF-GAN) on captions from MS-COCO. In a best-of-five vote, our model\u2019s sample was chosen as the most realistic 90.0% of the time, and was chosen as the image best matching a shared caption 93.3% of the time.</div>\nhow well the image matches the caption. Figure 6 shows the effect of increasing the number of samples N from which we select the top k images. This process can be seen as a kind of language-guided search (Andreas et al., 2017), and is also similar to the auxiliary text-image matching loss proposed by Xu et al. (2018). Unless otherwise stated, all samples used for both qualitative and quantitative results are obtained without temperature reduction (i.e., using t = 1) (except for Figure 2) and use reranking with N = 512.\n# 3. Experiments\n# 3.1. Quantitative Results\nWe evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN (Xu et al., 2018), DMGAN (Zhu et al., 2019), and DF-GAN (Tao et al., 2020), the last of which reports the best Inception Score (Salimans et al., 2016) and Fr\u00e9chet Inception Distance (Heusel et al., 2017) on MS-COCO. Figure 3 qualitatively compares samples from our model to those from prior work. We also conduct a human evaluation similar to the one used in Koh et al. (2021) to compare our approach to DF-GAN, the results of which are shown in Figure 7. Given a caption, the sample from our model receives the majority vote for better matching the caption 93% of the time. It also receives the majority vote for being more realistic 90% of the time. Figure 9(a) shows that our model also obtains an FID score on MS-COCO within 2 points of the best prior approach, despite having never been trained on the captions. Our training data incorporates a filtered subset of YFCC100M,\nWe evaluate our model zero-shot by comparing it to three prior approaches: AttnGAN (Xu et al., 2018), DMGAN (Zhu et al., 2019), and DF-GAN (Tao et al., 2020), the last of which reports the best Inception Score (Salimans et al., 2016) and Fr\u00e9chet Inception Distance (Heusel et al., 2017) on MS-COCO. Figure 3 qualitatively compares samples from our model to those from prior work.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fccf/fccfd9b7-8afc-462d-9db4-14178cca3327.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8. Zero-shot samples from our model on the CUB dataset.</div>\nand we found that it includes about 21% of the images in the MS-COCO validation set from a de-duplication procedure described in the next section. To isolate this effect, we compute the FID statistics for the validation set both with these images (solid lines) and without them (dashed lines), finding no significant change in the results. Training the transformer on the tokens from the dVAE encoder allows us to allocate its modeling capacity to the low-frequency information that makes images visually recognizable to us. However, it also disadvantages the model, since the heavy compression renders it unable to produce high-frequency details. To test the effect of this on the quantitative evaluations, we compute the FID and IS in Figure 9(a) after applying a Gaussian filter with varying radius to both the validation images and samples from the models. Our approach achieves the best FID by a margin of about 6 points with a slight blur of radius 1. The gap between our approach and others tends to widen as the blur radius is increased. We also obtain the highest IS when the blur radius is greater than or equal to two. Our model fares significantly worse on the CUB dataset, for which there is a nearly 40-point gap in FID between our model and the leading prior approach (Figure 9(b)). We found an 12% overlap rate for this dataset, and again observed no significant difference in the results after removing these images. We speculate that our zero-shot approach is less likely to compare favorably on specialized distributions such as CUB. We believe that fine-tuning is a promising direction for improvement, and leave this investigation to future work. Samples from our model for captions in this dataset are shown in Figure 8. Finally, Figure 9(c) shows clear improvements in FID and IS for MS-COCO as the sample size used for reranking with the contrastive model is increased. This trend continues up to a sample size of 32, after which we observe diminishing\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1bad/1bad105b-23b4-45dd-a314-1eb698e16618.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/73b2/73b2a562-17b3-4114-8f33-567f0c88ed5b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) FID and IS on MS-COCO as a function of blur radius.</div>\n<div style=\"text-align: center;\">(b) FID and IS on CUB as a function of blur radius.</div>\nFigure 9. Quantitative results on MS-COCO and CUB. Solid lines represent FID computed against the original validation sets, and dashed lines represent FID computed against validation sets with overlapping images removed (see Section 3.2). For MS-COCO, we evaluate all models on a subset of 30,000 captions sampled from the validation set. For CUB, we evaluate all models on all of the unique captions in the test set. We compute the FID and IS using the DM-GAN code, which is available at https://github.com/MinfengZhu/DM-GAN.\nreturns.\n# 3.2. Data Overlap Analysis\nWe used the deduplication procedure described in Radford et al. (2021) to determine which images to remove. For each validation image, we find the closest image in the training data using a contrastive model specifically trained for this task. We then sort the images in descending order by closeness to their nearest matches in the training data. After inspecting the results by hand, we determine the images to remove by manually selecting a conservative threshold designed to minimize the false negative rate.\n# 3.3. Qualitative Findings\nWe found that our model has the ability to generalize in ways that we did not originally anticipate. When given the caption \u201ca tapir made of accordion...\u201d (Figure 2a), the model appears to draw a tapir with an accordion for a body, or an accordion whose keyboard or bass are in the shape of a tapir\u2019s trunk or legs. This suggests that it has developed a rudimentary ability to compose unusual concepts at high levels of abstraction.\nOur model also appears to be capable of combinatorial generalization, such as when rendering text (Figure 2b) or when probed on sentences like \u201can illustration of a baby hedgehog in a christmas sweater walking a dog\u201d (Figure 2c). Prompts\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66c3/66c38e6f-bc74-49f3-9ed2-f20e09e40a60.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) FID and IS on MS-COCO as a function of the sample size used for reranking.</div>\nlike the latter require the model to perform variable binding (Smolensky, 1990; Greff et al., 2020) \u2013 it is the hedgehog that is in the christmas sweater, not the dog. We note, however, that the model performs inconsistently on the task, sometimes drawing both animals with christmas sweaters, or drawing a hedgehog walking a smaller hedgehog.\nTo a limited degree of reliability, we also find our model to be capable of zero-shot image-to-image translation controllable by natural language (Figure 2d). When the model is given the caption \u201cthe exact same cat on the top as a sketch at the bottom\u201d and the top 15 \u00d7 32 part of the image token grid for a photo of a cat, it is able to draw a sketch of a similar looking cat on the bottom.\nThis works with several other kinds of transformations, including image operations (e.g., changing the color of the image, converting it to grayscale, or flipping it upside-down) and style transfer (e.g., drawing the cat on a greeting card, a postage stamp, or a cell phone case). Some transformations, such as those that involve only changing the color of the animal, suggest that the model is capable of performing a rudimentary kind of object segmentation. We provide additional examples of zero-shot image-to-image translation in Section G.\n# 4. Conclusion\nWe investigate a simple approach for text-to-image generation based on an autoregressive transformer, when it is executed at scale. We find that scale can lead to improved generalization, both in terms of zero-shot performance relative to previous domain-specific approaches, and in terms of the range of capabilities that emerge from a single generative model. Our findings suggest that improving generalization as a function of scale may be a useful driver for progress on this task.\n# Acknowledgements\nWe would like to thank Matthew Knight for reviewing the code release for this work, and Rewon Child, John Schulman, Heewoo Jun, and Prafulla Dhariwal for helpful early feedback on the paper. We would also like to thank Jong Wook Kim for writing the PyTorch package for the contrastive model described in Radford et al. (2019) that we used to rerank the samples from our model.\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. Draw: A recurrent neural network for image generation. In International Conference on Machine Learning, pp. 1462\u20131471. PMLR, 2015.\n# References\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings in deep residual networks. In European conference on computer vision, pp. 630\u2013645. Springer, 2016.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint arXiv:1706.08500, 2017.\nJang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144, 2016.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\nKoh, J. Y., Baldridge, J., Lee, H., and Yang, Y. Text-toimage generation grounded by fine-grained user attention. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 237\u2013246, 2021.\nSennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015.\n",
    "paper_type": "method",
    "attri": {
        "background": "Text-to-image generation has traditionally focused on better modeling assumptions for training on fixed datasets. Previous methods have involved complex architectures and additional information such as object part labels. A new approach is necessary to improve the fidelity and flexibility of image generation.",
        "problem": {
            "definition": "The issue addressed in this paper is the need for a more effective method of generating images from textual descriptions that can generalize well to new categories without extensive retraining.",
            "key obstacle": "Existing methods often suffer from severe artifacts such as object distortion and illogical object placement, limiting their effectiveness in generating high-quality images."
        },
        "idea": {
            "intuition": "The idea is inspired by the potential of autoregressive transformers to model complex distributions effectively when trained on large datasets.",
            "opinion": "The proposed method involves training a 12-billion parameter autoregressive transformer on a large dataset of image-text pairs, allowing it to generate high-fidelity images from text descriptions.",
            "innovation": "This method differs from existing approaches by utilizing a transformer architecture that models text and image tokens as a single stream of data, leveraging scale to enhance performance."
        },
        "method": {
            "method name": "Autoregressive Transformer for Text-to-Image Generation",
            "method abbreviation": "AT-TIG",
            "method definition": "The method involves training a transformer to autoregressively model text and image tokens, using a two-stage training procedure with a discrete variational autoencoder.",
            "method description": "The core of the method is a two-stage training approach that compresses images into tokens and models the joint distribution of text and image tokens.",
            "method steps": [
                "Stage 1: Train a discrete variational autoencoder to compress images into a grid of image tokens.",
                "Stage 2: Concatenate text tokens with image tokens and train an autoregressive transformer to model the joint distribution."
            ],
            "principle": "The effectiveness of this method lies in its ability to capture low-frequency structures of images while allowing for high-quality image generation through a large-scale transformer model."
        },
        "experiments": {
            "evaluation setting": "The model was evaluated zero-shot on the MS-COCO dataset, comparing its performance against existing methods like AttnGAN, DMGAN, and DF-GAN.",
            "evaluation method": "Performance was assessed using metrics such as Inception Score and Fr\u00e9chet Inception Distance, along with human evaluations of image quality and caption matching."
        },
        "conclusion": "The experiments demonstrate that the proposed autoregressive transformer approach achieves high-quality image generation and generalizes effectively to new categories, suggesting that scaling models can drive progress in text-to-image generation.",
        "discussion": {
            "advantage": "The key advantages of this approach include its ability to generate high-fidelity images without extensive retraining and its flexibility in handling various image generation tasks.",
            "limitation": "A limitation of the method is its performance on specialized datasets, such as CUB, where it struggles compared to leading prior approaches.",
            "future work": "Future research could focus on fine-tuning the model for specialized distributions to improve performance on niche datasets."
        },
        "other info": {
            "additional notes": "The model uses mixed-precision training to optimize memory usage and throughput, and employs distributed optimization techniques to handle large parameter sizes."
        }
    },
    "mount_outline": [
        {
            "section number": "6.2",
            "key information": "The proposed method involves training a 12-billion parameter autoregressive transformer on a large dataset of image-text pairs, allowing it to generate high-fidelity images from text descriptions."
        },
        {
            "section number": "5.1",
            "key information": "The method involves training a transformer to autoregressively model text and image tokens, using a two-stage training procedure with a discrete variational autoencoder."
        },
        {
            "section number": "5.2",
            "key information": "The experiments demonstrate that the proposed autoregressive transformer approach achieves high-quality image generation and generalizes effectively to new categories."
        },
        {
            "section number": "7.4",
            "key information": "Future research could focus on fine-tuning the model for specialized distributions to improve performance on niche datasets."
        },
        {
            "section number": "4.1",
            "key information": "A limitation of the method is its performance on specialized datasets, such as CUB, where it struggles compared to leading prior approaches."
        }
    ],
    "similarity_score": 0.6801738281694923,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5229/5229ac48-3bef-42e2-a874-ea792340dfd0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/456d/456d073d-26ad-4583-8054-f0eca0c21e4d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3fd3/3fd367bf-a820-4c0f-8930-71a16de3f3aa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/299c/299c2968-463f-4067-8936-fc13811f49b4.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f81/6f812976-54af-4952-b214-abb5fee93941.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7e3/d7e36da0-9851-44d1-a51a-8d7253be7a1c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d69/1d69576e-e57d-4d35-ae08-5737d43cf309.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fccf/fccfd9b7-8afc-462d-9db4-14178cca3327.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1bad/1bad105b-23b4-45dd-a314-1eb698e16618.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/73b2/73b2a562-17b3-4114-8f33-567f0c88ed5b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66c3/66c38e6f-bc74-49f3-9ed2-f20e09e40a60.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0011_large/papers/Zero-shot text-to-image generation.json"
}