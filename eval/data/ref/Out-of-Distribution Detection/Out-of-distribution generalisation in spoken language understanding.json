{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2407.07425",
    "title": "Out-of-distribution generalisation in spoken language understanding",
    "abstract": "Test data is said to be out-of-distribution (OOD) when it unexpectedly differs from the training data, a common challenge in real-world use cases of machine learning. Although OOD generalisation has gained interest in recent years, few works have focused on OOD generalisation in spoken language understanding (SLU) tasks. To facilitate research on this topic, we introduce a modified version of the popular SLU dataset SLURP, featuring data splits for testing OOD generalisation in the SLU task. We call our modified dataset SLURP For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find end-to-end SLU models to have limited capacity for generalisation. Furthermore, by employing model interpretability techniques, we shed light on the factors contributing to the generalisation difficulties of the models. To improve the generalisation, we experiment with two techniques, which improve the results on some, but not all the splits, emphasising the need for new techniques. Index Terms: spoken language understanding, generalisation, out-of-distribution, model interpretability",
    "bib_name": "porjazovski2024outofdistributiongeneralisationspokenlanguage",
    "md_text": "# Out-of-distribution generalisation in spoken language understanding\n# Dejan Porjazovski, Anssi Moisio, Mikko Kurimo\nAalto University, Finland\ndejan.porjazovski@aalto.fi, anssi.moisio@aalto.fi, mikko.kurimo@aalto.fi\n# Abstract\nTest data is said to be out-of-distribution (OOD) when it unexpectedly differs from the training data, a common challenge in real-world use cases of machine learning. Although OOD generalisation has gained interest in recent years, few works have focused on OOD generalisation in spoken language understanding (SLU) tasks. To facilitate research on this topic, we introduce a modified version of the popular SLU dataset SLURP, featuring data splits for testing OOD generalisation in the SLU task. We call our modified dataset SLURP For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find end-to-end SLU models to have limited capacity for generalisation. Furthermore, by employing model interpretability techniques, we shed light on the factors contributing to the generalisation difficulties of the models. To improve the generalisation, we experiment with two techniques, which improve the results on some, but not all the splits, emphasising the need for new techniques. Index Terms: spoken language understanding, generalisation, out-of-distribution, model interpretability\n# 1. Introduction\nSpoken language understanding (SLU) systems are the backbone of human-computer interaction devices that need to understand the meaning of the utterance before taking an action. When these models are deployed for real-world use, their performance should be consistent even when presented with outof-distribution (OOD) data that differs from the training distribution in an unpredictable manner [1]. Multiple types of OOD generalisation capacities are desirable but not necessarily achieved by neural SLU systems. Length generalisation (see e.g. [2, 3]) is the capacity to process sequences that are longer (or shorter) than those seen in the training set. Out-of-vocabulary (OOV) generalisation is needed when the test set includes words or other units not seen in the training set [4, 5]. A third type is compositional generalisation (CG) [6, 7], required when test samples combine familiar units in novel ways. For example, novel combinations of slot types in a slot filling task has been shown to pose difficulties for SLU systems [8, 9]. More generally, neural (NLP) systems have been found often to fail in tasks that require CG [10, 11, 12], although they do have some capacity for CG [13]. In addition to the generalisation types applicable to both text and audio sequences, some types are specific to audio-based models, such as diverse accents [14], different age groups [15], and various acoustic environments [16]. The traditional pipeline SLU systems consist of two submodules: an automatic speech recognition (ASR) system that generates transcripts and a separate text-based natural language\nunderstanding (NLU) system that extracts meaning from the generated transcripts [17, 9]. Previous studies that explore generalisation in SLU employ a pipeline system, focusing on the transcripts instead of the original audio [18, 9]. As these studies focus on the text-based language understanding task, the proposed evaluation sets are mostly text-only data, and as such, they can not always be applied to the end-to-end (E2E) SLU models that rely solely on audio [19, 20, 5]. The main reason is that text is easy to segment due to word boundaries, whereas adding or removing segments of an audio signal corresponding to certain tokens in the transcript is challenging. Moreover, the audio models\u2019 generalisation criteria can differ from those of text-based models. Despite these challenges, a few previous works have evaluated the generalisation capabilities of E2E SLU models. In [4], a system was trained on three specific phrases and tested with also a new phrase, assessing the OOV generalisation. Similarly, in [5], E2E SLU models were tested on unique wordings not seen in training. The findings revealed a notable increase in error rates as the number of unseen wordings increased. These studies, however, predominantly focus on a singular type of generalisation. The authors of [21] noted that generalisation in SLU tasks may be divided into two types: generalising to diverse speakers and generalising to diverse phrases (e.g., ngrams). These two types may be orthogonal and thus can be assessed separately. We argue, in the same vein, that there are multiple types of generalisation that SLU models should ideally achieve. However, our data splitting methods differ from those in [21] in multiple ways. The n-gram-based splits in [21] may result in OOV words in the test set, or possibly novel combinations (n-grams) of familiar words, requiring compositional generalisation. We distinguish OOV splits from CG splits, allowing for a more fine-grained evaluation. Moreover, we focus on slightly more abstract units than n-grams, namely the scenario and action labels. Our methods and resources therefore aim to complement the previous works, adding to the collection of resources enabling development of more robust SLU systems. We introduce OOV, CG, and microphone mismatch splits of the SLURP dataset [22], which we call SLURP For OOD generalisation (SLURPFOOD). Utilising these splits, we assess SLU models\u2019 capacity for OOD generalisation. As a benchmark, we report results for pre-trained self-supervised models. We also leverage a model interpretability method (Integrated Gradients [23]) to determine the underlying causes contributing to the limited generalisation of the systems. To improve the generalisation, we explore two techniques: TOPK [24] and segmented processing. We make the SLURPFOOD splits, the models, and the code for reproducing all the experiments publicly available1.\n1https://github.com/aalto-speech/slurpfood\nTable 1: Statistics for the proposed split configurations. OOV is out-of-vocabulary, CG is compositional generalisation, and DA is Double-Action. The mic mismatch split is not included due to its similarity to the original splits. The table shows class overlaps (intersections of class sets) between the train, dev, and test subsets for each label type (scenario, action, intent). In the brackets are the similarities C\u03b1 (see Section 2) of the frequency distributions of labels in the different subsets.\nOriginal\nNon-OOV\nOOV\nNon-CG\nCG\nDA Non-CG\nDA CG\nSubset size\nTrain\n50627\n36287\n36286\n29750\n32651\n36064\n36064\nDev\n8690\n6216\n6216\n3302\n3610\n6192\n6192\nTest\n13078\n3444\n3444\n1480\n1480\n3500\n3500\nAction overlap\nTrain-Test\n46\n(1.00)\n22\n(0.71)\n2\n(0.32)\n44\n(0.99)\n39\n(0.83)\n39\n(0.97)\n35\n(0.93)\nDev-Test\n45\n(0.99)\n21\n(0.71)\n2\n(0.33)\n43\n(0.98)\n38\n(0.83)\n38\n(0.96)\n37\n(0.92)\nScenario overlap\nTrain-Test\n18\n(1.00)\n13\n(0.83)\n13\n(0.83)\n18\n(1.00)\n16\n(0.84)\n12\n(1.00)\n12\n(1.00)\nDev-Test\n18\n(1.00)\n13\n(0.81)\n13\n(0.81)\n18\n(1.00)\n16\n(0.85)\n12\n(1.00)\n12\n(1.00)\nIntent overlap\nTrain-Test\n59\n(1.00)\n23\n(0.88)\n0\n(0.00)\n57\n(0.99)\n41\n(0.39)\n27\n(0.74)\n0\n(0.00)\nDev-Test\n58\n(0.99)\n22\n(0.87)\n0\n(0.00)\n55\n(0.97)\n39\n(0.38)\n20\n(0.56)\n0\n(0.00)\n# 2. SLURPFOOD\nSLURP consists of 72395 recordings of 16520 utterances. Each sentence has 5 types of annotations: transcript, named entities, part-of-speech tags, scenario, and action. Scenario is a label that indicates the general situation or context for the sentence, for example \u2018qa\u2019 (question answering), \u2018audio\u2019, or \u2018transport\u2019. The action indicates what the speaker wants the (supposed) system to do (although often not very precisely), for example \u2018factoid\u2019 in the \u2018qa\u2019 scenario, \u2018volume up\u2019 in the \u2018audio\u2019 scenario, or \u2018taxi\u2019 in the \u2018transport\u2019 scenario. There is also a sixth, redundant annotation called intent, which is the combination of scenario and action. For more details about SLURP, see [22]. We propose four distinct split configurations to assess generalisation in SLU. The mismatched acoustic environment splits are applicable to the scenario and intent classification tasks. The CG splits can be applied to scenario classification or be adapted for intent classification by predicting the scenarios and actions separately, for example by using a multi-task model. The OOV and Double-Action CG (DA CG) splits can be used only for the scenario classification task because their test sets include some action classes that are not in the training sets. The statistics of the splits are presented in Table 1. OOV splits. To create the OOV splits, we first select a test set, shared between the OOV and non-OOV portions. In this task, the test subset is sampled from the original SLURP test set, ensuring there are enough different intents. To create the OOV training and dev subsets, we removed the intents present in the test set from the original training and dev subsets. This allows us to assess the model\u2019s OOV generalisation ability when the scenario is seen in training, but the task (action) is novel. For instance, the training set may contain \u2018launch super mario\u2019 where the scenario is \u2018play\u2019 and the action is \u2018game\u2019. Conversely, the test set could include an instruction \u2018please play the next episode in the podcast\u2019, maintaining the \u2018play\u2019 scenario, but in a combination with a novel \u2018podcast\u2019 action. To create the non-OOV training and development subsets to contrast with the OOV subsets, we replaced half of the difficult splits with elements containing intents present in the test set. While adding the new samples, we made sure that the scenario distribution remained the same as in the OOV split. CG splits. To generate the CG splits, we utilise the distribution-based compositionality assessment (DBCA) [11] framework. DBCA is a method to determine how compositional two datasets are by calculating the divergences of the distributions of atoms, primitive elements, on one hand, and\nthe divergences of the distributions of combinations of atoms, called compounds, on the other hand. To create a train-test split that requires compositional generalisation, the atom divergence should be similar between train and test splits, while compound distribution should diverge. In our CG splits, atoms are defined as the scenario and action labels, and the compound in each utterance is the combination of the two, i.e. the intent. Similarity between distributions P and Q is calculated in the same way as in [11], using the Chernoff coefficient C\u03b1(P\u2225Q) = \ufffd k p\u03b1 k q1\u2212\u03b1 k \u2208[0, 1] [25], with \u03b1 = 0.5 for the atom distribution similarity and \u03b1 = 0.1 for the compound distribution similarity. The divergence is defined as 1 \u2212C\u03b1. A greedy algorithm, similar to that by [26], splits the data aiming to maximise the train-test compound divergence while minimising the atom divergence. A non-CG split with minimal compound divergence, as well as minimal atom divergence, is also created to contrast with the CG split. In Table 1, we can see that for the CG splits, the distributions of the actions and scenarios are relatively similar between training and test sets, whereas the distributions of intents diverge. Double-Action CG splits. We provide a second split configuration for testing the CG. In this split, the task is to predict the scenario from a combination of two utterances with different actions but same scenario. Again, for both the CG and non-CG versions, we used a unified test set and modified the training and development portions accordingly. The train, development, and test portions are derived from the original SLURP splits. For the CG split, we ensured that no combination of the same two actions present in the test set appears in the training or development splits. To create the non-CG split, we removed half of the samples from the difficult set and replaced them with two action combinations present in the test set, preserving the class distribution as in the CG split. Microphone mismatch splits. In the original SLURP dataset, the utterances were captured either through a headset or from a distance, simulating diverse acoustic environments. We used this information to create splits for assessing SLU models\u2019 ability to generalise to mismatched acoustic environments. More specifically, we selected only the headset recordings from the official SLURP train and development splits. To test the models\u2019 classification ability in different acoustic environments, we created two test splits, derived from the official SLURP test set. The first one contains recordings made with a headset and the second one contains recordings made without a headset. We also made sure that the speakers were identical in both versions.\n# The number of samples in training, development, and test splits is 24209, 4173. and 5552, respectively. The rest of the statistics are identical to the original splits (see Table 1).\nThe number of samples in training, development, and test splits is 24209, 4173. and 5552, respectively. The rest of the statistics are identical to the original splits (see Table 1).\n# 3. Experiments and results\n# 3.1. Baseline systems\n3.1. Baseline systems\nThe primary goal of the proposed splits is to have resources for testing the generalisation ability of E2E SLU models. To test the generalisation ability of the models on the proposed splits, we created baseline E2E SLU systems for each of the splits and trained them on the scenario classification task. All four split configurations are designed for the sequence classification task. To do sequence classification, we fine-tuned the English pre-trained WavLM-base-plus model [27] (90.2M trainable parameters) by adding a classification layer and training it with the negative log-likelihood loss. We chose this model due to its superior performance on various speech-related tasks. To combine the utterances in the DA CG/non-CG splits, we processed them individually using the WavLM model. The embeddings of the two utterances are then averaged to get a single vector representation. All the models were trained with a single V100 GPU for 30 epochs, using a batch size of 10. In the OOV, CG and Mic mismatch, training for one epoch with the WavLM model took around 16 minutes, whereas for the DA CG it took 45 minutes. For more details about the hyperparameters, refer to the provided code repository.\nTable 2: Micro F1 scores for the scenario classification task. The results are obtained by averaging the scores from two runs with different seeds. The 95% confidence intervals calculated with the bootstrap method are given in the brackets.\nSplit\nWavLM\nTOPK\nSegment\nNon-OOV\n85.6 (85, 87)\n84.1 (83, 86)\n85.0 (84, 87)\nOOV\n52.7 (51, 55)\n54.1 (53, 56)\n53.4 (52, 55)\nNon-CG\n80.9 (79, 83)\n80.9 (79, 83)\n81.8 (80, 84)\nCG\n61.1 (59, 64)\n57.6 (56, 60)\n59.4 (57, 62)\nDA Non-CG\n88.9 (88, 90)\n88.7 (88, 90)\n89.0 (88, 90)\nDA CG\n82.6 (81, 84)\n87.4 (87, 89)\n86.3 (85, 88)\nMic Headset\n85.4 (85, 86)\n85.6 (85, 87)\n85.6 (85, 87)\nMic Other\n69.9 (69, 71)\n63.7 (63, 65)\n69.2 (68, 71)\n# 3.2. Results\nAs an evaluation metric, we used the micro F1 score. Additionally, we calculated the 95% confidence intervals using the bootstrap method2. The scenario classification results are presented in Table 2. On the OOV split, the baseline WavLM model performs significantly worse than the non-OOV, with a 32.9 percentage point performance degradation. The big performance gap between the easy and difficult splits suggests that the model struggles to generalise when presented with context unseen during training. On the CG/non-CG splits the difference in performance is smaller but still noticeable. On these splits, the WavLM got\n2Ferrer, L. and Riera, P. Confidence Intervals for evaluation in machine learning [Computer software]. https://github.com/ luferrer/ConfidenceIntervals\na performance decrease of 19.8 points. When using a bigger context, like in the case of the DA CG/non-CG splits, the gap in performance is more narrow, with a decrease of 6.3. When evaluating the baseline WavLM model on utterances with the same and different acoustic environments as those in training we can see again a large drop in performance. Namely, when presented with utterances that do not use a headset, the performance drops 15.5 percentage points. Since these splits do not require any modification to the architecture to perform intent classification, we trained the baseline on that task. The results showed that the performance dropped from 79.7% to 42.1% when presented with samples that do not use the headset.\n# 3.3. Why do the models fail to generalise?\nAs evident from the results, the WavLM model exhibits performance loss when evaluated on the OOD splits. This trend shows that even though the model has seen large amounts of data during pre-training, it still struggles to generalise. To investigate the reason why the model fails to generalise, we employed an interpretability technique called Integrated Gradients (IG) to determine the most important words for each prediction. IG is a technique for estimating the contribution of the input segments (in our case the word regions) to the final prediction. The attributions are calculated by creating interpolation steps from the baseline, in our case zeros, to the input and integrating along the steps. Then, integrated gradients are obtained by accumulating the gradients. For more technical details, refer to [23]. To obtain the most important word for each prediction, we took the word region with the highest attribution value. Since we are interested in the contribution of each word to the final prediction, we need to align the audio signal with the corresponding transcript. For this, we used an ASR-finetuned wav2vec2 model. We collected the most important word for each classified sample and counted the frequencies of these words within each scenario class. Figure 1 shows the 3 words most frequently determined as the most important by the IG method. We see the models often assign a high attribution value to stopwords, such as \u2018a\u2019 or \u2018the\u2019. Focusing on stopwords may indicate that the model has learned spurious correlations between input features and output classes. For example, the models might learn to classify the samples based on syntactic structures independent of their semantic content, which would lead to an inability to generalise to novel utterances that are structured differently but have the same intent, or vice versa. Comparing the important words for correct predictions in the CG split to those in the the IID (non-OOD) split (right-handside column of matrices, the 1st and 2nd row in each matrix), there is a tendency that the most important words for the CG split are semantically more relevant to the class than those for the IID split. For example, the word \u2018calendar\u2019 in the calendar class, or \u2018song\u2019 in music class, \u2018lights\u2019 in the IoT class, or \u2018podcast\u2019 in the play class. (Although there are 2 counterexamples: email and transport classes.) This could mean that in the CG split, the model has to use the semantically relevant words to predict the class, whereas in the IID split, the model can more often use stopwords to give correct classification. We take this to mean the CG split more readily assesses the models\u2019 capacity to learn to use features that help generalising to novel samples. Comparing the important words for correct predictions to those for incorrect predictions can also give some clues as to why the model confuses some classes. For example, the Music class seems to be confused with the Play class because of the word \u2018song\u2019 in the OOV split, and \u2018music\u2019 in the CG split.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/348e/348e0ece-f277-4edc-b573-fb783ad7c331.png\" style=\"width: 50%;\"></div>\nFigure 1: Top 3 words that are most frequently the most important word for the prediction (colour intensity signifies frequency), determined by the IG method, for the OOV and CG splits. The first two rows in each matrix show the most important words for the samples that were correctly predicted by the IID and OOD splits, respectively. The third row shows the most important words for the incorrectly classified samples. The last row shows the most commonly confused classes on the OOD split with respect to the true class.\n# 3.4. Towards improving the generalisation\nHoping to improve the generalisation of the WavLM model, we experimented with two approaches: TOPK [24] and segmented processing. TOPK presents a straightforward method that preserves the existing architecture without any modifications. It involves averaging solely the top-k highest losses within each mini-batch, rather than all individual losses. Through experimentation on a subset of the data, we explored values of k ranging from 2 to 6 to select the optimal parameter. The best results were obtained with a k=2, therefore we use that value in the results reported in Table 2. For the second approach, we apply segment-based process-\ning by obtaining the embedding vector from WavLM and splitting it along the temporal dimension in segments with overlapping windows. Then, we take the mean of each segment to get a single embedding per segment. Finally, we compute the mean of the embedding segments and pass it through the output layer. If the utterance is too short and can not be segmented, we use the whole, without segmenting. To determine the most optimal segment size, we conducted experiments by training on a subset of the training data. We experimented with segment sizes of 20, 40, 60, 80, 100, 120, and 140. The stride is always half of the segment size. We obtained the best results with a segment size of 120 and a stride of 60. The result presented in Table 2 show that both approaches improve the generalisation on some of the splits, but not on all of them. For instance, for the OOV splits, both TOPK and segmented processing produce better results than the baseline, with TOPK being better. On the CG splits generated with the DBCA method, both TOPK and segmented processing failed to improve the generalisation. However, for the DA CG split, both approaches outperformed the baseline. On the Mic other split, the TOPK approach performs significantly worse than the segmented processing. One reason for that could be due to the training set containing only samples recorded with a headset that have very little variance of the acoustic conditions, so the samples with highest losses are probably difficult not due to the acoustic environment but due to other factors instead.\n# 4. Limitations\nIn this study, we mostly focus on the scenario classification task, although intent classification is another task made possible by the intent labels in the SLURP data. Similarly, named entity annotations in the SLURP dataset could also be used to create a different type of CG splits. We aimed to shed light on the reasons for the poor generalisation results of the models by utilising the integrated gradients method, and we made some hypotheses based on the most important words, but these hypotheses are not yet conclusively tested, and therefore only tentatively suggest weaknesses of the systems.\n# 5. Conclusions\nWe introduced data splits to assess different types of OOD generalisation of SLU models. These splits systematically test the OOV, CG, and acoustic environment aspects of OOD generalisation. The empirical findings showed significant challenges in generalisation for the WavLM model across the OOD splits, underscoring the necessity for alternative modelling strategies. We investigated TOPK and segmented processing techniques, revealing improvements in some types of OOD generalisation, although still failing to improve generalisation to other types of OOD data. By analysing the most important words affecting the predictions, we noticed that the models often give high importance to the stopwords, indicating that they might learn spurious correlations. Moreover, we found that the important words in general differ between the OOD split types, providing different challenges to the models, which is corroborated by the fact that the best results are achieved by different model types depending on the split. Specifically, the CG split seems to require the system to focus on semantically relevant words more often than other splits do. In the future, we plan to use these findings of the weaknesses of the systems to develop methods for improving the generalisation of E2E SLU models.\n# 6. Acknowledgements\nWe are grateful for the Business Finland project LAREINA under Grant 7817/31/2022. The computational resources were provided by Aalto ScienceIT.\n# 7. References\n[1] J. Liu, Z. Shen, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui, \u201cTowards out-of-distribution generalization: A survey,\u201d arXiv preprint arXiv:2108.13624, 2021. [2] J. Ray Chowdhury and C. Caragea, \u201cMonotonic location attention for length generalization,\u201d in Proceedings of the 40th International Conference on Machine Learning. PMLR, 2023, pp. 28 792\u201328 808. [3] H. Zhou, A. Bradley, E. Littwin, N. Razin, O. Saremi, J. Susskind, S. Bengio, and P. Nakkiran, \u201cWhat algorithms can transformers learn? a study in length generalization,\u201d in The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS\u201923, 2023. [4] L. Lugosch, M. Ravanelli, P. Ignoto, V. S. Tomar, and Y. Bengio, \u201cSpeech Model Pre-Training for End-to-End Spoken Language Understanding,\u201d in Proc. Interspeech 2019, 2019, pp. 814\u2013818. [5] E. Palogiannidi, I. Gkinis, G. Mastrapas, P. Mizera, and T. Stafylakis, \u201cEnd-to-end architectures for asr-free spoken language understanding,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7974\u20137978. [6] J. A. Fodor and Z. W. Pylyshyn, \u201cConnectionism and cognitive architecture: A critical analysis,\u201d Cognition, vol. 28, no. 1-2, pp. 3\u201371, 1988. [7] D. Hupkes, V. Dankers, M. Mul, and E. Bruni, \u201cCompositionality decomposed: How do neural networks generalise?\u201d Journal of Artificial Intelligence Research, vol. 67, pp. 757\u2013795, 2020. [8] S. Broscheit, Q. Do, and J. Gaspers, \u201cDistributionally robust finetuning bert for covariate drift in spoken language understanding,\u201d in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2022, pp. 1970\u20131985. [9] A. Ray, Y. Shen, and H. Jin, \u201cCompositional Generalization in Spoken Language Understanding,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 750\u2013754. 10] B. Lake and M. Baroni, \u201cGeneralization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 2873\u20132882. 11] D. Keysers, N. Sch\u00a8arli, N. Scales, H. Buisman, D. Furrer, S. Kashubin, N. Momchev, D. Sinopalnikov, L. Stafiniak, T. Tihon, D. Tsarkov, X. Wang, M. van Zee, and O. Bousquet, \u201cMeasuring compositional generalization: A comprehensive method on realistic data,\u201d in International Conference on Learning Representations, 2020. [Online]. Available: https: //openreview.net/pdf?id=SygcCnNKwr 12] Y. Yao and A. Koller, \u201cStructural generalization is hard for sequence-to-sequence models,\u201d in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, 2022, pp. 5048\u20135062. 13] M. Lepori, T. Serre, and E. Pavlick, \u201cBreak it down: Evidence for structural compositionality in neural networks,\u201d Advances in Neural Information Processing Systems, vol. 36, 2023. 14] T. Viglino, P. Motlicek, and M. Cernak, \u201cEnd-to-End Accented Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 2140\u2013 2144. 15] A. Potamianos and S. Narayanan, \u201cRobust recognition of children\u2019s speech,\u201d IEEE Transactions on speech and audio processing, vol. 11, no. 6, pp. 603\u2013616, 2003. 16] R. Haeb-Umbach, J. Heymann, L. Drude, S. Watanabe, M. Delcroix, and T. Nakatani, \u201cFar-field automatic speech recognition,\u201d Proceedings of the IEEE, vol. 109, no. 2, pp. 124\u2013148, 2020.\n[17] N. Gupta, G. Tur, D. Hakkani-Tur, S. Bangalore, G. Riccardi, and M. Gilbert, \u201cThe at&t spoken language understanding system,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 1, pp. 213\u2013222, 2005. [18] J. Gaspers, A. Kumar, G. Ver Steeg, and A. Galstyan, \u201cTemporal generalization for spoken language understanding,\u201d in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, 2022, pp. 37\u201344. [19] D. Serdyuk, Y. Wang, C. Fuegen, A. Kumar, B. Liu, and Y. Bengio, \u201cTowards end-to-end spoken language understanding,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754\u20135758. [20] P. Haghani, A. Narayanan, M. Bacchiani, G. Chuang, N. Gaur, P. Moreno, R. Prabhavalkar, Z. Qu, and A. Waters, \u201cFrom audio to semantics: Approaches to end-to-end spoken language understanding,\u201d in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 720\u2013726. [21] S. Arora, A. Ostapenko, V. Viswanathan, S. Dalmia, F. Metze, S. Watanabe, and A. W. Black, \u201cRethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding,\u201d in Proc. Interspeech 2021, 2021, pp. 1264\u20131268. [22] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, \u201cSLURP: A spoken language understanding resource package,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Online: Association for Computational Linguistics, Nov. 2020, pp. 7252\u20137262. [Online]. Available: https://aclanthology.org/2020.emnlp-main.588 [23] M. Sundararajan, A. Taly, and Q. Yan, \u201cAxiomatic attribution for deep networks,\u201d in International conference on machine learning. PMLR, 2017, pp. 3319\u20133328. [24] K. Kawaguchi and H. Lu, \u201cOrdered SGD: A new stochastic optimization framework for empirical risk minimization,\u201d in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 669\u2013679. [25] J. Chung, P. Kannappan, C. Ng, and P. Sahoo, \u201cMeasures of distance between probability distributions,\u201d Journal of mathematical analysis and applications, vol. 138, no. 1, pp. 280\u2013292, 1989. [Online]. Available: https://www.sciencedirect. com/science/article/pii/0022247X89903351 [26] A. Moisio, M. Creutz, and M. Kurimo, \u201cEvaluating morphological generalisation in machine translation by distributionbased compositionality assessment,\u201d in Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa). T\u00b4orshavn, Faroe Islands: University of Tartu Library, May 2023, pp. 738\u2013751. [27] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale selfsupervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Out-of-distribution (OOD) generalisation is a common challenge in machine learning, particularly in spoken language understanding (SLU) tasks. While there has been growing interest in OOD generalisation, it has not been extensively explored in SLU, necessitating a benchmark to assess and improve the generalisation capabilities of SLU models.",
            "purpose of benchmark": "The benchmark is designed to facilitate research on OOD generalisation in SLU by providing modified data splits for evaluating model performance on unseen data distributions."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of SLU models' ability to generalise to OOD data, specifically focusing on tasks like out-of-vocabulary (OOV) and compositional generalisation (CG).",
            "key obstacle": "Existing benchmarks primarily focus on text-based evaluations, which do not adequately assess the generalisation capabilities of end-to-end SLU models that rely on audio input."
        },
        "idea": {
            "intuition": "The creation of the benchmark was inspired by the observation that SLU models often fail to generalise to novel utterances and contexts, particularly in real-world applications.",
            "opinion": "The authors emphasize the importance of this benchmark for advancing understanding and capabilities in OOD generalisation within SLU systems.",
            "innovation": "The benchmark introduces novel data splits that target specific types of generalisation, allowing for more granular evaluation of SLU models compared to previous benchmarks.",
            "benchmark abbreviation": "SLURPFOOD"
        },
        "dataset": {
            "source": "The dataset is a modified version of the SLURP dataset, featuring specific splits for testing OOD generalisation in SLU tasks.",
            "desc": "The dataset includes 72,395 recordings of 16,520 utterances, with annotations for transcripts, named entities, part-of-speech tags, scenarios, and actions.",
            "content": "The dataset contains audio data along with corresponding textual annotations that relate to the SLU tasks.",
            "size": "72,395",
            "domain": "Spoken Language Understanding",
            "task format": "Scenario Classification"
        },
        "metrics": {
            "metric name": "Micro F1 Score",
            "aspect": "Accuracy of model predictions on various splits of the dataset.",
            "principle": "The Micro F1 Score was chosen as it provides a balanced measure of precision and recall across classes, which is crucial for evaluating classification tasks.",
            "procedure": "Model performance was evaluated by calculating the Micro F1 Score on the test splits, with confidence intervals obtained using the bootstrap method."
        },
        "experiments": {
            "model": "The primary model tested was the WavLM-base-plus, a pre-trained end-to-end SLU model.",
            "procedure": "The model was fine-tuned for the scenario classification task using the proposed data splits and trained on a single V100 GPU for 30 epochs.",
            "result": "The results showed significant performance degradation on OOD splits, highlighting the models' struggles with generalisation.",
            "variability": "Variability was accounted for by averaging results from multiple runs with different seeds and calculating confidence intervals."
        },
        "conclusion": "The introduction of SLURPFOOD provides a necessary resource for assessing OOD generalisation in SLU models, revealing significant challenges and the need for improved modelling strategies.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by systematically evaluating different types of generalisation in SLU, providing insights into model performance and weaknesses.",
            "limitation": "The focus on scenario classification limits the exploration of other potential tasks within the SLURP dataset, such as intent classification.",
            "future work": "Future research will aim to develop methods to enhance the generalisation capabilities of SLU models based on the findings from this benchmark."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Out-of-distribution (OOD) generalisation is a common challenge in machine learning, particularly in spoken language understanding (SLU) tasks."
        },
        {
            "section number": "1.2",
            "key information": "The benchmark is designed to facilitate research on OOD generalisation in SLU by providing modified data splits for evaluating model performance on unseen data distributions."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the challenge of SLU models' ability to generalise to OOD data, specifically focusing on tasks like out-of-vocabulary (OOV) and compositional generalisation (CG)."
        },
        {
            "section number": "3.1",
            "key information": "The benchmark introduces novel data splits that target specific types of generalisation, allowing for more granular evaluation of SLU models compared to previous benchmarks."
        },
        {
            "section number": "4.1",
            "key information": "The results showed significant performance degradation on OOD splits, highlighting the models' struggles with generalisation."
        },
        {
            "section number": "7.1",
            "key information": "The focus on scenario classification limits the exploration of other potential tasks within the SLURP dataset, such as intent classification."
        },
        {
            "section number": "7.2",
            "key information": "Future research will aim to develop methods to enhance the generalisation capabilities of SLU models based on the findings from this benchmark."
        }
    ],
    "similarity_score": 0.6960234724060247,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Out-of-distribution generalisation in spoken language understanding.json"
}