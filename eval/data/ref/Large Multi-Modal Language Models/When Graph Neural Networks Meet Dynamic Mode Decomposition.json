{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.05593",
    "title": "When Graph Neural Networks Meet Dynamic Mode Decomposition",
    "abstract": "Graph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper into this perspective by connecting the dynamics in GNNs to modern Koopman theory and its numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can estimate a low-rank, finite-dimensional linear operator based on multiple states of the system, effectively approximating potential nonlinear interactions between nodes in the graph. This approach allows us to capture complex dynamics within the graph accurately and efficiently. We theoretically establish a connection between the DMD-estimated operator and the original dynamic operator between system states. Building upon this foundation, we introduce a family of DMDGNN models that effectively leverage the low-rank eigenfunctions provided by the DMD algorithm. We further discuss the potential of enhancing our approach by incorporating domain-specific constraints such as symmetry into the DMD computation, allowing the corresponding GNN models to respect known physical properties of the underlying system. Our work paves the path for applying advanced dynamical system analysis tools via GNNs. We validate our approach through extensive experiments on various learning tasks, including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs. We also empirically verify that our proposed models can serve as powerful encoders for link prediction tasks. The results demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance, highlighting the effectiveness of integrating DMD into GNN frameworks.",
    "bib_name": "shi2024graphneuralnetworksmeet",
    "md_text": "# When Graph Neural Networks Meet Dynamic Mode Decomposition\nDai Shi \u2217\u2020\nUniversity of Sydney, Australia\n 8 Oct 2024\nJunbin Gao University of Sydney, Australia\nUniversity of Sydney, Australia\n# Abstract\nGraph Neural Networks (GNNs) have emerged as fundamental tools for a wide range of prediction tasks on graph-structured data. Recent studies have drawn analogies between GNN feature propagation and diffusion processes, which can be interpreted as dynamical systems. In this paper, we delve deeper into this perspective by connecting the dynamics in GNNs to modern Koopman theory and its numerical method, Dynamic Mode Decomposition (DMD). We illustrate how DMD can estimate a low-rank, finite-dimensional linear operator based on multiple states of the system, effectively approximating potential nonlinear interactions between nodes in the graph. This approach allows us to capture complex dynamics within the graph accurately and efficiently. We theoretically establish a connection between the DMD-estimated operator and the original dynamic operator between system states. Building upon this foundation, we introduce a family of DMDGNN models that effectively leverage the low-rank eigenfunctions provided by the DMD algorithm. We further discuss the potential of enhancing our approach by incorporating domain-specific constraints such as symmetry into the DMD computation, allowing the corresponding GNN models to respect known physical properties of the underlying system. Our work paves the path for applying advanced dynamical system analysis tools via GNNs. We validate our approach through extensive experiments on various learning tasks, including directed graphs, large-scale graphs, long-range interactions, and spatial-temporal graphs. We also empirically verify that our proposed models can serve as powerful encoders for link prediction tasks. The results demonstrate that our DMD-enhanced GNNs achieve state-of-the-art performance, highlighting the effectiveness of integrating DMD into GNN frameworks.\n# 1 Introduction\nGraph Neural Networks (GNNs) [38, 18] have become fundamental tools for processing graph structured data across a wide range of learning tasks. Recently, numerous studies [64, 14, 30, 70, 16\n\u2217Equal contribution. Dai Shi is the corresponding author. \ufffddai.shi@sydney.edu.au. \u2020In memory of MeiMei \ufffd, whose love and companionship will always be remembered.\nPreprint. Under review.\nhave established a strong connection between GNN feature propagation and diffusion processes. Building upon this connection, many of these works have integrated complex physical processes into graph diffusion to mitigate inherent computational issues in GNNs, such as over-smoothing [52] and over-squashing [60]. While these approaches have yielded significant theoretical and empirical results from the feature propagation perspective, there remains a gap in analyzing these dynamics through the lens of physics, particularly from a dynamical systems viewpoint. From the dynamical systems perspective, a carefully analyzed and refined dynamic can potentially enhance GNN performance by providing deeper insights into feature propagation over graphs. Motivated by this, we delve deeper into the physics of GNN dynamics by employing Dynamic Mode Decomposition (DMD) [67], a powerful numerical tool from fluid dynamic theory. DMD has become fundamental for approximating the Koopman operator [39], which is essentially designed to approximate the non-linear dynamic using an (infinite-dimensional) linear operator. Specifically, DMD finds a low-rank estimation of the system\u2019s original operator by leveraging multiple system states (e.g., snapshots), and the resulting DMD modes capture the principal components driving the underlying physics. Applying DMD to GNN feature propagation enables us to capture underlying dynamical patterns. In addition, DMD allows for a reduction in the number of learnable parameters in terms of spectral filtering and decreased complexity in spatial aggregation with low-rank updates. Consequently, the resulting DMD-enhanced GNNs (DMD-GNNs) not only capture the characteristics of the original dynamics by leveraging the estimated DMD modes but also have the potential to adapt to more complex tasks (e.g., long-range graphs [19]) in which the original model usually delivers poor performance via lesser computational cost.\nhave established a strong connection between GNN feature propagation and diffusion processes. Building upon this connection, many of these works have integrated complex physical processes into graph diffusion to mitigate inherent computational issues in GNNs, such as over-smoothing [52] and over-squashing [60]. While these approaches have yielded significant theoretical and empirical results from the feature propagation perspective, there remains a gap in analyzing these dynamics through the lens of physics, particularly from a dynamical systems viewpoint. From the dynamical systems perspective, a carefully analyzed and refined dynamic can potentially enhance GNN performance by providing deeper insights into feature propagation over graphs. Motivated by this, we delve deeper into the physics of GNN dynamics by employing Dynamic Mode Decomposition (DMD) [67], a powerful numerical tool from fluid dynamic theory. DMD has become fundamental for approximating the Koopman operator [39], which is essentially designed to approximate the non-linear dynamic using an (infinite-dimensional) linear operator. Specifically, DMD finds a low-rank estimation of the system\u2019s original operator by leveraging multiple system states (e.g., snapshots), and the resulting DMD modes capture the principal components driving the underlying physics. Applying DMD to GNN feature propagation enables us to capture underlying dynamical patterns. In addition, DMD allows for a reduction in the number of learnable parameters in terms of spectral filtering and decreased complexity in spatial aggregation with low-rank updates. Consequently, the resulting DMD-enhanced GNNs (DMD-GNNs) not only capture the characteristics of the original dynamics by leveraging the estimated DMD modes but also have the potential to adapt to more complex tasks (e.g., long-range graphs [19]) in which the original model usually delivers poor performance via lesser computational cost. Contribution In this paper, we establish a novel link between feature dynamics in graph neural networks and the Koopman operator learning theory through DMD. We begin by formulating the dynamic system, Koopman operator, and DMD in Section 3 and show the link between GNN and dynamic system in Section 4. In Section 5, we introduce our newly designed data-driven graph DMD approach, demonstrating how it captures the principal components that drive the underlying complex physics on the graph. Accordingly, a family of DMD-GNNs can be smoothly derived from DMD with a flexible choice of initial dynamics. In Section 6, we analyze the properties of graph DMD from a dynamical systems perspective, showing that the manifold containing DMD outputs is locally topologically conjugate to a specific spectral submanifold tangent to the system origin. This reveals the underlying geometry between our graph DMD and the actual characteristics of the data. We further investigate how different choices of initial dynamics affect the range of the DMD output spectrum, which is essential for DMD-GNNs in fitting both homophily and heterophily graphs. In Section 7, we evaluate the performance of DMD-GNNs on various learning tasks, including node classification on citation networks, long-range datasets, and node regression on spatial-temporal graphs (STGs), along with tests of parameter sensitivities. We also show that DMD-GNNs can be powerful encoders for the link prediction tasks. Finally, in Section 8, we explore the potential of incorporating additional constraints on DMD and deploying physics-informed DMD for directed graphs, showing the model\u2019s potential of imitating complex physical dynamics, which leads to a range of future research directions at the intersection of DMD and GNNs.\nContribution In this paper, we establish a novel link between feature dynamics in graph neural networks and the Koopman operator learning theory through DMD. We begin by formulating the dynamic system, Koopman operator, and DMD in Section 3 and show the link between GNN and dynamic system in Section 4. In Section 5, we introduce our newly designed data-driven graph DMD approach, demonstrating how it captures the principal components that drive the underlying complex physics on the graph. Accordingly, a family of DMD-GNNs can be smoothly derived from DMD with a flexible choice of initial dynamics. In Section 6, we analyze the properties of graph DMD from a dynamical systems perspective, showing that the manifold containing DMD outputs is locally topologically conjugate to a specific spectral submanifold tangent to the system origin. This reveals the underlying geometry between our graph DMD and the actual characteristics of the data. We further investigate how different choices of initial dynamics affect the range of the DMD output spectrum, which is essential for DMD-GNNs in fitting both homophily and heterophily graphs. In Section 7, we evaluate the performance of DMD-GNNs on various learning tasks, including node classification on citation networks, long-range datasets, and node regression on spatial-temporal graphs (STGs), along with tests of parameter sensitivities. We also show that DMD-GNNs can be powerful encoders for the link prediction tasks. Finally, in Section 8, we explore the potential of incorporating additional constraints on DMD and deploying physics-informed DMD for directed graphs, showing the model\u2019s potential of imitating complex physical dynamics, which leads to a range of future research directions at the intersection of DMD and GNNs.\n# 2 Related Works\nGraph Neural Diffusion Models Many recent works have explored the link between the feature propagation in GNNs to the so-called physical diffusion process, leading to a variety of diffusionbased or physics-informed models [12, 16, 61, 30]. By integrating complex physical dynamic systems into the graph domain, these methods often achieve superior performance across different learning tasks. The analysis within these works is mainly conducted in the graph feature domain through the message-passing paradigm [25]. However, the discussion on whether the benefits of incorporated dynamics are sufficiently utilized in GNN is rarely conducted and can only be observed in the recent works [70]. In addition, attention-based GNNs (e.g., GAT [68]) and transformer-based diffusion models (e.g., Difformer [72]) barely generate node-level attention scores based on their similarities on each layer yet rarely consider interactions between multiple states of the system. In this work, we analyze some commonly implemented GNNs and their dynamics through DMD, showing that a low-rank estimate of the operators that act on the original GNN dynamics is sufficient to induce identical or even better learning performances. The theoretical results in our work establish a bridge between DMD and the operators of the underlying dynamics.\nKoopman Operator and Dynamic Mode Decomposition The Koopman operator theory [39] has become one of the most widely applied approaches for analyzing modern dynamical systems [10]. Fundamentally, Koopman theory posits that any finite-dimensional nonlinear dynamic can be studied as an infinite-dimensional linear operator, denoted as K. To identify finite-dimensional representations of K, numerous numerical methods have been developed, with DMD emerging as one of the most popular. DMD has become a leading tool in aerodynamics and fluid physics [5, 20]. From a machine learning perspective, data-driven Koopman autoencoders [48] have been developed to learn both the system\u2019s measurement functions and the operator simultaneously. Furthermore, the spectral characteristics (i.e., eigenvalues and eigenvectors) of the estimated Koopman operator have been leveraged in recent advancements in time series analysis [8, 46, 42]. Theoretically, the work by [6] extends the DMD algorithm to incorporate more information about the underlying physics (e.g., symmetry, low-rank structures) into its outputs, resulting in a suite of physics-informed DMD methods. Additionally, [28] proved that the estimates computed from DMD belong to the so-called slow decay submanifold that is tangent to the slow-spectral subspaces, leading to a deeper understanding of the DMD outputs and the underlying physics.\n# 3 Dynamic Systems, Koopman Operator and DMD\nWe consider an autonomous dynamic system which can be defined as\n \u2208  \u2208C in which x = x(t) is a d-dimensional vector serving as the state of the system at t. An evolution or trajectory of the system {x(t)}t\u2208R+ can induce a so-called flow mapping Ft : Rd \u2192Rd that takes the state x(0) at time 0 to the state x(t) at time t > 0.\nIn 1931, Bernard O. Koopman demonstrated that it is possible to represent a nonlinear dynamical system in (1) by using an infinite-dimensional linear operator that acts on a Hilbert space of measurement functions of the state x [39]. Specifically let us consider the smooth functions: \u03d5(x) \u2208C1(Rd, Rm), where \u03d5 and \u03d5(x) are often named as the measurement function and observations, respectively, (or interchangeably named as observables in some works [55]). The Koopman operator Kt is an infinite-dimensional linear operator that maps a measurement function \u03d5 into a new function Kt\u03d5 such that its function values at a state x (imagining it is at time 0) is given by \u03d5\u2019s function value at the state given by the flow map Ft after time t, i.e.,\nK \u2200 \u2208 This can be written as Kt\u03d5 = \u03d5 \u25e6Ft. It is easy to prove that, for any two measurement functions \u03d51 and \u03d52, Kt(a\u03d51 + b\u03d52) = a\u03d51 + b\u03d52 where a, b \u2208R. When considering all the possible advancing time t along the flow maps, one can prove that {Kt}t\u2208R+ forms a semi-group which has an infinitesimal generator K. In literature, this generator is called the Koopman operator of the underlying dynamical system, which satisfies the Koopman dynamical system: d dt\u03d5 = K\u03d5. For our purpose, we are more interested in the discrete nonlinear system defined by\nThe discrete system (3) itself defines a discrete path X = {xk}. In the same merit of (2), the Koopman operator K for the nonlinear discrete system can be defined as\nK That is the value of the mapped function K\u03d5 at the state in step k is given by the value of measurement function \u03d5 at the state in the next along the path X. As K is a infinite-dimensional linear operator in the measurement function space, this paves the way such that we analyze the unlderlying discrete nonlinear dynamical system in measurement space. With a given set of finite number of observed data \u03a6(X(k)) = {\u03d5(xk)}, the Dynamic Mode Decomposition (DMD), as one of the most popular numerical methods for approximating the Koopman operator, originally seeks for a finite-dimensional linear operator K, such that\nwhere \u03a6(X(k +1)) is the dataset of one step shift version of dataset \u03a6(X(k)). DMD and its variants have been widely deployed via various fields such as fluid dynamics [4, 47], time series forecasting [46], environmental engineering [43], and solving partial differential equations [41]\n(1)\n(3)\n(4)\n(5)\n# 4 How GNNs Resonates with Dynamic Systems\nThroughout this paper, we denote G(V, E) as graphs that can be weighted and directed, where V and E are the set of nodes and edges. Consider a multilayer GNN on the graph G(V, E), and let H(\u2113) \u2208RN\u00d7d\u2113be the node feature matrix at layer \u2113, and A and L \u2208RN\u00d7N be the adjacency and Laplacian matrices of the graph, respectively. In general, most GNNs propagate the node signals through either spatial aggregation [38] or spectral filtering [18], denoted as:\nwhere one can have H(0) = X, the original feature matrix or H(0) = XW, and the generic function F maps node features from one layer to another. When F(H) = AH or its variants, e.g., A is reweighed based on the attention mechanism or rewired according to the graph topology, (6) represents those spatial GNNs [38, 68, 66]. On the other hand, if F(H) = Udiag(\u03b8)U\u22a4H or its variants, e.g., F(H) = Usin2(diag(\u03b81))U\u22a4H + Ucos2(diag(\u03b82))U\u22a4H, then (6) becomes those spectral or multi-scale GNNs [31]. In addition, it is well-studied that many GNN dynamics in (6) are equivalent to the discretized version of some continuous diffusion processes [30]. Specifically, let hi \u2208Rd be the signal over node i, the so-called graph diffusion process can be denoted as:\nwhere the function Si,j(hi(t), hj(t), t) : Rd \u00d7 Rd \u00d7 [0, \u221e) \u2192R defines the diffusivity coefficient controlling the diffusion strength pariwisely for any given t. In the simplest case, when Si,j(hi, hj, t) = 1, \u2200(i, j) \u2208E the diffusion process can be written as \u2202hi \u2202t = div(\u2207H)i = \u2212(LH)i, in which we denote div and \u2207as the divergence and gradient operator on the graph. The solution of the diffusion equation is given by the so-called heat kernel, i.e., X(t) = exp(\u2212tL)H(0), suggesting an exponential decay of the feature dissimilarities known as over-smoothing (OSM) [52]. Accordingly, GNNs are required to find the balance between the diffusion and reaction processes, with the former homogenizes (smoothing) and later differentiates (sharpening) node features. One way to achieve this goal is to assign a certain source term to the diffusion process in (7), yielding the following:\n# \u2202hi(t) \u2202t = \u03b1 \ufffd Si,j(hi(t), hj(t), t)(hj(t) \u2212hi(t)) + \u03b2 R(hi(t), t),\nwhere we let R : Rd \u00d7 [0, +\u221e) \u2192Rd be a reaction function that acts only on the node features and \u03b1, \u03b2 \u2208R+ are hyperparameters to balance two terms. One can verify that when the system is dominated by the diffusion term, e.g., large \u03b1 and small \u03b2, the corresponding GNNs will have more smoothing effects on the node features, whereas when the reaction term is dominated, node features tend to be dissimilar to each other, see [31, 26, 16] for more detailed analysis. Furthermore, let \u03b1 = \u03b2 = 1, when R(hi(t), t) = hi(t) \u2299(1 \u2212hi(t)), we reach the Fisher information of the node features [24], and if R(hi(t), t) = hi(t) \u2299(1 \u2212hi(t) \u2299hi(t)), then we have the source term presented as Allen-Cahn term [70]. We refer to equation (10) in [16] and Appendix B for more examples. In practice, (8) can be implemented by the so-called MLPin and MLPout paradigm as: H(0) = MLPin(X), H(T) = H(0) + \ufffdT 0 P(H(t))dt, Y = MLPout(H(T)), (9) in which we let P(H(t)) = \u2212\u03b1 LH(t) + \u03b2 R(H(t), t). Fixing \u03b1 = \u03b2 = 1, then its Euler discretization yields H(\u2113+1) = H(\u2113)\u2212LH(\u2113)+R(H(\u2113), \u2113) = AH(\u2113)+R(H(\u2113), \u2113), yielding a generalized version of (6). Finally, if R(H(\u2113), \u2113) = RH(\u2113) where R is any matrix, then the diffusion and reaction terms can be combined as (A+R)H(\u2113), resulting a linear F, i.e., F(ahi+bhj) = aF(hi)+bF(hj). Otherwise, in the case, e.g., R(H(\u2113), \u2113) = H(\u2113) \u2299(I \u2212H(\u2113) \u2299H(\u2113)), where the diffusion and reaction terms can not be combined, resulting as a non-linear feature propagation. Clearly, (6) can be seen as a special case of discrete dynamical system (3). Similarly the diffusion processes such as (7) and (8) are described as the continuous dynamical system (1). Next we will explore how the Koopman operator theory and its numerical approximation DMD can facilitate analyzing GNNs dynamics.\nwhere we let R : Rd \u00d7 [0, +\u221e) \u2192Rd be a reaction function that acts only on the node features and \u03b1, \u03b2 \u2208R+ are hyperparameters to balance two terms. One can verify that when the system is dominated by the diffusion term, e.g., large \u03b1 and small \u03b2, the corresponding GNNs will have more smoothing effects on the node features, whereas when the reaction term is dominated, node features tend to be dissimilar to each other, see [31, 26, 16] for more detailed analysis. Furthermore, let \u03b1 = \u03b2 = 1, when R(hi(t), t) = hi(t) \u2299(1 \u2212hi(t)), we reach the Fisher information of the node features [24], and if R(hi(t), t) = hi(t) \u2299(1 \u2212hi(t) \u2299hi(t)), then we have the source term presented as Allen-Cahn term [70]. We refer to equation (10) in [16] and Appendix B for more examples. In practice, (8) can be implemented by the so-called MLPin and MLPout paradigm as:\n(6)\n(7)\n(8)\n(9)\n# 5 Dynamic Mode Decomposition in GNNs\n# 5.1 The Data-Driven DMD Algorithm\nOur primary goal is to leverage DMD to estimate a best-fitted finite-dimensional matrix K to approximate the infinite-dimensional Koopman operator K. In the meantime, we also expect the process of estimating K can capture sufficient graph information by considering multiple rather than individual states of the system. Let\u2019s assume H = \u03a6(X), i.e., the graph features are the actual observation induced from \u03a6, and denote the operator F such that H(\u2113+ 1) = FH(\u2113), thus we have\nwhere we let H(\u2113)\u2020 be the pseudo-inverse of H(\u2113). To show how DMD approximate F, we start with the singular value decomposition (SVD) of H(\u2113): H(\u2113) = M\u03a3V\u2217, where we denote the upper case \u2217as the conjugate transpose or Hermitian, and the matrix M, \u03a3 and V\u2217are with the size of N \u00d7 N, N \u00d7 d and d \u00d7 d, respectively. We note that given N is larger than d in most of cases (e.g., X in Cora is with the size 2708 \u00d7 1433), the rank of H(\u2113) is at most d. Accordingly, we have\nwhere we let H(\u2113)\u2020 be the pseudo-inverse of H(\u2113). In addition, we have the following\nK = M\u2217FM = M\u2217H(\u2113+ 1)V\u03a3\u22121,\nsuggesting a projection of F onto the column space of M\u2217. Let K = M\u2217F, and the actual rank of H(\u2113) = r, then (12) directly suggests that in practice, to estimate the unknown F from system states, i.e., H(\u2113) and H(\u2113+ 1), one only need to focus on the K which is with reduced dimensionality of r \u00d7 r which is obtained by min(N, d). Denoting the eigendecomposition of KU(K) = U(K)\u039b(K) since the rank of H(\u2113) is r, therefore the rank of K is also r. Accordingly, one can check that the first r eigenvalues of K are equal to the original F, whereas the rest N \u2212r eigenvalue of K is zero. The corresponding (projected) DMD modes 3 can be denoted as the columns of \u03a8 = MU(K) = H(\u2113)V\u03a3\u22121U(K) \u2208RN\u00d7r. One can verify that if the columns of H(\u2113+ 1) are spanned by those of H(\u2113), then the DMD modes and eigenvalues obtained from the eigendecomposition of K are the eigenvectors and eigenvalues of the operator F defined in (10) Accordingly, DMD provides a data-driven low-dimensional estimation of the operator F, with both eigenvalues and eigenvectors based on H(\u2113) and H(\u2113+ 1).\nThe illustration of DMD from the previous section suggests that to deploy DMD, one needs to assign an initial feature dynamic on the graph so that the snapshots (i.e., H(\u2113) and H(\u2113+1)) can be collected. Through this paradigm, DMD can produce K as a low-rank estimation for approximating the original non-linear dynamics. That is\nin which we note that the dimension of K is N \u00d7N, however both N \u2212r eigenvectors and eigenvalues of K are all zeros based on (12). One can also leverage DMD via different feature dynamics such as oscillation [53], wave equation [22] and quantum processing [69] to unleash the underlying physics. More importantly, DMD provides the estimated dynamic modes and eigenvalues based on the data matrices from the initial dynamics, therefore, the feature propagation via the corresponding DMD-GNNs can expressed as spectral filtering process by leveraging the estimated \u03a8, that is\n# H(\u2113+ 1) = \u03a8diag(\u03b8)\u03a8\u22a4H(\u2113)W(\u2113),\nand in practice, such spectral filtering process can be further approximated via polynomial approximation, e.g., via the first r eigenvalues [18]. We further note that DMD also provides a flexible truncation rate [35]. Specifically, a rate \u03be \u2208[0, 1] is assigned to control the rank of truncation, e.g.,\nand in practice, such spectral filtering process can be further approximated via polynomial approximation, e.g., via the first r eigenvalues [18]. We further note that DMD also provides a flexible truncation rate [35]. Specifically, a rate \u03be \u2208[0, 1] is assigned to control the rank of truncation, e.g., 3One can also denote the so-called exact DMD modes by \u03a8 = H(\u2113+ 1)V\u03a3\u22121U(K), and verify that if H(\u2113) and H(\u2113+ 1) shares the same column space, then the projected DMD modes will be equal to the exact DMD modes. Accordingly, in this work, we use the term DMD modes for simplicity reasons.\n3One can also denote the so-called exact DMD modes by \u03a8 = H(\u2113+ 1)V\u03a3\u22121U(K), and verify that if H(\u2113) and H(\u2113+ 1) shares the same column space, then the projected DMD modes will be equal to the exact DMD modes. Accordingly, in this work, we use the term DMD modes for simplicity reasons.\n(10)\n(11)\n(13)\n(14)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ffb5/ffb50364-aced-403c-9b89-b76bba5d4ca0.png\" style=\"width: 50%;\"></div>\n\u03be = 0.85 means the rank is the number of the largest singular values that are needed to reach the 85% of spectral energy (i.e., \ufffd i \u039b2 ii). Accordingly, a higher value of \u03be indicates more eigenvalues are considered in K, suggesting a wider range, including both higher and lower frequency components for spectral filtering in (14). On the other hand, in the case when \u03be is smaller, only those lower frequency components are preserved. Nevertheless, we found that practically setting \u03be \u2208[0.7, 0.9] is flexible enough for letting DMD-GNNs fit most of the graph benchmarks, see Section 6 and 7.4 for more theoretical and empirical justifications. Summary of the Model Procedure We visualize the steps on how our DMD-GNNs are designed in Figure 1. Initially, the underlying graph features x are measured by the measurement function \u03a6, leading to the observed feature h, which is further propagated by the initial GNNs to supply the inputs of DMD. DMD then takes multiple system states, e.g., h(\u2113) and h(\u2113+ 1) to produce the low-rank estimation of the GNN dynamics, namely DMD modes. These DMD modes (i.e., \u03a8) are leveraged via DMD-GNNs to propagate node features via spectral filtering.\n\u03be = 0.85 means the rank is the number of the largest singular values that are needed to reach the 85% of spectral energy (i.e., \ufffd i \u039b2 ii). Accordingly, a higher value of \u03be indicates more eigenvalues are considered in K, suggesting a wider range, including both higher and lower frequency components for spectral filtering in (14). On the other hand, in the case when \u03be is smaller, only those lower frequency components are preserved. Nevertheless, we found that practically setting \u03be \u2208[0.7, 0.9] is flexible enough for letting DMD-GNNs fit most of the graph benchmarks, see Section 6 and 7.4 for more theoretical and empirical justifications.\nSummary of the Model Procedure We visualize the steps on how our DMD-GNNs are designed in Figure 1. Initially, the underlying graph features x are measured by the measurement function \u03a6, leading to the observed feature h, which is further propagated by the initial GNNs to supply the inputs of DMD. DMD then takes multiple system states, e.g., h(\u2113) and h(\u2113+ 1) to produce the low-rank estimation of the GNN dynamics, namely DMD modes. These DMD modes (i.e., \u03a8) are leveraged via DMD-GNNs to propagate node features via spectral filtering.\n# 5.3 Towards Leveraging Multiple States Via Spatial-Temporal Graphs (STG)\nRecall that the DMD algorithm produces a low-rank estimate of the system operator via multiple snapshots of the system, causing its wide application in time series forecasting [40]. In terms of graph-based forecasting tasks, this can correspond to the node-level regression task via STGs. Specifically, the task can be interpreted as predicting the future time series window Hf = {ht0+1, ht0+2, ..., ht0+f} given the past context window Hc = {ht0\u2212c+1, ht0\u2212c+2, ..., ht0}, where f and c are the length of future and past windows, by considering GNNs as a solver for the approximating the transaction between them [11, 45], that is {ht0+1, ht0+2, ..., ht0+f} \u2248 GNN({ht0\u2212c+1, ht0\u2212c+2, ..., ht0}), in which the graph structure, usually as prior knowledge in STGs, is leveraged consistently via the GNN dynamics. However, it is preferable to have the graph structure reweighed [72] or rewired [2] especially when the prediction task is highly time-dependent, e.g., the traffic flow (speed) varies between rush hours and other time intervals. In this case, DMD shares a similar motivation by offering a data-driven estimated eigenspace for spectral filtering based on the selected initial dynamics and snapshots, resulting in a dynamic STG forecasting paradigm [36].\n# 6 DMD Recovers the Underlying Geometry\nRecall that in Section 5.1 we briefly showed the relationship between the DMD modes and eigenvalues in K and those in the generic F. In this section, we provide further analysis to show that\nOur analysis adopts the settings in [28]. First, without loss of generality, we assume that the underlying dynamic system contains a fixed point at the origin, i.e., f(0) = 0. We will also assume that the measurement function \u03a6 = id e.g., \u03a6(X) = X. Accordingly, one can rewrite the dynamic in (1) as\n(15)\n|| \ufffd \ufffd || \ufffd  |\ufffd| than |x| near the origin. In addition, we also assume that A is symmetric and the spectrum of A can be partitioned at a specific index (set as d for simplicity). That is \u03bbN \u2264\u00b7 \u00b7 \u00b7 \u03bbd+1 \u2264\u03bbd \u2264\u00b7 \u00b7 \u00b7 \u03bb1 < 0, and this suggests the existence of d-dimensional, normally attracting slow-spectral subspace S = span{e1, \u00b7 \u00b7 \u00b7 , ed}, where we denote e be the basis vector. Similarly, one can denote the corresponding fast-decay subspace denoted as F = span{ed+1, \u00b7 \u00b7 \u00b7 , eN}. We further let U(A) = [U(A)S, U(A)F ] be the corresponding eigenvectors in A, with U(A)S \u2208RN\u00d7d, U(A)F \u2208RN\u00d7(N\u2212d). Then we have the following conclusion.\nLemma 1 (Informal). With mild conditions, further assume that rank(H) = d and |U(A)F X(\u2113)|, |U(A)F X(\u2113+ 1)| \u2264|U(A)SX|1+\u03c4 for some \u03c4 \u2208(0, 1], suggesting in the underlying dynamics the subspace outside S can quickly shrink out. Then DMD can capture the principal component that drives the underlying dynamic. Specifically, let the output of DMD be denoted as D (e.g., K \u2286D), then D is with the form\nand D is locally topologically conjugated with order O(|U(A)SX|\u03c4) error to the linearized dynamic on a d-dimensional, slow attracting spectral submanifold M(S) \u2208C1 tangent to S at x = 0.\nProof and list of conditions for inducing Lemma 1 in Appendix A.1, (16) directly shows that DMD captures the principal component (i.e., \u039bs) that drives the dynamic system from its spectral subspace. We further remark that the results of Lemma 1 can be smoothly applied to the discretized dynamic by assuming the spectrum range of A to be |\u03bbN| \u2264\u00b7 \u00b7 \u00b7 \u2264|\u03bbd+1| \u2264|\u03bbd| \u2264\u00b7 \u00b7 \u00b7 |\u03bb1| < 1, similarly, we show more detailed clarification in Appendix A.1.\nGuidance for GNN Dynamic, Parameter Initialization and Heterophily Adaptation The conclusion in Lemma 1 provides guidance to select the initial dynamic and DMD-GNN filtering parameter initialization. For example, let\u2019s consider two different initial dynamics H(\u2113+ 1) = \ufffdAH(\u2113) and H(\u2113+ 1) = \ufffdA2H(\u2113), where we let \ufffdA \u2208RN\u00d7N be the normalized adjacency matrix. From the spectral graph theory, we have the eigenvalues of \ufffdA (denoted as \u039b( \ufffdA)) ranging from \u22121 to 1. The solutions of the corresponding continuous dynamics of them are\n \ufffd \ufffd H(t) = e\u2212(I\u2212\u039b( \ufffd A))tH(0) and H(t) = e\u2212(I\u2212\u039b2( \ufffd A))tH(0),\n    and it is well-known that the corresponding eigenvalues of normalized Laplacian \ufffdL is with the range of [0, 2]. However, one can verify that the range of the eigenvalues in I \u2212\u039b2( \ufffdA) is [0, 1] Therefore, with a fixed dimension of the slow decay subspace (i.e., fixed truncation rate), the second dynamic i.e., H(\u2113+ 1) = \ufffdA2H(\u2113) provides a smaller range of the eigenvalues and this is particularly useful for the graphs that require feature smoothing rather than sharpening dynamic in GNN, e.g., homophily graphs. Similarly, one can always bring the variation back to add the source term to the initial dynamic. For example, let the initial dynamic as H(\u2113+ 1) = \ufffdAH(\u2113) + H(\u2113) = ( \ufffdA + I)H(\u2113), and the solution of the continuous system is H(t) = e\u2212\u039b( \ufffd A)H(0), as \u039b( \ufffdA) \u2208[\u22121, 1], hence corresponding slow decay subspace is back to the wider range, suggesting a potential mixed dynamic i.e., negative eigenvalues for sharpening and positive eigenvalues for smoothing, which benefits DMD-GNNs for fitting heterophily graphs.\n# 7 Experiments\nWe apply the proposed DMD GNNs to various learning tasks: 1) Node classification on both homophilic and heterophilic graphs; 2) Node classification on long-range graphs [19]; 3) spatialtemporal dynamic predictions; 4) As an efficient encoder for link prediction tasks. We conduct the parameter sensitivity analysis to test the model\u2019s sensitivity on the truncation rate \u03be. We include detailed experiment setting information, and some further interpretation of our results via e.g., epidemiology and physics in Appendix C. All experiments are conducted in Python 3.11 on one NVIDIA\u00aeRTX 4090 GPU with 16384 CUDA cores and 24GB memory size.\n(16)\n(17)\n<div style=\"text-align: center;\">Table 1: Performances of DMD-GNNs via homophilic and heterophilic graphs, the best performances are marked in bold</div>\nare marked in bold\nMethods\nCora\nCiteseer\nPubmed\nTexas\nWisconsin\nCornell\nOGB-arXiv\nMLP\n55.1\u00b11.3\n59.1\u00b10.5\n71.4\u00b10.4\n92.3\u00b10.7\n91.8\u00b13.1\n91.3\u00b10.7\n55.0\u00b10.3\nGCN\n81.5\u00b10.5\n70.9\u00b10.5\n79.0\u00b10.3\n75.7\u00b11.0\n66.7\u00b11.4\n66.5\u00b113.8\n72.7\u00b10.3\nGAT\n83.0\u00b10.7\n72.0\u00b10.7\n78.5\u00b10.3\n78.8\u00b10.9\n71.0\u00b14.6\n76.0\u00b11.0\n72.0\u00b10.5\nGPRGNN\n83.8\u00b10.9\n75.9\u00b11.2\n79.8\u00b10.8\n75.9\u00b16.2\n89.9\u00b13.0\n85.0\u00b15.2\n70.4\u00b11.5\nAPPNP\n83.5\u00b10.7\n75.9\u00b10.6\n79.0\u00b10.3\n83.9\u00b10.7\n90.1\u00b13.5\n89.8\u00b10.6\n70.3\u00b12.5\nH2GCN\n83.4\u00b10.5\n73.1\u00b10.4\n79.2\u00b10.3\n85.9\u00b14.6\n87.9\u00b14.2\n85.1\u00b16.4\n72.8\u00b12.4\nSAGE\n74.5\u00b10.8\n67.2\u00b11.0\n76.8\u00b10.6\n79.3\u00b11.2\n64.8\u00b15.2\n71.4\u00b11.2\n70.6\u00b11.6\nGRAND++\n82.9\u00b11.4\n70.8\u00b11.1\n79.2\u00b11.5\n81.4\u00b13.5\n88.6\u00b12.1\n75.6\u00b13.2\n74.1\u00b12.3\nFramelets\n83.3\u00b10.5\n71.0\u00b10.6\n79.4\u00b10.4\n82.3\u00b12.5\n88.9\u00b13.2\n72.6\u00b10.3\n71.9\u00b10.2\nDMD-GCN\n82.6\u00b10.7\n71.4\u00b12.6\n79.3\u00b10.9\n84.2\u00b12.6\n85.2\u00b12.1\n83.6\u00b13.1\n73.9\u00b12.8\nDMD-SGC\n84.1\u00b10.4\n72.6\u00b10.8\n79.4\u00b11.4\n81.2\u00b12.5\n83.0\u00b11.8\n80.0\u00b11.9\n74.1\u00b10.9\nDMD++\n82.3\u00b10.3\n73.2\u00b10.4\n79.9\u00b10.7\n92.6\u00b13.4\n91.9\u00b12.6\n91.4\u00b11.7\n74.4\u00b11.5\nDMD-ACMP\n82.9\u00b10.9\n73.0\u00b12.1\n81.2\u00b11.5\n89.4\u00b13.8\n89.4\u00b12.2\n88.1\u00b12.4\n75.5\u00b10.8\nTable 2: Results on long-range graph datasets, accuracies are reported via Macro-F1 scores. Best performances are highlighted in bold.\nt performances are highlighted in bold.\nMethods\nCOCO-SP\nPascalVOC-SP\nMLP\n0.031\u00b10.016\n0.114\u00b10.023\nGCN\n0.079\u00b10.025\n0.238\u00b10.016\nGPRGNN\n0.044\u00b10.015\n0.152\u00b10.024\nSGC\n0.056\u00b10.011\n0.216\u00b10.039\nDMD-GCN\n0.081\u00b10.015\n0.243\u00b10.014\nDMD-SGC\n0.083\u00b10.012\n0.217\u00b10.036\nDMD++\n0.091\u00b10.009\n0.241\u00b10.017\n# 7.1 Node Classification on Homo and Heterophilic graphs\nWe report the performances of DMD-GNNs via both homophilic and heterophilic graphs for later the connected nodes are often with distinguished labels. The baseline models include: two-layer MLP, GCN, GAT, GPRGNN [15], H2GCN [78], GraphSage [29], ACMP, GRAND++ and Graph Framelets [76]. For datasets, we include the homophilic datasets Cora, Citeseer and Pubmed, and for heterophilic graphs we select Texas, Wisconsin and Cornell. In addition, to illustrate DMD-GNNs scalability, we test our models via OGB-arXiv [34]. For DMD-GNNs, we include DMD-GCN for which the initial feature dynamic is conducted via one layer of GCN without learnable weight matrices, i.e., H(\u2113+ 1) = \ufffdAH(\u2113). Followed by the generic form in (8), in terms of the initial non-linear dynamics, we selected the ACMP dynamic H(\u2113+ 1) = (A \u2212B)H(\u2113) + H(\u2113) \u2299(I \u2212H(\u2113) \u2299H(\u2113)), leading to the DMD-ACMP model. We also define DMD-SGC with the initial dynamic as H(\u2113+ 1) = \ufffdAsH(\u2113)[71], where s stands for a certain power of the adjacency due to SGC\u2019s specialty via large-scale graph datasets, and we fix s = 2. Moreover, to let the initial dynamic appropriately capture the graph characteristics, we define the DMD++ as H(\u2113+ 1) = \u03b1 \ufffdA2H(\u2113) + (1 \u2212\u03b1)H(\u2113), suggesting a feature dynamic with two-hops of neighboring information (i.e., SGC) and source term. Additionally, we fixed the truncation rate \u03be = 0.85 for homophilic graphs (as well as for Ogbn-arXiv) and \u03be = 0.7 for heterophilic graphs. The results are presented in Table 1, and one can check that DMD-GNNs outperform the baseline models across the majority of datasets. In addition, DMD-SGC shows higher/lower accuracy compared to DMD-GCN via homophilic/heterophilic graphs, suggesting that a stronger smoothing/sharpening effect, i.e., \ufffdAH compared to \ufffdA2H is preferred in homophilic/heterophilic graphs. Finally, DMD-GNNs that are induced from the original dynamics that contain diffusion-reaction process (e.g., ACMP) show better performances than those DMD-GNNs induced from relatively simple dynamics (e.g., GCN), this implies that DMD-GNNs can inherit the advantages of their original models.\nTable 3: Results on the link prediction tasks by treating DMD-GNNs as an graph feature encoder. The best performances are highlighted in bold.\nTable 3: Results on the link prediction tasks by treating DMD-GNNs as an graph feature encoder.\nThe best performances are highlighted in bold.\nMethods\nCora\nCiteseer\nChameleon Squirrel\nMLP\n75.0\u00b11.1 78.3\u00b1 0.8 76.9\u00b10.4\n73.2\u00b10.2\nGCN\n76.9\u00b10.8 77.9\u00b10.9 78.7\u00b10.5\n74.7\u00b10.1\nSAGE\n75.1\u00b10.5 76.3\u00b11.2 82.1\u00b10.4\n75.3\u00b10.3\nAPPNP\n76.0\u00b10.9 75.9\u00b11.1 78.9\u00b10.8\n73.6\u00b10.2\nDMD-GCN 75.7\u00b10.4 78.7\u00b10.6 76.1\u00b10.4\n78.8\u00b10.4\nDMD-SGC 77.4\u00b10.2 77.2\u00b10.7 78.3\u00b10.7\n74.3\u00b11.2\nDMD++\n75.3\u00b10.6 76.6\u00b10.6 77.5\u00b10.3\n78.1\u00b10.5\n<div style=\"text-align: center;\">Table 4: Mean and standard deviation of MSE on spatial-temporal prediction datasets. The best performances are highlighted in bold.</div>\nperformances are highlighted in bold.\nDataset\nMLP\nGCN\nGAT\nGAT-KNN\nGCN-KNN\nChebNet\nGrand++\nFramelet\nDMD-GCN\nDMD-SGC\nDMD-ACMP\nDMD++\nChickenpox\n0.924\n(\u00b10.001)\n0.923\n(\u00b10.001)\n0.924\n(\u00b10.002)\n0.926\n(\u00b10.004)\n0.926\n(\u00b10.004)\n0.916\n(\u00b10.014)\n0.991\n(\u00b10.009)\n0.923\n(\u00b10.03)\n0.978\n(\u00b10.093)\n0.935\n(\u00b10.014)\n0.924\n(\u00b10.001)\n0.910\n(\u00b10.002)\nCovid\n0.956\n(\u00b10.034)\n0.956\n(\u00b10.029)\n1.052\n(\u00b10.081)\n0.861\n(\u00b10.045)\n1.475\n(\u00b10.028)\n0.991\n(\u00b10.078)\n1.271\n(\u00b10.073)\n0.938\n(\u00b10.081)\n0.760\n(\u00b10.025)\n0.805\n(\u00b10.067)\n0.809\n(\u00b10.011)\n0.784\n(\u00b10.013)\nWikiMath\n1.073\n(\u00b10.042)\n1.292\n(\u00b10.125)\n1.339\n(\u00b10.073)\n0.826\n(\u00b10.070)\n1.023\n(\u00b10.058)\n1.589\n(\u00b10.048)\n1.044\n(\u00b10.328)\n0.988\n(\u00b10.069)\n0.848\n(\u00b10.031)\n0.901\n(\u00b10.028)\n0.934\n(\u00b10.015)\n0.805\n(\u00b10.042)\n# 7.2 Performance on Long Range Graph Benchmarks\nAs DMD estimates the lower-rank graph adjacency matrix via a data-driven manner, thus, the resulting matrix K from (12) is, in general, denser than the original graph adjacency, leading the DMD-GNNs to own the natural advantage in terms of long-range graph learning tasks for which traditional GNNs tend to lose their accuracy due to reduced sensitivity between node features caused by the relatively long hops or distances between them, also known as the over-squashing issue [60, 66]. In this experiment, we test the performance of DMD via two long-range graph datasets, namely PascalVOC-SP and COCO-SP (details in Appendix C.2). We did not include DMD-ACMP in this experiment due to the potential high variation of its hyperparameters. We evaluate the model performance via Macro-F1 scores and the final results are averaged from 10 runs. The learning outcomes are presented in Table. 2. One can see DMD-GNNs show superior performances by surpassing baseline models via COCO-SP, whereas limited improvements can be observed via PascalVOC-SP. We noticed that this might be due to the fact that in both datasets, the initial feature dimension is less than the number of classes, and DMD-GNNs are conducted via the spectral filtering paradigm, hence limited free parameters can be adjusted. To further investigate this, in Appendix C.2.4 we slightly change the architecture of DMD-GNNs by conducting spectral convolution first followed by two MLP layers, and study its impact on testing performances.\n# 7.3 Spatial-Temporal Dynamics Prediction for Infectious Disease Epidemiology\nWe test DMD-GNNs in three spatial-temporal datasets. These datasets are a series of graph-structured data with each of them containing an integer observation, e.g., the Chickenpox and COVID prevalence. The task is to predict the future, e.g., disease occurrence based on the historical records (snapshots). For example, in the Chickenpox dataset, the initial graph is with the nodes as countries and edges as direct neighborhood relationships. Followed by the settings from [72], the node features are lagged weekly counts of the chickenpox cases (e.g., 4-lags). For a more appropriate comparison, we also deploy GCN and GAT via KNN graphs, however, we do not use the KNN graph for DMD-GNNs. Table 4 shows the prediction accuracy of DMD-GNNs compared to different competitors. Other than GCN and GAT models, we also include ChebNet [18], Grand++, Graph Framelets DMD-GNNs show remarkable performances among all included datasets, indicating the potential of deploying DMD and related approaches to identifying the principal spreading pattern in terms of infectious disease epidemiology, see Appendix C.3.2 for more details.\n# 7.4 Sensitivity Analysis\nIn this experiment, we test DMD-GNN\u2019s sensitivities on the parameter \u03be \u2208[0.2, 0.8], which controls the rate of the truncation in DMD. The results are shown in Figure 2 (left). One can observe that in homophilic graphs, the best results for DMD-GNNs are achieved in a relatively large value of \u03be (i.e., using a wider spectrum) compared to heterophilic graphs in which learning accuracy dropped after best performances with lesser \u03be are achieved. This aligns with our previous claim in Section 6. That is the graph structural information as well as its spectrum is with higher/lower importance via homophilic/heterophilic graphs.\n# 7.5 DMD-GNNs for Link Prediction\nIn this section, we demonstrate that DMD-GNNs can serve as a powerful encoder for link prediction tasks by effectively capturing complex relational structures in graphs. Based on the work in [37], we design a model in which during the encoding process, negative links are randomly added to the original graph to enhance the model\u2019s discriminative capabilities. The model is then tasked with\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0d4b/0d4bb52c-0dbb-49c0-a8c1-1c6bf311455d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Results for hyperparameter sensitivity and comparison between DMD-GNNs and PIDMDGNNs via directed and undirected graphs.</div>\na binary classification problem, distinguishing between positive links from the original edges and negative links from the artificially added edges. Using node embeddings generated by the DMD-GNN encoder, the decoder predicts link existence across all edges including the negative links by computing the dot product between pairs of node embeddings for each edge. This operation aggregates the values across the embedding dimensions to produce a single scalar that represents the probability of edge existence for each edge. We select Cora, Citeseer, Chameleon and Squirrel [50] for the testing datasets and fixed the quantity of \u03be same as the ones used in node classification. The results are presented in Table. 3. One interesting observation here is the DMD-GNNs included from a relatively less complex initial dynamic (e.g., GCN) show better link prediction accuracy compared to others, and MLP shows comparable learning outcomes compared to all GNN models.\n# 8 Model Extensions and Further Directions\nDMD with Additional Constraints One can check that DMD approach defined in Section 5.1 can also be viewed as [54, 67] arg minrank(K)\u2264d \u2225H(\u2113+ 1) \u2212KH(\u2113)\u2225F . However, when information about the underlying dynamic is known, e.g., K should be symmetric or shift equivariant, one may prefer to align these constraints to the above optimization problem, yielding the so-called physicsinformed DMD [6] as arg minK\u2208M \u2225H(\u2113+ 1) \u2212KH(\u2113)\u2225F , where we let M be the generic notion of the potential manifold that contains K. The manifold constraint on K leads to variations in terms of the estimated eigenvectors, i.e., \u03a8, indicating a family of physics-informed DMD-GNNs (PIDMDGNNs), that can potentially show better performances in different learning tasks. In addition, one can let K satisfy the so-called coupling conditions [58], such that K1 = \u00b5, K\u22a41 = \u03bd, then the problem can be treated as an optimal transport problem between features, and these features may not need to be sourced from the same graph, e.g., transportation between two knowledge graphs. In this case, K serves as a graph-structured transportation plan between two distributions.\nOn Directed Graphs and Reconstruct Complex Physical Systems The DMD algorithm described in Section 5.1 is not necessarily produce a symmetric K. This suggests DMD algorithm may be naturally suitable for direct graphs. To verify this idea and motivate future research directions, we briefly test the performances of DMD-GNNs and PIDMD-GNNs (with symmetric constraints) via two directed graphs: Computer and Photo in which, unlike the node classification experiment where the graph is programmed to be undirected, we preserve the directions of the links. The results presented in Figure. 2 show that PIDMD-GNNs underperform/outperform DMD-GNNs via directed/undirected graphs. In Appendix C.5 we also show that PIDMD-GNNs own the potential to reconstruct complex physical dynamics such as Schr\u00f6dinger\u2019s equation.\n# 9 Conclusion\nIn this work, we introduced a novel integration of DMD with GNNs, demonstrating its efficacy in enhancing GNN performance across various graph learning tasks. By leveraging DMD to approximate the underlying dynamics of graph data, we are able to capture key dynamical patterns through a low-rank representation, which in turn enables more effective feature propagation in GNNs. Our proposed DMD-GNN models demonstrate superior performance across various tasks, underscoring the benefits of incorporating the advanced dynamic system analysis tool into graph learning.\n[1] Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S\u00fcsstrunk. Slic superpixels compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence, 34(11):2274\u20132282, 2012. [2] Ahmad Ali, Yanmin Zhu, and Muhammad Zakarya. Exploiting dynamic spatio-temporal graph convolutional neural networks for citywide traffic flows prediction. Neural networks, 145:233\u2013247, 2022. [3] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021. [4] Hassan Arbabi and Igor Mezic. Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the koopman operator. SIAM Journal on Applied Dynamical Systems, 16(4):2096\u20132126, 2017. [5] Omri Azencot, N Benjamin Erichson, Vanessa Lin, and Michael Mahoney. Forecasting sequential data using consistent koopman autoencoders. In International Conference on Machine Learning, pages 475\u2013485. PMLR, 2020. [6] Peter J Baddoo, Benjamin Herrmann, Beverley J McKeon, J Nathan Kutz, and Steven L Brunton. Physics-informed dynamic mode decomposition. Proceedings of the Royal Society A, 479(2271):20220576, 2023. [7] Pradeep Kr Banerjee, Kedar Karhadkar, Yu Guang Wang, Uri Alon, and Guido Mont\u00fafar. Oversquashing in GNNs through the lens of information contraction and graph expansion. In The 58th Annual Allerton Conference on Communication, Control, and Computing, pages 1\u20138. IEEE, 2022. [8] Nimrod Berman, Ilan Naiman, and Omri Azencot. Multifactor sequential disentanglement via structured koopman autoencoders. In The Eleventh International Conference on Learning Representations, 2023. [9] Mitchell Black, Zhengchao Wan, Amir Nayyeri, and Yusu Wang. Understanding oversquashing in GNNs through the lens of effective resistance. In International Conference on Machine Learning, pages 2528\u20132547. PMLR, 2023. 10] Steven L Brunton, Marko Budi\u0161i\u00b4c, Eurika Kaiser, and J Nathan Kutz. Modern koopman theory for dynamical systems. arXiv preprint arXiv:2102.12086, 2021. 11] Khac-Hoai Nam Bui, Jiho Cho, and Hongsuk Yi. Spatial-temporal graph neural network for traffic forecasting: An overview and open research issues. Applied Intelligence, 52(3):2763\u2013 2774, 2022. 12] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb, and Emanuele Rossi. Grand: Graph neural diffusion. In International Conference on Machine Learning, pages 1407\u20131418. PMLR, 2021. 13] Benjamin Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Xiaowen Dong, and Michael Bronstein. Beltrami flow and neural diffusion on graphs. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 1594\u20131609. Curran Associates, Inc., 2021. 14] Benjamin Paul Chamberlain, James Rowbottom, Maria Goronova, Stefan Webb, Emanuele Rossi, and Michael M Bronstein. Grand: Graph neural diffusion. Proceedings of the 38th International Conference on Machine Learning, (ICML) 2021, 18-24 July 2021, Virtual Event, 2021. 15] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. In Proceedings of International Conference on Learning Representations, 2021. 16] Jeongwhan Choi, Seoyoung Hong, Noseong Park, and Sung-Bae Cho. Gread: Graph neural reaction-diffusion networks. In International Conference on Machine Learning, pages 5722\u2013 5747. PMLR, 2023. 17] Andreea Deac, Marc Lackenby, and Petar Veli\u02c7ckovi\u00b4c. Expander graph propagation. In Learning on Graphs Conference, pages 38\u20131. PMLR, 2022.\n[18] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. Advances in Neural Information Processing Systems, 29, 2016. [19] Vijay Prakash Dwivedi, Ladislav Ramp\u00e1\u0161ek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. Advances in Neural Information Processing Systems, 35:22326\u201322340, 2022. [20] Hamidreza Eivazi, Luca Guastoni, Philipp Schlatter, Hossein Azizpour, and Ricardo Vinuesa. Recurrent neural networks and koopman-based frameworks for temporal predictions in a loworder model of turbulence. International Journal of Heat and Fluid Flow, 90:108816, 2021. [21] Lars Eld\u00e9n and Haesun Park. A procrustes problem on the stiefel manifold. Numerische Mathematik, 82(4):599\u2013619, 1999. [22] Moshe Eliasof, Eldad Haber, and Eran Treister. Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations. Advances in neural information processing systems, 34:3836\u20133849, 2021. [23] Lukas Fesser and Melanie Weber. Mitigating over-smoothing and over-squashing using augmentations of forman-ricci curvature. In Learning on Graphs Conference, pages 19\u20131. PMLR, 2023. [24] Ronald Aylmer Fisher. The wave of advance of advantageous genes. Annals of eugenics, 7(4):355\u2013369, 1937. [25] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In International Conference on Machine Learning, pages 1263\u20131272. PMLR, 2017. [26] Francesco Di Giovanni, James Rowbottom, Benjamin Paul Chamberlain, Thomas Markovich, and Michael M. Bronstein. Understanding convolution on graphs via energies. Transactions on Machine Learning Research, 2023. [27] Jhony H Giraldo, Konstantinos Skianis, Thierry Bouwmans, and Fragkiskos D Malliaros. On the trade-off between over-smoothing and over-squashing in deep graph neural networks. In Proceedings of the 32nd ACM international conference on information and knowledge management, pages 566\u2013576, 2023. [28] George Haller and B\u00e1lint Kasz\u00e1s. Data-driven linearization of dynamical systems. Nonlinear Dynamics, pages 1\u201325, 2024. [29] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. Advances in Neural Information Processing Systems, 30, 2017. [30] Andi Han, Dai Shi, Lequan Lin, and Junbin Gao. From continuous dynamics to graph neural networks: Neural diffusion and beyond. Transactions on Machine Learning Research, 2024. Survey Certification. [31] Andi Han, Dai Shi, Zhiqi Shao, and Junbin Gao. Generalized energy and gradient flow via graph framelets. arXiv:2210.04124, 2022. [32] Philip Hartman. On local homeomorphisms of euclidean spaces. Bol. Soc. Mat. Mexicana, 5(2):220\u2013241, 1960. [33] Nicholas J Higham. The symmetric procrustes problem. BIT Numerical Mathematics, 28:133\u2013 143, 1988. [34] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. Advances in Neural Information Processing Systems, 33:22118\u201322133, 2020. [35] Sara M Ichinaga, Francesco Andreuzzi, Nicola Demo, Marco Tezzele, Karl Lapo, Gianluigi Rozza, Steven L Brunton, and J Nathan Kutz. Pydmd: A python package for robust dynamic mode decomposition. arXiv preprint arXiv:2402.07463, 2024. [36] Boris Ivanovic and Marco Pavone. The trajectron: Probabilistic multi-agent trajectory modeling with dynamic spatiotemporal graphs. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2375\u20132384, 2019. [37] Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\n[38] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, 2017. [39] Bernard O Koopman. Hamiltonian systems and transformation in hilbert space. Proceedings of the National Academy of Sciences, 17(5):315\u2013318, 1931. [40] J Nathan Kutz, Steven L Brunton, Bingni W Brunton, and Joshua L Proctor. Dynamic mode decomposition: data-driven modeling of complex systems. SIAM, 2016. [41] J Nathan Kutz, Joshua L Proctor, and Steven L Brunton. Koopman theory for partial differential equations. arXiv preprint arXiv:1607.07076, 2016. [42] Henning Lange, Steven L Brunton, and J Nathan Kutz. From fourier to koopman: Spectral methods for long-term time series prediction. Journal of Machine Learning Research, 22(41):1\u2013 38, 2021. [43] Cruz Y Li, Zengshun Chen, Xuelin Zhang, KT Tim, and Chongjia Lin. Koopman analysis by the dynamic mode decomposition in wind engineering. Journal of Wind Engineering and Industrial Aerodynamics, 232:105295, 2023. [44] Lequan Lin and Junbin Gao. A magnetic framelet-based convolutional neural network for directed graphs. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1\u20135. IEEE, 2023. [45] Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. Diffusion models for timeseries applications: a survey. Frontiers of Information Technology & Electronic Engineering, 25(1):19\u201341, 2024. [46] Yong Liu, Chenyu Li, Jianmin Wang, and Mingsheng Long. Koopa: Learning non-stationary time series dynamics with koopman predictors. Advances in Neural Information Processing Systems, 36, 2024. [47] Igor Mezi\u00b4c. Analysis of fluid flows via spectral properties of the koopman operator. Annual review of fluid mechanics, 45(1):357\u2013378, 2013. [48] Indranil Nayak, Debdipta Goswami, Mrinal Kumar, and Fernando Teixeira. Temporallyconsistent koopman autoencoders for forecasting dynamical systems. arXiv preprint arXiv:2403.12335, 2024. [49] George Panagopoulos, Giannis Nikolentzos, and Michalis Vazirgiannis. Transfer graph neural networks for pandemic forecasting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 4838\u20134845, 2021. [50] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding. Journal of Complex Networks, 9(2):cnab014, 2021. [51] Benedek Rozemberczki, Paul Scherer, Yixuan He, George Panagopoulos, Alexander Riedel, Maria Astefanoaei, Oliver Kiss, Ferenc Beres, Guzman Lopez, Nicolas Collignon, et al. Pytorch geometric temporal: Spatiotemporal signal processing with neural machine learning models. In Proceedings of the 30th ACM international conference on information & knowledge management, pages 4564\u20134573, 2021. [52] T Konstantin Rusch, Michael M Bronstein, and Siddhartha Mishra. A survey on oversmoothing in graph neural networks. arXiv:2303.10993, 2023. [53] T Konstantin Rusch, Ben Chamberlain, James Rowbottom, Siddhartha Mishra, and Michael Bronstein. Graph-coupled oscillator networks. In International Conference on Machine Learning, pages 18888\u201318909. PMLR, 2022. [54] Peter J Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of fluid mechanics, 656:5\u201328, 2010. [55] Peter J Schmid. Dynamic mode decomposition and its variants. Annual Review of Fluid Mechanics, 54(1):225\u2013254, 2022. [56] Peter H Sch\u00f6nemann. A generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):1\u201310, 1966. [57] Zhiqi Shao, Dai Shi, Andi Han, Yi Guo, Qibin Zhao, and Junbin Gao. Unifying over-smoothing and over-squashing in graph neural networks: A physics informed approach and beyond. arXiv:2309.02769, 2023.\n[58] Dai Shi, Junbin Gao, Xia Hong, ST Boris Choy, and Zhiyong Wang. Coupling matrix manifolds assisted optimization for optimal transport problems. Machine Learning, 110:533\u2013558, 2021. [59] Dai Shi, Yi Guo, Zhiqi Shao, and Junbin Gao. How curvature enhance the adaptation power of framelet GCNs. arXiv:2307.09768, 2023. [60] Dai Shi, Andi Han, Lequan Lin, Yi Guo, and Junbin Gao. Exposition on over-squashing problem on GNNs: Current methods, benchmarks and challenges. arXiv:2311.07073, 2023. [61] Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, and Junbin Gao. Design your own universe: A physics-informed agnostic method for enhancing graph neural networks. International Journal of Machine Learning and Cybernetics, 2024. [62] Dai Shi, Zhiqi Shao, Junbin Gao, Zhiyong Wang, and Yi Guo. Frameless graph knowledge distillation. IEEE Transactions on Neural Networks and Learning Systems, 2024. [63] Dai Shi, Zhiqi Shao, Yi Guo, Qibin Zhao, and Junbin Gao. Revisiting generalized p-laplacian regularized framelet gcns: Convergence, energy dynamic and as non-linear diffusion. Transactions on Machine Learning Research, 2023. [64] Matthew Thorpe, Tan Minh Nguyen, Hedi Xia, Thomas Strohmer, Andrea Bertozzi, Stanley Osher, and Bao Wang. GRAND++: Graph neural diffusion with a source term. In International Conference on Learning Representations, 2022. [65] Mathieu Blondel Tianlin Liu, Joan Puigcerver. Sparsity-constrained optimal transport. In Proceedings of the Eleventh International Conference on Learning Representations (ICLR), 2023. [66] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In International Conference on Learning Representations, 2021. [67] Jonathan H Tu. Dynamic mode decomposition: Theory and applications. PhD thesis, Princeton University, 2013. [68] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua Bengio. Graph attention networks. In International Conference on Learning Representations, 2018. [69] Guillaume Verdon, Trevor McCourt, Enxhell Luzhnica, Vikash Singh, Stefan Leichenauer, and Jack Hidary. Quantum graph neural networks. arXiv preprint arXiv:1909.12264, 2019. [70] Yuelin Wang, Kai Yi, Xinliang Liu, Yu Guang Wang, and Shi Jin. ACMP: Allen-cahn message passing with attractive and repulsive forces for graph neural networks. In The Eleventh International Conference on Learning Representations, 2023. [71] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning, pages 6861\u20136871. PMLR, 2019. [72] Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi Yan. DIFFormer: Scalable (graph) transformers induced by energy constrained diffusion. In International Conference on Learning Representations (ICLR), 2023. [73] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. arXiv preprint arXiv:1904.07785, 2019. [74] Mengxi Yang, Shi Dai, Xuebin Zheng, Jie Yin, and Junbin Gao. Quasi-framelets: Another improvement to graph neural networks. International Journal of Machine Learning and Cybernetics, 2024. [75] Kaisheng Zeng, Chengjiang Li, Lei Hou, Juanzi Li, and Ling Feng. A comprehensive survey of entity alignment for knowledge graphs. AI Open, 2:1\u201313, 2021. [76] Xuebin Zheng, Bingxin Zhou, Junbin Gao, Yuguang Wang, Pietro Li\u00f3, Ming Li, and Guido Montufar. How framelets enhance graph neural networks. In International Conference on Machine Learning, pages 12761\u201312771. PMLR, 2021. [77] Xuebin Zheng, Bingxin Zhou, Yu Guang Wang, and Xiaosheng Zhuang. Decimated framelet system on graphs and fast g-framelet transforms. Journal of Machine Learning Research, 23:18\u20131, 2022.\n78] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793\u20137804, 2020.\n[78] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. Advances in Neural Information Processing Systems, 33:7793\u20137804, 2020.\nEXPERIMENT DETAILS 1 C.1 NODE CLASSIFICATION ON BOTH TYPES OF GRAPHS . . . . . . . . . . . . . . 1 C.1.1 BASELINES AND DMD-GNNS IMPLEMENTATION DETAILS . . . . . . . 1 C.2 PERFORMANCE ON LONG RANGE GRAPH BENCHMARKS (LRGB) . 2 C.2.1 OVER-SQUASHING IN GNNS . . . . . . . . . . . . . . . . . . . . . . . . 2 C.2.2 DATASETS AND DATA PRE-PROCESSING . . . . . . . . . . . . . . . . . . 2 C.2.3 TRAINING DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 C.2.4 DMD-GNN IMPLEMENTATION AND HYPERPARAMETERS . . . . . . . . . 2 C.3 SPATIAL-TEMPORAL DYNAMIC PREDICTIONS . . . . . . . . . . . . . . . . . . . 2 C.3.1 DATASETS AND DATA PRE-PROCESSING . . . . . . . . . . . . . . . . . . 2 CHICKENPOX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 COVID . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 WIKIMATH . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 C.3.2 INFLUENCE ON INFECTIOUS DISEASE EPIDEMIOLOGY . . . . . . . . . . 2 C.4 LINK PREDICTION DETAILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 C.4.1 TASK INFORMATION AND DATA PRE-PROCESSING . . . . . . . . . . . . 2 C.4.2 DMD-GNN IMPLEMENTATION AND HYPERPARAMETERS . . . . . . . . 2 C.5 DIRECT GRAPH AND PHYSICS INFORMED DMD . . . . . . . . . . . . . . . . . 2 C.5.1 PHYSICS INFORMED DMD AND PIDMD-GNN FOR SCIENCE . . . . . . 2 C.5.2 ILLUSTRATIVE EXAMPLE: SCHR\u00d6DINGER EQUATION . . . . . . . . . . 2 C.5.3 OPTIMIZATION WITHIN PIDMD . . . . . . . . . . . . . . . . . . . . . . 2\n17\n# A THEORETICAL JUSTIFICATIONS OF DMDS\n# A.1 THEORETICAL PROOFS\nLemma 2 (Formal Version of Lemma 1). Assuming that the system defined in (15) has a stable hyperbolic fixed point at origin x = 0, f \u2208C2 in a neighborhood of the origin. Let \u03a6(0) = X and for some (generic) integer d, rank [DX|S] = d, suggesting no rank degeneracy in the slow decayed subspace S. Further assume that rank(H) = d and |U(A)F X(\u2113)|, |U(A)F X(\u2113+ 1)| \u2264 |U(A)SX|1+\u03c4 for some \u03c4 \u2208(0, 1], suggesting in the underlying dynamics the subspace outside S can quickly shrink out. Then the estimation of DMD, denoted as D (e.g., K \u2286D) is with the form\nDAA  O|A| and D is locally topologically conjugated with order O(|U(A)SX|\u03c4) error to the linearized dynamic on a d-dimensional, slow attracting spectral submanifold M(S) \u2208C1 tangent to S at x = 0.\nOur proof follows directly from the one used in proving Theorem 1 in the work [28], with the case that A is symmetric and \u03a6 = id, i.e., \u03a6(X) = X, where we generically denote X as the underlying system states. We briefly show our proof of self-completeness.\nProof. By the assumption of a stable hyperbolic fixed point and the C1 linearization theorem [32], any trajectory in a neighborhood of the origin in the nonlinear system (15) converges at an exponential rate e\u03bbd+1t to a d-dimensional attracting spectral submanifold M(S), tangent to a d-dimensional attracting slow spectral subspace S of the linearized system at the origin. Based on Theorem 1 in [28], we have the form of the estimations of DMD as the submanifold defined as\nDAA O|A| where we let T(A)S and P(A)S \u2208RN\u00d7d be the right and left eigenvectors of A restricted by S, respectively. Given we assumed the A is symmetric, we have T(A)S = P(A)S = U(A)S. Therefore the above equation can be further simplified to\nDAA O|A| Further, because the assumption that D\u03a6 = id, hence the derivative D = D\u03a6(X) w.r.t X is I, therefore we have\nDAA O|A| Further, because the assumption that D\u03a6 = id, hence the derivative D = D\u03a6(X) w.r.t X is I, therefore we have\nD and this completes the proof.\n# B GNNS WITH DIFFERENT TYPES OF SOURCE TERM\nIn this section, we provide more examples regarding GNNs with source terms, which serve as a generic notion to those GNNs that contain two terms with one term homogenizing the node feature and another term bringing the variation back to the system to preserve the dissimilarities between features. In terms of the spatial GNNs, this paradigm typically refers to the diffusion-reaction paradigm mentioned in the paper. Based on the summary provided in [16] and [30], we include some of recently developed GNNs for self-completeness. On the other hand, in terms of spectral GNNs, this GNN structure may refer to multi-scale GNNs like Wavelet GNNs [73] or Framelet GNNs [59, 77, 57, 63]. Given there are many GNNs in this field, in this work, we only include the formulation of the models that we included in our experiments, for more details, please refer to the recent reports [30]. In terms of the notations, for all formulated GNNs, we denote them via normalized adjacency/Laplacian matrices, i.e., \ufffdA and \ufffdL, respectively, although notation could be a bit different within their original publications. We further note that we omit to introduce those basic yet state-of-the-art GNNs such as MLP, GCN, GAT and GPRGNN. Finally, we start by showing the formulation of the APPNP model which has the feature propagation as:\n \u2212 \ufffd where we let L be the number of layers. The core idea of APPNP is to balance the diffusion and reaction terms by \u03b1 and 1 \u2212\u03b1, and this approach has been adopted by many successive models, such as ACMP [70]: H(\u2113+ 1) = ( \ufffdA \u2212B)H(\u2113) + H(\u2113) \u2299(I \u2212H(\u2113) \u2299H(\u2113)) (23\n(18)\n(19)\n(20)\n(21)\n21)\n(22)\n(23)\nin which, a matrix B is involved with adjacency matrix \ufffdA for adjusting the attractive and repulsive forces between nodes, and the so-called Allen-Cahn term H(\u2113) \u2299(I \u2212H(\u2113) \u2299H(\u2113)) is leveraged as the source term. Similar to APPNP, the graph sample and aggregate (SAGE) [29] is with the propagation rule as:\n \ufffd where the source term is generated from the (embedded) feature matrix in addition to the original GCN propagation. We then formulate the GRAND++, which serves as the initial work on bringing the source term to the diffusion process of the graph \u2202H \u2202t = div( \ufffdA(H) \u00b7 \u2207H) + S which owns discretized version as:\n \ufffd GRAND++ : H(\u2113+ 1) = \ufffdAH(\u2113) + S,\n \ufffd in which the S is the source term with each row of the S is defined as Si = \ufffd j\u2208I \u03b4ij(H(0)j \u2212\u00afH(0) with I \u2286V a selected node subset used as the source term and \u00afH(0) = 1 |I| \ufffd j\u2208I H(0)j is the average signal. \u03b4ij denotes the initial transition probability from node j to i. Finally, we include H2GCN as\nHfinal i = Combine(Hi(0), Hi(1), \u00b7 \u00b7 \u00b7 Hi(L)),\n \u00b7 \u00b7 \u00b7 where we let h be the neighboring order (hops) that H2GCN uses to aggregate, and the representation is formed as a combination of all intermediate representations. Lastly, another embedding of Hfinal is leveraged to make the final prediction of the labels. In terms of spectral GNNs, we only include ChebNet [18] and graph framelets. Both two models propagate the node features via spectral filtering approach. For example, we have\n \u00b7 \u00b7 \u00b7 where we let h be the neighboring order (hops) that H2GCN uses to aggregate, and the representation is formed as a combination of all intermediate representations. Lastly, another embedding of Hfinal is leveraged to make the final prediction of the labels.\nin which we let diag(\u03b8) be the filtered graph spectrum. We note that the process of the ChebyNet can be empirically approximated by the corresponding polynomials and one usually applies feature embedding, e.g., MLP(H), prior to the spectral filtering process for computational convenience. Similarly, framelet GNN attempts to balance the feature smoothing and sharpening effects induced from the spectral filtering on both low and high pass domains, that is\nwhere we let W be the framelet decomposition matrices such that \ufffd (r,j)\u2208I W\u22a4 r,jWr,j = I for P = {(r, j) : r = 1, ..., L, j = 0, 1, ..., J} \u222a{(0, J)}. For example, one can denote the framelet Haar type filter as\nand let \u03b8ii > 0, one can check that compared to the low pass filtering process cos(diag(\u03b8))U\u22a4H(\u2113)W(\u2113), one can treat the high pass filtering process as a reaction process to bring the variation back to the model.\n# C EXPERIMENT DETAILS\nIn this section, we show details of our experiments, and in the next section, we present some extra tests in addition to our current results.\n# C.1 NODE CLASSIFICATION ON BOTH TYPES OF GRAPHS\nTable. 5 shows the summary statistics of the included static graphs, in which we let H(G) be their homophily index which indicates the uniformity of labels in neighborhoods. For example, a higher homophily index suggests that the connected nodes within the graph are more likely with the same label. we note that we also show the summary statistics of CS and Photo as they have been utilized via\n(24)\n(25)\n(26)\n(27)\n(28)\n(29)\n(30)\n<div style=\"text-align: center;\">Table 5: Summary statistics of datasets, the column H(G) is the homophil</div>\n<div style=\"text-align: center;\">Table 5: Summary statistics of datasets, the column H(G) is the homophily index. Datasets #Classes #Features #nodes #Edges H(G) Ranks</div>\n HG\nDatasets\n#Classes #Features #nodes #Edges\nH(G) Ranks\nCora\n7\n1433\n2708\n5429\n0.825\n421\nCiteseer\n6\n3703\n3327\n4372\n0.717\n819\nPubmed\n3\n500\n19717\n44338\n0.792\n295\nCS\n15\n6805\n18333\n100227 0.832 1750\nPhoto\n8\n745\n7487\n126530 0.849\n402\nOgbn-arxiv\n47\n100\n169343 1166243 0.681\n128\nWisconsin\n5\n251\n499\n1703\n0.150\n21\nTexas\n5\n1703\n183\n279\n0.097\n35\nCornell\n5\n1703\n183\n277\n0.386\n16\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d55/4d559b84-e8eb-48f9-9f5c-d31e46b7ff63.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Number of feature dimensions and the actual number of ranks selected from the DMD. From the first row (\u03be = 0.85): Cora, Citeseer, Pubmed; second row (\u03be = 0.7): Wisconsin, Texas and Cornell.</div>\nSection 8 for testing the performance difference between DMD-GNNs and PIDMD-GNNs. Finally, as we have shown in the main page, we fixed SVD Rank within the SVD of H(\u2113) as 0.85 for the homophilic graphs (as well as for Ogbn-arXiv)) and 0.7 for heterophilic graphs. In Table. 5 we also show the number of actual ranks that are used for DMD-GNNs, and these ranks directly determine the number of parameters for the spectral filtering within DMD-GNNs, e.g., (14). Figure. 3 visualizes the actual number of the ranks (highlighted in orange) via six included datasets. From both table and figure, one can see that DMD significantly reduced the number of singular values. Especially in the heterophilic graphs, DMD-GNNs only require less than 40 learnable parameters for spectral filtering. This directly verified our claim in Section 5.2. That is, for homophilic graphs, in which more graph structural information is needed for GNNs, one may need a higher value of \u03be for the truncation compared to the heterophilic graphs.\n# C.1.1 BASELINES AND DMD-GNNS IMPLEMENTATION DETAI\nAll included baselines are implemented using torch.geometric, except graph framelets with we adapt from the work in [77, 74]. As there are many ways of implementing graph framelets, in this work, we only consider the easiest case, i.e., Haar filter, which leads to the propagation denoted in (30). We set the number of layers of all included baselines as 2, and the baseline outcomes are collected from the pre-published results. In terms of DMD-GNNs, all of our proposed models adopt a spectral filtering paradigm, as illustrated in (14). Hence, in most of our conducted experiments, we let all DMD-GNNs with one single layer. More specifically, for all tested datasets, we do normalization on both feature and graph adjacency to adjust the potential scaling issues. In terms of the ACMP\n<div style=\"text-align: center;\">Table 6: Summary statistics of the selected LRGBs [19]</div>\nDataset\nTotal Graphs\nTotal Nodes\nAverage Nodes\nTotal Edges\nPascalVOC-SP\n11355\n5443545\n479.40\n30777444\nCOCO-SP\n123286\n58793216\n476.8\n332091902\ndynamic, we fixed Bij = 0.0001 and 0.5 for homophilic and heterophilic graphs, respectively bas on the conclusion in [70]\nFor all included datasets, we followed the standard data split scheme. In terms of the model training and testing, we leverage the ADAM as the optimizer. We also found that in this experiment, our proposed DMD-GNN models are not sensitive to hyperparameters such as hidden dimension, learning rate, weight decay, dropout, etc, suggesting the effectiveness of incorporating DMD to refine the initial dynamics on the graph. Finally, the maximum number of epochs is set as 200 for all included datasets except 500 for Ogbn-arXiv. The",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of enhancing Graph Neural Networks (GNNs) by integrating Dynamic Mode Decomposition (DMD) to improve performance on graph-structured data. Previous methods have struggled with computational issues like over-smoothing and over-squashing. The authors argue that a deeper analysis of GNN dynamics through DMD can provide significant insights and improvements.",
        "problem": {
            "definition": "The core problem is the inefficiency of existing GNNs in capturing complex dynamics in graph data, particularly in long-range interactions and large-scale graphs.",
            "key obstacle": "The main challenge is the inability of traditional GNNs to effectively model nonlinear interactions and dynamics due to their reliance on simplistic feature propagation mechanisms."
        },
        "idea": {
            "intuition": "The idea stems from the observation that GNN feature propagation resembles diffusion processes, which can be analyzed through dynamical systems theory.",
            "opinion": "The proposed method integrates DMD into GNNs to leverage the low-rank eigenfunctions of the DMD algorithm, enhancing the GNN's ability to capture complex dynamics.",
            "innovation": "The primary innovation is the establishment of a link between GNNs and the Koopman operator learning theory via DMD, allowing for a more nuanced understanding of graph dynamics."
        },
        "method": {
            "method name": "DMD-enhanced Graph Neural Networks (DMD-GNNs)",
            "method abbreviation": "DMD-GNN",
            "method definition": "DMD-GNNs utilize Dynamic Mode Decomposition to approximate the underlying dynamics of graph data, integrating low-rank representations into GNN architectures.",
            "method description": "The core of the method involves applying DMD to capture the principal components of graph dynamics, which are then used to inform feature propagation in GNNs.",
            "method steps": [
                "Collect multiple states of the graph data.",
                "Apply DMD to estimate the low-rank operator.",
                "Use the DMD modes to inform the GNN's feature propagation mechanism."
            ],
            "principle": "The effectiveness of this method lies in its ability to reduce the complexity of the GNN while improving its capacity to model complex interactions through low-rank approximations."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on various datasets, including citation networks and spatial-temporal graphs, comparing DMD-GNNs against standard GNN architectures.",
            "evaluation method": "Performance was assessed using metrics like accuracy and Macro-F1 scores across different graph learning tasks, including node classification and link prediction."
        },
        "conclusion": "The results demonstrate that DMD-GNNs achieve state-of-the-art performance across various tasks, validating the integration of DMD as a powerful enhancement to traditional GNN methodologies.",
        "discussion": {
            "advantage": "The key advantages of DMD-GNNs include improved performance on long-range graphs and reduced sensitivity to over-smoothing and over-squashing effects.",
            "limitation": "One limitation noted is the potential complexity in tuning the DMD parameters for different types of graphs, which may require careful consideration.",
            "future work": "Future research could explore the incorporation of additional domain-specific constraints into the DMD computation and further applications of DMD-GNNs in other complex dynamical systems."
        },
        "other info": {
            "info1": "The paper establishes a theoretical foundation connecting GNN dynamics to DMD.",
            "info2": {
                "info2.1": "DMD-GNNs are validated through extensive experiments on various datasets.",
                "info2.2": "The authors suggest exploring physics-informed DMD for directed graphs."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper establishes a theoretical foundation connecting Graph Neural Network (GNN) dynamics to Dynamic Mode Decomposition (DMD)."
        },
        {
            "section number": "2.2",
            "key information": "The core problem is the inefficiency of existing GNNs in capturing complex dynamics in graph data, particularly in long-range interactions and large-scale graphs."
        },
        {
            "section number": "2.3",
            "key information": "Key obstacle includes the inability of traditional GNNs to effectively model nonlinear interactions and dynamics due to their reliance on simplistic feature propagation mechanisms."
        },
        {
            "section number": "3.1",
            "key information": "DMD-GNNs utilize Dynamic Mode Decomposition to approximate the underlying dynamics of graph data, integrating low-rank representations into GNN architectures."
        },
        {
            "section number": "5.1",
            "key information": "The primary innovation is the establishment of a link between GNNs and the Koopman operator learning theory via DMD, allowing for a more nuanced understanding of graph dynamics."
        },
        {
            "section number": "5.4",
            "key information": "One limitation noted is the potential complexity in tuning the DMD parameters for different types of graphs, which may require careful consideration."
        },
        {
            "section number": "7.1",
            "key information": "The key advantages of DMD-GNNs include improved performance on long-range graphs and reduced sensitivity to over-smoothing and over-squashing effects."
        },
        {
            "section number": "7.3",
            "key information": "Future research could explore the incorporation of additional domain-specific constraints into the DMD computation and further applications of DMD-GNNs in other complex dynamical systems."
        }
    ],
    "similarity_score": 0.5792089455611352,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/When Graph Neural Networks Meet Dynamic Mode Decomposition.json"
}