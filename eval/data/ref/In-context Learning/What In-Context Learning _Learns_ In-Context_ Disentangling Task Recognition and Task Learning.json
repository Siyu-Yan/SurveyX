{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.09731",
    "title": "What In-Context Learning \"Learns\" In-Context: Disentangling Task Recognition and Task Learning",
    "abstract": "Large language models (LLMs) exploit incontext learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations \u2013 even without ground-truth labels \u2013 and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL\u2019s performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.1",
    "bib_name": "pan2023incontextlearninglearnsincontext",
    "md_text": "# What In-Context Learning \u201cLearns\u201d In-Context: Disentangling Task Recognition and Task Learning\nJane Pan Tianyu Gao Howard Chen Danqi Chen Department of Computer Science, Princeton University {jp7224,tianyug,howardchen,danqic}@cs.princeton.edu\n# Abstract\nLarge language models (LLMs) exploit incontext learning (ICL) to solve tasks with only a few demonstrations, but its mechanisms are not yet well-understood. Some works suggest that LLMs only recall already learned concepts from pre-training, while others hint that ICL performs implicit learning over demonstrations. We characterize two ways through which ICL leverages demonstrations. Task recognition (TR) captures the extent to which LLMs can recognize a task through demonstrations \u2013 even without ground-truth labels \u2013 and apply their pre-trained priors, whereas task learning (TL) is the ability to capture new input-label mappings unseen in pre-training. Using a wide range of classification datasets and three LLM families (GPT-3, LLaMA and OPT), we design controlled experiments to disentangle the roles of TR and TL in ICL. We show that (1) models can achieve non-trivial performance with only TR, and TR does not further improve with larger models or more demonstrations; (2) LLMs acquire TL as the model scales, and TL\u2019s performance consistently improves with more demonstrations in context. Our findings unravel two different forces behind ICL and we advocate for discriminating them in future ICL research due to their distinct nature.1\narXiv:2305.09731v1\n# 1 Introduction\nLarge language models (LLMs) have demonstrated the ability to perform in-context learning (ICL), i.e., \u201clearning\u201d to perform a task purely from examples in the context without any parameter updates (Brown et al., 2020). This powerful and flexible phenomenon enables LLMs to be used as general-purpose models that can perform any task with a small set of labeled examples. However, there is still no consensus on how incontext learning works. Some previous work hy-\n1Our code is publicly available at https://github.com/ princeton-nlp/WhatICLLearns.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e27/1e27dace-3f73-409e-b314-946912d1d057.png\" style=\"width: 50%;\"></div>\nFigure 1: We perform experiments in three settings: RANDOM (top), ABSTRACT (middle), and GOLD (bottom). Our experiments demonstrate that task recognition (TR; shown by RANDOM) does not scale with model sizes and number of demonstrations, while task learning (TL; shown by ABSTRACT) does.\npothesizes that during pre-training, LLMs implicitly learn tasks required for downstream applications, and the in-context demonstrations merely provide information that allow the model to recognize which task is required (Xie et al., 2022). Min et al. (2022) show empirical evidence of this hypothesis by demonstrating that ICL performance is insensitive to the usage of ground-truth labels. On the other hand, Aky\u00fcrek et al. (2023); von Oswald et al. (2022) construct theories that Transformer-based models may perform implicit gradient descent to update an \u201cinner-model\u201d, and Dai et al. (2023) demonstrate similarities between in-context learning and explicit fine-tuning through a series of metrics on real-world datasets. Such hypotheses assume the correct input-output mappings are important and ICL actually performs implicit learning over demonstrations. In this paper, we disentangle ICL into task\nrecognition (TR), which recognizes the task from demonstrations and applies LLMs\u2019 pre-trained priors, and task learning (TL), which learns a new input-label mapping from demonstrations. In common ICL scenarios where ground-truth labels are provided, TR and TL take effect simultaneously. We propose two settings to tease them apart: 1) RANDOM, where the labels are uniformly sampled from the label space (Min et al., 2022), in order to restrict LLMs to only apply TR; 2) ABSTRACT, where the labels are replaced with abstract symbols (e.g., numbers or letters) that never co-occurred with the inputs in pre-training. We focus on how the two abilities in ICL evolve with two factors \u2013 model sizes and numbers of demonstrations, which have been neglected in related literature. Through extensive experiments with a series of classification datasets on GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), and OPT (Zhang et al., 2022), we find:\nrecognition (TR), which recognizes the task from demonstrations and applies LLMs\u2019 pre-trained priors, and task learning (TL), which learns a new input-label mapping from demonstrations. In common ICL scenarios where ground-truth labels are provided, TR and TL take effect simultaneously. We propose two settings to tease them apart: 1) RANDOM, where the labels are uniformly sampled from the label space (Min et al., 2022), in order to restrict LLMs to only apply TR; 2) ABSTRACT, where the labels are replaced with abstract symbols (e.g., numbers or letters) that never co-occurred with the inputs in pre-training. We focus on how the two abilities in ICL evolve with two factors \u2013 model sizes and numbers of demonstrations, which have been neglected in related literature. Through extensive experiments with a series of classification datasets on GPT-3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), and OPT (Zhang et al., 2022), we find: \u2022 The gap between GOLD and RANDOM is small with smaller models, corroborating with Min et al. (2022). However, with larger models and more examples, the gap becomes larger. This suggests TR plays a significant role in ICL, but it does not scale with increasing parameters or examples. \u2022 LLMs also perform TL, which emerges with larger models and more demonstrations. With the largest model and more than 16 examples, ABSTRACT outperforms RANDOM, pointing to a paradigm shift in in-context learning at scale. Together, our findings provide a better way to understand ICL behaviors.2\n\u2022 The gap between GOLD and RANDOM is small with smaller models, corroborating with Min et al. (2022). However, with larger models and more examples, the gap becomes larger. This suggests TR plays a significant role in ICL, but it does not scale with increasing parameters or examples. \u2022 LLMs also perform TL, which emerges with larger models and more demonstrations. With the largest model and more than 16 examples, ABSTRACT outperforms RANDOM, pointing to a paradigm shift in in-context learning at scale. Together, our findings provide a better way to understand ICL behaviors.2\n# 2 Task Recognition and Task Learning\nAn LLM (parameterized by \u03b8) performs ICL by conditioning on the input-label pair demonstrations Ddemo = (x1, y1, x2, y2, . . . , xK, yK) and the test input xtest to predict the label ytest \u223cp\u03b8(y | Ddemo , xtest), where the demonstrations elicit a mapping f : X \u2192Y, x \u2208X, y \u2208Y. We delineate two ways an LLM can leverage in-context demonstrations: task recognition and task learning. Task recognition (TR) represents models\u2019 ability to recognize the mapping f purely by observing the input distribution {xi}K i=1 and the label distribution {yi}K i=1, without the provided (xi, yi) pairs.\n2We discuss the differences between our work and Min et al. (2022); Yoo et al. (2022) in Section 5, detailing how our findings deviate and converge with existing results.\n# The LLM then applies its pre-trained priors to the recognized f. Formally, when only TR is enabled,\np\u03b8(y | xtest, {xi, yi}K i=1) =p\u03b8(y | xtest, {xi}K i=1, {yi}K i=1),\n | {} i=1 =p\u03b8(y | xtest, {xi}K i=1, {yi}K i=1), which suggests TR does not rely on the pair information. For example, an input distribution of movie reviews and a label distribution of \u201cThe sentiment is positive/negative\u201d can be easily recognized as a sentiment classification task due to their prevalence during pre-training, and LLMs can make reasonable predictions without explicitly \u201clearning\u201d the task via ground-truth demonstrations. This leads to observations that the model can still perform well even when we provide wrong input-label mappings, e.g., \u201cThe movie is great. The sentiment is negative\u201d (Min et al., 2022). Task learning (TL), on the other hand, characterizes how the model learns a new mapping from the input-label pairs through demonstrations. Unlike TR, TL allows models to learn novel mappings and thus correct input-label pairs will be crucial. We posit that the two mechanisms occur under separate conditions, as recognizing an already learned task is easier than learning a new mapping. Models are able to perform TR at a small scale, but this ability does not drastically improve with increasing model sizes and demonstrations; on the other hand, TL improves significantly when model sizes and numbers of demonstrations increase. To show the above phenomenon, we disentangle TR and TL through label space manipulation, including three different setups (examples in Figure 1): \u2022 GOLD: the standard ICL setting where we use natural prompts and gold input-label pairs. This setup reflects both TR and TL abilities. \u2022 RANDOM: similar to Min et al. (2022), we use the same natural prompts as GOLD and sample demonstration labels uniformly at random from the label space. This setup reflects TR only. \u2022 ABSTRACT: we use minimal prompts (which provide no task information) and characters with no clear semantic meanings (e.g. numbers, letters, and random symbols) as the label for each class. We found that even abstract labels may have biases in pre-training, e.g., \u201c0\u201d is biased towards negative. Hence, for each prompt x1, y1, . . . , xK, yK, we randomly sample a 1-1 mapping \u03c6 : Y \u2192Y\u2217 to avoid any bias, and no task-specific information is leaked in either the prompt template or the label\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cbc2/cbc2a035-b659-4dd8-9416-53126aa58b8a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Performance vs. Number of Demonstrations Random (TR) Abstract (TL) Gold (TR + TL) Random Guess</div>\nFigure 2: Averaged accuracy across 16 datasets for GPT-3 (left), LLaMA (middle), and OPT (right). Top graphs plot model sizes from small to large against performance, using 32 examples. Variance is calculated across three prompts. Bottom graphs plot #demonstrations against performance for davinci, LLaMA-65B, and OPT-66B.\nspace. To evaluate the model\u2019s ABSTRACT performance, we measure its accuracy using \u03c6(ytest) as target labels. Since these input-label mappings are never seen in pre-training, it reflects the TL ability. In the following sections, we conduct comprehensive experiments with the above three different settings under two axes \u2013 model sizes and numbers of demonstrations \u2013 and show how TR and TL manifest under different conditions.\n# 3 Experimental Setup 3.1 Datasets\n# 3 Experimental Setup\n# 3.1 Datasets\nWe experiment on 16 classification datasets across 4 type of tasks: sentiment analysis, toxicity detection, natural language inference/paraphrase detection, and topic/stance classification. All datasets and references are in Appendix A. Our dataset selection largely follows Min et al. (2022), but we exclude multi-choice datasets since it is difficult to apply our ABSTRACT experiments on them.\n# 3.2 Models\nWe use three state-of-the-art LLM families: GPT3 (Brown et al., 2020), LLaMA (Touvron et al., 2023), and OPT (Zhang et al., 2022). We use\nGPT-3 ada (350M), babbage (1.3B), curie (6.7B), and davinci (175B) via the OpenAI API. For OPT, we use checkpoints from the Transformers library (Wolf et al., 2020), with model sizes of 350M, 2.7B, 6.7B, 13B, 30B, and 66B parameters. For LLaMA, we use model sizes of 7B, 13B, 33B, and 65B parameters.3\n# 3.3 Task Setup\nWe adopt the sample-based evaluation protocol: for each test example, we sample a different set of demonstrations from the training set. We manually design 3 prompt templates for each type of classification tasks in a similar style to the prompts from Min et al. (2022). We report the mean by averaging across datasets and prompts, and standard variation across different prompts for each datapoint. For GPT-3, we sample 150 examples for each dataset. We use fewer examples due to budget constraints, and GPT-3 presents lower variance than other model families. For OPT and LLaMA, we sample 1,350 examples for all datasets.\n3For GPT-3, we use the non-instruction legacy models for fair comparison to OPT and LLaMA models. We did not run experiments on the largest OPT-175B model due to computational constraints.\nWe design two kinds of prompts: natural language prompts (Table 1), which are similar to the manual prompts in Min et al. (2022), and minimal prompts (Table 3), which remove any natural language instructions for the task. For ABSTRACT, we tested three types of label choices: numbers (0, . . . , N \u22121, where N is the number of classes), letters (N letters from A, B, C, . . . ), and symbols (first N symbols of \u201c@\u201d, \u201c#\u201d, \u201c$\u201d, \u201d%\u201d, \u201c*\u201d, and \u201c\u2227\u201d). For each test example, we randomly sample a new mapping between labels and abstract characters. We report the number abstract labels in all the main results and compare the three forms in \u00a74.2.\n# 4 Results\nFigure 2 shows our main results with GPT-3, LLaMA, and OPT with our 3 settings: GOLD, RANDOM, and ABSTRACT. Below we summarize the trends of TR and TL across different conditions.\n# 4.1 Main Results\nSummary of overall trends. We first verify that GOLD consistently performs the best across model families and number of demonstrations, which is expected given that the GOLD setting provides the model with all information. Overall, the RANDOM curves do not increase with either model sizes or number of demonstrations, remaining largely flat; considering the scenario with small model sizes and few examples (K = 8), there is an insignificant gap between RANDOM and GOLD. Meanwhile, the ABSTRACT curves demonstrate an increasingly steep slope as the model sizes and the number of demonstrations grow; with small models or small K, ABSTRACT mostly underperforms RANDOM, whereas ABSTRACT with largest models and K = 32 performs well above RANDOM (and may even be competitive with GOLD). We note that the OPT curves demonstrate significant variance, which we hypothesize to be a result of the models potentially being under-trained. We elaborate the takeaways on TR and TL below.\nscales. For all model families, the RANDOM setting shows similar performance at all sizes and numbers of demonstrations. Moreover, TR performance is significantly stronger than the random baseline, even with small models and few examples. For instance, even the smallest 350M parameter models are able to recognize the task using\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b9f/1b9f7957-c7e1-4b93-ae94-b74f5de63163.png\" style=\"width: 50%;\"></div>\nFigure 3: Performance of three types of ABSTRACT labels: numbers, letters, and symbols on davinci and OPT-66B.\n<div style=\"text-align: center;\">Figure 3: Performance of three types of ABSTRACT labels: numbers, letters, and symbols on davinci and OPT-66B.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/262d/262da082-8696-44c3-b9c8-0ea324cb4531.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Average results of sentiment analysis datasets (left) vs. natural language inference datasets (right) on GPT-3 models, with K = 32.</div>\njust 8 examples, drawing around 10 points of average performance lead against the random baseline for GPT-3 ada and 5 points for OPT-350M. This shows that task recognition from in-context examples does not drastically scale with model sizes or numbers of examples.\nTask learning is enabled with scale. We observe that TL is dependent on model sizes: smaller models perform roughly the same across all numbers of demonstrations (see Figure 6). On the other hand, larger models can utilize the provided mapping information and perform TL, as ABSTRACT (TL) performance increases drastically with larger sizes (first row of Figure 2). When using a larger model, the results also improve as the number of demonstration increases (second row of Figure 2). With only 16 examples, OPT-66B and davinci are able to match the performance of GOLD while using a new label mapping. While LLaMA-65B\u2019s ABSTRACT is not as competitive as its GOLD, the trend of improving ABSTRACT performance with larger size s or larger K is clear. This suggests that TL is only enabled by scales and further improves with more demonstrations.\n# 4.2 Further Analysis\nThe trends for task learning generalize across different types of abstract labels. In Figure 3, we show ABSTRACT results with number, letter, and symbol labels respectively. We observe that all three versions show a similar trend and coincide with our main results. Numbers and letters perform consistently better than symbols. This may be because letters and numbers appear more frequently in the pre-training corpus, and therefore make for a more \"natural\" label space. Task difficulty affects the trends. We notice that ABSTRACT scales better with sizes and examples when the task is simpler. In Figure 4 we compare two types of tasks: sentiment analysis and natural language inference (NLI). Since NLI is more difficult, we observe that it produces a flatter ABSTRACT curve, suggesting that the model relies more on the natural prompts and pre-training priors to solve those tasks. We demonstrate the full task-type breakdown results in \u00a7C.\nThe trends for task learning generalize across different types of abstract labels. In Figure 3, we show ABSTRACT results with number, letter, and symbol labels respectively. We observe that all three versions show a similar trend and coincide with our main results. Numbers and letters perform consistently better than symbols. This may be because letters and numbers appear more frequently in the pre-training corpus, and therefore make for a more \"natural\" label space.\n# 5 Related Work\nMany works have attempted to deepen empirical or theoretical understanding of ICL since its emergence in Brown et al. (2020). For instance, Xie et al. (2022) present a theoretical framework where latent \u201cconcepts\" parameterize each document in pre-training. They posit that all concepts have been learned in pre-training; thus, ICL is the result of implicit Bayesian inference, where the LM uses incontext demonstrations as evidence to identify the correct concept. Min et al. (2022) present empirical evidence for this framework by showing that only limited information, rather than true input-label mappings, is needed to perform ICL. Other works investigate the impact of the pretraining corpus on ICL. Chan et al. (2022) identify properties of the pre-training distribution that enable ICL behavior, including burstiness, label multiplicity, and a long-tailed class distribution \u2013 all of which are satisfied by natural language. Razeghi et al. (2022) show that the frequencies of terms in the pre-training corpora is positively correlated with model performance. More recently, several works have explored theoretical frameworks in which ICL can be seen as implicit gradient descent, treating a forward pass over the in-context demonstrations as an \u201cupdate\u201d to an implicit internal model. (Aky\u00fcrek et al., 2023; von\nOswald et al., 2022; Dai et al., 2023). For mechanistic perspectives on ICL, Olsson et al. (2022) and Bansal et al. (2022) identify induction heads (subnetworks that perform in-context pattern recognition) in small and large models, respectively. While our conclusions align with aspects of previous studies, our work contributes novel insights on multiple axes. Min et al. (2022) also show that even small models can perform TR and argue that the performance gap between GOLD and RANDOM is consistently small, but most of their experiments are on \u226413B models with 16 demonstrations; we show that as model sizes scale, GOLD tends to improve while RANDOM does not. Thus, the performance deficit of RANDOM grows as models become larger. Yoo et al. (2022) also perform similar experiments to RANDOM and ABSTRACT, but they do not deeply investigate the effect of model sizes or numbers of demonstrations. Contemporary work Wei et al. (2023) obtain similar results; additionally, they show that instruction-tuning strengthens the model\u2019s semantic priors more than it improves TL. However, they primarily focus on closed-source models, whereas we also conduct experiments on public models such as LLaMA and OPT. Collectively, our findings offer a comprehensive understanding of how ICL works across scales.\n# 6 Conclusion\nWhile previous work often studies ICL as an umbrella term, regardless of model sizes and numbers of examples, we argue that there are two distinct characterizations of ICL \u2013 task recognition and task learning \u2013 and demonstrate that they emerge under different conditions. Even small models are capable of performing TR, but this ability does not scale. On the other hand, TL is an emergent ability of large models; small models are unable to perform TL even when provided with more demonstrations, whereas large models can leverage more demonstations to improve their TL performance. We suggest that future work on ICL should distinguish the two phenomena and clearly state the conditions under which the experiments are conducted.\n# Limitations\nThough LLMs with in-context learning are capable of all kinds of NLP tasks, this work is limited to classification tasks because they are easier to be adapted to our RANDOM and ABSTRACT setup. We leave other types of NLP tasks as future work.\nAnother limitation of our work lies in the definition and discussion of task learning. Though we empirically show that large models are capable of acquiring a novel mapping to abstract labels like numbers or letters, how models \u201clearn\u201d mechanistically is still elusive. As suggested in previous work, LLMs may conduct implicit gradient descent over demonstrations, or they may alternatively map the patterns shown in the demonstrations back to concepts learned in pre-training. To some extent, these mechanisms could be considered an advanced form of \u201ctask recognition\u201d. This work only designs experiments to better observe and disentangle TR and TL, and we look forward to further studies that reveal more insights about the mechanistic innerworkings of these phenomena in ICL.\n# Acknowledgements\nWe thank the members of the Princeton NLP group for their valuable advice, thoughts, and discussions. We also appreciate the helpful feedback given by the anonymous reviewers and the area chairs. This project was partially supported by the National Science Foundation under Award IIS-2211779, and a Sloan Fellowship.\n# References\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas,\nTengyu Ma, and Denny Zhou. 2023.\nWhat learn-\ning algorithm is in-context learning? investigations\nwith linear models. In International Conference on\nLearning Representations (ICLR).\nHritik Bansal, Karthik Gopalakrishnan, Saket Dingli-\nwal,\nSravan Bodapati,\nKatrin Kirchhoff,\nand\nDan Roth. 2022.\nRethinking the role of scale\nfor in-context learning: An interpretability-based\ncase study at 66 billion scale.\narXiv preprint\narXiv:2212.09095.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019.\nSemEval-2019 task 5:\nMultilin-\ngual detection of hate speech against immigrants\nand women in Twitter. In Proceedings of the 13th\nInternational Workshop on Semantic Evaluation,\npages 54\u201363. Association for Computational Lin-\nguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015.\nA large an-\nnotated corpus for learning natural language infer-\nence. In Empirical Methods in Natural Language\nProcessing (EMNLP), pages 632\u2013642.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah,\nJared\nD\nKaplan,\nPrafulla\nDhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems\n(NeurIPS), volume 33, pages 1877\u20131901.\nStephanie Chan, Adam Santoro, Andrew Lampinen,\nJane Wang, Aaditya Singh, Pierre Richemond,\nJames McClelland, and Felix Hill. 2022.\nData\ndistributional properties drive emergent in-context\nlearning in transformers.\nIn Advances in Neural\nInformation Processing Systems (NeurIPS), vol-\nume 35, pages 18878\u201318891.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shum-\ning Ma, Zhifang Sui, and Furu Wei. 2023.\nWhy\ncan GPT learn in-context?\nlanguage models im-\nplicitly perform gradient descent as meta-optimizers.\nIn ICLR 2023 Workshop on Mathematical and\nEmpirical Understanding of Foundation Models.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nHector Levesque,\nErnest Davis,\nand Leora Mor-\ngenstern. 2012.\nThe winograd schema chal-\nlenge.\nIn Thirteenth International Conference on\nthe Principles of Knowledge Representation and\nReasoning.\nP. Malo, A. Sinha, P. Korhonen, J. Wallenius, and\nP. Takala. 2014.\nGood debt or bad debt: Detect-\ning semantic orientations in economic texts. Journal\nof the Association for Information Science and\nTechnology.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zam-\nparelli. 2014.\nA SICK cure for the evaluation of\ncompositional distributional semantic models.\nIn\nInternational Conference on Language Resources\nand Evaluation (LREC), pages 216\u2013223.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work? In Empirical\nMethods in Natural Language Processing (EMNLP),\npages 11048\u201311064.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. SemEval-\n2018 task 1:\nAffect in tweets.\nIn Proceedings\nof the 12th International Workshop on Semantic\nEvaluation, pages 1\u201317. Association for Computa-\ntional Linguistics.\nIoannis Mollas, Zoe Chrysopoulou, Stamatis Karlos,\nand Grigorios Tsoumakas. 2020.\nEthos: an on-\nline hate speech detection dataset.\narXiv preprint\narXiv:2006.08328.\nCatherine\nOlsson,\nNelson\nElhage,\nNeel\nNanda,\nNicholas Joseph, Nova DasSarma, Tom Henighan,\nBen Mann, Amanda Askell, Yuntao Bai, Anna Chen,\net al. 2022. In-context learning and induction heads.\narXiv preprint arXiv:2209.11895.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning.\nIn\nFindings of Empirical Methods in Natural Language\nProcessing (EMNLP), pages 840\u2013854. Association\nfor Computational Linguistics.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Empirical Methods in Natural Language\nProcessing (EMNLP), pages 3687\u20133697.\nEmily Sheng and David Uthus. 2020.\nInvestigating\nsocietal biases in a poetry composition system. In\nProceedings of the Second Workshop on Gender\nBias in Natural Language Processing, pages 93\u2013106.\nAssociation for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1631\u20131642.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin,\nEdouard Grave,\nand Guillaume Lample. 2023.\nLlama: Open and efficient foundation language mod-\nels. arXiv preprint arXiv:2302.13971.\nJohannes von Oswald, Eyvind Niklasson, Ettore Ran-\ndazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev,\nAndrey Zhmoginov, and Max Vladymyrov. 2022.\nTransformers learn in-context by gradient descent.\narXiv preprint arXiv:2212.07677.\nEllen M Voorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Association\nfor Computing Machinery Special Interest Group\nin Information Retrieval (ACM SIGIR), pages 200\u2013\n207.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, et al. 2023.\nLarger\nlanguage models do in-context learning differently.\narXiv preprint arXiv:2303.03846.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen,\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f0a/3f0aec63-e954-40a1-ada2-4bfd4927c53f.png\" style=\"width: 50%;\"></div>\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning.\nIn Empirical Methods in Natural Language\nProcessing (EMNLP), pages 38\u201345.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022.\nAn explanation of\nin-context\nlearning\nas\nimplicit\nbayesian\ninfer-\nence.\nIn International Conference on Learning\nRepresentations (ICLR).\nKang Min Yoo, Junyeob Kim, Hyuhng Joon Kim,\nHyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-\ngoo Lee, and Taeuk Kim. 2022. Ground-truth labels\nmatter: A deeper look into input-label demonstra-\ntions. In Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2422\u20132437.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. OPT: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nWe use a total of 16 datasets. Sentiment analysis includes SST-2 (Socher et al., 2013), financial_phrasebank (Malo et al., 2014), emotion (Saravia et al., 2018), and poem_sentiment (Sheng and Uthus, 2020) Topic/stance classification includes TREC (Voorhees and Tice, 2000), tweet_eval_atheist, and tweet_eval_feminist (Mohammad et al., 2018; Basile et al., 2019). Toxicity detection includes tweet_eval_hate, ethos_race, ethos_gender, ethos_national_origin, and ethos_religion (Mollas et al., 2020) Natural language inference/paraphrase detection includes SICK (Marelli et al., 2014), SNLI (Bowman et al., 2015), WNLI (Levesque et al., 2012), and MRPC (Dolan and Brockett, 2005). We sample from the training set to construct the prompts; following Min et al. (2022), we use the development set for evaluation, using sampled max(1350, dataset_size) examples.\n# B Prompt Templates\nFor each task category (e.g. sentiment classification, topic detection), we manually design three natural language templates. Depending on exact specifications for the dataset, templates may be adjusted to better reflect the task (e.g. \"Is this atheist?\" for tweet_eval_atheist). We apply these templates to the natural language label sets (GOLD and RANDOM). All prompts are presented in Table 1. We also design two task-agnostic variations on three minimal templates for ABSTRACT: one for single-sentence tasks and one for multi-sequence tasks (e.g. NLI tasks). We use these minimal templates on the abstract language label sets in order to prevent the model from being exposed to any information regarding the task from the prompt design. All minimal templates are presented in Table 3 All prompts are designed to be answered with single-token responses (e.g. \"Yes/No\", \"True/False\", \"positive/negative/neutral\", \"0/1/2\", \"A/B/C\") so that we can directly check models\u2019 last token prediction results instead of applying decoding methods.\n# C More Results\nWe demonstrate average model performance with respect to number of parameters in Figure 5. It is clear that small models struggle to perform ABSTRACT, regardless of how many examples,\nwhereas the largest models (especially GPT-3 Davinci and OPT-66B) are able to perform ABSTRACT. Additionally, their performance improves even more when more demonstrations are provided. We demonstrate average model performance with respect to numbers of demonstrations in Figure 6. We can see a clear trend that RANDOM (TR) does not change much but ABSTRACT improves drastically with more examples, especially for GPT3 Davinci and OPT-66B. Figure 7 shows all the ABSTRACT results and demonstrates a similar trend to what \u00a74.2 describes. Figure 8, Figure 9, Figure 10, and Figure 11 show task-type breakdown results. Though individual task-type results are more noisy, we can make a similar observation compared to the main result \u2013 ABSTRACT (TL) scales better with sizes and numbers of examples compared to RANDOM (TR).\nType\nTemplate #\nExample\n1\n<s>\nThe sentiment is <positive/negative>\nSentiment\nAnalysis\n2\n<s>\nSentiment: <positive/negative>\n3\n<s>\nThe sentiment of the text is <positive/negative>\n1\n<s>\nIs this hate speech? <Yes/No>\nHate Speech\n2\n<s>\nIs the sentence hateful? <Yes/No>\n3\n<s>\nThe sentence contains hate speech. True or False?\nThe answer is <True/False>\n1\n<s>\nThe stance is feminist. True or False?\nThe answer is <True/False>\nStance\nDetection\n2\n<s>\nDoes the sentence express a feminist view? <Yes/No>\n3\n<s>\nIs the stance feminist? <Yes/No>\n1\n<s>\nThe topic is <label>\nTopic\nDetection\n2\n<s>\nThe sentence is about <label>\n3\n<s>\nSentence topic: <label>\nType\nTemp. #\nExample\nEntailment\n1\n<s1>\nThe question is: <s2>?\nTrue or False?\nThe answer is <True/False>\n2\nHypothesis: <s1>\nPremise: <s2>?\nDo the sentences show entailment?\n<Yes/No>\n3\nThe hypothesis is: <s1>\nThe premise is: <s2>?\nIs this entailment?\n<Yes/No>\nNLI\n1\n<s1>\nThe question is: <s2>\nTrue, False, or Unknown?\nThe answer is <True/False/Unknown>\n2\nHypothesis: <s1>\nPremise: <s2>?\nGiven the premise, is the hypothesis true? Yes, No, or Unknown?\nThe answer is: <Yes/No/Unknown>\n3\nThe hypothesis is: <s1>\nThe premise is: <s2>?\nAccording to the premise, the hypothesis is true. True, False, or Unknown?\nThe answer is: <True/False/Unknown>\nParaphrase\nDetection\n1\n<s1>\nThe question is: <s2>\nTrue or False?\nThe answer is: <True/False/>\n2\nSentence 1: <s1>\nSentence 2: <s2>\nThese sentences are paraphrases. True or False?\nThe answer is: <True/False/>\n3\nText: <s1>\nConsider this sentence: <s2>\nDoes it paraphrase the text?\n<Yes/No>\nTable 2: Natural prompts used as input in GOLD and RANDOM settings for multi-sentence datasets. <s1> and <s2> denote the input sequences; labels are illustrated in red.\nTable 2: Natural prompts used as input in GOLD and RANDOM settings for multi-se denote the input sequences; labels are illustrated in red.\nType\nTemplate #\nExample\n1\n<sentence>\n<label>\nMinimal\n(single\nsentence)\n2\n<sentence>\nLabel: <label>\n3\nSentence: sentence>\nLabel: <label>\n1\n<sentence1> [SEP] <sentence2>\n<label>\nMinimal\n(multiple\nsentences)\n2\n<sentence1> [SEP] <sentence2>\nLabel: <label>\n3\nSentence 1: <sentence1>\nSentence 2: <sentence2>\nLabel: <label>\n<div style=\"text-align: center;\">Table 3: Minimal prompts used for ABSTRACT.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5941/5941067b-d517-483d-a6c7-c1c5e579dc6c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Averaged accuracy across 16 datasets for GPT-3 (top), LLaMA (middle), and OPT (bottom). X-axis shows model sizes from small to large. We run experiments with 8 (left), 16 (middle), and 32 (right) demonstrations respectively. Variance is calculated across three prompts.</div>\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.52\n0.51\n0.54\n0.45\n0.23\n0.4\n0.4\n0.38\n0.41\n0.44\n0.34\n0.43\nbabbage\n0.51\n0.52\n0.54\n0.38\n0.37\n0.43\n0.46\n0.29\n0.49\n0.53\n0.34\n0.57\ncurie\n0.55\n0.54\n0.6\n0.28\n0.33\n0.32\n0.39\n0.32\n0.4\n0.56\n0.36\n0.56\ndavinci\n0.56\n0.55\n0.59\n0.34\n0.33\n0.33\n0.4\n0.4\n0.38\n0.4\n0.39\n0.44\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.23\n0.4\n0.4\n0.38\n0.41\n0.44\n0.34\n0.43\n0.56\n0.39\n0.64\n0.62\nbabbage\n0.37\n0.43\n0.46\n0.29\n0.49\n0.53\n0.34\n0.57\n0.45\n0.39\n0.55\n0.54\ncurie\n0.33\n0.32\n0.39\n0.32\n0.4\n0.56\n0.36\n0.56\n0.54\n0.42\n0.63\n0.53\ndavinci\n0.33\n0.33\n0.4\n0.4\n0.38\n0.4\n0.39\n0.44\n0.4\n0.56\n0.44\n0.52\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.41\n0.44\n0.34\n0.43\n0.56\n0.39\n0.64\n0.62\n0.52\n0.71\n0.62\n0.57\nbabbage\n0.49\n0.53\n0.34\n0.57\n0.45\n0.39\n0.55\n0.54\n0.51\n0.52\n0.58\n0.56\ncurie\n0.4\n0.56\n0.36\n0.56\n0.54\n0.42\n0.63\n0.53\n0.52\n0.6\n0.48\n0.54\ndavinci\n0.38\n0.4\n0.39\n0.44\n0.4\n0.56\n0.44\n0.52\n0.51\n0.54\n0.47\n0.52\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.56\n0.39\n0.64\n0.62\n0.52\n0.71\n0.62\n0.57\n0.76\n0.68\n0.54\n0.74\nbabbage\n0.45\n0.39\n0.55\n0.54\n0.51\n0.52\n0.58\n0.56\n0.62\n0.63\n0.51\n0.61\ncurie\n0.54\n0.42\n0.63\n0.53\n0.52\n0.6\n0.48\n0.54\n0.55\n0.56\n0.6\n0.54\ndavinci\n0.4\n0.56\n0.44\n0.52\n0.51\n0.54\n0.47\n0.52\n0.5\n0.48\n0.53\n0.62\nTable 4: Single dataset accuracies across the GPT-3 model family, using 8 examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb58/eb5801ea-6dfc-4f31-b483-c7ee96e5e9dc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Averaged accuracy across 16 datasets for GPT-3 (top), LLaMA (middle), and OPT (bottom). x-axis shows number of demonstrations in the prompt. For each model, we run experiments for RANDOM (left), ABSTRACT(middle), and GOLD (right) demonstrations. Variance is calculated across three templates.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a155/a1550340-50b0-4c89-9ecb-67fd55009541.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance of three types of ABSTRACT labels: numbers, letters, and symbols on OPT and GPT-3</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/59ab/59abf4a1-f969-4c47-a444-295d4c7b6a0e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Average performance of sentiment analysis datasets.</div>\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.51\n0.51\n0.52\n0.44\n0.37\n0.48\n0.4\n0.42\n0.41\n0.37\n0.44\n0.44\nbabbage\n0.48\n0.54\n0.55\n0.36\n0.41\n0.31\n0.44\n0.33\n0.48\n0.54\n0.38\n0.54\ncurie\n0.54\n0.58\n0.62\n0.28\n0.48\n0.3\n0.33\n0.38\n0.32\n0.56\n0.41\n0.56\ndavinci\n0.56\n0.6\n0.64\n0.34\n0.42\n0.39\n0.29\n0.44\n0.38\n0.46\n0.49\n0.49\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.37\n0.48\n0.4\n0.42\n0.41\n0.37\n0.44\n0.44\n0.54\n0.53\n0.67\n0.68\nbabbage\n0.41\n0.31\n0.44\n0.33\n0.48\n0.54\n0.38\n0.54\n0.43\n0.53\n0.63\n0.56\ncurie\n0.48\n0.3\n0.33\n0.38\n0.32\n0.56\n0.41\n0.56\n0.5\n0.55\n0.71\n0.54\ndavinci\n0.42\n0.39\n0.29\n0.44\n0.38\n0.46\n0.49\n0.49\n0.38\n0.63\n0.49\n0.51\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.41\n0.37\n0.44\n0.44\n0.54\n0.53\n0.67\n0.68\n0.52\n0.76\n0.68\n0.52\nbabbage\n0.48\n0.54\n0.38\n0.54\n0.43\n0.53\n0.63\n0.56\n0.53\n0.61\n0.58\n0.54\ncurie\n0.32\n0.56\n0.41\n0.56\n0.5\n0.55\n0.71\n0.54\n0.56\n0.55\n0.49\n0.55\ndavinci\n0.38\n0.46\n0.49\n0.49\n0.38\n0.63\n0.49\n0.51\n0.6\n0.56\n0.5\n0.57\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.54\n0.53\n0.67\n0.68\n0.52\n0.76\n0.68\n0.52\n0.75\n0.69\n0.57\n0.77\nbabbage\n0.43\n0.53\n0.63\n0.56\n0.53\n0.61\n0.58\n0.54\n0.61\n0.55\n0.54\n0.62\ncurie\n0.5\n0.55\n0.71\n0.54\n0.56\n0.55\n0.49\n0.55\n0.59\n0.49\n0.55\n0.59\ndavinci\n0.38\n0.63\n0.49\n0.51\n0.6\n0.56\n0.5\n0.57\n0.59\n0.54\n0.67\n0.63\nable 5: Single dataset accuracies across the GPT-3 model family, using \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/79ca/79cac69b-c1c1-40ca-afc3-267e82cf16f4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Average performance of natural language inference/paraphrase detection datasets.</div>\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.48\n0.52\n0.53\n0.4\n0.37\n0.42\n0.41\n0.38\n0.42\n0.24\n0.45\n0.27\nbabbage\n0.53\n0.58\n0.52\n0.32\n0.38\n0.35\n0.42\n0.35\n0.38\n0.44\n0.4\n0.5\ncurie\n0.54\n0.59\n0.66\n0.26\n0.47\n0.31\n0.38\n0.4\n0.43\n0.57\n0.41\n0.57\ndavinci\n0.57\n0.64\n0.66\n0.29\n0.51\n0.37\n0.28\n0.49\n0.37\n0.43\n0.52\n0.49\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.37\n0.42\n0.41\n0.38\n0.42\n0.24\n0.45\n0.27\n0.55\n0.56\n0.69\n0.66\nbabbage\n0.38\n0.35\n0.42\n0.35\n0.38\n0.44\n0.4\n0.5\n0.51\n0.58\n0.65\n0.51\ncurie\n0.47\n0.31\n0.38\n0.4\n0.43\n0.57\n0.41\n0.57\n0.52\n0.56\n0.71\n0.51\ndavinci\n0.51\n0.37\n0.28\n0.49\n0.37\n0.43\n0.52\n0.49\n0.35\n0.68\n0.5\n0.63\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.42\n0.24\n0.45\n0.27\n0.55\n0.56\n0.69\n0.66\n0.55\n0.73\n0.69\n0.61\nbabbage\n0.38\n0.44\n0.4\n0.5\n0.51\n0.58\n0.65\n0.51\n0.57\n0.63\n0.6\n0.59\ncurie\n0.43\n0.57\n0.41\n0.57\n0.52\n0.56\n0.71\n0.51\n0.59\n0.65\n0.5\n0.61\ndavinci\n0.37\n0.43\n0.52\n0.49\n0.35\n0.68\n0.5\n0.63\n0.6\n0.63\n0.51\n0.62\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nada\n0.55\n0.56\n0.69\n0.66\n0.55\n0.73\n0.69\n0.61\n0.73\n0.65\n0.63\n0.77\nbabbage\n0.51\n0.58\n0.65\n0.51\n0.57\n0.63\n0.6\n0.59\n0.64\n0.57\n0.56\n0.65\ncurie\n0.52\n0.56\n0.71\n0.51\n0.59\n0.65\n0.5\n0.61\n0.63\n0.44\n0.61\n0.69\ndavinci\n0.35\n0.68\n0.5\n0.63\n0.6\n0.63\n0.51\n0.62\n0.65\n0.6\n0.7\n0.71\nable 6: Single dataset accuracies across the GPT-3 model family, using \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78d3/78d34b55-6cb4-487d-8d00-93c8353f4dce.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Average performance of toxicity detection datasets.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/65ee/65ee2eb5-a237-4ad1-a9d6-51d38646f00f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">gure 11: Average performance of topic/stance classification datasets.</div>\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.49\n0.51\n0.53\n0.43\n0.34\n0.48\n0.41\n0.31\n0.45\n0.33\n0.34\n0.29\nOPT-2.7B\n0.52\n0.55\n0.56\n0.43\n0.36\n0.45\n0.47\n0.34\n0.5\n0.52\n0.34\n0.55\nOPT-6.7B\n0.53\n0.53\n0.57\n0.26\n0.33\n0.27\n0.33\n0.39\n0.36\n0.46\n0.36\n0.48\nOPT-13B\n0.55\n0.52\n0.61\n0.4\n0.35\n0.4\n0.49\n0.35\n0.47\n0.36\n0.3\n0.37\nOPT-30B\n0.52\n0.54\n0.55\n0.28\n0.24\n0.35\n0.4\n0.34\n0.46\n0.53\n0.31\n0.55\nOPT-66B\n0.52\n0.55\n0.53\n0.29\n0.38\n0.32\n0.44\n0.37\n0.42\n0.44\n0.36\n0.47\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.34\n0.48\n0.41\n0.31\n0.45\n0.33\n0.34\n0.29\n0.48\n0.36\n0.48\n0.6\nOPT-2.7B\n0.36\n0.45\n0.47\n0.34\n0.5\n0.52\n0.34\n0.55\n0.54\n0.42\n0.56\n0.49\nOPT-6.7B\n0.33\n0.27\n0.33\n0.39\n0.36\n0.46\n0.36\n0.48\n0.63\n0.44\n0.74\n0.55\nOPT-13B\n0.35\n0.4\n0.49\n0.35\n0.47\n0.36\n0.3\n0.37\n0.59\n0.44\n0.69\n0.62\nOPT-30B\n0.24\n0.35\n0.4\n0.34\n0.46\n0.53\n0.31\n0.55\n0.56\n0.43\n0.61\n0.44\nOPT-66B\n0.38\n0.32\n0.44\n0.37\n0.42\n0.44\n0.36\n0.47\n0.33\n0.44\n0.46\n0.45\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.45\n0.33\n0.34\n0.29\n0.48\n0.36\n0.48\n0.6\n0.49\n0.66\n0.65\n0.51\nOPT-2.7B\n0.5\n0.52\n0.34\n0.55\n0.54\n0.42\n0.56\n0.49\n0.53\n0.49\n0.51\n0.56\nOPT-6.7B\n0.36\n0.46\n0.36\n0.48\n0.63\n0.44\n0.74\n0.55\n0.54\n0.59\n0.53\n0.57\nOPT-13B\n0.47\n0.36\n0.3\n0.37\n0.59\n0.44\n0.69\n0.62\n0.53\n0.63\n0.55\n0.53\nOPT-30B\n0.46\n0.53\n0.31\n0.55\n0.56\n0.43\n0.61\n0.44\n0.49\n0.42\n0.46\n0.57\nOPT-66B\n0.42\n0.44\n0.36\n0.47\n0.33\n0.44\n0.46\n0.45\n0.55\n0.53\n0.44\n0.55\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.48\n0.36\n0.48\n0.6\n0.49\n0.66\n0.65\n0.51\n0.71\n0.66\n0.53\n0.73\nOPT-2.7B\n0.54\n0.42\n0.56\n0.49\n0.53\n0.49\n0.51\n0.56\n0.52\n0.48\n0.51\n0.5\nOPT-6.7B\n0.63\n0.44\n0.74\n0.55\n0.54\n0.59\n0.53\n0.57\n0.61\n0.53\n0.52\n0.62\nOPT-13B\n0.59\n0.44\n0.69\n0.62\n0.53\n0.63\n0.55\n0.53\n0.61\n0.55\n0.52\n0.62\nOPT-30B\n0.56\n0.43\n0.61\n0.44\n0.49\n0.42\n0.46\n0.57\n0.47\n0.46\n0.51\n0.46\nOPT-66B\n0.33\n0.44\n0.46\n0.45\n0.55\n0.53\n0.44\n0.55\n0.38\n0.49\n0.55\n0.56\nTable 7: Single dataset accuracies across the OPT model family, using 8 examples.\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.52\n0.53\n0.55\n0.47\n0.37\n0.49\n0.42\n0.42\n0.44\n0.33\n0.36\n0.35\nOPT-2.7B\n0.52\n0.56\n0.58\n0.44\n0.44\n0.47\n0.51\n0.39\n0.46\n0.55\n0.39\n0.57\nOPT-6.7B\n0.52\n0.57\n0.57\n0.22\n0.39\n0.28\n0.39\n0.43\n0.41\n0.48\n0.42\n0.54\nOPT-13B\n0.58\n0.54\n0.62\n0.32\n0.44\n0.38\n0.41\n0.39\n0.41\n0.36\n0.4\n0.36\nOPT-30B\n0.51\n0.57\n0.57\n0.34\n0.4\n0.35\n0.41\n0.32\n0.5\n0.55\n0.45\n0.56\nOPT-66B\n0.5\n0.57\n0.54\n0.25\n0.47\n0.31\n0.47\n0.44\n0.48\n0.49\n0.38\n0.51\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.37\n0.49\n0.42\n0.42\n0.44\n0.33\n0.36\n0.35\n0.45\n0.4\n0.47\n0.65\nOPT-2.7B\n0.44\n0.47\n0.51\n0.39\n0.46\n0.55\n0.39\n0.57\n0.53\n0.5\n0.58\n0.45\nOPT-6.7B\n0.39\n0.28\n0.39\n0.43\n0.41\n0.48\n0.42\n0.54\n0.66\n0.53\n0.8\n0.59\nOPT-13B\n0.44\n0.38\n0.41\n0.39\n0.41\n0.36\n0.4\n0.36\n0.6\n0.53\n0.72\n0.54\nOPT-30B\n0.4\n0.35\n0.41\n0.32\n0.5\n0.55\n0.45\n0.56\n0.56\n0.52\n0.64\n0.35\nOPT-66B\n0.47\n0.31\n0.47\n0.44\n0.48\n0.49\n0.38\n0.51\n0.3\n0.57\n0.49\n0.44\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.44\n0.33\n0.36\n0.35\n0.45\n0.4\n0.47\n0.65\n0.52\n0.71\n0.71\n0.52\nOPT-2.7B\n0.46\n0.55\n0.39\n0.57\n0.53\n0.5\n0.58\n0.45\n0.59\n0.47\n0.41\n0.62\nOPT-6.7B\n0.41\n0.48\n0.42\n0.54\n0.66\n0.53\n0.8\n0.59\n0.56\n0.71\n0.62\n0.61\nOPT-13B\n0.41\n0.36\n0.4\n0.36\n0.6\n0.53\n0.72\n0.54\n0.53\n0.62\n0.5\n0.55\nOPT-30B\n0.5\n0.55\n0.45\n0.56\n0.56\n0.52\n0.64\n0.35\n0.57\n0.43\n0.38\n0.63\nOPT-66B\n0.48\n0.49\n0.38\n0.51\n0.3\n0.57\n0.49\n0.44\n0.59\n0.51\n0.4\n0.6\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.45\n0.4\n0.47\n0.65\n0.52\n0.71\n0.71\n0.52\n0.76\n0.73\n0.51\n0.76\nOPT-2.7B\n0.53\n0.5\n0.58\n0.45\n0.59\n0.47\n0.41\n0.62\n0.52\n0.45\n0.54\n0.54\nOPT-6.7B\n0.66\n0.53\n0.8\n0.59\n0.56\n0.71\n0.62\n0.61\n0.69\n0.64\n0.61\n0.74\nOPT-13B\n0.6\n0.53\n0.72\n0.54\n0.53\n0.62\n0.5\n0.55\n0.58\n0.55\n0.53\n0.58\nOPT-30B\n0.56\n0.52\n0.64\n0.35\n0.57\n0.43\n0.38\n0.63\n0.5\n0.41\n0.59\n0.51\nOPT-66B\n0.3\n0.57\n0.49\n0.44\n0.59\n0.51\n0.4\n0.6\n0.46\n0.46\n0.59\n0.55\nTable 8: Single dataset accuracies across the OPT model family, using 16 examples.\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.53\n0.53\n0.55\n0.42\n0.35\n0.42\n0.43\n0.33\n0.4\n0.36\n0.34\n0.35\nOPT-2.7B\n0.51\n0.59\n0.59\n0.31\n0.42\n0.42\n0.43\n0.39\n0.42\n0.53\n0.4\n0.57\nOPT-6.7B\n0.55\n0.59\n0.6\n0.26\n0.29\n0.24\n0.4\n0.39\n0.42\n0.49\n0.44\n0.53\nOPT-13B\n0.56\n0.58\n0.59\n0.25\n0.45\n0.36\n0.39\n0.38\n0.42\n0.4\n0.38\n0.37\nOPT-30B\n0.52\n0.59\n0.57\n0.32\n0.47\n0.42\n0.47\n0.42\n0.47\n0.54\n0.45\n0.6\nOPT-66B\n0.48\n0.58\n0.51\n0.27\n0.5\n0.26\n0.4\n0.46\n0.5\n0.45\n0.43\n0.47\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.35\n0.42\n0.43\n0.33\n0.4\n0.36\n0.34\n0.35\n0.44\n0.38\n0.44\n0.67\nOPT-2.7B\n0.42\n0.42\n0.43\n0.39\n0.42\n0.53\n0.4\n0.57\n0.51\n0.56\n0.58\n0.46\nOPT-6.7B\n0.29\n0.24\n0.4\n0.39\n0.42\n0.49\n0.44\n0.53\n0.68\n0.61\n0.82\n0.63\nOPT-13B\n0.45\n0.36\n0.39\n0.38\n0.42\n0.4\n0.38\n0.37\n0.61\n0.6\n0.72\n0.48\nOPT-30B\n0.47\n0.42\n0.47\n0.42\n0.47\n0.54\n0.45\n0.6\n0.57\n0.57\n0.7\n0.4\nOPT-66B\n0.5\n0.26\n0.4\n0.46\n0.5\n0.45\n0.43\n0.47\n0.37\n0.64\n0.57\n0.41\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.4\n0.36\n0.34\n0.35\n0.44\n0.38\n0.44\n0.67\n0.51\n0.73\n0.71\n0.51\nOPT-2.7B\n0.42\n0.53\n0.4\n0.57\n0.51\n0.56\n0.58\n0.46\n0.55\n0.49\n0.43\n0.6\nOPT-6.7B\n0.42\n0.49\n0.44\n0.53\n0.68\n0.61\n0.82\n0.63\n0.65\n0.74\n0.62\n0.65\nOPT-13B\n0.42\n0.4\n0.38\n0.37\n0.61\n0.6\n0.72\n0.48\n0.56\n0.57\n0.44\n0.64\nOPT-30B\n0.47\n0.54\n0.45\n0.6\n0.57\n0.57\n0.7\n0.4\n0.55\n0.42\n0.36\n0.66\nOPT-66B\n0.5\n0.45\n0.43\n0.47\n0.37\n0.64\n0.57\n0.41\n0.63\n0.52\n0.36\n0.67\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nOPT-350M\n0.44\n0.38\n0.44\n0.67\n0.51\n0.73\n0.71\n0.51\n0.77\n0.74\n0.52\n0.79\nOPT-2.7B\n0.51\n0.56\n0.58\n0.46\n0.55\n0.49\n0.43\n0.6\n0.54\n0.41\n0.56\n0.48\nOPT-6.7B\n0.68\n0.61\n0.82\n0.63\n0.65\n0.74\n0.62\n0.65\n0.78\n0.65\n0.64\n0.77\nOPT-13B\n0.61\n0.6\n0.72\n0.48\n0.56\n0.57\n0.44\n0.64\n0.5\n0.45\n0.53\n0.5\nOPT-30B\n0.57\n0.57\n0.7\n0.4\n0.55\n0.42\n0.36\n0.66\n0.46\n0.4\n0.71\n0.54\nOPT-66B\n0.37\n0.64\n0.57\n0.41\n0.63\n0.52\n0.36\n0.67\n0.49\n0.4\n0.69\n0.56\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.59\n0.53\n0.64\n0.33\n0.31\n0.37\n0.41\n0.43\n0.45\n0.32\n0.36\n0.38\n13B\n0.63\n0.53\n0.65\n0.31\n0.34\n0.28\n0.43\n0.34\n0.44\n0.39\n0.41\n0.41\n30B\n0.64\n0.58\n0.72\n0.38\n0.47\n0.52\n0.57\n0.49\n0.65\n0.37\n0.43\n0.41\n65B\n0.69\n0.58\n0.72\n0.4\n0.42\n0.58\n0.54\n0.42\n0.58\n0.38\n0.46\n0.41\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.31\n0.37\n0.41\n0.43\n0.45\n0.32\n0.36\n0.38\n0.64\n0.4\n0.7\n0.65\n13B\n0.34\n0.28\n0.43\n0.34\n0.44\n0.39\n0.41\n0.41\n0.42\n0.35\n0.61\n0.61\n30B\n0.47\n0.52\n0.57\n0.49\n0.65\n0.37\n0.43\n0.41\n0.65\n0.38\n0.79\n0.69\n65B\n0.42\n0.58\n0.54\n0.42\n0.58\n0.38\n0.46\n0.41\n0.6\n0.44\n0.83\n0.69\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.45\n0.32\n0.36\n0.38\n0.64\n0.4\n0.7\n0.65\n0.56\n0.73\n0.61\n0.53\n13B\n0.44\n0.39\n0.41\n0.41\n0.42\n0.35\n0.61\n0.61\n0.52\n0.66\n0.59\n0.5\n30B\n0.65\n0.37\n0.43\n0.41\n0.65\n0.38\n0.79\n0.69\n0.52\n0.76\n0.65\n0.52\n65B\n0.58\n0.38\n0.46\n0.41\n0.6\n0.44\n0.83\n0.69\n0.55\n0.75\n0.65\n0.56\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.64\n0.4\n0.7\n0.65\n0.56\n0.73\n0.61\n0.53\n0.7\n0.71\n0.52\n0.78\n13B\n0.42\n0.35\n0.61\n0.61\n0.52\n0.66\n0.59\n0.5\n0.64\n0.71\n0.54\n0.78\n30B\n0.65\n0.38\n0.79\n0.69\n0.52\n0.76\n0.65\n0.52\n0.77\n0.67\n0.56\n0.86\n65B\n0.6\n0.44\n0.83\n0.69\n0.55\n0.75\n0.65\n0.56\n0.77\n0.73\n0.6\n0.87\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.61\n0.58\n0.66\n0.33\n0.49\n0.37\n0.41\n0.35\n0.45\n0.31\n0.43\n0.36\n13B\n0.6\n0.58\n0.66\n0.27\n0.5\n0.34\n0.4\n0.34\n0.42\n0.37\n0.42\n0.41\n30B\n0.67\n0.67\n0.74\n0.37\n0.54\n0.53\n0.47\n0.5\n0.62\n0.36\n0.51\n0.42\n65B\n0.66\n0.62\n0.73\n0.37\n0.56\n0.6\n0.52\n0.53\n0.6\n0.38\n0.55\n0.42\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.49\n0.37\n0.41\n0.35\n0.45\n0.31\n0.43\n0.36\n0.65\n0.46\n0.72\n0.6\n13B\n0.5\n0.34\n0.4\n0.34\n0.42\n0.37\n0.42\n0.41\n0.41\n0.39\n0.59\n0.56\n30B\n0.54\n0.53\n0.47\n0.5\n0.62\n0.36\n0.51\n0.42\n0.64\n0.49\n0.84\n0.6\n65B\n0.56\n0.6\n0.52\n0.53\n0.6\n0.38\n0.55\n0.42\n0.56\n0.54\n0.87\n0.62\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.45\n0.31\n0.43\n0.36\n0.65\n0.46\n0.72\n0.6\n0.53\n0.72\n0.57\n0.59\n13B\n0.42\n0.37\n0.42\n0.41\n0.41\n0.39\n0.59\n0.56\n0.51\n0.66\n0.59\n0.5\n30B\n0.62\n0.36\n0.51\n0.42\n0.64\n0.49\n0.84\n0.6\n0.58\n0.74\n0.6\n0.65\n65B\n0.6\n0.38\n0.55\n0.42\n0.56\n0.54\n0.87\n0.62\n0.58\n0.75\n0.66\n0.65\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.65\n0.46\n0.72\n0.6\n0.53\n0.72\n0.57\n0.59\n0.67\n0.65\n0.59\n0.78\n13B\n0.41\n0.39\n0.59\n0.56\n0.51\n0.66\n0.59\n0.5\n0.73\n0.69\n0.54\n0.78\n30B\n0.64\n0.49\n0.84\n0.6\n0.58\n0.74\n0.6\n0.65\n0.74\n0.65\n0.64\n0.85\n65B\n0.56\n0.54\n0.87\n0.62\n0.58\n0.75\n0.66\n0.65\n0.78\n0.73\n0.64\n0.85\n<div style=\"text-align: center;\">tweet_eval_hate Random Abstract Gold</div>\nTable 11: Single dataset accuracies across the LLaMA model family, using 16 examples.\ntweet_eval_hate\ntweet_eval_atheism\ntweet_eval_feminist\nsick\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.58\n0.58\n0.64\n0.33\n0.51\n0.35\n0.4\n0.38\n0.47\n0.36\n0.46\n0.4\n13B\n0.6\n0.59\n0.68\n0.3\n0.46\n0.37\n0.41\n0.42\n0.46\n0.36\n0.42\n0.42\n30B\n0.65\n0.64\n0.73\n0.32\n0.53\n0.6\n0.48\n0.51\n0.63\n0.35\n0.55\n0.42\n65B\n0.64\n0.68\n0.78\n0.38\n0.51\n0.6\n0.45\n0.49\n0.63\n0.36\n0.62\n0.43\nfinancial_phrasebank\nethos_race\nethos_gender\nethos_religion\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.51\n0.35\n0.4\n0.38\n0.47\n0.36\n0.46\n0.4\n0.64\n0.5\n0.74\n0.61\n13B\n0.46\n0.37\n0.41\n0.42\n0.46\n0.36\n0.42\n0.42\n0.38\n0.38\n0.56\n0.65\n30B\n0.53\n0.6\n0.48\n0.51\n0.63\n0.35\n0.55\n0.42\n0.61\n0.61\n0.88\n0.66\n65B\n0.51\n0.6\n0.45\n0.49\n0.63\n0.36\n0.62\n0.43\n0.52\n0.66\n0.88\n0.59\nethos_national_origin\nsnli\nsst2\ntrec\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.47\n0.36\n0.46\n0.4\n0.64\n0.5\n0.74\n0.61\n0.59\n0.67\n0.47\n0.62\n13B\n0.46\n0.36\n0.42\n0.42\n0.38\n0.38\n0.56\n0.65\n0.53\n0.73\n0.67\n0.57\n30B\n0.63\n0.35\n0.55\n0.42\n0.61\n0.61\n0.88\n0.66\n0.6\n0.74\n0.55\n0.6\n65B\n0.63\n0.36\n0.62\n0.43\n0.52\n0.66\n0.88\n0.59\n0.63\n0.76\n0.58\n0.66\nrte\nwnli\nmrpc\npoem\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\nRandom\nAbstract\nGold\n7B\n0.64\n0.5\n0.74\n0.61\n0.59\n0.67\n0.47\n0.62\n0.69\n0.65\n0.64\n0.79\n13B\n0.38\n0.38\n0.56\n0.65\n0.53\n0.73\n0.67\n0.57\n0.76\n0.7\n0.62\n0.83\n30B\n0.61\n0.61\n0.88\n0.66\n0.6\n0.74\n0.55\n0.6\n0.8\n0.57\n0.65\n0.82\n65B\n0.52\n0.66\n0.88\n0.59\n0.63\n0.76\n0.58\n0.66\n0.77\n0.63\n0.73\n0.87\ndataset accuracies across the LLaMA model family, using 32 examples\nble 12: Single dataset accuracies across the LLaMA model family, usin\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding the mechanisms behind in-context learning (ICL) in large language models (LLMs), which can perform tasks based on few examples without parameter updates.",
        "problem": {
            "definition": "The problem lies in the lack of clarity on how LLMs utilize in-context demonstrations to recognize tasks and learn new mappings.",
            "key obstacle": "The main challenge is distinguishing between task recognition (TR) and task learning (TL) within the context of ICL."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to clarify the different roles of TR and TL in ICL, as previous works have not adequately differentiated them.",
            "opinion": "The authors propose that ICL can be decomposed into TR, which applies pre-trained knowledge to recognize tasks, and TL, which learns new mappings from the demonstrations.",
            "innovation": "The primary improvement over previous methods is the systematic disentangling of TR and TL, showing that they function under different conditions."
        },
        "Theory": {
            "perspective": "The theoretical perspective involves understanding the distinct mechanisms of TR and TL, suggesting that they operate independently.",
            "opinion": "The authors assume that recognizing a previously learned task is easier than learning a new mapping, leading to different performance characteristics.",
            "proof": "The paper provides empirical evidence through controlled experiments that demonstrate the separate effects of TR and TL as model sizes and demonstration numbers vary."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using 16 classification datasets across various tasks, utilizing three LLM families: GPT-3, LLaMA, and OPT.",
            "evaluation method": "The evaluation involved three setups: GOLD (natural prompts with correct labels), RANDOM (natural prompts with randomly sampled labels), and ABSTRACT (minimal prompts with abstract symbols)."
        },
        "conclusion": "The findings reveal that TR does not improve with larger models or more demonstrations, while TL significantly benefits from model scaling and increased demonstrations, suggesting a paradigm shift in ICL understanding.",
        "discussion": {
            "advantage": "The main advantage is the clear differentiation between TR and TL, providing insights into how LLMs can be optimized for ICL.",
            "limitation": "A limitation is that the study is focused solely on classification tasks, leaving other types of NLP tasks unexplored.",
            "future work": "Future research should explore TL's mechanisms further and apply the findings to a broader range of NLP tasks beyond classification."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding the mechanisms behind in-context learning (ICL) in large language models (LLMs), which can perform tasks based on few examples without parameter updates."
        },
        {
            "section number": "1.2",
            "key information": "The problem lies in the lack of clarity on how LLMs utilize in-context demonstrations to recognize tasks and learn new mappings."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective involves understanding the distinct mechanisms of task recognition (TR) and task learning (TL), suggesting that they operate independently."
        },
        {
            "section number": "3.3",
            "key information": "The primary improvement over previous methods is the systematic disentangling of TR and TL, showing that they function under different conditions."
        },
        {
            "section number": "5.1",
            "key information": "The experiments were conducted using 16 classification datasets across various tasks, utilizing three LLM families: GPT-3, LLaMA, and OPT."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is that the study is focused solely on classification tasks, leaving other types of NLP tasks unexplored."
        },
        {
            "section number": "7",
            "key information": "The findings reveal that TR does not improve with larger models or more demonstrations, while TL significantly benefits from model scaling and increased demonstrations, suggesting a paradigm shift in ICL understanding."
        }
    ],
    "similarity_score": 0.7732700675358849,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/What In-Context Learning _Learns_ In-Context_ Disentangling Task Recognition and Task Learning.json"
}