{
    "from": "google",
    "scholar_id": "QcVAY-buVZwJ",
    "detail_id": null,
    "title": "Large language model can interpret latent space of sequential recommender",
    "abstract": " ABSTRACT\n\nSequential recommendation is to predict the next item of interest for a user, based on her/his interaction history with previous items. In conventional sequential recommenders, a common approach is to model item sequences using discrete IDs, learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g., image, audio, 3D points), a compelling research question arises: \u201cCan LLMs understand and work with hidden representations from ID-based sequential recommenders?\u201d. To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically, with the multimodal pairs (i.e., representations of interaction sequence and text narrations), RecInterpreter first uses a lightweight adapter to map the representations into the token embedding space of the LLM. Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence. Taking a step further, we propose a sequence-residual prompt instead, which guides the LLM in identifying the residual item by contrasting the representations before and after integrating this residual into the existing sequence. Empirical results showcase that our RecInterpreter enhances the exemplar LLM, LLaMA, to understand hidden representations from ID-based sequential recommenders, especially when guided by our sequence-residual prompts. Furthermore, RecInterpreter enables LLaMA to instantiate the oracle items generated by generative recommenders like DreamRec, concreting the item a user would ideally like to interact with next. Codes are available at https://github.com/YangZhengyi98/RecInterpreter.\n\nAn Zhang National University of Singapore Singapore, Singapore anzhang@u.nus.edu\n\nCCS CONCEP",
    "bib_name": "yang2023large",
    "md_text": "# Large Language Model Can Interpret Latent Space of Sequential Recommender\n\nZhengyi Yang University of Science and Technology of China Hefei, China yangzhy@mail.ustc.edu.cn\nJiancan Wu University of Science and Technology of China Hefei, China wujcan@gmail.com\nYanchen Luo University of Science and Technology of China Hefei, China luoyanchen@mail.ustc.edu.cn\nAn Zhang National University of Singapore Singapore, Singapore anzhang@u.nus.edu\nJizhi Zhang University of Science and Technology of China Hefei, China cdzhangjizhi@mail.ustc.edu.cn\nYancheng Yuan The Hong Kong Polytechnic University Hong Kong, China yancheng.yuan@polyu.edu.hk\n\nJizhi Zhang University of Science and Technology of China Hefei, China cdzhangjizhi@mail.ustc.edu.cn\n\ncdzhangjizhi@mail.ustc.edu.cn\n\nXiang Wang University of Science and Technology of China Hefei, China xiangwang1223@gmail.com\nXiangnan He University of Science and Technology of China Hefei, China xiangnanhe@gmail.com\n\nxiangwang1223@gmail.com\n\n# ABSTRACT\n\nSequential recommendation is to predict the next item of interest for a user, based on her/his interaction history with previous items. In conventional sequential recommenders, a common approach is to model item sequences using discrete IDs, learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g., image, audio, 3D points), a compelling research question arises: \u201cCan LLMs understand and work with hidden representations from ID-based sequential recommenders?\u201d. To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically, with the multimodal pairs (i.e., representations of interaction sequence and text narrations), RecInterpreter first uses a lightweight adapter to map the representations into the token embedding space of the LLM. Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence. Taking a step further, we propose a sequence-residual prompt instead, which guides the LLM in identifying the residual item by contrasting the representations before and after integrating this residual into the existing sequence. Empirical results showcase that our RecInterpreter enhances the exemplar LLM, LLaMA, to understand hidden representations from ID-based sequential recommenders, especially when guided by our sequence-residual prompts. Furthermore, RecInterpreter enables LLaMA to instantiate the oracle items generated by generative recommenders like DreamRec, concreting the item a user would ideally like to interact with next. Codes are available at https://github.com/YangZhengyi98/RecInterpreter.\n\nAn Zhang National University of Singapore Singapore, Singapore anzhang@u.nus.edu\n\nCCS CONCEPTS\n\u2022 Information systems \u2192 Recommender systems; Retrieval models and ranking.\n\nKEYWORDS\n\nSequential Recommendation, Large Language Models\n\n# 1 INTRODUCTION\n\nSequential recommendation \u2014 predicting the next item of interest based on a sequence of items that a user interacted with before \u2014 has been a fundamental task in both academia and industry [7, 16, 20]. Scrutinizing leading sequential recommenders [5, 16, 20, 35, 42], we can summarize a typical pipeline: 1) assign discrete IDs to items and initialize learnable vectors (aka. item embeddings) to represent different items, and 2) learn the hidden representation based on each sequence of item embeddings, as Figure 1(a) shows. Such representations, derived from the ID-modeling paradigm, are able to encode sequential patterns of user behaviors, having greatly facilitated the next-item recommendation. With the meteoric rise of Large Language Models (LLMs) (e.g., GPT4 [30], LLaMA [36]), aligning diverse modalities \u2014 such as images, audios, and 3D points \u2014 with text can empower LLMs to understand and reason about other modalities [2, 9, 12, 13, 17, 28, 49]. Central to such an alignment is transforming the hidden representations from the modality-specific encoders (e.g., images encoded by ViT [8] or Stable Diffusion [34], audios encoded by HiFiGAN [21]) into the text token embeddings of an LLM [12, 49]. This allows for the LLM to reason over the input modality and generate the textual responses correspondingly. Although multi-modal comprehension is becoming a focal point of LLMs, the capability to interpret hidden representations from sequential recommenders remains\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b201/b2011f4e-573f-4b15-99cc-5e26a1c88c39.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) LLM4Rec\n</div>\n<div style=\"text-align: center;\">(a) Sequential Recommendation (SeqRec)\n</div>\nparision of conventional sequential recommendation, LLM4Rec and RecInterpreter. Flame denotes tunable snowflake indicates frozen modules.\n\n<div style=\"text-align: center;\">Figure 1: Comparision of conventional sequential recommendation, LLM4Rec and RecInterpreter. Fl modules, while snowflake indicates frozen modules.\n</div>\nmostly unexplored. This is largely due to the current LLMs-forRecommendation (LLM4Rec) studies [1, 3, 10, 11, 25, 39], which primarily focus on recasting the user-item interactions as text prompts and feeding them into LLMs for recommendation or reranking, as Figure 1(b) illustrates. However, this paradigm shields LLMs from accessing or deciphering the hidden representations from recommender models. Naturally, an compelling research question arises: \u201cCan LLMs Understand Representations from Recommenders?\u201d To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Here we select LLaMA7B [36] as a prime example of open-source LLMs, which offer access to its hidden states and support backpropagation. In terms of sequential recommenders, we harness the representative models trained solely on item ID sequences, including GRU4Rec [16], Caser [35], SASRec [20], and DreamRec [43]. Having the LLM and recommender frozen, one straightforward solution to bridge their gap is the alignment training [2, 29, 49] with the paired multimodal data (i.e.,  representations of item ID sequences and text narrations). Following the leading alignment strategies [9, 29, 49], RecInterpreter has two key components: 1) train a lightweight adapter to map the recommendation representations into the token embedding space of the LLM, and 2) inject these recommendation-specific tokens into a text prompt to ask the LLM for a textual elucidation. Next, we will elaborate on these components. Specifically, as a bridge, the adapter fuses the spaces of the LLM and recommender into a joint token embedding space, wherein tokens represent both text and user behavior. Moreover, we simply set it as a single linear projection layer to train, where the model parameters of the recommenders and LLaMA are frozen. This lightweight design not only reaches convergence faster than training from scratch, but also inherits the reasoning capabilities of the LLM. Having the recommendation-specific tokens, we first propose the sequence-recovery prompt, which tries to recover the whole\n\n<div style=\"text-align: center;\">(c) Rec Interpreter\n</div>\nitem sequence. Here is an example of the prompt in the movie recommendation scenario:\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/357f/357fbeab-2d4e-45dd-9a52-d0f5a9de360f.png\" style=\"width: 50%;\"></div>\nwhere <SeqH> is the hidden representation of watching history encoded by a sequential recommender (e.g.,  the hidden representation in Figure 1). While we empirically show that LLaMA could understand some interactions from the hidden representation, it is hard to recover all the interactions, since the hidden representation is highly compressed. To this end, we carefully craft a sequenceresidual prompt tailored for sequential recommenders. This prompt is designed to guide LLaMA in identifying the residual item by comparing the representations before and after the sequence incorporates said residual.\n\n# Sequence-Residual Prompt Example\n\n\u201cA user has watched a series of movies, which can be represented as <SeqH1>. After watching another movie, the watching history can be represented as <SeqH2>. What is the additional movie the user watched?\u201d\n\nwhere <SeqH1> and <SeqH2> are the hidden representations before and after the sequence integrates with the residual. Surprisingly, our empirical evaluations show that LLaMA exhibits a significant aptitude for deciphering the representations from sequential recommenders, especially following our instructions. When presented with an item ID sequence and another extended by a target item ID, LLaMa can clearly tell their representation difference and yield the textual description of the target item. Consequently, we may safely reach the conclusion that LLMs could be inspired to understand the representation space of sequential recommenders, which inherently encapsulate rich patterns of user\n\nLarge Language Model Can Interpret Latent Space of Sequential Recommender\n\nbehaviors. Moreover, since the linear projection is the only tunable component, it is affordable for online service providers to interpret their own recommenders with LLMs, which is flexible for them to investigate further how to utilize LLMs in their platforms. Furthermore, another interesting research question emerges: \u201cCan LLMs Instantiate the Generated Items from Generative Recommenders?\u201d. Our RecInterpreter presents a straightforward solution to decode the generated results of generative recommender systems. Take DreamRec [43] \u2014 one of the latest in the generative recommendation \u2014 as an example. Here, the generated oracle item is encoded as vector representation, which lacks explicit interpretation. Using our RecInterpreter framework that can differentiate between representations before and after a user\u2019s engagement with a new item, we could append the generated oracle item at the end of the interaction sequence, and let LLaMA interpret the oracle item with a text description. Experiments demonstrate that our RecInterpreter framework can decode reasonable oracle items not limiting in the candidate set, which completes the full promise of DreamRec as a generative recommender.\n\n# 2 RELATED WORK\n\nThis section reviews the work on multimodal language models, and then discusses the work on sequential recommendation, especially the integration of LLMs.\n\n# 2.1 Multimodal Language Models\n\nRecent advances in LLMs have demonstrated remarkable few/zeroshot reasoning capabilities in Neural Language Processing tasks [30, 33, 48]. Meanwhile, different modalities (including vision, video, audio and etc.), have been evolving their models rapidly to better accommodate different tasks [8, 21, 34]. More recently, researchers find that models of different modalities can be unified with LLMs by making the hidden representations perceivable for LLMs, leading to a promising direction, multimodal language models [2, 12, 28, 40, 49]. Along this research line, the pioneer work, Flamingao [2] demonstrates that the vision encoder NFNet [6] could be understood by LLMs through inserting tunable gated cross-attention dense blocks among the layers of LLMs, which has been proven effective in GPT4Vision [31]. MiniGPT4 [49] further shows that a single linear layer is enough to make LLaMA [36] to interpret hidden representations encoded by ViT [8]. Similarly, TANGO [12] suggests that the audio backbone model HiFiGAN [21] can be unified by LLMs. Besides, Video-ChatGPT [28] and VideoChat [23] imply that pre-trained video encoders are also perceivable for LLMs.\n\n# 2.2 Sequential Recommendation\n\nSequential recommendation aims at inferring users\u2019 preferences based on their interaction sequences. Previous work has explored encoding the interaction sequences with different model architectures, such as Recurrent Neural Network (RNN) [16], Convolutional Neural Network (CNN) [35], and Transformer encoder [20]. Moreover, recent work also designs auxiliary learning tasks like causal inference [44, 47], data augmentation [41], and robust learning [42]. Advances in LLMs have drawn increasing research attention to leveraging LLMs for sequential recommendation. Some research\n\ndirectly employs In-Context Learning to assess the recommendation performance of LLM or enhance traditional recommendations, thereby measuring the recommendation capabilities of LLMs [15, 18, 27, 45]. Furthermore, some recent studies [3, 4] are concerned that LLMs lack recommendation-specific knowledge in the pre-training phase, thus proposing to leverage LLMs to implement specific tuning techniques to enhance their recommendation performance [3, 4, 26, 38, 46]. However, these approaches mostly shield LLMs from accessing or deciphering the hidden representations from frozen recommender models. Therefore, inspiring LLMs to interpret hidden representations from sequential recommenders remains largely unexplored.\n\n# 3 INSPIRE LLMS TO UNDERSTAND SEQUENTIAL RECOMMENDERS\n\nIn this section, we outline our RecInterpreter framework to harness the capabilities of LLMs for comprehending traditional sequential recommenders. We first introduce a sequence-recovery task, aiming to empower LLaMA to reconstruct the items in an interaction sequence based solely on its hidden representation. Taking a step further, we propose a novel sequence-residual task, which guides LLaMA to pinpoint the residual item by contrasting the hidden representations before and after integrating this residual into the existing sequence. Finally, we highlight how RecInterpreter can explicitly decode the embeddings of an unseen oracle item, when employed within generative recommender settings.\n\n# 3.1 Sequence-Recovery Prompting\n\nTo validate LLaMA\u2019s capability in understanding sequential recommenders, we draw inspiration from prior multi-modal alignment studies [2, 9, 49?] and propose a sequence recovery task. This objective is to encourage LLaMA to reconstruct the items in an interaction sequence in text, based solely on its hidden representation, as depicted in Figure 2. Next, we will elaborate on our steps.\n\ninteraction sequence s = [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5a] that involves the consumed items, we employ a well-trained sequential recommender, such as SASRec and DreamRec, to derive the hidden representation of the sequence. Formally, this sequence encoding process can be formulated as follows: The interaction sequence s is first vectorized as E s = [e \ud835\udc60 1, e \ud835\udc60 2, . . . , e \ud835\udc60 \ud835\udc5a] by the sequential recommender, and we can acquire the hidden representation through:\n\n# h s = Seq-Enc (E s),\n\n(1)\n\nwhere Seq-Enc (\u00b7) is the sequence encoder of conventional sequential recommender, and h s \u2208 R \ud835\udc51 is the \ud835\udc51-dimentional representation of sequence \ud835\udc60 (e.g., \ud835\udc51 is set as 64 or 256 in SASRec).\n\nRepresentation Adaptaion vis Lightweight Adapter. We train a lightweight adapter to project the hidden representation h s into the text token embedding space of LLaMA. Here we implement the adapter as a linear projection layer, whose design ensures that the input dimension aligns with \ud835\udc51, while the output dimension matches LLaMA\u2019s token embedding size (i.e., 4096). Thus the hidden representation is transformed as:\n\ufffd h s = Linear-Proj \ud835\udf03 (h s). (2)\n\n\ufffd h s = Linear-Proj \ud835\udf03 (h s).\n\n(2)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba17/ba1757dd-212c-4054-b308-710977562dad.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/149d/149d783b-2cd7-42f7-a8e5-d46fd0a9a326.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">LLaMA\n</div>\n<div style=\"text-align: center;\">lustration of the sequence recovery framework. We provide the task-specific textual prompts and the hidden ion of the interaction sequence projected by a linear layer, targeting at inspiring LLaMA to recover the interactions ual response. Flame denotes tunable modules, while snowflake indicates frozen modules.\n</div>\n<div style=\"text-align: center;\">Figure 2: Illustration of the sequence recovery framework. We provide the task-specific textual p representation of the interaction sequence projected by a linear layer, targeting at inspiring LLaMA to with a textual response. Flame denotes tunable modules, while snowflake indicates frozen modu\n</div>\nIn this way, the adapter serves as a bridge, integrating the spaces of LLaAM and the recommender system. This leads to a unified token embedding space, where tokens can signify either textual content or user interactions. The deeper exploration of such adapters, such as Q-former [22], is an avenue we plan to explore in future work.\nPrompt Design for the Adapter Training. Here we design the sequence-recovery prompts, which are composed of text tokens interleaved with the projected sequence representation \ufffd h s. Here is an example of the sequence-recovery prompt in the movie recommendation scenario:\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f69/1f69145a-a1fa-4559-bf09-b44980cc0b0b.png\" style=\"width: 50%;\"></div>\nSequence-Recovery Prompt\nInput\nPrompt\nA person has watched a series of movies.\nThe watching list can be represented as:\n<SeqH>. Describe this watching history of\nthe person in detail.\nTarget\nResponse\nThis user has watched Twelve Monkeys,\nCat People, Cape Fear, Abyss, Candyman,\nOmen, Nightmare on Elm Street, Shining,\nEmpire Strikes Back, To Kill a Mocking-\nbird in the previous.\nwhere <SeqH> is the projected hidden representation (i.e., \ufffd h s). It is worth noting that the prompt involves two key components: 1) the input prompt, which contains the projected hidden representation of the sequence \ufffd h s; and 2) the target response, which offers the detailed textual narration of \ufffd h s. Within the autoregressive framework of LLaMA, we calculate the training objective by\n\n# regressing the target prompt X \ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 based on the condition of the input prompt X \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61 [36]:\n\n\ud835\udc56 =1 \ud835\udc5d (X \ud835\udc56 \ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 | X \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61, X [1: \ud835\udc56 \u2212 1] \ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61),\n\ud835\udc5d (X \ud835\udc47\ud835\udc4e\ud835\udc5f\ud835\udc54\ud835\udc52\ud835\udc61 | X \ud835\udc3c\ud835\udc5b\ud835\udc5d\ud835\udc62\ud835\udc61) = \ud835\udc41 \ufffd\n\n(3)\n\nwhere \ud835\udc41 is the number of tokens in the target prompt. During the training phase, we provide both the input prompt and the target response with the objective of learning to generate descriptions for the projected sequence embedding \ufffd h s. During the inference phase, we provide only the input prompt containing the projected sequence embedding \ufffd h s, and acquire the output text as the understanding of \ufffd h s.\n\n# \ufffd\n3.2 Sequence-Residual Prompting\n\nIt is challenging for LLaMA to understand all items from a simple hidden representation of the interaction sequence, since the datasets of recommendation are usually very sparse. Although we empirically show that LLaMA can understand the interactions to a large extent under the sequence-recovery framework, we would also like to refine the framework to encourage LLaMA to understand sequential recommender more delicately. Drawing inspiration from Flamingo [2], which suggests that LLMs could better process images if the hidden representations of two similar images are provided at the same time with their differences, we propose to inspire LLaMA to understand sequential recommenders by identifying the residual item based on hidden representations before and after a sequence integrates the residual, as illustrated in Figure 3. Then we elaborate on the sequence-residual prompting step by step:\n\nLarge Language Model Can Interpret Latent Space of Sequential Recommender\n\nLarge Language Model Can Interpret Latent Space of Sequential Recommender\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6d8/d6d8f212-082c-49a6-a1f7-57dccf347baa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">3: Illustration of the sequence residual framework. We provide the task-specific textual prompts and the hid ntations before and after the sequence incorporates a residual item. The two hidden representations are projected b linear Layer. Flame denotes tunable modules, while snowflake indicates frozen modules.\n</div>\n<div style=\"text-align: center;\">Figure 3: Illustration of the sequence residual framework. We provide the task-specific tex representations before and after the sequence incorporates a residual item. The two hidden rep shared linear Layer. Flame denotes tunable modules, while snowflake indicates frozen mo\n</div>\nSequence Encoding via Sequential recommenders. Given an interaction sequence s = [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5a], we could design a circumstance, that a user has interacted with [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5a \u2212 1] and then interacts with a residual item \ud835\udc60 \ud835\udc5a. The sequential recommender could encode s 1 = [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5a \u2212 1] and s 2 = [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5a] as h 1 \ud835\udc60 and h 2 \ud835\udc60 respectively:\n\nh s 1 = Seq-Enc (E 1) and h s 2 = Seq-Enc (E 2),\n\n(4)\n\nwhere E 1 and E 2 are the vactorized sequence of s 1 and s 2.\nRepresentation Adaptaion vis Lightweight Adapter. We also employ a linear projection layer as the lightweight adapter, which could project h s 1 and h s 2 to be \ufffd h s 1 and \ufffd h s 2:\n\n\ufffd h s 1 = Linear-Proj \ud835\udf03 (h s 1) and \ufffd h s 2 = Linear-Proj \ud835\udf03 (h s 2), (5\n\n(5)\n\nwhere the parameters of linear layer are shared by h s 1 and h s 2.\nPrompt Design for the Adapter Training. Here we design more delicate sequence-residual prompts, which inspire LLaMA to identify the residual item \ud835\udc60 \ud835\udc5a by comparing \ufffd h s 1 and \ufffd h s 2. Here is an example of the sequence-residual prompt in the movie recommendation scenario:\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f23/7f230944-53eb-4a9b-9c9a-984179b28b3b.png\" style=\"width: 50%;\"></div>\nSequence-Residual Prompt\nInput\nPrompt\nA person has watched a series of movies.\nThe watching list can be represented as\nList1: <SeqH1>. After watching another\nmovie, the watching list can further be\nrepresented as List2: <SeqH2>. What is the\nmovie in List2 but not in List1?\nTarget\nResponse\nThis user watched movie Twelve Monkeys\nin List2 but not in List1.\nwhere <SeqH1> and <SeqH2> would be replaced with \ufffd h s 1 and \ufffd h s 2, and Twelve Monkeys is the residual item in the example. Similar to the sequence-recovery prompting, we provide both the input prompt and target response during the training phase, and only the input prompt during the inference phase.\n\n# 3.3 Instantiate Oracle Items\n\nHaving shown that LLaMA can be inspired to identify the residual item by comparing two hidden representations of designed sequences, we then elaborate on how to benefit generative recommender with this sequence-residual prompting.\n\nBrief on DreamRec. In DreamRec [43], one of the latest in generative recommendation, an oracle item could be generated through the guided diffusion process. However, the oracle item is represented as a hidden vector without explicit interpretation, thus the completion of the recommendation task is compromised by finding the nearest items of the oracle item in the candidate set, which\n\nfails to achieve the full promise of DreamRec to generative items beyond the candidates [43]. Drawing inspiration from the proposed sequence-residual prompting, we could let LLaMA provide the description of the oracle item generated by DreamRec, thus directly acquiring the recommendation results.\nConstruct Sequence-Residual Task with Oracle Item. Let s = [\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5b] be a historical interaction sequence of a user, and DreamRec can generate the vector representation of the corresponding oracle item as e \ud835\udc60 \u2217. As described in Section 3.2, the well-trained sequence-residual framework could identify the residual item between two hidden representations before and after the sequence interacts with a new item. Therefore, we assume that the user would interact with the oracle item and construct the vectorized sequence E \u2217 = [e \ud835\udc60 1, e \ud835\udc60 2, . . . , e \ud835\udc60 \ud835\udc5b, e \ud835\udc60 \u2217]. Applying the sequential encoder of DreamRec and the linear projection adapter, we have:\n\n[\ud835\udc60 1,\ud835\udc60 2, . . . ,\ud835\udc60 \ud835\udc5b] be a historical interaction sequence of a user, and DreamRec can generate the vector representation of the corresponding oracle item as e \ud835\udc60 \u2217. As described in Section 3.2, the well-trained sequence-residual framework could identify the residual item between two hidden representations before and after the sequence interacts with a new item. Therefore, we assume that the user would interact with the oracle item and construct the vectorized sequence E \u2217 = [e \ud835\udc60 1, e \ud835\udc60 2, . . . , e \ud835\udc60 \ud835\udc5b, e \ud835\udc60 \u2217]. Applying the sequential encoder of DreamRec and the linear projection adapter, we have:\n\n# h s = Seq-Enc (E) and h s \u2217 = Seq-Enc (E \u2217),\n\n(6)\n\n# and: \ufffd\n\n# \ufffd h s = Linear-Proj \ud835\udf03 (h s) and \ufffd h s \u2217 = Linear-Proj \ud835\udf03 (h s \u2217).\n\n\ufffd h s = Linear-Proj \ud835\udf03 (h s) and \ufffd h s \u2217 = Linear-Proj \ud835\udf03 (h s \u2217).\n\n(7)\n\n\ufffd \ufffd\nTo this end, LLaMA could identify the oracle item e \ud835\udc60 \u2217 with textual descriptions by comparing \ufffd h s and \ufffd h s \u2217 with the sequence-residual prompting framework.\n\n\ufffd \ufffd\nTraining and Inference. The training phase remains the same as the sequence-residual prompting framework, i.e., we utilize the sequences in the dataset to construct the contrastive hidden representations pairs for training. During the inference phase, we would feed \ufffd h s and \ufffd h s \u2217 into LLaMA, and then LLaMA could respond with a textual description about the oracle item. Therefore, we can complete the explicit decoding of the generated oracle items, which has not been achieved by DreamRec.\n\n# 4 EXPERIMENT\n\nIn this section, we conduct experiments to demonstrate our approach to inspire LLMs to understand sequential recommenders through interpreting the hidden representations. Then we show how to facilitate generative recommendation by providing a textual description of generated items.\n\n# 4.1 Experimental Setings\n\n4.1.1 Datasets.  We use two datasets from real-world recommendation scenarios: MovieLens and Steam:\n\nSince tuning the projection layer requires backpropagation from LLaMA, the training phase is more time-consuming than conventional recommenders, and the size of datasets should not be too large. Therefore, we select the MovieLens100K dataset in our experiment. For the Steam dataset, we first remove users who have less than 20 reviews, which keeps the same as the processing of\n\n1 https://grouplens.org/datasets/movielens/\n\n<div style=\"text-align: center;\">Table 1: Statistics of datasets.\n</div>\nDataset\nMovieLens\nSteam\n#sequences\n943\n11,938\n#items\n1,682\n3,581\n#interactions\n100,000\n274,726\nMovieLens. Then we sample 1/3 of users and 1/3 of games and preserve their interactions to acquire a moderate size of dataset. For both datasets, we first sort all sequences in chronological order and then split the data into training, validation, and testing data at the ratio of 8:1:1. This splitting strategy ensures that later interactions would not appear in the training set, avoiding any potential of information leakage [19]. The statistics of datasets are illustrated in Table 1.\n\n4.1.2 Implementation details. We implement all approaches with Python 3.10, PyTorch 2.0.0, and transformers 4.28.0 in a single Nvidia GeForce A40. We preserve the last 10 interactions as the historical sequence. For sequences with less than 10 interactions, we would pad them to 10 with a padding token. We first train the sequential recommenders (GRU4Rec [16], Caser [35], SASRec [20] and DreamRec [43]) on the training datasets. We use Adam optimizer, the learning rate is tuned as 0.001 and the batch size is set as 256. We adopt L2 regularization for all models other than DreamRec and the coefficient is searched in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7], since DreamRec does not require L2 regularization [43]. The embedding size is searched in [16, 64, 256, 1024]. The sequential recommenders would be frozen after training. We would utilize the frozen encoders in the pre-trained sequential recommenders to obtain the hidden representations of interaction sequences. Let \ud835\udc3f and \ud835\udc37 denote the length of the sequences and the dimension of the item embeddings respectively. For Caser, the size of the hidden representation is 1 \u00d7 (\ud835\udc37 + \ud835\udc5b \ud835\udc53 \u00d7 \ud835\udc60 \ud835\udc53), where \ud835\udc5b \ud835\udc53 and \ud835\udc60 \ud835\udc53 are the number and size of convolutional kernels respectively [35], we directly employ a linear layer to transfer the hidden representation to be the size of token embedding of LLaMA. For GRU4Rec, SASRec, and DreamRec, they adopt sequence-to-sequence models (RNN or Transformer encoder) as sequence encoders, and the size of hidden representations is \ud835\udc3f \u00d7 \ud835\udc37 [16, 20]. Therefore, we first acquire the linear combination of the hidden representations by employing a convolutional filter of size \ud835\udc3f \u00d7 1 to acquire a 1 \u00d7 \ud835\udc37 representation, and then adopt the linear projection similar to Caser. In the training phase of RecInterpreter, we adopt a warmup learning rate schedule: the learning is set as 0.0001 at the 1st epoch, increases linearly to 0.0005 at the 5th epoch, and remains unchanged. We search the L2 regularization coefficient in the range of [1e-4, 1e5, 1e-6]. We select LLaMA-7B [36] as the LLM in our experiment. For the sequence recovery framework, we set the maximum generated token length as 100, since each hidden representation contains several items. For the sequence residual framework, we set the maximum generated token length as 50, since the residual item contains only one item. It takes about 2 hours and 6 hours to train the model for a single epoch in the MovieLens dataset and Steam dataset respectively. And training for 20 epochs is generally enough for convergence.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a9b/0a9b08ea-9536-4548-a046-21cd1911d160.png\" style=\"width: 50%;\"></div>\nFigure 4: The distribution of the number of recovered items on the MovieLens (ML) and Steam datasets, wi SASRec, and DreamRec as the sequential recommenders. \u2018Pop\u2019 denotes that the 10 most popular movies or ga data are provided as the sequence recovery results. The average number of items in the test sequences is 9.99 dataset, and 8.89 for the Steam dataset.\n\n# 2 Sequence-Recovery Result\n\nThe straightforward approach to show whether LLaMA could understand the hidden representations of sequential recommenders is to let LLaMA recover the items encoded in the hidden representations with textual descriptions. In the testing data, each interaction sequence contains 10 movies or video games, and the sequence recovery task is to recover these items based on the hidden representations of the sequential recommenders. It is worth noting that sequence recovery is not a trivial task, since each dataset contains thousands of items and the hidden representation is highly compressed. We illustrate the result in Figure 4, and we present the observed cases from the MovieLens and Steam datasets in the inference phase in Figure 5. From Figure 4 we can observe that:\n\n\u2022 If we naively provide the most popular items in the training data as the recovery of interaction sequences, they can hardly match the real interacted items. In the MovieLens dataset, among 74.74% of the test samples, the popularity-based recovery strategy can not recover any items. Similarly, in the Steam dataset, the ratio of recovering 0 items based on popularity is 58.56%. These results suggest that the sequence recovery task is quite challenging, which a simple heuristic of popularity can hardly handle.\n\u2022 In general, with our designed sequence recovery framework, LLaMA shows the capability of understanding hidden representations of sequential recommenders. Notably, in the MovieLens dataset, LLaMA could recover more than 5 items from the hidden representations of Caser, SASRec, and DreamRec for over 35% of all test samples, and the percentage of recovering more\n\nthan 3 items from the hidden representations could reach 80%. Therefore, we could safely draw the conclusion that the hidden representations of interaction sequences encoded by sequential recommender are also perceivable for LLaMA, just as the hidden representations of images, audios, and videos.\n\u2022 In the comparison of the MovieLens and Steam datasets, we can observe that LLaMA shows a better understanding of the MovieLens data than the Steam data. One reason is that the game titles in the Steam dataset are more complex than the movie titles in the MovieLens dataset. Specifically, the movie titles in the MovieLens dataset are quite simple and clear containing only English words. However, the game titles in the Steam dataset are more complicated. Plenty of the game titles contain the version or provider information, such as  \u201cSwords and Sorcery - Underworld Definitive Edition\u201d. Besides, some of the game titles contain other languages than English, such as Chinese or Japanese. Therefore, it might be harder for LLaMA to understand the Steam dataset.\n\u2022 In the comparison of different sequential recommenders, we can observe that LLaMA could interpret the hidden representations of DreamREc most precisely. The reason can be that DreamRec adopts the diffusion model for generative recommendation, and diffusion has shown remarkable performance in generation tasks [24, 34]. Among the other three sequential recommenders, LLaMA can better understand Caser, SASRec than GRU4Rec. The reason comes from their different model architectures. Specifically, Caser adopts CNN to capture the sequential patterns, and CNN has displayed the impressive capability of encoding the global information [14]. The Transformer encoder employed\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc2c/fc2ccb36-3dae-4b2c-8953-1e20b4687bcd.png\" style=\"width: 50%;\"></div>\nFigure 5: Two cases of the sequence recovery task in MovieLens and Steam datasets. <SeqH>  denotes the hidden representation of the interaction sequence after the projection layer. The blue text in the response denotes the correctly recovered movies and games from only the hidden representation.\nTable 2: The result of sequence residual on the MovieLens and Steam datasets. We calculate the accuracy of correctly identifying the residual item in the test data as the evaluation metrics.\n\nDataset\nGRU4Rec\nCaser\nSASRec\nDreamRec\nMovieLens\n52.63%\n78.95%\n93.68%\n97.89%\nSteam\n17.11%\n55.03%\n52.60%\n86.33%\nby SASRec is one of the widely adopted sequence-to-sequence architectures [37], which is also the fundamental component in the framework of LLaMA [36]. However, RNN may suffer from issues such as vanishing gradient and exploding gradient [32], thus presenting obstacles for LLaMA to well understand its hidden representations.\n\n# 4.3 Sequence-Residual Result\n\nWe have shown that LLaMA could understand the interactions from the hidden representation, but it is hard to recover all items, since the number of candidates is very large and the hidden representation is highly compressed. Therefore, we design the sequenceresidual prompting framework, i.e., inspiring LLaAM to identify the residual item by comparing the representations before and after the sequence incorporates the said residual. We illustrate the result in Table 2, from which we can observe:\n\u2022 LLaMA could identify the residual item based on the hidden representations of DreamRec with high accuracy (97.89% at the MovieLens dataset and 86.33% at the Steam dataset.), which verifies the effectiveness of the proposed sequence-residual framework. Besides, the remarkable accuracy also suggests that DreamRec could better encode the interaction sequences, owing to the\n\nsuperiority of the diffusion model. Among the conventional recommenders, the accuracy of residual item identification is higher based on Caser and SASRec than GRU4Rec. The reason is similar to the analysis in the sequence-recovery framework, that Caser and SASRec employ more effective model architectures (CNN and Transformer encoder) than GRU4Rec (RNN).\nLLaMA also understands the MovieLens dataset better than the Steam data w.r.t. the sequence-residual task. The reason comes from that the titles of video games are more complicated than the titles of movies. However, DreramRec can also be well understood by LLaMA under the sequence-residual framework (with the accuracy of 86.33%), which further indicates that the complicated game titles are also distinguishable with the hidden representations learned by DreamRec.\n\n# 4.4 Instantiate Oracle items\n\nSince our sequence residual framework could identify the residual item from hidden representations before and after the sequence incorporates the residual item, we could replace the residual item with the generated oracle item by DreamRec. Accordingly, our RecInterpreter could provide a text description of the oracle item, and instantiate it as the recommendation results. Similar to the arguments in DreamRec [43] that the generated oracle items are not limited in the candidate set, we also discover that the instantiations of the oracle items through our RecInterpreter may not exist in the item set of the datasets. Specifically, we empirically find that 28.13% of the instantiations are beyond the movies in the MovieLens dataset, and 48.67% of the instantiations are beyond the video games in the Steam dataset. Therefore, it is hard to evaluate the performance of DreamRec based on the instantiation of RecInterpreter with traditional evaluation metrics such as Hit Ratio (HR) or normalized discounted cumulative gain (NDCG) [20]. To this end, we further conduct the evaluation with the assistance of ChatGPT. Specifically, given an interaction sequence of a user, SASRec could assign preference scores for the candidate items, and select the item with the highest score as the recommendation result. DreamRec could -generate an oracle item in the form of vector representation, which can be instantiated through the proposed RecInterpreter. Afterward, we could ask ChatGPT which item the user prefers among the recommended items provided by SASRec and DreamRec together with a randomly sampled item from the candidate set. An example of the prompts in the MovieLens dataset shows as follows:\n\nChatGPT Evaluation Prompt Example\n\u201cA person has watched a series of movies: <Watching History>. Which of the following movies does this person prefer? <Movie1>, <Movie2>, or <Movie3>. Please pick one.\u201d\n\n\u201cA person has watched a series of movies: <Watching History>. Which of the following movies does this person prefer? <Movie1>, <Movie2>, or <Movie3>. Please pick one.\u201d\n\nwhere <Watching History> is the movie titles in the interactions, and <Movie1>, <Movie2> and <Movie3> are the movie titles of the three recommendation result. The results are illustrated in Table 3. We can observe that under the evaluation of ChatGPT, DreamRec outperforms SASRec in the MovieLens dataset, and achieves comparable performance with\n\nLarge Language Model Can Interpret Latent Space of Sequential Recommender\n\nTable 3: The distribution of ChatGPT\u2019s selection from the random strategy, SASRec, and DreamRec.\n\nDataset\nRandom\nSASRec\nDreamRec\nMovieLens\n13.68%\n35.79%\n50.53%\nSteam\n0.76%\n51.15%\n48.09%\nSASRec in the Steam dataset. Besides, the learned recommenders SASRec and DreamRec both outperform the naive baseline of random sampling. This reasonable result suggests that RecInterpreter provides an effective approach to instantiate the generated oracle items of DreamRec, which completes the full promise of generative recommenders to provide recommendations beyond the constraint of candidate items.\n\n# 5 CONCLUSION AND LIMITATIONS\n\nWe propose RecInterpreter, inspiring LLMs to understand the hidden representation of conventional sequential recommenders. RecInterpreter draws inspiration from recent advances in multi-modal language models, that hidden representation of modality-specific encoders, such as image encoders and audio encoders, could be perceived by LLMs through simple projection. Therefore, RecInterpreter designs the sequence-recovery and sequence-residual promptings, allowing for LLaMA to understand the hidden representation of sequential recommenders. Besides, RecInterpreter provides a novel scheme to instantiate the generated oracle items, completing the full promise of generative recommendation. Meanwhile, RecInterpreter also has a few limitations: 1) the projection is simply set as a linear layer; and 2) the size of the datasets is not large enough. We believe these can be resolved in further research with more advanced adapters such as Q-former [22], and larger datasets, with sufficient computation resources. Moreover, as an initial attempt to inspire LLMs to understand the hidden representation of recommenders, RecInterpreter provides many research opportunities. For example, online service providers could apply RecInterpreter to their own recommenders, and explore the application of LLMs in their platforms from this perspective. Besides, researchers can design other prompting frameworks other than the sequence-recovery and sequence-residual, to better guide LLMs to understand sequential recommenders. Moreover, exploring the understanding of other recommenders with LLMs, such as collaborative filtering models and conversational recommenders, can also serve as a promising research direction.\n\n# REFERENCES\n\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. AI Open 4 (2023), 80\u201390.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00e9n Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. In NeurIPS.\n[3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems. CoRR abs/2308.08434\n\n[1] Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. AI Open 4 (2023), 80\u201390.\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Kar\u00e9n Simonyan. 2022. Flamingo: a Visual Language Model for Few-Shot Learning. In NeurIPS.\n[3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, and Qi Tian. 2023. A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems. CoRR abs/2308.08434\n\n(2023).\n[4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. In RecSys. ACM, 1007\u20131014.\n[5] Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H. Chi. 2018. Latent Cross: Making Use of Context in Recurrent Recommender Systems. In WSDM. 46\u201354.\n[6]  Andy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. 2021. HighPerformance Large-Scale Image Recognition Without Normalization. In ICML (Proceedings of Machine Learning Research, Vol. 139). 1059\u20131071.\n[7] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In RecSys. 191\u2013198.\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR. OpenReview.net.\n[9]  Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal Language Model. In ICML (Proceedings of Machine Learning Research, Vol. 202). 8469\u20138488.\n[10] Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of large language models (llms). arXiv preprint arXiv:2307.02046 (2023).\n[11] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt & Predict Paradigm (P5). In RecSys. ACM, 299\u2013315.\n[12] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. 2023. Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model. CoRR abs/2304.13731 (2023).\n[13] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, and Pheng-Ann Heng. 2023. Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following. CoRR abs/2309.00615 (2023).\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR. 770\u2013778.\n[15] Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023. Large language models as zero-shot conversational recommenders. arXiv preprint arXiv:2308.10053 (2023).\n[16] Bal\u00e1zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2016. Session-based Recommendations with Recurrent Neural Networks. In ICLR.\n[17] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 2023. 3D-LLM: Injecting the 3D World into Large Language Models. CoRR abs/2307.12981 (2023).\n[18] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023. Large language models are zero-shot rankers for recommender systems. arXiv preprint arXiv:2305.08845 (2023).\n[19] Yitong Ji, Aixin Sun, Jie Zhang, and Chenliang Li. 2023. A Critical Study on Data Leakage in Recommender System Offline Evaluation. ACM Trans. Inf. Syst. 41, 3 (2023), 75:1\u201375:27.\n[20]  Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM. 197\u2013206.\n[21]  Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. In NeurIPS.\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. In ICML (Proceedings of Machine Learning Research, Vol. 202). PMLR, 19730\u201319742.\n[23] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. 2023. VideoChat: Chat-Centric Video Understanding. CoRR abs/2305.06355 (2023).\n[24] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. 2022. Diffusion-LM Improves Controllable Text Generation. In NeurIPS.\n[25] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al. 2023. How Can Recommender Systems Benefit from Large Language Models: A Survey. arXiv preprint arXiv:2306.05817 (2023).\n[26] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023. ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation. arXiv preprint arXiv:2308.11131 (2023).\n\n[27] Junling Liu, Chaoyong Liu, Renjie Lv, Kangdi Zhou, and Yan Bin Zhang. 2023. Is ChatGPT a Good Recommender? A Preliminary Study. ArXiv abs/2304.10149 (2023). https://api.semanticscholar.org/CorpusID:258236609\n[28] Muhammad Maaz, Hanoona Abdul Rasheed, Salman H. Khan, and Fahad Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. CoRR abs/2306.05424 (2023).\n[29] Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Tushar Nagarajan, Matt Smith, Shashank Jain, Chun-Fu Yeh, Prakash Murugesan, Peyman Heidari, Yue Liu, et al. 2023. AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model. CoRR abs/2309.16058 (2023).\n[30] OpenAI. 2023. GPT-4 Technical Report. CoRR abs/2303.08774 (2023). [31] OpenAI. 2023. GPT-4V(ision) System Card. Retrieved September 25, 2023 from\nhttps://cdn.openai.com/papers/GPTV_System_Card.pdf\n[32] Razvan Pascanu, Tom\u00e1s Mikolov, and Yoshua Bengio. 2013. On the difficulty of training recurrent neural networks. In ICML (3), Vol. 28. 1310\u20131318.\n[33] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction Tuning with GPT-4. CoRR abs/2304.03277 (2023).\n[34] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR. IEEE, 10674\u201310685.\n[35] Jiaxi Tang and Ke Wang. 2018. Personalized top-n sequential recommendation via convolutional sequence embedding. In WSDM. 565\u2013573.\n[36] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971 (2023).\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS. 5998\u20136008.\n[38] Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, and Enhong Chen. 2023. Exploring large language model for graph data understanding in online job recommendations. arXiv preprint arXiv:2307.05722 (2023).\n\n[39] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A Survey on Large Language Models for Recommendation. arXiv preprint arXiv:2305.19860 (2023).\n[40]  Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. NExTGPT: Any-to-Any Multimodal LLM. CoRR abs/2309.05519 (2023).\n[41] Xu Xie, Fei Sun, Zhaoyang Liu, Shiwen Wu, Jinyang Gao, Jiandong Zhang, Bolin Ding, and Bin Cui. 2022. Contrastive Learning for Sequential Recommendation. In ICDE. 1259\u20131273.\n[42] Zhengyi Yang, Xiangnan He, Jizhi Zhang, Jiancan Wu, Xin Xin, Jiawei Chen, and Xiang Wang. 2023. A Generic Learning Framework for Sequential Recommendation with Distribution Shifts. In SIGIR. ACM, 331\u2013340.\n[43] Zhengyi Yang, Jiancan Wu, Zhicai Wang, Xiang Wang, Yancheng Yuan, and Xiangnan He. 2023. Generate What You Prefer: Reshaping Sequential Recommendation via Guided Diffusion. In NeurIPS.\n[44] An Zhang, Fangfu Liu, Wenchang Ma, Zhibo Cai, Xiang Wang, and Tat-Seng Chua. 2023. Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting. CoRR abs/2303.03187 (2023).\n[45] Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. In RecSys. ACM, 993\u2013999.\n[46] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023).\n[47] Yang Zhang, Fuli Feng, Xiangnan He, Tianxin Wei, Chonggang Song, Guohui Ling, and Yongdong Zhang. 2021. Causal Intervention for Leveraging Popularity Bias in Recommendation. In SIGIR. 11\u201320.\n[48] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).\n[49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models. CoRR abs/2304.10592 (2023).\n\n",
    "paper_type": "method",
    "attri": {
        "background": "Sequential recommendation is a fundamental task aimed at predicting the next item of interest for a user based on their interaction history with previous items. Traditional methods use discrete IDs to model item sequences, but recent advancements in large language models (LLMs) suggest a new approach may be necessary to interpret hidden representations from these recommenders.",
        "problem": {
            "definition": "The problem addressed in this paper is the inability of existing sequential recommendation methods to effectively interpret and utilize hidden representations from ID-based models.",
            "key obstacle": "The main challenge is that current LLMs primarily focus on user-item interactions as text prompts, preventing them from accessing and understanding the underlying hidden representations generated by recommender systems."
        },
        "idea": {
            "intuition": "The idea is inspired by the success of LLMs in understanding various modalities through alignment with text, prompting the question of whether LLMs can also decipher hidden representations from sequential recommenders.",
            "opinion": "The proposed idea involves using a framework called RecInterpreter to leverage LLMs for interpreting the hidden representation space of sequential recommenders, enabling better understanding and utilization of user interaction data.",
            "innovation": "RecInterpreter distinguishes itself from existing methods by directly mapping hidden representations into the token embedding space of LLMs, allowing them to generate textual descriptions based on these representations."
        },
        "method": {
            "method name": "RecInterpreter",
            "method abbreviation": "RI",
            "method definition": "RecInterpreter is a framework that employs a lightweight adapter to map hidden representations from sequential recommenders into the token embedding space of LLMs, facilitating their interpretation.",
            "method description": "The core of RecInterpreter is to inspire LLMs to understand and generate descriptions for hidden representations of sequential recommenders through sequence-recovery and sequence-residual prompting.",
            "method steps": [
                "Train a lightweight adapter to project hidden representations into the token embedding space of the LLM.",
                "Design sequence-recovery prompts to encourage the LLM to reconstruct item sequences from hidden representations.",
                "Utilize sequence-residual prompts to help the LLM identify residual items by contrasting representations before and after integrating new items."
            ],
            "principle": "The effectiveness of RecInterpreter lies in its ability to bridge the gap between the representation spaces of LLMs and sequential recommenders, allowing for a richer understanding of user behavior and preferences."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized two datasets, MovieLens and Steam, focusing on real-world recommendation scenarios. The MovieLens dataset contained 943 sequences and 100,000 interactions, while the Steam dataset had 11,938 sequences and 274,726 interactions.",
            "evaluation method": "The performance of RecInterpreter was assessed by measuring its ability to recover item sequences and identify residual items through designed prompts, with results compared against baseline methods."
        },
        "conclusion": "The experiments demonstrated that RecInterpreter significantly enhances the ability of LLaMA to understand hidden representations from sequential recommenders, with promising results in both sequence recovery and residual identification tasks.",
        "discussion": {
            "advantage": "The key advantages of RecInterpreter include its ability to effectively interpret hidden representations and generate meaningful textual descriptions, enabling improved recommendations beyond traditional methods.",
            "limitation": "Limitations include the simplicity of the projection method, which is currently a linear layer, and the relatively small size of the datasets used for training.",
            "future work": "Future research directions may involve developing more sophisticated adapter architectures and utilizing larger datasets to enhance the performance and applicability of RecInterpreter."
        },
        "other info": {
            "info1": "The framework is open-source and available at https://github.com/YangZhengyi98/RecInterpreter.",
            "info2": {
                "info2.1": "The study highlights the potential of LLMs in recommender systems, paving the way for further exploration in this area.",
                "info2.2": "Empirical results indicate that LLaMA exhibits a better understanding of simpler datasets, such as MovieLens, compared to more complex datasets like Steam."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The proposed idea involves using a framework called RecInterpreter to leverage LLMs for interpreting the hidden representation space of sequential recommenders, enabling better understanding and utilization of user interaction data."
        },
        {
            "section number": "4.2",
            "key information": "RecInterpreter distinguishes itself from existing methods by directly mapping hidden representations into the token embedding space of LLMs, allowing them to generate textual descriptions based on these representations."
        },
        {
            "section number": "5.1",
            "key information": "The effectiveness of RecInterpreter lies in its ability to bridge the gap between the representation spaces of LLMs and sequential recommenders, allowing for a richer understanding of user behavior and preferences."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions may involve developing more sophisticated adapter architectures and utilizing larger datasets to enhance the performance and applicability of RecInterpreter."
        },
        {
            "section number": "11",
            "key information": "The experiments demonstrated that RecInterpreter significantly enhances the ability of LLaMA to understand hidden representations from sequential recommenders, with promising results in both sequence recovery and residual identification tasks."
        }
    ],
    "similarity_score": 0.7751551277040303,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b201/b2011f4e-573f-4b15-99cc-5e26a1c88c39.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/357f/357fbeab-2d4e-45dd-9a52-d0f5a9de360f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba17/ba1757dd-212c-4054-b308-710977562dad.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/149d/149d783b-2cd7-42f7-a8e5-d46fd0a9a326.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1f69/1f69145a-a1fa-4559-bf09-b44980cc0b0b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d6d8/d6d8f212-082c-49a6-a1f7-57dccf347baa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7f23/7f230944-53eb-4a9b-9c9a-984179b28b3b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0a9b/0a9b08ea-9536-4548-a046-21cd1911d160.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fc2c/fc2ccb36-3dae-4b2c-8953-1e20b4687bcd.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large language model can interpret latent space of sequential recommender.json"
}