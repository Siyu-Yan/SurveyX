{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.13987",
    "title": "Focused Large Language Models are Stable Many-Shot Learners",
    "abstract": "In-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We theoretically and experimentally confirm that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FOCUSICL based on model perplexity of demonstrations. Comprehensive experiments validate that FOCUSICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with manyshot demonstrations.",
    "bib_name": "yuan2024focusedlargelanguagemodels",
    "md_text": "Peiwen Yuan1, Shaoxiong Feng2, Yiwei Li1, Xinglin Wang1, Yueqi Zhang1 Chuyi Tan1, Boyuan Pan2, Heda Wang2, Yao Hu2, Kan Li1\u2217 1School of Computer Science and Technology, Beijing Institute of Technology q1 r1 q2 r2 q qN rN d1 d2 dN _ ...\n{peiwenyuan,liyiwei,wangxinglin,zhangyq,tanchuyi,likan}@bit.edu.c {shaoxiongfeng2023,whd.thu}@gmail.com {panboyuan,xiahou}@xiaohongshu\n# Abstract\nIn-Context Learning (ICL) enables large language models (LLMs) to achieve rapid task adaptation by learning from demonstrations. With the increase in available context length of LLMs, recent experiments have shown that the performance of ICL does not necessarily scale well in many-shot (demonstration) settings. We theoretically and experimentally confirm that the reason lies in more demonstrations dispersing the model attention from the query, hindering its understanding of key content. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering to avoid attention being diverted by unimportant contents at token-level and operates hierarchical attention to further ensure sufficient attention towards current query at demonstration-level. We also design an efficient hyperparameter searching strategy for FOCUSICL based on model perplexity of demonstrations. Comprehensive experiments validate that FOCUSICL achieves an average performance improvement of 5.2% over vanilla ICL and scales well with manyshot demonstrations.\narXiv:2408.13987v1\n# 1 Introduction\nThe rapid development of large language models (LLMs) has facilitated the emergence and enhancement of their In-Context Learning (ICL) abilities (Wei et al., 2022a; Dong et al., 2023). As a trainingfree method, ICL can achieve fast model adaptation on specific tasks based on several demonstrations prefixed to the query, formally denoted as ICL(response|demos, query). Intuitively, more demonstrations can help LLMs better understand the task and increase the likelihood of finding demonstrations that aid in responding queries, thus leading to better performance. Theoretically, a similar conclusion can be drawn. Previous studies (Dai et al., 2023; Irie et al., 2022; von Oswald\n*Corresponding author.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ec28/ec287ac9-8995-4a23-a3b7-0f2862315d19.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The average model attention for query is dispersed by the increased number of demonstrations, causing inadequate understanding of query.</div>\net al., 2023; Aky\u00fcrek et al., 2023) have theoretically inferred that ICL can be viewed as an implicit finetuning process, with demonstrations analogous to training samples. On this basis, as finetuning has been validated to comply with the scaling law (Hernandez et al., 2021) where performance increases with the number of training samples, the performance of ICL should also positively correlates with the number of demonstrations, which has been experimentally verified by previous studies (Bertsch et al., 2024; Duan et al., 2023). However, with the increase in available context length of LLMs (Reid et al., 2024), some studies (Zhao et al., 2023; Agarwal et al., 2024) observe counterexamples when scaling the demonstration numbers from few-shot to many-shot. Agarwal et al. (2024) finds that the optimal number of demonstrations for six out of eleven benchmarks is not the maximum number they have tested. Our experimental results (Figure 5) also indicate that the model performance might decline with increased demonstrations when applying ICL, exhibiting an inverse-scaling phenomenon (McKenzie et al., 2023). These findings indicate that LLMs are not stable many-shot learners.\nTo understand this gap, we revisit the derivation of Dai et al. (2023) that formally equates ICL with finetuning and identify that their approximation of standard attention operation as linear attention operation will ignore the competition for attention between demonstrations and the query when generating the response. Since this approximation is key to the equivalence of ICL and finetuning, we hypothesize that the reason why ICL does not adhere to the scaling law like finetuning is that more demonstrations can divert attention away from the query. Inadequate attention and understanding of the query can naturally lead to inferior response. To verify our hypothesis, we first conduct experiments confirming that increasing the number of demonstrations does lead to a decrease in model attention towards queries (Figure 1). We further experiment by adding blank spaces within the demonstrations and confirm that: the more blank spaces added, the more attention towards queries distracted by blanks, resulting in lower response accuracy (Figure 2). Inspired by the way humans benefit from ignoring irrelevant contents and integrating insights from multiple examples when solving problems, we propose FOCUSICL to avoid the attention dispersion issue faced by ICL. Specifically, at the token-level, FOCUSICL conducts triviality filtering by adaptively masking unimportant tokens of demonstrations based on attention distribution, allocating the attention to more important contents. At the demonstration-level, FOCUSICL performs hierarchical attention mechanism by dividing demonstrations into multiple batches and respectively conducting intra-batch and inter-batch attention operations. The limited demonstration number within each batch ensures sufficient attention to the query, while inter-batch attention integrates the benefits from a larger number of demonstrations. We further introduce an efficient hyperparameter searching strategy for FOCUSICL according to model perplexity of demonstrations. Our experiments across three LLMs on five benchmarks confirm that FOCUSICL achieves an average performance improvement of 5.2% over ICL by avoiding attention dispersion, with lower inference overhead. This demonstrates the effectiveness, efficiency, and generalizability of FOCUSICL. Furthermore, we observe that FOCUSICL achieves performance scaling with the number of demonstrations by maintaining attention on critical parts, making demonstration number a possible\nscaling direction for LLM-based AGI. Finally, we propose a unified perspective to understand the divergent phenomena observed in previous studies, where more demonstrations lead to either improved (Bertsch et al., 2024) or deteriorated (Agarwal et al., 2024) performance in ICL. Based on experimental results, we conclude that the performance of ICL initially benefits but subsequently suffers from more demonstrations. The weaker the model and the closer the relationship between samples, the later the sweet spot for the number of demonstrations occurs. Our contributions are summarized as follows:\n1. We analyze that the reason more demonstrations may lead to a decline in ICL performance is that they degrade the model understanding of query by dispersing its attention. 2. We propose FOCUSICL to achieve rational attention allocation via triviality filtering operation and hierarchical attention mechanism, making LLMs stable many-shot learners. 3. We conduct comprehensive experiments and analyses to validate the effectiveness, efficiency, generalizability and scalability of FOCUSICL.\n# 2 Background\nFormalization of ICL We follow (Dong et al., 2023) to define the general ICL paradigm. Given an LLM M and a query q, we choose N demonstrations from a candidate set Sdemos = {(qi, ri)}M i=1 to attain the response r from M as follows:\n\ufffd \ufffd\ufffd \ufffd where Sampling(\u00b7) denotes certain sampling strategy and Cat[\u00b7] denotes the operation of concatenation.\n# Scaling Demonstration Number\ntions on context window (2048 \u223c4096), early studies (Brown et al., 2020; Lu et al., 2022) on ICL are limited to few-shot scenarios where they generally observe gains from more demonstrations. As the context window expands recently, counterexamples occur. Agarwal et al. (2024) finds that the best performance of Gemini 1.5 Pro is achieved under settings where demonstration number is not the maximum one tested in over half of the benchmarks. Zhao et al. (2023) discoveries that increasing the number of demonstrations does not nec-\nessarily improve model performance across five LLMs. We observe similar phenomena in Figure 5.\n# 3 Revisiting\nIn this section, we explore what impedes LLMs from becoming stable many-shot learners.\n# 3.1 Approximating ICL as Finetuning\nSince Dai et al. (2023) derives that ICL is formally equivalent to finetuning, with demonstrations analogous to training samples, we decide to revisit their derivation process below to explore why finetuning satisfies scaling laws (Hernandez et al., 2021) while ICL does not.\nFinetuning Let W 0, \u2206W FT \u2208Rdout\u00d7din be the initialized parameter matrix and the update matrix, and x \u2208Rdin be the input representation. The output of certain linear layer optimized by gradient descent can be formulated as follows:\n(2)\nICL For each attention head of M, let hi \u2208 Rdin be the representation of the ith input token, W q, W k, W v be the projection matrices for computing the queries, keys and values. We denote hi\u2208demosW k, hi\u2208demosW v, hi\u2208qW k, hi\u2208qW v as Dk, Dv, Qk, Qv, respectively. To generate r, the output of hr can be derived below:\n(3)\nDai et al. (2023) approximate the standard attention to linear attention by removing the softmax operation for ease of qualitative analysis. Since hrW qQvQ\u22a4 k is the attention result in the zeroshot learning (ZSL) setting and hrW qDvD\u22a4 k is the extra outcome from demonstrations, they are denoted as hrW ZSL and hr\u2206W ICL respectively. Comparing Eq. (3) with Eq. (2), we can understand ICL as finetuning by treating the \u2206W ICL generated from demonstrations as the \u2206W FT generated from training samples.\n3.2 Ignorance of Attention Competition From Eq. (3) we can further derive as follows:\nwhich means that the existence of demonstrations does not affect the outcome from q. However, when we no longer approximate standard attention as linear attention, we arrive at the opposite conclusion:\n(5)\nwhere:\n\ufffd \ufffd \ufffd  \ufffd \ufffd \ufffd With the existence of \u03bb(hr) in Eq. (5), an increase in the number of demonstrations will lead to a larger \u03bb(hr), thereby decreasing the model attention towards q. At the same time, ICL does not necessarily adhere to the scaling law as it is no longer formally equivalent to finetuning. Therefore, we hypothesize that more demonstrations can divert model attention from the key contents (query), leading to possible performance decrease.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d839/d83964c3-77d9-4f91-941c-b45d722f5e00.png\" style=\"width: 50%;\"></div>\nFigure 2: Accuracy and attention of LONGCHAT-7BV1.5-32K with varying number of spaces added per demonstration. Demonstration number is set as 100.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/48ed/48edc782-c3c5-497b-9a6b-709fb7fcfe0e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Overall illustration of FOCUSICL.</div>\n# 3.3 Experimental Evidence for Hypothesis\nTo validate our hypothesis, we first investigate whether the model attention towards the query decreases with the increase of demonstration number. To avoid potentially unreliable results caused by data contamination (Jiang et al., 2024), our exploratory experiments are conducted with longchat7b-v1.5 (Li et al., 2023a) (32k context window) on the proposed COUNTA benchmark (See details in \u00a75.1), which requires the model to Count the number of character \u2018A\u2019 in the five candidates. As shown in Figure 1, the average attention weight of model towards each token in the query decreases by scaling up the demonstration number, corresponding to Eq. (5). We further explore how the model\u2019s lack of attention towards the query affects the quality of the response. Specifically, we add several blank spaces at the end of each demonstration. This format maintains the ICL paradigm and the meaningless blank spaces will not introduce additional information. As shown in Figure 2, we find that more blank spaces disperse the model attention towards the query similar to the demonstrations, which in turn leads to a decline in accuracy. Based on the experiments above, we have confirmed our hypothesis.\n# 4 Methodology\nTo mitigate the impact of LLMs\u2019 attention being dispersed by many-shot demonstrations, we propose FOCUSICL. The core idea behind FOCUSICL is to allocate model attention to more important contents at token-level by triviality filtering (\u00a74.1) and at demonstration-level by hierarchical attention (\u00a74.2), as shown in Figure 3.\n# 4.1 Triviality Filtering\nHumans benefit from selectively ignoring irrelevant parts (trivialities) of demonstrations to avoid attention dispersion. In contrast, the standard attention mechanism of LLMs fails to completely ignore (assign zero attention weight to) trivialities and leverage the prior that the tokens of query are generally important, for which we propose triviality filtering operation. To predict response r for given query q, in each attention layer, we first calculate the attention scores s as follows:\n(7)\nInstead of directly applying softmax on s like standard attention operation, we filter the trivialities in the demonstrations according to a pre-set threshold p in advance as follows:\nwhere \u02c6hr is the outcome of hr. By applying triviality filtering operation, useless parts of demonstrations are assigned zero attention weights thus LLMs can focus on leveraging relevant contents of the demonstrations to solve the current query. To achieve a broad impact, apart from r, we also apply triviality filtering operation on tokens belong to responses of demonstrations by autoregressively treating {(qi, ri)}k\u22121 i=1 as demonstrations of (qk, rk), k \u2208[2, N].\n# 4.2 Hierarchical Attention\nWhen there are numerous examples, humans draw inspirations for problem-solving from different ex-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5d8/e5d86672-bb96-47af-ad38-ee14cd1a1381.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Input details of FOCUSICL.</div>\namples separately and then integrate the insights to avoid distracting attention by focusing on too many examples simultaneously. Motivated by this, we introduce hierarchical attention mechanism for LLMs to learn from many-shot demonstrations while focusing on current query. We first split the demonstrations into T batches, where each one comprises B consecutive demonstrations. Without editing the token order, we change the position indexes to ensure that each batch is logically adjacent to the query (Figure 4). To ensure that batches are mutually invisible to each other, we use a mask matrix, allowing us to parallelly apply intra-batch attention within each batch i and query as follows:\nBy controlling the batch size B, we can ensure that the model maintains enough attention towards the query within each batch. To further integrate insights from different batches, we conduct interbatch attention as follows:\n(10)\n\ufffd \ufffd The sum of the attention scores for all tokens within each batch can reflect the amount of useful information contained in that batch for the current query. Based on this, we calculate the weighted sum of \u02c6hi r to attain the final output of the attention layer.\n# 4.3 Hyperparameter Searching\nTo efficiently find suitable values of filtering threshold p and batch size B for different LLMs and tasks, we propose a hyperparameter searching strategy as shown in Algorithm 1. By treating qi as current query and S1:i\u22121 as demonstrations, the model perplexity 1 (ppl) of ri can reflect the LLMs\u2019 capability when demonstration number is i \u22121 (lower ppl indicates better performance). Thus, we choose the p that yields the lowest average ppl and B that first leads an increasing trend in ppl as our hyperparameter choices. We generally set Sp as\n1We don\u2019t use accuracy because the accuracy obtained under teacher forcing will overestimate the model performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7fb/d7fba2cf-802c-4384-89d4-12bb57d812fe.png\" style=\"width: 50%;\"></div>\nAlgorithm 1 Hyperparameter Searching.\nRequire: Candidate filtering threshold set Sp, LLM M\nDemonstration set Sdemos, Demonstration number N\nEnsure: Suitable filtering threshold p and batch size B\n1: D(p, i) \u21900 for p \u2208Sp, i \u2208[0, N \u22121]\n2: for p \u2208Sp do:\n3:\nfor i \u21901, 5 do:\n4:\nS1:N \u2190RandomSelect(Sdemos, N)\n5:\n# calculate average ppl of responses in S1:N\n6:\nppl1:N \u2190M(ICLFormat(S1:N))\n7:\nD(p, j\u22121) \u2190D(p, j\u22121)+pplj for j \u2208[1, N]\n8:\nend for\n9:\nD(p, i) \u2190D(p, i) + D(p, i + 1) for i \u2208[0, N \u22122]\n10:\n\u00afD(p, i) \u2190D(p, i) \u2212D(p, i \u22122) for i \u2208[2, N \u22122]\n11: end for\n12: p \u2190argmin(p|sum(D(p)))\n13: B \u2190argmin(i| \u00afD(p, i) > 0)\n[0, 0.1, 0.2, 0.3, 0.4] and run each setting 5 times to stabilize the results, resulting in a total of 25 inference overhead for hyperparameter searching, which is relatively low compared with the thousands of evaluation samples.\n# 5 Experiments\nCentered around FOCUSICL, we will empirically demonstrate its performance on different LLMs and tasks in \u00a75.2, verify whether it can help LLMs scale well with demonstration number in \u00a75.3, and delve into its working mechanism in \u00a75.4. We also investigate the choice of hyperparameters in Appendix \u00a7A.1.\n# 5.1 Experimental Settings Benchmarks We conduct experiments on the fol lowing benchmarks:\n\u2022 CSQA (Talmor et al., 2019) is a high-quality benchmark for commonsense reasoning task. \u2022 PIQA (Bisk et al., 2020) concentrates on testing physical commonsense answering ability. \u2022 CountA is our proposed benchmark to avoid the impact of data contamination (Jiang et al., 2024), making experimental results more comprehensive and reliable. It requires the model to count the number of character \u2019A\u2019 in the five candidates. \u2022 ARC (Clark et al., 2018) includes questions that require extensive knowledge and reasoning to answer. \u2022 GSM8K (Cobbe et al., 2021) serves as a testbed for evaluating multi-step mathematical reasoning (chain-of-thought) ability.\nMethod\nCSQA PIQA CountA ARC GSM8K Avg.\nICL\n47.58 57.42\n79.04\n62.43\n9.93\n51.28\nEARLYSTOP 47.89 57.44\n81.28\n62.43\n11.14\n52.04\nSTRUCTICL 50.25 59.02\n86.77\n64.05\n11.25\n54.27\nTRIVIALITY 48.97 58.65\n85.68\n63.13\n11.00\n53.49\nFOCUSICL 50.70 60.83\n91.94\n64.55\n12.28\n56.06\n<div style=\"text-align: center;\">CSQA PIQA CountA ARC GSM8K Avg.</div>\n<div style=\"text-align: center;\">Table 1: Accuracy (%) of LONGCHAT-7B-V1.5-32K with compared methods across benchmarks.</div>\nMethod\nCSQA PIQA CountA ARC GSM8K Avg.\nICL\n60.72 60.09\n82.20\n77.11\n16.30\n59.23\nEARLYSTOP 61.36 60.20\n82.20\n78.14\n17.44\n59.87\nSTRUCTICL 61.44 61.81\n84.78\n78.05\n17.12\n60.64\nTRIVIALITY 61.51 61.03\n84.43\n77.78\n17.36\n60.42\nFOCUSICL 62.57 67.88\n85.13\n78.51\n17.74\n62.37\nTable 2: Accuracy (%) of VICUNA-7B-V1.5-16K with compared methods across benchmarks.\nabove and use the training set as the demonstration candidate set Sdemos.\n\u2022 ICL. We use a unified ICL (Brown et al., 2020) input format for all the methods for fair comparisons, as shown in Appendix \u00a7E. \u2022 EARLYSTOP. Zhao et al. (2023) proposes to pick the optimal demonstration number according to the performance on a validation set. \u2022 STRUCTICL. Hao et al. (2022) share a similar idea with us of dividing demonstrations into batches. Differently, their designs focus on extending available context length.\n\u2022 ICL. We use a unified ICL (Brown et al., 2020) input format for all the methods for fair comparisons, as shown in Appendix \u00a7E. \u2022 EARLYSTOP. Zhao et al. (2023) proposes to pick the optimal demonstration number according to the performance on a validation set. \u2022 STRUCTICL. Hao et al. (2022) share a similar idea with us of dividing demonstrations into batches. Differently, their designs focus on extending available context length.\nDetails We conduct experiments with three widely used long-context LLMs: LONGCHAT-7BV1.5-32K (Li et al., 2023a), VICUNA-7B-V1.516K (Zheng et al., 2023) and LLAMA-3-8BINSTRUCT (AI@Meta, 2024). We choose the maximum available number of demonstrations for evaluation based on the 40 GB memory of the A100 GPU (Table 9). The hyper parameter searching results are listed in Table 11. We use random sampling decoding strategy (T=0.1) and report the outcomes averaged over 5 runs (randomly selecting demonstrations) for credible results.\n# 5.2 Main Results\nOur main experimental results are presented in Tables 1, 2, and 3. The compared methods exhibit similar performance trends across different LLMs.\n<div style=\"text-align: center;\">CSQA PIQA CountA ARC GSM8K Avg.</div>\nMethod\nCSQA PIQA CountA ARC GSM8K Avg.\nICL\n74.90 75.86\n98.10\n90.00\n66.64\n81.10\nEARLYSTOP 75.54 77.09\n98.10\n90.47\n71.21\n82.48\nSTRUCTICL 75.12 77.05\n98.16\n90.70\n69.43\n82.09\nTRIVIALITY 75.25 76.38\n98.22\n90.40\n68.03\n81.56\nFOCUSICL 76.00 78.29\n98.34\n91.02\n71.89\n83.11\nTable 3: Accuracy (%) of LLAMA-3-8B-INSTRUCT with compared methods across benchmarks.\nBaselines Under most settings, EARLYSTOP outperforms ICL, consistent with the observations of Agarwal et al. (2024) and Zhao et al. (2023) that more demonstrations does not necessarily lead to better performance. Compared to EARLYSTOP which avoids the negative impact of attention dispersion by not introducing more demonstrations, STRUCTICL leverages all the given demonstrations through structured input to achieve slightly better performance.\nOurs However, due to the lack of insights into the reasons behind performance degradation of ICL with more demonstrations, the baselines fail to maintain the model attention on critical input parts while fully leveraging all demonstrations. In contrast, by introducing triviality filtering operation and hierarchical attention mechanism to achieve the above vision, FOCUSICL outperforms the compared baselines, achieving an average of 5.2% (3.31 points) performance improvement over ICL across three LLMs. The results of the T-test also indicate that FOCUSICL is significantly superior to baselines, with a p-value less than 0.05. This validates the effectiveness and generalizability of FOCUSICL.\nAblations We also report the performance of only performing triviality filtering operation as an ablation study. The results show that FOCUSICL benefits 1.29 points improvement from the triviality filtering operation and 2.02 points improvement from the hierarchical attention mechanism.\nEfficiency By performing hierarchical attention mechanism, demonstrations between different batches does not need direct interactions, which can save a significant amount of inference overhead. Assuming each demonstration has an average of L tokens, the overhead of attention operation between N demonstrations for ICL is:\n(11)\nwhere \u2206denotes a computational cost unit. The overhead for FOCUSICL with batch size as B is:\nTherefore, the overhead ratio of FOCUSICL to ICL in encoding demonstrations is B : N (N is generally several times larger than B), while the overhead in other aspects is roughly the same. This demonstrates the efficiency of FOCUSICL.\n# 5.3 Scaling with More Demonstrations\nThe recent significant advancements in LLMs mainly stem from scaling up in dimensions of model size and training data size. However, given the limitations of computation resource and data production speed, we are in eager need of exploring other potential scaling dimensions to continuously enhance the performance of LLMs. As shown in Figure 5, the demonstration number is not a stable scaling dimension when applying ICL, as the performance can sometimes exhibit an inverse-scaling phenomenon with more demonstrations. In contrast, FOCUSICL enables LLMs to become stable many-shot learners by directing their attention to important contents, thereby achieving good scalability in the dimension of demonstration number. It should be noted that we find the advantage of FOCUSICL over ICL continues to grow as the number of demonstrations increases. This means that if we have more resources to conduct experiments with more demonstrations, the advantage of FOCUSICL over ICL can be larger.\n# 5.4 Working Mechanism of FOCUSICL\nTo gain a deeper understanding of the working mechanism of FOCUSICL, we explore it from aspects of attention and hidden state distributions, following the experimental settings in \u00a73.3.\nAttention Distribution The primary purpose of FOCUSICL is to prevent the model attention from being scattered by the increased demonstrations, thereby ensuring a proper understanding of key contents. Therefore, we observe the attention weights allocated by the model towards the query as the number of demonstrations increases. As shown in Figure 6, by ignoring unimportant parts of the demonstrations and introducing the hierarchical attention mechanism, FOCUSICL consistently maintains sufficient attention towards the query.\nHidden States Distribution We further investigate the distribution of the hidden states of the last input token at the penultimate model layer through Principal Component Analysis (PCA). Intuitively, the distribution of the hidden states of the last token mainly depends on the current problem to be solved and should be independent of the demonstration number. However, as shown in Figure 7, we find that the hidden states of ICL change systematically with an increasing number of demonstrations, whereas FOCUSICL does not exhibit such behavior. We think that the systematic decline in attention towards the query in ICL with an increasing number of demonstrations continuously affects the hidden states during response generation, thereby impacting the quality of the generated response. In contrast, FOCUSICL avoids this issue by maintaining sufficient attention to the query as shown above, ultimately benefiting from more demonstrations.\n# 5.5 Further Discussion\nBased on our existing insights and experimental results, we attempt to understand the divergent phenomena of ICL observed in previous studies where more demonstrations sometimes lead to better performance, while sometimes the opposite occurs. We think the main reason leading to the above phenomena comes from the double-edged sword effect of learning from more demonstrations: on the one hand, they can help the model better understand the task and increase the likelihood of finding useful knowledge; on the other hand, they might also distract the model, leading to insufficient attention and understanding of current query. We consider that two aspects can influence the balance between the two effects:\n# Weak models require more demonstrations to understand the task. As shown in Figure 5, we\nunderstand the task. As shown in Figure 5, we observe that the optimal number of demonstrations for LONGCHAT-7B-V1.5-32K is greater compared to the other two models across most benchmarks. Considering that its performance is also the worst, we believe the reason for the aforementioned situation is that weaker models require more demonstrations to help them better understand the task.\nMore demonstrations are needed when they have a closer relationship. We also notice that the LLMs are more demonstration-hungry on CountA compared to other benchmarks as shown in Figure 5. We analyze that the correlation between samples in other benchmarks is relatively\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b7d4/b7d466c1-9f53-4953-9175-e2f4cb423b82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Accuracy Demonstration Number</div>\n<div style=\"text-align: center;\">Figure 5: FOCUSICL helps different LLMs scale well with many-shot demonstrations compared with ICL.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/69a4/69a42601-cd70-479e-ac81-1b17b492ef87.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Average model attention towards token of q with varying demonstration numbers.</div>\nweak, and even a single demonstration is sufficient to clarify the task format. In contrast, the demonstrations in CountA are closely related, collectively determining what the task definition is. In this scenario, LLMs cannot discern the complete task information if only given a few demonstrations. To sum up, when the samples are closely related, the model needs more demonstrations to analyze the correlations among them, so as to better understand and complete the task.\n# 6 Conclusions\nNoticing that the performance of LLMs under many-shot ICL does not consistently improve with more demonstrations, we analyze and validate the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2c78/2c78370a-726c-4768-9b42-1fe83c1b07c8.png\" style=\"width: 50%;\"></div>\nFigure 7: The PCA distribution results of the hidden states of the last input token from the penultimate layer of ICL (above) and FOCUSICL (below) with varying numbers of demonstrations.\nunderlying reason as follows: more demonstrations can disperse the model attention to critical con-\ntents, resulting in an insufficient understanding of the query. Inspired by how humans learn from examples, we propose a training-free method FOCUSICL, which conducts triviality filtering at tokenlevel and hierarchical attention at demonstrationlevel to rationally allocate model attention in each layer. Comprehensive experiments indicate that focused LLMs are stable many-shot learners, making demonstration number a possible scaling dimension for LLM-based AGI.\n# Limitations\nFrom an objective perspective , we think there are two main limitations of this paper:\n1. Although we have extended the demonstration number to nearly or even beyond 100, due to computational resource limitations, we are unable to conduct experiments with larger demonstration numbers. We will further verify the applicability of FOCUSICL with larger demonstration numbers in the future.\n2. This work primarily discusses LLMs that apply the standard transformer decoder architecture. We look forward to further exploring the scaling performance with the demonstration number and the applicability of FOCUSICL on other variants of LLMs, such as the encoder-decoder architecture and sliding window attention, in the future.\n# Ethics Statement\nAll of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study.\n# References\nRishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Stephanie C. Y. Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal M. P. Behbahani, Aleksandra Faust, and Hugo Larochelle. 2024. Many-shot in-context learning. CoRR, abs/2404.11018.\n# AI@Meta. 2024. Llama 3 model card.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023. What learning algorithm is in-context learning? investigations\nwith linear models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew R Gormley, and Graham Neubig. 2024. Incontext learning with long-context models: An indepth exploration. arXiv preprint arXiv:2405.00200. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7432\u2013 7439. AAAI Press. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 4005\u2013 4019. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234. Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, and Kar Yan Tam. 2023. Exploring the relationship between in-context learning and instruction tuning. CoRR, abs/2311.10367.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. 2021. Scaling laws for transfer. CoRR, abs/2102.01293.\nKazuki Irie, R\u00f3bert Csord\u00e1s, and J\u00fcrgen Schmidhuber. 2022. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9639\u20139659. PMLR.\nMinhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and Sanmi Koyejo. 2024. Investigating data contamination for pretraining language models. CoRR, abs/2401.06059.\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023a. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following.\n# Yiwei Li, Shaoxiong Feng, Bin Sun, and Kan Li. 2023b. Heterogeneous-branch collaborative learning for di-\nYiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin Sun, Heda Wang, and Kan Li. 2024b. Escape sky-high cost: Early-stopping selfconsistency for multi-step reasoning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net.\nHui Liu, Wenya Wang, Hao Sun, Chris Xing Tian, Chenqi Kong, Xin Dong, and Haoliang Li. 2024. Unraveling the mechanics of learning-based demonstration selection for in-context learning. CoRR, abs/2406.11890.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086\u2013 8098. Association for Computational Linguistics.\nan R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu, Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, Derik Kauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The Floating Droid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim, Samuel R. Bowman, and Ethan Perez. 2023. Inverse scaling: When bigger isn\u2019t better. CoRR, abs/2306.09479.\nMachel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530.\nhad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 2655\u20132671. Association for Computational Linguistics.\nBin Sun, Yitong Li, Fei Mi, Weichao Wang, Yiwei Li, and Kan Li. 2023. Towards diverse, relevant and coherent open-domain dialogue generation via hybrid latent variables. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 13600\u201313608. AAAI Press.\nBin Sun, Yitong Li, Fei Mi, Weichao Wang, Yiwei Li, and Kan Li. 2023. Towards diverse, relevant and coherent open-domain dialogue generation via hybrid latent variables. In Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023, Washington, DC, USA, February 7-14, 2023, pages 13600\u201313608. AAAI Press. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. Commonsenseqa: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4149\u20134158. Association for Computational Linguistics. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 35151\u201335174. PMLR. Xinglin Wang, Yiwei Li, Shaoxiong Feng, Peiwen Yuan, Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024. Integrate the essence and eliminate the dross: Finegrained self-consistency for free-form language generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 11782\u201311794. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022b. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022.\nPeiwen Yuan, Xinglin Wang, Shaoxiong Feng, Boyuan Pan, Yiwei Li, Heda Wang, Xupeng Miao, and Kan Li. 2024c. Generative dense retrieval: Memory can be a burden. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julian\u2019s, Malta, March 17-22, 2024, pages 2835\u20132845. Association for Computational Linguistics.\nFei Zhao, Taotian Pang, Zhen Wu, Zheng Ma, Shujian Huang, and Xinyu Dai. 2023. Dynamic demonstrations controller for in-context learning. CoRR, abs/2310.00385.\nianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023.\n# A Additional Experimental Results\n# A.1 Hyperparameters\nTo investigate the influence of hyperparameters, we report the results of LONGCHAT-7B-V1.5-32K on GSM8K benchmark with varying hyperparameter settings.\nFiltering Threshold As shown in Table 7, with the increase of filtering threshold p, the model\u2019s performance first improves and then declines. This is because, when p is relatively small, the model benefits from ignoring unimportant content and focusing its attention on more beneficial parts. However, when p becomes larger, the model might overlook potentially useful information in the demonstrations, leading to a decrease in performance.\nBatch Size As shown in Table 8, a similar inverted U-shaped curve phenomenon occurs when scaling the batch size while maintaining the overall demonstration number fixed. As the batch size decreases from 80, the model attention to the query continues to increase, which can lead to a certain improvement in model performance. However, when the batch size is too small, the model may fail to fully understand the task definition due to excessive lack of interaction between demonstrations, consistent with the findings of Bertsch et al. (2024). Luckily, through our proposed hyperparameter searching strategy, we can efficiently attain suitable hyperparameters for the given tasks and LLMs.\n# A.2 Further Analyses of TRIVIALITY\nWhen we identify tokens that are unhelpful for answering the current query through attention, TRIVIALITY directly masks them to prevent the model\u2019s attention from being distracted. Another more intuitive approach is to filter out demonstrations with minimal attention. We compared these two methods, and the results are shown in the Table 4. It can be seen that TRIVIALITY, which operates at a finer granularity at the token level, achieves better results. Additionally, we conducted the following experiments to further validate the motivation that tokens with low attention are unimportant and should be masked. We set the following settings below on CountA with LONGCHAT-7B-V1.5-32K:\n\u2022 No Masking.\n\u2022 Masking 40% of tokens with the lowest attention. \u2022 Masking 40% of tokens with the highest attention. \u2022 Randomly masking 40% of tokens.\nThe experimental results in Table 5 demonstrate the following: compared to No Masking, randomly masking reduces accuracy from 79.04% to 35.00%. Masking high-attention tokens leads the model to repeatedly output the word \u2019nobody\u2019, indicating a loss of problem-solving ability. Conversely, masking low-attention tokens significantly improves performance. To further analyze the underlying reasons, we calculated the model\u2019s perplexity across different settings. We found that random masking and masking high-attention tokens significantly increase model perplexity, likely due to the loss of critical token information. In contrast, masking lowattention tokens decreases model perplexity, indicating that filtering trivial tokens based on posterior attention information helps the model perform tasks more confidently.\nMethod ICL ICL-DROP TRIVIALITY FOCUSICL\nAccuracy 9.93\n10.79\n11.00\n12.28\nTable 4: Accuracy (%) of different methods on GSM8K with LONGCHAT-7B-V1.5-32K. ICL-drop indicates the ICL method with dropping the 10 demonstrations with lowest average attention weights.\nMethod\nAccuracy PPL\nNo Masking\n79.04\n0.610\nMask Low-attention Tokens\n85.68\n0.572\nMask High-attention Tokens\n0.00\n10.921\nRandom Masking\n35.00\n1.636\nTable 5: Accuracy (%) of different methods on GSM8K with LONGCHAT-7B-V1.5-32K.\n# A.3 FOCUSICL with Demonstrations Retrieval\nPrevious research (Rubin et al., 2022; Liu et al., 2024; Ye et al., 2023) have shown that selecting demonstrations relevant to the current query can enhance the performance of ICL. We investigated whether combining FOCUSICL with demonstration retrieval could yield better results. For simplicity, we used BERT embeddings rather than other complex retrieval methods (Yuan et al., 2024c) to\nretrieve the most relevant demonstrations. We then compared the experimental results using both ICL and FocusICL, as shown in Table 6. Retrieving relevant demonstrations resulted in a 1.13% improvement for ICL and a 1.53% enhancement for FocusICL. This improvement is likely attributed to the hierarchical attention mechanism\u2019s ability to more effectively utilize demonstrations with substantial informative content.\nMethod\nICL FOCUSICL\nRandom Demonstrations\n47.58\n50.70\nRelevant Demonstrations\n48.71\n52.23\nTable 6: Accuracy (%) of different methods on CSQA with LONGCHAT-7B-V1.5-32K.\n# B Derivation Details\nThe derivation details of Equation (5) are as follows:\n \u2212 \ufffd \ufffd\ufffd outcome from q + \u03bb(hr) Att (hrW q, Dk, Dv) \ufffd \ufffd\ufffd \ufffd outcome from demos ,\n(13)\nFiltering Threshold 0.0\n0.1\n0.2\n0.3\n0.4\nFOCUSICL\n11.90 12.28 12.03 12.05 11.88\nTable 7: Accuracy (%) of LONGCHAT-7B-V1.5-32K when applying FOCUSICL with varying filtering threshold and batch size as 8.\nBatch Size\n2\n4\n8\n16\n80\nFOCUSICL 10.46 10.99 12.28 11.45 11.00\nTable 8: Accuracy (%) of LONGCHAT-7B-V1.5-32K when applying FOCUSICL with varying batch sizes and filtering threshold as 0.1. It should be noted that the overall demonstration number is fixed as 80.\nwhere:\n# C Inverse-scaling Phenomena with Gemini\nDue to the limitations of computational resources and the unavailability of closed-source models, our experiments are primarily conducted on 7-8B open source LLMs. However, by utilizing APIs, we additionally explore the performance changes of more powerful models as the number of demonstrations increased, further validating the generalizability of the argument that LLMs are not stable many-shot learners. We choose to experiment with GEMINI 1.5 PRO for its long available context window (1M tokens). We test GEMINI 1.5 PRO on MATH benchmark (Hendrycks et al., 2021), which contains 7 subsets with 5 difficulty levels that can thoroughly evaluating the math reasoning abilities of LLMs. We use greedy searching decoding strategy with and report the outcomes averaged over 5 runs for credible results. As shown in Figure 8, obvious inverse-scaling phenomenon appears in 5 out of 7 subsets, with Precalculus and Intermediate Algebra as exceptions. This validates the generalizability of the argument that LLMs are not stable many-shot learners. Meanwhile, we observe that across different difficulty levels, GEMINI 1.5 PRO presents similar performance changing trends. Figure 8 clearly shows such phenomenon. This indicates that the task difficulty does not affects the optimal demonstration number of certain task.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a6c/9a6c3a37-37dc-4256-a7ca-b93216aaa568.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(g) Precalculus</div>\n<div style=\"text-align: center;\">Figure 8: Performance of Gemini on different subset of MATH with varying demonstration numbers.</div>\n# D Further Discussions\nFocusICL can be seen as a method that achieves performance gains through increased computation (more demonstrations). Similar approaches include Self-Consistency (Wang et al., 2023, 2024; Li et al., 2024b,a) and Chain-of-Thought (Wei et al., 2022b). In our experiments, we have confirmed that the\n<div style=\"text-align: center;\">(h) Average</div>\ngains brought by FOCUSICL are decoupled from those of Chain-of-Thought. We will further explore the interplay between FOCUSICL and other methods in the future. We tested the performance of FOCUSICL in tasks such as QA and inference in the experimental section. In the future, we will delve into exploring\nMethod CSQA PIQA CountA ARC GSM8K\nN\n128\n96\n448\n108\n80\nTable 9: The total demonstration number N of different\nbenchmarks in our experiments.\nMethod\nCSQA PIQA CountA ARC GSM8K\nTraining size 9741 16113\n3000\n2241\n7473\nTesting size\n1221\n1838\n1000\n567\n1319\n<div style=\"text-align: center;\">Method CSQA PIQA CountA ARC GSM8K</div>\nMethod CSQA PIQA CountA ARC GSM8K\nN\n128\n96\n448\n108\n80\n<div style=\"text-align: center;\">Table 9: The total demonstration number N of different benchmarks in our experiments.</div>\nMethod\nCSQA PIQA CountA ARC GSM8K\nTraining size 9741 16113\n3000\n2241\n7473\nTesting size\n1221\n1838\n1000\n567\n1319\nthe application of FOCUSICL in evaluation (Yuan et al., 2024b,a, 2023) and dialogue (Li et al., 2022; Sun et al., 2023; Li et al., 2023b) tasks.\n# E Prompt Template\nThe following is a template ICL input format when demonstration number is 2.\n### Human: I\u2019m getting warm because I increased the thermostat in my bedroom. What might I be doing soon? Answer Choices: (a) feeling comfortable (b) overheat (c) increase of temperature (d) pleasure (e) starting fire\n# ### Assistant: A\n### Human: Where might I hear and see information on current events? Answer Choices: (a) internet (b) television (c) newspaper (d) book (e) radio\n### Assistant: B\n# ### Assistant: B\n### Human: If somebody buys something and gives it to me as a free gift, what is the cost status of the gift? Answer Choices: (a) deadly (b) imprisoned (c) paid for (d) expensive (e) in prison\n### Assistant:\nModel\nLONGCHAT-7B VICUNA-7B LLAMA-3-8B\n-V1.5-32K\n-V1.5-16K\n-INSTRUCT\nParams\np\nB\np\nB\np\nB\nCSQA\n0.1\n32\n0.2\n16\n0.4\n32\nPIQA\n0.1\n32\n0.1\n8\n0.4\n2\nCountA 0.4\n112\n0.4\n224\n0.4\n112\nARC\n0.4\n16\n0.4\n0.1\n0.4\n12\nGSM8K 0.1\n8\n0.1\n8\n0.4\n8\nTable 11: The results of hyperparameter searching strategy across varing tasks and LLMs.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of In-Context Learning (ICL) in large language models (LLMs), highlighting that while ICL allows for rapid task adaptation through demonstrations, its performance does not scale well with many-shot settings. Previous methods have shown that increasing the number of demonstrations can disperse model attention away from queries, leading to poorer performance, thus necessitating a new approach.",
        "problem": {
            "definition": "The problem addressed in this paper is the decline in performance of LLMs when the number of demonstrations in ICL increases, which leads to inadequate understanding of the query due to attention dispersion.",
            "key obstacle": "The main difficulty is that existing methods do not effectively manage the attention of LLMs, causing critical content related to the query to be overshadowed by irrelevant information from additional demonstrations."
        },
        "idea": {
            "intuition": "The idea for FOCUSICL was inspired by human learning processes, where ignoring irrelevant information and focusing on important examples can enhance understanding and problem-solving.",
            "opinion": "FOCUSICL is a training-free method that employs triviality filtering and hierarchical attention to allocate the model's attention more effectively, thereby improving performance in many-shot demonstration scenarios.",
            "innovation": "The key innovation of FOCUSICL lies in its dual approach: triviality filtering at the token level to mask unimportant content, and hierarchical attention at the demonstration level to ensure sufficient focus on relevant examples, contrasting with traditional ICL methods."
        },
        "method": {
            "method name": "FOCUSICL",
            "method abbreviation": "FOCUSICL",
            "method definition": "FOCUSICL is a method designed to optimize attention allocation in LLMs by filtering out trivial content and employing a hierarchical attention mechanism to enhance understanding of queries in many-shot settings.",
            "method description": "FOCUSICL combines triviality filtering and hierarchical attention to maintain model focus on critical content while benefiting from multiple demonstrations.",
            "method steps": [
                "Calculate attention scores for tokens in demonstrations.",
                "Apply triviality filtering to mask unimportant tokens based on a threshold.",
                "Split demonstrations into batches for hierarchical attention.",
                "Perform intra-batch attention to focus on the current query.",
                "Integrate insights from different batches through inter-batch attention."
            ],
            "principle": "The effectiveness of FOCUSICL is based on the principle that by minimizing attention on irrelevant content and optimizing focus on the query, the model can achieve better understanding and performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on various benchmarks including CSQA, PIQA, CountA, ARC, and GSM8K using three LLMs: LONGCHAT-7B-V1.5-32K, VICUNA-7B-V1.5-16K, and LLAMA-3-8B-INSTRUCT.",
            "evaluation method": "The performance of FOCUSICL was evaluated by comparing accuracy across benchmarks against baseline methods, measuring how well the model maintained attention on the query with increasing demonstration numbers."
        },
        "conclusion": "The experiments demonstrate that FOCUSICL improves average performance by 5.2% over traditional ICL by effectively managing attention, confirming its potential as a stable many-shot learner and suggesting that demonstration number can be a viable scaling dimension for LLMs.",
        "discussion": {
            "advantage": "FOCUSICL stands out by effectively preventing attention dispersion, allowing LLMs to leverage multiple demonstrations without losing focus on critical content, leading to improved performance.",
            "limitation": "The main limitation is the inability to test FOCUSICL with larger demonstration numbers due to computational constraints, which may affect the generalizability of results.",
            "future work": "Future research will explore the applicability of FOCUSICL with larger demonstration sizes and its performance on other LLM architectures, such as encoder-decoder models."
        },
        "other info": [
            {
                "info1": "The datasets used in the study were publicly available and did not contain harmful content."
            },
            {
                "info2": {
                    "info2.1": "The authors confirm that no annotators were employed for data collection.",
                    "info2.2": "The study ensures compliance with ethical standards in research."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-Context Learning (ICL) allows for rapid task adaptation through demonstrations, but its performance does not scale well with many-shot settings."
        },
        {
            "section number": "1.3",
            "key information": "Large language models (LLMs) are crucial in ICL, but increasing the number of demonstrations can disperse model attention away from queries, leading to poorer performance."
        },
        {
            "section number": "3.1",
            "key information": "FOCUSICL is a method designed to optimize attention allocation in LLMs by filtering out trivial content and employing a hierarchical attention mechanism."
        },
        {
            "section number": "3.2",
            "key information": "The effectiveness of FOCUSICL is based on minimizing attention on irrelevant content and optimizing focus on the query to achieve better understanding and performance."
        },
        {
            "section number": "4.1",
            "key information": "FOCUSICL employs triviality filtering and hierarchical attention to enhance understanding in many-shot demonstration scenarios."
        },
        {
            "section number": "6.1",
            "key information": "Existing methods do not effectively manage the attention of LLMs, causing critical content related to the query to be overshadowed by irrelevant information."
        },
        {
            "section number": "6.4",
            "key information": "Future research will explore the applicability of FOCUSICL with larger demonstration sizes and its performance on other LLM architectures."
        }
    ],
    "similarity_score": 0.7235472689157487,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Focused Large Language Models are Stable Many-Shot Learners.json"
}