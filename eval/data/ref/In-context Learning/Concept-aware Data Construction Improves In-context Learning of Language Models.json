{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.09703",
    "title": "Concept-aware Data Construction Improves In-context Learning of Language Models",
    "abstract": "Many recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs' ability to perform a new task solely from natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in small-scale, synthetic settings.\n  In this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (CoAT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using CoAT, pre-trained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functional deficiencies of the previous models. Finally, we show that concept-aware in-context learning is more effective for a majority of new tasks when compared to traditional instruction tuning, resulting in a performance comparable to the previous in-context learners using magnitudes of more training data.",
    "bib_name": "tefnik2024conceptawaredataconstructionimproves",
    "md_text": "# Concept-aware Data Construction Improves In-context Learning of Language Models\n\nMichal \u0160tef\u00e1nik \u2663\u2217 and Marek Kadl\u02c7c\u00edk \u2663 and Petr Sojka \u2663\n\n\u2663 Faculty of Informatics Masaryk University, Czech Republic\n\nAbstract\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0657/0657810e-07bf-442e-97ae-5fb4032892ec.png\" style=\"width: 50%;\"></div>\nMany recent language models (LMs) are capable of in-context learning (ICL), manifested in the LMs\u2019 ability to perform a new task solely from a natural-language instruction. Previous work curating in-context learners assumes that ICL emerges from a vast over-parametrization or the scale of multi-task training. However, recent theoretical work attributes the ICL ability to concept-dependent training data and creates functional in-context learners even in smallscale, synthetic settings.\nIn this work, we practically explore this newly identified axis of ICL quality. We propose Concept-aware Training (C o AT), a framework for constructing training scenarios that make it beneficial for the LM to learn to utilize the analogical reasoning concepts from demonstrations. We find that by using C o AT, pretrained transformers can learn to better utilise new latent concepts from demonstrations and that such ability makes ICL more robust to the functional deficiencies of the previous models.\nFinally, we show that concept-aware in-context learners are much more e ff ective in in-context learning a majority of unseen tasks compared to traditional instruction tuning, and fare comparably also to previous in-context learners trained in large-scale multitask learning requiring magnitudes of more training data.\n\n# 1 Introduction\n\nThe in-context learning (ICL), as initially uncovered by Brown et al. (2020), is a setting requiring language models (LMs) to infer and apply correct functional relationships from the pairs of inputs and outputs (i.e. demonstrations) presented in userprovided input prompt (Li et al., 2023b). Given that a small set of demonstrations can be obtained for any machine learning task, in-context learning presents a much more versatile and practical alternative to training task-specific models.\n* Corresponding author: stefanik.m@mail.muni.cz\n\n<div style=\"text-align: center;\">Predicted label:\n</div>\nFigure 1: Example of training instruction constructed from synthetic TeaBReaC dataset where demonstrations share analogical reasoning chain. In Concept-aware Training (C o AT), we construct such examples to train in-context learners to utilise latent reasoning concepts whenever available in demonstrations.\nModern in-context learners may perform ICL with quality comparable to task-specialized models (Zhao et al., 2023; \u0160tef\u00e1nik et al., 2023). However, it remains unclear why some LMs are able of ICL in such quality while others are not; Initial work introducing GPT3 (Brown et al., 2020) followed by Thoppilan et al. (2022); Chowdhery et al. (2022); inter alia explains ICL as an emergent consequence of models\u2019 scale. But more recent LMs (Sanh et al., 2022; Wang et al., 2022; Wei et al., 2021; Ouyang et al., 2022) are based on 10 to 100 times smaller models and reach comparable ICL quality, instead attributing the ICL ability to a vast volume and diversity of pre-training tasks and instructions. On the contrary, theoretical studies uncover different determinants of ICL quality than the model or data scale, relating ICL to specific data qualities, such as the occurrence of cases that can not be explained by mere statistical co-occurrence of tokens. Notably, Xie et al. (2022) specify this as the occurrence of training exemplars that can only be resolved by identifying latent concepts, i.e. underlying functional relations that explain the correct prediction. In this and other work surveyed in Sec\n\ntion 2, authors prove that ICL can also emerge with both small data and small models. Our work explores the practical potential of concept-dependent data on the quality and robustness of in-context learning. In Section 3, we propose and implement a data construction framework that encourages  the occurrence of concept dependencies in training data, and hence, requires  models to learn to utilise latent concepts that explain these irregularities (Fig. 1). We call this framework Concept-aware Training (CoAT). In Sections 4, we explore the impact of C o AT in controlled settings. We find that (i) it is possible to train language models for in-context learning of unseen  latent concepts and (ii) that such conceptaware in-context learning is more robust  to the functional deficiencies of existing in-context learners. Finally, on a set of over 70 tasks of SuperGLUE and Natural-Instructions, we find that C o AT can also improve practical in-context learning performance over traditional instruction tuning approach; in many cases, C o AT enables ICL of otherwise not learnable tasks, and with only two training tasks reaches ICL performance comparable to in-context learners of similar or larger size trained on massive collections of over 1,600 tasks.\n\n# 2 Background\n\nMethods for training in-context learners Incontext learning ability, including few-shot ICL, was first uncovered in GPT3 (Brown et al., 2020) trained unsupervisedly for causal language modelling. With no other substantial di ff erences to previous GPT models, the emergence of ICL was attributed to GPT3\u2019s scale, having grown to over 170billion parameters since GPT2 (\u2248 800M params). Not long after, a pivotal work of Schick and Sch\u00fctze (2020) on a Pattern-exploiting training (PET) has shown that even much smaller (110M) models like BERT (Devlin et al., 2019) can be finetuned using self-training in a similarly small data regime, first disputing the assumption on the necessity of the scale in rapidly learning new tasks. A line of generation models further undermined the assumption of the size conditioning of ICL. Among the first, Min et al. (2022a) fine-tune smaller models (<1B parameters) on a large mixture of tasks in the few-shot instructional format and find that such models can perform previously unseen tasks. Following approaches (Sanh et al., 2022; Wang et al., 2022) also train smaller models\n\nfor instruction following on large mixtures of tasks, assuming that the model\u2019s ability to in-context learn an unseen task emerges from a large diversity of instructions and task types. A recently popularised reinforcement learning approach of I nstruct GPT (Ouyang et al., 2022) also presents an adaptation of an instruction-following objective, training on mixtures of instructions with automatic feedback. Recently, the instruction following was extended by joint training on programming code generation (Chen et al., 2021) and by Chain-of-Thought (CoT) targets (Wei et al., 2022), where the model is trained to respond with a sequence of naturallanguage steps deducing the answer (Zhao et al., 2023; Kadl\u02c7c\u00edk et al., 2023). These extensions were empirically shown to enhance ICL ability (Fu and Khot, 2022) and were adopted by F lan models (Chung et al., 2022).\n\nfor instruction following on large mixtures of tasks, assuming that the model\u2019s ability to in-context learn an unseen task emerges from a large diversity of instructions and task types. A recently popularised reinforcement learning approach of I nstruct GPT (Ouyang et al., 2022) also presents an adaptation of an instruction-following objective, training on mixtures of instructions with automatic feedback. Recently, the instruction following was extended by joint training on programming code generation (Chen et al., 2021) and by Chain-of-Thought (CoT) targets (Wei et al., 2022), where the model is trained to respond with a sequence of naturallanguage steps deducing the answer (Zhao et al., 2023; Kadl\u02c7c\u00edk et al., 2023). These extensions were empirically shown to enhance ICL ability (Fu and Khot, 2022) and were adopted by F lan models (Chung et al., 2022).\nAnalyses of ICL Recent studies shed some light on the functioning of ICL in LMs through controlled experimentation, finding that the LMs\u2019 decision-making in ICL does not align with humans. Notably, Lu et al. (2022) report on the sensitivity of LMs to the specific formulation of the instructions in the prompt, while Liu et al. (2022) measures sensitivity to the ordering of in-context demonstrations. Further, we find that LMs perform ICL comparably well when the labels of the demonstrations are randomly shu ffl ed (Min et al., 2022b) or when the presented CoT sequences do not make sense (Wang et al., 2023). We note that such behaviours di ff er from learning a functional relation from demonstrations that we expect from in-context learners (Li et al., 2023b) and can be exploited to lead models to incorrect predictions. Nevertheless, other studies report that under the right conditions, LMs are able to learn functional relationships solely from the input prompt; For instance, Aky\u00fcrek et al. (2023); Li et al. (2023c) show that Transformers can be trained to accurately learn regression functions solely from the prompt.\nXie et al. (2022) identify the key covariate of ICL quality in the occurrence of training examples where correct predictions are conditioned by latent concepts. Consider a pre-training example \u2018Albert Einstein was [MASK]\u2019; The correct prediction for [MASK] can be resolved if the model can extract and apply  a latent reasoning concept from context, such as that the context exhibits a concept of nationalities and hence, [MASK] best substitutes \u2018German\u2019. Such concept dependencies occur in\n\nAnalyses of ICL Recent studies shed some light on the functioning of ICL in LMs through controlled experimentation, finding that the LMs\u2019 decision-making in ICL does not align with humans. Notably, Lu et al. (2022) report on the sensitivity of LMs to the specific formulation of the instructions in the prompt, while Liu et al. (2022) measures sensitivity to the ordering of in-context demonstrations. Further, we find that LMs perform ICL comparably well when the labels of the demonstrations are randomly shu ffl ed (Min et al., 2022b) or when the presented CoT sequences do not make sense (Wang et al., 2023). We note that such behaviours di ff er from learning a functional relation from demonstrations that we expect from in-context learners (Li et al., 2023b) and can be exploited to lead models to incorrect predictions. Nevertheless, other studies report that under the right conditions, LMs are able to learn functional relationships solely from the input prompt; For instance, Aky\u00fcrek et al. (2023); Li et al. (2023c) show that Transformers can be trained to accurately learn regression functions solely from the prompt.\nXie et al. (2022) identify the key covariate of ICL quality in the occurrence of training examples where correct predictions are conditioned by latent concepts. Consider a pre-training example \u2018Albert Einstein was [MASK]\u2019; The correct prediction for [MASK] can be resolved if the model can extract and apply  a latent reasoning concept from context, such as that the context exhibits a concept of nationalities and hence, [MASK] best substitutes \u2018German\u2019. Such concept dependencies occur in\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eee2/eee2dc2d-1ba4-4a01-9a95-5a5f91e17093.png\" style=\"width: 50%;\"></div>\n2: Demonstrations selection in Concept-aware training (C o AT): From all samples of the training datase st (1) filter out ones sharing a specific reasoning concept \ufffd with predicted sample (x pred, y pred). From th, we (2) iteratively pick the candidate demonstration(s) c i such that the trained model \u0398 \u2019s probability o ating the correct prediction y pred if we pick c i among demonstrations is minimal.\n\nlanguage sparsely but naturally, allowing the emergence of a certain ICL quality in LMs (Wies et al., 2023). Later work attributes ICL to similar data properties labelled under the terms of statistical burstiness (Chan et al., 2022) or compositionality (Hahn and Goyal, 2023). Our work builds upon this theory, but compared to previous studies limited to in-silico experiments, we elevate the idea of concept-aware training to real-world settings, with publicly available datasets and pre-trained language models. We are first to measure the impact of concept-aware data construction in extrinsic evaluation over 70 diverse tasks and show its potential to substantially enhance data e ffi ciency and robustness in training in-context learners, compared to previous work using magnitudes of more data and compute.\n\n# 3 Concept-Aware Training\n\nAiming to create language models able to learn a new latent reasoning concept in-context, we propose a Concept-Aware Training (C o AT) as an instruction-tuning framework specifying  conditions for a selection of few-shot demonstrations for the training instructions (Figure 2). We assume the format of training prompts widely used in the previous work training incontext few-shot learners, constructing training instructions from k demonstrations consisting of the input texts x with labels y followed by the predicted sample\u2019s input text x pred:\n[x 1, y 1, \u27e8 sep \u27e9, . . . , x k, y k, \u27e8 sep \u27e9, x pred] \u2192 y pred\nIn this setting, C o AT proposes to filter in-context demonstrations sequentially by two conditions.\n\nThe main condition, denoted as informativeness condition, assures to pick demonstrations exhibiting a specific reasoning concept C that is shared between a picked demonstration (x i, y i) and the predicted example (x pred, y pred), thus picking only the demonstrations whose reasoning pattern is  informative for the correct prediction. Such settings make it beneficial for the trained model to learn to extract and apply concepts presented in demonstrations. However, as the sole informativeness condition may easily pick demonstrations very similar or identical to the predicted sample, we propose a second, non-triviality condition. This condition chooses from the informative demonstrations the ones with which it is \u2018di ffi cult\u2019 for the model to respond correctly. This condition avoids the occurrence of in-context demonstrations identical to the predicted sample and may also increase the heterogeneity of di ff erent concepts that co-occur among the demonstrations, avoiding the over-reliance on the presence of a small set of specific concepts in small-data settings. We note that a body of previous work proposes better ways for picking in-context demonstrations during the inference (without updating the model) (Li et al., 2023a; Gupta et al., 2023; Luo et al., 2024). While some of these strategies are applicable in picking informative demonstrations in CoAT, note that this line of work is complementary to ours in assuming an existing in-context learner. More importantly, our motivation is substantially di ff erent; CoAT uses demonstrations of instruction training as a vehicle for creating training cases conditioned on latent concepts. This idea is not restricted to instruction tuning, but we note that instruction\n\nWhat constitutes a concept applicable in CoAT? We broadly define the term concept as an arbitrary functional relation of input and prediction (Xie et al., 2022) that holds robustly for any sample of a given task. Hypothetically, once the model learns to model a specific concept perfectly, it will never produce a prediction that violates this concept (\u0160tef\u00e1nik and Kadl\u02c7c\u00edk, 2023). In practice, attempts to clearly present useful concepts to the model can be obstructed by other covariates (Mikula et al., 2024), such as frequent predictive co-occurrences of tokens. Thus, we propose to pick CoAT\u2019s concepts among features that are unlikely to be substitutable by non-robust covariates, presenting as best candidates to be features conditioned on a deep or holistic decomposition of the input. Note that the goal of CoAT is not to represent chosen training concepts in the model\u2019s weights  explicitly but to improve the model\u2019s ability to extract and apply available concepts from demonstrations. Towards this goal, CoAT fundamentally assumes that the ability to extract and apply one concept transfers to other  concepts beyond the training distribution. We verify this ability later in Section 4.4.\n\n# 3.1 Proposed Implementation\n\nIn our experiments, we implement the proposed C o AT framework in two training stages: First, we train LM on a scalable synthetic QA dataset, which, contrary to traditional QA datasets, contains annotations of reasoning concepts. Second, we refresh the LM\u2019s ability to work with natural language prompts by further training on a QA dataset with only natural language inputs. Hence, contrary to previous work utilising massive multitask training, in total, we only use two QA datasets.\n\n# Informativeness condition\n\nlection of annotated reasoning concepts in a TeaBReaC dataset of Trivedi et al. (2022), containing more than 900 unique explanations over a large set of synthetic QA contexts. Each TeaBReaC\u2019s explanation maps a natural question to the answer span through a sequence of declarative reasoning steps, such as \u201cselect \u2192 group \u2192 project\u201d. Within C o AT, we use these explanations as the shared concepts C (Fig. 1); In the training prompts, all demonstrations exhibit the same reasoning chain as the predicted sample.\n\nTo restore the model\u2019s ability to work with a natural language, in the second step, we fit the resulting model to natural inputs by further fine-tuning on AdversarialQA dataset (Bartolo et al., 2021); As the annotations of reasoning concepts in general QA datasets are scarce, in this case, we naively use the initial word of the question (\u201cWho\u201d, \u201cWhere\u201d,. . . ) as the shared concept, aware that such-grouped samples are not always mutually informative.\nNon-triviality condition In both training stages, we implement the non-triviality condition in the following steps. (i) We select a random subset of 20 samples that passed the informativeness condition (denoted X info). (ii) From X info, we iteratively pick a sequence of i \u2208 1.. k demonstrations (with k: 2 \u2264 k \u2264 8) as follows:\n1. For each sample (x j, y j) \u2208 X info, we use the training model to compute a likelihood  of generating the correct prediction y pred if a given sample (x j, y j) is included among demonstrations. Whenever y pred contains more than one token, we compute the likelihood as the  average of the likelihoods of all y pred \u2019s tokens in the teacher-forced generation.\n2. In each step i, we pick among the demonstrations a sample with which the likelihood of generating correct prediction is minimal.\n\nAn overview of this process is depicted in Figure 2, with a schematic example of a training prompt in Figure 1. Full training prompts that our implementation of CoAT constructs in training on each dataset can be found in Table 2 in the Appendix.\n\n# 4 Experiments\n\nOur experiments provide empirical evidence towards answering three research questions (RQs):\n\n# 1. Can we improve models\u2019 ability to benefit from new reasoning concepts in-context?\n\n2. Are the concept-aware in-context learners more robust to known functional artifacts?\n\n# 3. Can concept-aware in-context learning also improve performance in real-world tasks?\n\nThe first two RQs validate our assumptions on concept-aware training: that (1) the implementation of CoAT indeed improves models\u2019 utilisation of both seen and out-of-distribution concepts from demonstrations, and that (2) such an ability can\n\nmake the in-context learning of a CoAT-trained language model more robust to artefacts revealed in previous in-context learners (Wei et al., 2023). Finally, in (3), we assess whether the enhanced models\u2019 ability to rely more on latent concepts can also improve the practical quality of low-resource in-context learning.\n\n# 4.1 Training and Evaluation\n\nTo maximise comparability with the previous work, we fine-tune our models from m T5 pre-trained models of Xue et al. (2021). In both training stages (Sec. 3.1), we fine-tune all model parameters in a teacher-forced next-token prediction until convergence of evaluation loss. * We further detail the training parameters in Appendix A. In all experiments, we construct evaluation prompts from k =  3 demonstrations chosen consistently for all models, with prompts including the options for expected labels. We complement all our evaluations with confidence intervals from the bootstrapped evaluation (population n = 100, repeats r =  200). We specify evaluation setup separately for each experiment (\u00a7 4.4\u2013 4.6) with further details and examples in Appendix B.\n\n# 4.2 Baselines\n\nWe assess the impact C o AT\u2019s main design choices against two baselines, allowing us to measure the impact of both its data construction conditions.\n\nRandom demonstrations selection (T k random) We evaluate the impact of all C o AT\u2019s components against a baseline trained in identical settings but picking the in-context demonstrations randomly with uniform probability over the whole training set. This baseline reproduces the methodology of a majority of the referenced work on instruction tuning, including T k-I nstruct (Wang et al., 2022) and F lan (Chung et al., 2022). Apart from the demonstration selection, all other settings, including training data, remain identical (\u00a7 4.1) to assure comparability with C o AT models.\n\n# Demonstrations passing only informativeness\n\ncondition (T k info) In this baseline, we perform ablation of C o AT\u2019s non-triviality condition (Sec. 3) by picking the demonstrations passing only the informativeness condition. Hence, such-picked demonstrations in the training instructions are informative for the prediction but can exhibit cases\n\n* Implementations of CoAT training and concept-learning evaluations are on https://github.com/MIR-MU/CoAT\n\nwhere some of the demonstrations are similar or even identical to the predicted sample, making it trivial for the model to perform correct prediction. All other training settings are unchanged (\u00a7 4.1).\n\n# 4.3 Other evaluated models\n\nWe also evaluate three recent in-context learners for which we can assess which datasets were used in their training mix: (1) T0 of Sanh et al. (2022) trained on a mixture of 35 datasets of di ff erent tasks in zero-shot settings, mostly of QA type, mapped into a self-containing human-understandable interaction format; (2) T k-I nstruct of Wang et al. (2022) pre-trained in a few-shot format similar to ours, on a mixture of 1,616 diverse tasks, and (3) F lan models of Chung et al. (2022) that further extend data settings of T k-I nstruct to a total of 1,836 tasks, including chain-of-thought labels, i.e. a step-by-step reasoning chain mapping input prompt to a label. All these models are based on the same pretrained model (T5), making the results comparable to the level of fine-tuning methodology. T k I nstruct and F lan  use the data construction reproduced in our T k random baseline, but applied in vastly larger data settings.\n\n# 4.4 C o AT\u2019s ability to improve models utilisation of latent concepts (RQ1)\n\nWe pose that if the model can utilize a new reasoning concept C from demonstrations, it will be able to improve  the prediction in cases where the demonstrations use the same C as the predicted sample. Thus, to evaluate if training with C o AT improves models\u2019 utilisation of concepts, we evaluate models\u2019 performance in a few-shot setting where we ensure that the demonstrations share a specific latent concept with the predicted sample. Then, we quantify models\u2019 ability to improve from the concept by computing the di ff erence in accuracy between such concept-sharing evaluation and conventional evaluation using randomly chosen demonstrations. We perform the first analysis on TeaBReaC with annotated reasoning chains as concepts C shared between demonstrations and predicted sample (Fig. 1), but to evaluate generalization to unseen concepts, we filter out all samples with reasoning chains that were present in training. This results in 316 evaluation scenarios presenting models with 14 previously unseen reasoning patterns. In this setting, we compare the concept-improving ability of C o AT models with T k random baseline.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/073e/073e8432-b861-4de0-9a5d-27ab7f8b002a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: In-context learning of new concepts: Relative change of performance of models when presented with demonstrations exhibiting a reasoning concept informative for prediction. Evaluation with (left) synthetic TeaBReaC samples, and (right) diverse concepts of  natural datasets (\u00a7 4.4).\n</div>\nFigure 3: In-context learning of new concepts: Relative change of performance of models when presented with demonstrations exhibiting a reasoning concept informative for prediction. Evaluation with (left) synthetic TeaBReaC samples, and (right) diverse concepts of  natural datasets (\u00a7 4.4).\n\nThe important limitation of evaluation with TeaBReaC\u2019s concepts is that it remains unclear whether evaluation with synthetic  samples is representative for concept learning in a natural language. Hence, we also perform this analysis with samples and concepts of natural-language tasks. Previous work of \u0160tef\u00e1nik and Kadl\u02c7c\u00edk (2023) evaluated ICL ability over four di ff erent functional concepts, all extracted from explanations  of naturallanguage datasets. We adopt the concepts of this work and evaluate models for in-context learning of the following concepts: (i) reasoning logic of NLI samples of GLUE-Diagnostic dataset (Wang et al., 2018), (ii) entity relations  annotated in human explanations (Inoue et al., 2020) in the HotpotQA dataset (Yang et al., 2018), (iii) functional operations annotated in general elementary-grade tests of OpenBookQA (Mihaylov et al., 2018), and (iv) shared facts in science exams of WorldTree dataset (Jansen et al., 2018; Xie et al., 2020). Examples of prompts with concept-sharing demonstrations for these datasets are shown in Table 3. Identically to the case of synthetic concepts, we evaluate the ability of C o AT models to benefit from these concepts when exhibited in demonstrations and compare to uncontrolled demonstrations\u2019 selection (T k random) used in previous work.\n\n# Results\n\nConcept-aware training improves the ability to benefit from unseen concepts Figure 3 evaluates models\u2019 ability to improve from presented concepts as the relative di ff erence in performance between\n\nrandom and concept-sharing demonstration selection. First, evaluation with unseen TeaBReaC concepts (left) assesses models\u2019 ability to extrapolate the utilisation of latent concepts to 14 previously unseen reasoning chains. Both C o AT and randomdemonstration models (\u00a7 4.2) can improve from concepts presented in demonstrations. However, the improvement of C o AT-trained models is significantly larger and exceeds gains of T k random by 2-fold and 4-fold with the smaller and larger model, respectively. This comparison verifies that C o AT\u2019s data construction really improves our targeted skill of better utilizing concepts of demonstrations.\n\n# CoAT applied with synthetic data also improves\n\nthe use of natural concepts Evaluation of improvements on selected natural concepts (Figure 3; right) shows that concept-learning ability obtained with synthetic TeaBReaC concepts transfers to natural-language settings, as the C o AT-trained models can benefit from concepts significantly more than models trained without concept-aware data construction (T k random). Despite that, evaluations over the individual reasoning concepts (Figure 7 in Appendix C.3) reveal that even C o AT models can not benefit robustly from all concepts. Nevertheless, we note that in the cases where C o AT models do not improve, also none  of the baselines benefit from presented concepts. This might be attributed to several reasons: (i) the presented concepts are not really  informative for prediction, (ii) our training data allowed the models to memorize relevant knowledge and, hence, do not need (and benefit from) the concepts\u2019 exposition, or (iii) our training concepts were simply not su ffi cient to generalize over these new concepts.\n\n# 4.5 Robustness of concept-aware in-context learners (RQ2)\n\nAs we overviewed in Section 2, other work reports functional deficiencies of previous in-context learners, including surprising insensitivity of incontext learners to the assigned demonstrations\u2019 labels (Min et al., 2022b). Wei et al. (2023) attribute this to models\u2019 over-reliance on the semantic priors obtained in pre-training, which may override learning of the functional concepts. Such behaviour is defective, because the ability to learn functional relations is necessary for robust and interpretable in-context learning of truly unseen tasks. To evaluate the impact of concept-aware training on models\u2019 reliance on their semantic priors,\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1296/1296c90d-c004-4799-aec4-c8caf468e0a2.png\" style=\"width: 50%;\"></div>\nFigure 4: Models\u2019 reliance on semantic priors: Relative change of models\u2019 performance when we (left) replace  labels with \u2018non-sensical\u2019 tokens with no correspondence to the semantics of the task, such as \u2018 foo \u2019, \u2018 bar \u2019, etc.; and (right) flip the original labels, so that e.g. \u2018 negative \u2019 label corresponds to a positive-sentiment sample. C o AT models can in-context learn the inputoutput mapping similarly well with non-sensical labels and rely on the labels\u2019 semantics significantly less than previous in-context learners (in grey).\n\nwe follow the setup of Wei et al. (2023) and assess reliance on labels \u2019 semantics in a standard few-shot evaluation (\u00a7 4.1), with one of the two modifications; (i) We change the labels to tokens with irrelevant meaning for the prediction task, such as \u2018Foo\u2019, \u2018Bar\u2019 etc. (ii) We shu ffl e the labels so that semantically incorrect labels are assigned in the demonstrations, but the input-label mapping remains consistent. In both settings, the task\u2019s functional relation can still be recovered from demonstrations, but the sole reliance on semantics will either not help or will mislead the model. In this setting, we evaluate three model types: (i) C o AT-trained models, (ii) models with uncontrolled data construction (T k random & previous work), and (iii) models with uncontrolled data construction, but fine-tuned only on a natural QA dataset (denoted T k-QA). We perform the evaluation over 8 SuperGLUE tasks with discrete labels.\nResults Figure 4 shows the results. Evaluation with non-sensical labels (left) reveals that all models pre-trained on a synthetic TeaBReaC dataset (T k random, and T k-C o AT) are more robust to the labels\u2019 semantics than our natural-language baseline (T k-QA). However, a comparison of T k random and T k-C o AT suggests that T k-C o AT\u2019s preference for learning functional relations is a\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5cb7/5cb7579c-8db8-4aa6-9ba4-17eff63bd57c.png\" style=\"width: 50%;\"></div>\nFigure 5: E ff ectiveness of Concept-aware training: Natural-Instructions: Win rate of models utilising Concept-aware training (C o AT; \u00a7 3) and traditional instruction tuning (T k-R andom; \u00a7 4.2) evaluated on (top) all and (bottom) reasoning tasks of Natural-Instructions collection. Values indicate the number of tasks where the referenced model reaches significantly higher accuracy than the other. For the similar tasks, the di ff erence in models\u2019 performance is not statistically significant.\ncomposition of both using a synthetic dataset in pre-training and C o AT\u2019s data construction. A comparison to previous models reveals that all multitask models experience substantially larger decay in performance than our models. We suspect this feature could be a bias specific to massive multi-task learning emerging when label semantics can explain a large portion of training data. This result is consistent with Wei et al. (2023), but contrary to their conclusions, we show that ICL robust to semantic distractions does not emerge exclusively with very large (\u2265 100B) model scale. Nevertheless, we note that the smaller C o AT model still relies on labels\u2019 semantics when recognizable (Flipped labels; Fig. 4 right), less than previous models, but comparable to our baselines.\n\nFigure 5: E ff ectiveness of Concept-aware training: Natural-Instructions: Win rate of models utilising Concept-aware training (C o AT; \u00a7 3) and traditional instruction tuning (T k-R andom; \u00a7 4.2) evaluated on (top) all and (bottom) reasoning tasks of Natural-Instructions collection. Values indicate the number of tasks where the referenced model reaches significantly higher accuracy than the other. For the similar tasks, the di ff erence in models\u2019 performance is not statistically significant.\n\ncomposition of both using a synthetic dataset in pre-training and C o AT\u2019s data construction. A comparison to previous models reveals that all multitask models experience substantially larger decay in performance than our models. We suspect this feature could be a bias specific to massive multi-task learning emerging when label semantics can explain a large portion of training data. This result is consistent with Wei et al. (2023), but contrary to their conclusions, we show that ICL robust to semantic distractions does not emerge exclusively with very large (\u2265 100B) model scale. Nevertheless, we note that the smaller C o AT model still relies on labels\u2019 semantics when recognizable (Flipped labels; Fig. 4 right), less than previous models, but comparable to our baselines.\n\n# 4.6 Practical e ff ectiveness of concept-aware in-context learners (RQ3)\n\nFinally, we assess the practical quality of conceptaware ICL on previously unseen tasks in a simulated low-resource application with only three randomly-chosen demonstrations. We evaluate on samples from two collections of tasks: (i) SuperGLUE (Wang et al., 2019) consisting of 10 tasks requiring a variety of reasoning skills, and (ii) a test split of Natural-Instructions (Wang et al., 2022) from which we pick 60 extractive tasks. For SuperGLUE tasks, we verbalize both the demonstrations and predicted sample using all available templates within PromptSource library (Bach et al., 2022) and report results for the best-performing template\n\nAxG\nAx-b\nWSC\nCB\nRTE\nWiC\nReCoRD\nBoolQ\nCOPA\nMultiRC\nTk-random-1B 49.4\u00b15.2 43.6\u00b14.8 52.7\u00b15.1 21.8\u00b13.9 29.3\u00b14.6 18.0\u00b14.0 15.3\u00b13.8\n34.0\u00b15.0 74.7\u00b13.4\n5.1\u00b12.4\nTk-random-3B 50.2\u00b15.4 57.5\u00b14.8 52.0\u00b15.5 47.8\u00b15.1 48.9\u00b14.8 50.1\u00b14.4 16.3\u00b17.3\n62.8\u00b14.6 75.5\u00b12.8\n2.1\u00b11.5\nTk-info-1B\n50.0\u00b14.2 42.6\u00b15.7 52.0\u00b14.3 47.2\u00b13.9 49.2\u00b14.8 53.2\u00b14.5 15.5\u00b14.0\n19.6\u00b12.3 61.5\u00b12.3\n3.2\u00b11.2\nTk-info-3B\n50.8\u00b14.6 57.2\u00b14.9 53.5\u00b14.8 47.3\u00b15.4 54.7\u00b14.9 53.6\u00b14.7 22.6\u00b14.5\n64.4\u00b14.8 76.3\u00b13.0\n2.7\u00b12.1\nTk-CoAT-1B\n50.4\u00b15.3 52.7\u00b14.6 53.6\u00b15.2 46.9\u00b14.9 53.7\u00b14.9 53.5\u00b15.3 17.0\u00b13.5\n63.8\u00b15.4 76.1\u00b13.2 11.4\u00b12.6\nTk-CoAT-3B\n57.9\u00b14.9 57.2\u00b14.8 53.6\u00b14.5 60.4\u00b14.8 52.0\u00b15.4 56.9\u00b15.0 23.1\u00b13.8\n63.6\u00b14.3 81.3\u00b13.3 56.9\u00b13.6\nTable 1: E ff ectiveness of concept-aware training: SuperGLUE: ROUGE-L scores of ICL models evaluated in few-shot setting on SuperGLUE tasks (Wang et al., 2019), trained using (i) random demonstrations sampling used in previous work, (ii) informative demonstrations sampling (\u00a7 4.2) and (iii) informative + non-trivial sampling (C o AT; \u00a7 3). Underlined are the best results per each task and model size. See Table 5 for a comparison to previous models.\n\nfor each model. For Natural-Instructions tasks, we prefix the demonstrations with the instruction provided with each task. To maximise evaluation reliability over all models, we analyse the error cases and choose to report the results in ROUGE-L for SuperGLUE, and in a standard accuracy for Natural-Instructions. We specify the metrics selection analysis and other evaluation details in Appendix B, with prompt examples in Table 4. As a primary reference point, we again compare the results of C o AT-trained models to T k random, where we can make sure that all other training configurations except for the data construction method are identical. Further, we compare to T k info  (without Non-triviality condition; \u00a7 4.2) to also evaluate the importance of the non-triviality  condition. Finally, to provide additional context to our results, we also compare the performance of C o AT-trained models to previous in-context learners (\u00a7 4.3).\nResults Figure 5 compares the accuracy of C o ATtrained models to our baselines: (i) without systematic demonstrations selection (T k-R andom) and (ii) without the non-triviality condition (T k-I nfo), over 60 tasks of NaturalInstructions collection. In comparison to T k-R andom, C o AT models reach significantly higher accuracy on 41 and 45 of 60 tasks, with comparable performance on a majority (13 and 14) of other tasks. The di ff erence is further magnified on reasoning tasks, which we argue might better evaluate models\u2019 ability to in-context learn a functional  relation of the new task. A comparison of T k-I nfo with T k-R andom shows that the performance on reasoning tasks is mainly fostered by the C o AT\u2019s informativeness condition, but in a full task collection, T k-C o AT still outperforms T k-I nfo in 19 out of 60 tasks. Evaluations on other task segments can be found in Appendix C.2. In the evaluation over the tasks of SuperGLUE collection (Table 1), we additionally report the specific values of ROUGE-L that our baselines\n\nand CoAT models achieve. With a single exception, models utilising a concept-based selection of demonstrations (T k-C o AT and T k-I nfo) consistently reach higher scores than T k-R andom. Our analyses of models\u2019 predictions reveal that in 7 out of 20 evaluations, T k-R andom models fail to follow the task\u2019s instruction, consequentially responding out of valid label space. T k-C o AT is shown to mitigate this issue in all cases except for a smaller C o AT-trained model on MultiRC. A comparison of T k-C o AT with T k-I nfo shows that non-triviality condition is more substantial for a smaller model, but the models of both sizes benefit similarly from the concept-sharing selection of demonstrations.\n\n# Comparison to multitask learners\n\ncontextualizes the performance of C o AT models trained on two datasets of a single (QA) task with existing instructional models trained on massive mixtures of 35\u20131,836 tasks. Over all the NI tasks (Fig. 6; top), C o AT models outperform multitask learners on a majority of tasks in 3 of 6 competitions. CoAT models are outperformed by F lan models but perform at least similarly for the  majority  of the tasks in 5 out of 6 competitions. The evaluation on reasoning tasks (Fig. 6; middle) supports our hypothesis that C o AT particularly promotes improvements in in-context learning of new reasoning abilities, winning on reasoning tasks over F lan and T k-I nstruct in a comparable number of cases than the opponents. Finally, we look at a few tasks with  unseen labels for both T k-I nstruct and F lan models (Fig. 6; bottom) where multitask learners can not rely on shortcuts based on unseen tasks\u2019 labels. Here, the results of competition between C o AT with F lan models turns over, with C o AT models performing significantly better on 4 out of 6 tasks. While this sole segment is not big enough for robust conclusions, the results further support our claim that\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9383/9383721e-4354-45c7-bb1a-eccf49ed9634.png\" style=\"width: 50%;\"></div>\nFigure 6: Performance comparison to previous work: Natural-Instructions collection: Win rate of C o AT models trained using two (2) tasks and previous incontext learners trained on mixtures of 35 (T0), 1,616 (T k-I nstruct) and 1,836 tasks (T k-F lan). Values denote the number of tasks where the model reaches significantly better accuracy. Evaluations over (top) all tasks, (middle) reasoning tasks, (bottom) tasks with labels not present in the training mix of T k-I nstruct and T k-F lan.\n\nconcept-based ICL is more robust  to semantic distractions (RQ2). Table 5 in Appendix C details models\u2019 scores on SuperGLUE tasks, providing further evidence on overall comparability of C o AT models to multitask learners. For instance, a comparison with T k-I nstruct reveals that C o AT\u2019s 1B and 3B models reach higher absolute scores on 3 and 5 out of the 7 T k-I nstruct \u2019s unseen tasks.\n\n# 5 Conclusion\n\nInspired by data-centric theories on emergence of in-context learning, we propose and implement a Concept-aware Training framework for constructing training scenarios that challenge language models to learn to utilise latent concepts from in-context prompts. We show that language models can be trained to benefit from unseen concepts (RQ1), and that such ICL is more robust in learning  functional relations of a new task from demonstrations (RQ2). Finally, in extrinsic evaluation over 70 tasks, we demonstrate the practical e ffi ciency of concept-dependent training data, with C o AT mod\n\nels bringing significant improvements on 41 and 45 out of 60 Natural-Instructions tasks, or 6 and 5 of 10 SuperGLUE tasks (RQ3), while reaching a performance comparable to multitask learning requiring magnitudes of more data. More broadly, our work pioneers an alternative direction for scaling the quality of in-context learning to the previously explored model and data scale axes. We wish towards inspire future work to a more proactive approach to refining training data properties so that fitting such data necessitates the emergence of the specific, robust abilities of the models, such as the concept-learning ability. Specifically, future work can build upon our findings in researching ways to upscale conceptdependent data in unsupervised settings, allowing for pre-training more robust language models with a fraction of data and computing budget.\n\nAlthough our main objective is to assess the e ffi ciency of concept-aware training, we acknowledge the limitations of our comparison to the previous work, where several aspects convolute the representative comparison of di ff erent in-context learners: (i) each of the multitask learners was trained on a di ff erent, yet massive set of tasks, making it difficult to find a broader collection that is new for multiple models; For this purpose, we surveyed three standard collections used for few-shot evaluation: CLUES (Mukherjee et al., 2021), RAFT (Alex et al., 2021) and FLEX (Bragg et al., 2021), but found in total only three tasks unseen by the multitask learners of previous work, all of the same type (classification). Therefore, in our evaluations, we use (a) Tk-Instruct\u2019s own evaluation set and (b) SuperGLUE, which significantly overlaps with the training tasks of previous work. (ii) many aspects make it \u201ceasier\u201d for the model to improve, including the domain of labels or prompt format matching the training distribution (relevant to T k-I nstruct and F lan evaluated on Natural-Instructions). Another aspect that we neglect in our experiments in favour of more in-depth analyses is the impact of pretraining projected into the properties of the foundation model that we use. We pick T5 as a base model to maximise comparability with previous work. While we do not identify any concrete reason to assume that C o AT would perform worse with other base models, one should note that our results do not provide any evidence in this respect.\n\nFinally, we note that the applicability of C o AT is conditioned by the availability of the annotated concepts C in the training datasets, which might be di ffi cult to obtain for natural-language datasets. Our implementation circumvents this issue by using a synthetically curated dataset. Hence, we simultaneously show that concept-aware abilities can also be obtained in the restrictive settings of synthetic-dataset pre-training, where we note that the volume and variability of the synthetic dataset can be scaled further much easier than the natural dataset(s) (Trivedi et al., 2022). Nevertheless, our experiments do not provide any empirical evidence for answering to what extent  could further extension of synthetically-generated datasets, possibly covering even more complex concepts, scale to further performance gains.\n\n# Ethical Considerations & Broader Impact\n\nThe primary motivation of our work is to minimise the computing demands for the creation of accurate in-context learners by deepening our understanding of the covariates of the resulting quality. We believe that our presented method, as well as the future data-e ffi cient methods improving our understanding of in-context learning, will enable the democratization of the creation of robust and accurate in-context learning models for both research and industry. Finally, we note that data-e ffi cient methods for training ICLs (as opposed to multitask training) might open possibilities for creating more accurate ICLs specialized to languages outside English, where training datasets are scarce. We look forward for the future work that will explore the potential of data-e ffi cient instruction tuning specifically on the target-language datasets, creating in-context learners specially tailored for target languages outside English.\n\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2023.  What learning algorithm is in-context learning? Investigations with linear models. In The Eleventh International Conference on Learning Representations.\nNeel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C. Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, Michael Noetel, and Andreas Stuhlm\u00fcller. 2021. RAFT: A real-world few-shot text classification benchmark. In Thirty-fifth Conference on Neural\n\nInformation Processing Systems Datasets and Benchmarks Track (Round 2).\n\nStephen H. Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Ra ff el, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged S. Al-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Xiangru Tang, Mike Tian-Jian Jiang, and Alexander M. Rush. 2022. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and Douwe Kiela. 2021. Improving Question Answering Model Robustness with Synthetic Adversarial Data Generation. In  Proceedings of the 2021 Conference EMNLP, pages 8830\u20138848, Online and Punta Cana, Dominican Republic. ACL.\nJonathan Bragg, Arman Cohan, Kyle Lo, and Iz Beltagy. 2021. FLEX: Unifying Evaluation for Few-Shot NLP. In  Advances in Neural Information Processing Systems, volume 34, pages 15787\u201315800. Curran Associates, Inc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Je ff rey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In  Advances in NIPS, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nStephanie C.Y. Chan, Adam Santoro, Andrew Kyle Lampinen, Jane X Wang, Aaditya K Singh, Pierre Harvey Richemond, James McClelland, and Felix Hill. 2022. Data Distributional Properties Drive Emergent In-Context Learning in Transformers. In Advances in Neural Information Processing Systems.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.\n\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code.\n\nMorikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Je ff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Je ff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling InstructionFinetuned Language Models. arXiv e-prints, page arXiv:2210.11416.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the Surprising Di ffi culty of Natural Yes / No Questions. In  Proceedings of the 2019 Conference of the North American Chapter of the ACL: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936, Minneapolis, Minnesota. ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proc. of the 2019 Conference of the NAACL: Human Language Technologies, pages 4171\u20134186, Minneapolis, USA. ACL.\nHao Fu, Yao; Peng and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu\u2019s Notion.\n\nShivanshu Gupta, Matt Gardner, and Sameer Singh. 2023.  Coverage-based Example Selection for InContext Learning. In Findings of the ACL: EMNLP 2023, pages 13924\u201313950, Singapore. ACL.\nMichael Hahn and Navin Goyal. 2023. A Theory of Emergent In-Context Learning as Implicit Structure Induction.\nNaoya Inoue, Pontus Stenetorp, and Kentaro Inui. 2020.\nR4C: A benchmark for evaluating RC systems to get the right answer for the right reason. In Proceedings of the 58th Annual Meeting of the ACL, pages 6740\u2013 6750, Online. ACL.\nPeter Jansen, Elizabeth Wainwright, Steven Marmorstein, and Clayton Morrison. 2018. WorldTree: A corpus of explanation graphs for elementary science questions supporting multi-hop inference. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\nMarek Kadl\u02c7c\u00edk, Michal \u0160tef\u00e1nik, Ondrej Sotolar, and Vlastimil Martinek. 2023. Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12101\u201312108, Singapore. ACL.\nXiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023a.  Unified Demonstration Retriever for InContext Learning. In Proceedings of the 61st Annual Meeting of the ACL (Volume 1: Long Papers), pages 4644\u20134668, Toronto, Canada. ACL.\nYingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023b.  Transformers as Algorithms: Generalization and Stability in In-context Learning.\nYingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. 2023c.  Transformers as Algorithms: Generalization and and Stability in In-context Learning.\nChin-Yew Lin. 2004.  ROUGE: A Package for Automatic Evaluation of Summaries. In  Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. ACL.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. ACL.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming FewShot Prompt Order Sensitivity. In Proceedings of the\n\nPapers), pages 8086\u20138098, Dublin, Ireland. ACL.\nMan Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran Kazemi. 2024. In-context Learning with Retrieved Demonstrations for Language Models: A Survey.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering. In Proceedings of the 2018 Conference EMNLP, pages 2381\u20132391, Brussels, Belgium. ACL.\nLuk\u00e1\u0161 Mikula, Michal \u0160tef\u00e1nik, Marek Petrovi\u02c7c, and Petr Sojka. 2024. Think Twice: Measuring the E ffi ciency of Eliminating Prediction Shortcuts of Question Answering Models. In Proceedings of the 18th Conference of the European Chapter of the ACL (Volume 1: Long Papers), pages 2179\u20132193, St. Julian\u2019s, Malta. ACL.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022a. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022b. Rethinking the role of demonstrations: What makes in-context learning work?\nSubhabrata Mukherjee, Xiaodong Liu, Guoqing Zheng, Saghar Hosseini, Hao Cheng, Greg Yang, Christopher Meek, Ahmed Hassan Awadallah, and Jianfeng Gao. 2021. Few-Shot Learning Evaluation in Natural Language Understanding. In NeurIPS 2021.\nLong Ouyang, Je ff  Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In  Proceedings of the 36th International Conference on NIPS, NIPS \u201922, pages 27730\u2013 27744, Red Hook, NY, USA. Curran Associates Inc.\nMohammad Taher Pilehvar and Jose Camacho-Collados. 2019.  WiC: the word-in-context dataset for evaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of the North American Chapter of the ACL: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, Minneapolis, Minnesota. ACL.\nRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender Bias in Coreference Resolution. In Proceedings of the 2018 Conference of the North American Chapter of the ACL: Human Language Technologies, New Orleans, Louisiana. ACL.\n\nVictor Sanh, Albert Webson, Colin Ra ff el, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Cha ffi n, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022.  Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Representations.\n\nTimo Schick and Hinrich Sch\u00fctze. 2020. Exploiting cloze questions for few shot text classification and natural language inference.\n\n# Timo Schick and Hinrich Sch\u00fctze. 2020. Exploiting cloze questions for few shot text classification and natural language inference.\n\nMichal \u0160tef\u00e1nik and Marek Kadl\u02c7c\u00edk. 2023.  Can Incontext Learners Learn a Reasoning Concept from Demonstrations? In  Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations (NLRSE), pages 107\u2013115, Toronto, Canada. ACL.\n\nMichal \u0160tef\u00e1nik, V\u00edt Novotn\u00fd, Nikola Groverov\u00e1, and Petr Sojka. 2022.  Adaptor: Objective-Centric Adaptation Framework for Language Models. In  Proceedings of the 60th Annual Meeting of the ACL: System Demonstrations, pages 261\u2013269, Dublin, Ireland. ACL.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, ChungChing Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Ho ff man-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise AgueraArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. LaMDA: Language Models for Dialog Applications.\n\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022.  Super-NaturalInstructions: Generalization via Declarative Instructions on 1600 + NLP Tasks. In Proceedings of the 2022 Conference EMNLP, pages 5085\u20135109, Abu Dhabi, United Arab Emirates. ACL.\n\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners.\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In Advances in Neural Information Processing Systems.\n\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning di ff erently.\n\n# Noam Wies, Yoav Levine, and Amnon Shashua. 2023\nThe Learnability of In-Context Learning.\n\nNoam Wies, Yoav Levine, and Amnon Shashua. 2023.\nThe Learnability of In-Context Learning.\n\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. In Proc. of the 2020 Conf. EMNLP: System Demonstrations, pages 38\u201345. ACL.\n\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An Explanation of In-context Learning as Implicit Bayesian Inference. In  International Conference on Learning Representations.\n\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A Survey of Large Language Models.\n\n# A Training details\n\nTable 2 shows a full training example for each stage of training: (1) TeaBReaC with synthetic contexts (top) and (2) AdversarialQA with natural-language contexts (bottom). In all our training setups, we fine-tune all model parameters for teacher-forced next-token prediction, conventionally used in training sequence-to-sequence language models. In the two training stages (TeaBReaC and AdversarialQA), we use a learning rate of 5 e \u2212 5 and 2 e \u2212 5, respectively. Other parameters remain identical between stages: e ff ective batch size = 30 samples and early stopping  with the patience of 2,000 updates based on evaluation loss on a standardized validation set of each dataset. We do not report the absolute values of evaluation loss as these are not directly comparable. In C o AT training, we use a random subsample of 20 informative examples as a candidate set for a selection of non-trivial demonstrations. Other parameters of training configuration default to Training Arguments of Transformers library (Wolf et al., 2020) in version 4.19.1. For readability, we implement the relatively complex demonstrations\u2019 selection as a new objective of the Adaptor library (\u0160tef\u00e1nik et al., 2022). The picked demonstrations are encoded into a format consistent with the evaluation.\n\n# B Evaluation details\n\nTables 3 shows an example of an instruction for each evaluation that we perform within the conceptlearning evaluation. For readability, we only shorten the examples of HotpotQA, where we omit some sources of data available for the model. In the case of TeaBReaC not shown in this table, the evaluation prompt format is the same as in training (Table 2), whereas we make sure that the reasoning chains of evaluation samples di ff er from the training.\n\nSuperGLUE Evaluation format As mentioned in Section 4.1, we verbalize both the demonstrations and predicted sample using all available templates of PromptSource library (Bach et al., 2022), obtaining prompts for each demonstration prompt x i and its label y i in a free-text form. The prompts commonly contain the full-text match of the possible labels as options for the model. Following the example of Wang et al. (2022), we additionally prepend the demonstrations and\n\nlabels with keywords \u201cInput\u201d and \u201cPrediction\u201d and separate demonstrations with new lines. Thus, the resulting input \u2192 output pairs in evaluation take this format:\n\n\u201cInput: x 1 Prediction: y 1 <newline> Input: x 2 Prediction: y 2 <newline> Input: x 3 Prediction: y 3 <newline> Input: x pred Prediction: \u201d \u2192 \u201cy pred\u201d\n\nwhere demonstrations (x i, y i) are picked randomly but consistently between all evaluated models.\n\n# Natural-Instructions Evaluation format\n\nevaluations on Natural-Instructions, we closely follow the example of Wang et al. (2022) and additionally prepend the sequence of demonstrations with an instruction provided for each task:\n\n\u201c<task instruction> <newline> Input: x 1 Prediction: y 1 <newline> Input: x 2 Prediction: y 2 <newline> Input: x 3 Prediction: y 3 <newline> Input: x pred Prediction: \u201d \u2192 \u201cy pred\u201d\n\nwhere the <task instruction>  contains the instruction as would be given to the annotators of the evaluation task, usually spanning between 3\u20136 longer sentences. The demonstrations are again picked randomly but consistently between models. Examples of evaluation prompts for both SuperGLUE and Natural-Instructions can be found in Table 4.\n\n# Evaluation metrics selection\n\ntraining in-context few-shot learners is not consistent in the use of evaluation metrics, and the choice usually boils down to either using the exact-match accuracy (Sanh et al., 2022; Chung et al., 2022) or ROUGE-L of Lin (2004) (Wang et al., 2022), evaluating the longest common sequence of tokens. We investigate these two options with the aim of not penalising the models for minor discrepancies in the output format (in the accuracy case) but avoiding false positive evaluations in predictions that are obviously incorrect (in the ROUGE case). Investigation of the models\u2019 predictions reveals that the selection of the metric makes a large difference only in the case of T k-I nstruct models, where the situation di ff ers between SuperGLUE and Natural-Instructions, likely due to the character of the evaluation prompts. (1) On SuperGlue, e.g. on MultiRC task, for the evaluation prompt: \"Does answer sound like a valid\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b21/9b21e978-15c7-4e73-8553-420f0126b680.png\" style=\"width: 50%;\"></div>\n\u201c131\u201d\n\u201cInput: how many points did the Monte Vesuvio\" score in their two highest scoring matches? Context: scores of games of Pentagon\". 99 scores of games of monte vesuvio\". 67 scores of games of Pentagon\". 6 scores of games of monte vesuvio\". 76 scores of games of Pentagon\". 37 scores of games of monte vesuvio\". 56 scores of games of Pentagon\". 8 scores of games of Pentagon\". 90 scores of games of Pentagon\". 20 Answer: Prediction: 143 [2 more examples] Input: how many points did the Bell 212 score in their two highest scoring games? Context: scores of games of bell 212. 90 scores of games of S-50. 54 scores of games of bell 212. 41 scores of games of bell 212. 36 scores of games of S-50. 23 scores of games of bell 212. 6 scores of games of bell 212. 2 scores of games of S-50. Prediction:\u201d\n\u201cBritish\u201d\n\u201cInput: Who was the Speaker in 1909? Context: Second, Democrats have always elevated their minority floor leader to the speakership upon reclaiming majority status. Republicans have not always followed this leadership succession pattern. In 1919, for instance, Republicans bypassed James R. Mann, R-IL, who had been minority leader for eight years, and elected Frederick Gillett, R-MA, to be Speaker. Mann \"had angered many Republicans by objecting to their private bills on the floor;\" also he was a prot\u00e9g\u00e9 of autocratic Speaker Joseph Cannon, R-IL (1903\u20131911), and many Members \"suspected that he would try to re-centralize power in his hands if elected Speaker.\" More recently, although Robert H. Michel was the Minority Leader in 1994 when the Republicans regained control of the House in the 1994 midterm elections, he had already announced his retirement and had little or no involvement in the campaign, including the Contract with America which was unveiled six weeks before voting day. Prediction: Joseph Cannon, R-IL. [2 more examples] Input: Who created the legal system still in use in Florida? Context: As a result of these initiatives northeastern Florida prospered economically in a way it never did under Spanish rule. Furthermore, the British governors were directed to call general assemblies as soon as possible in order to make laws for the Floridas and in the meantime they were, with the advice of councils, to establish courts. This would be the first introduction of much of the English-derived legal system which Florida still has today including trial by jury, habeas corpus and county-based government. Neither East Florida nor West Florida would send any representatives to Philadelphia to draft the Declaration of Independence. Florida would remain a Loyalist stronghold for the duration of the American Revolution. Prediction:\u201d\n\n<div style=\"text-align: center;\">AdversarialQA Matching question-word \u201cWho\u201d\n</div>\nxamples of training instructions with expected outputs, for both our datasets applied in training. Note ared reasoning concept is not a part of the model\u2019s input.\n\nanswer to the question: question\", T k-I nstruct-3B in our evaluation predicts \"Yes.\" or \"Yes it is\" (instead of \"Yes\"), or \"No not at all\" (instead of \"No\"), likely due to the resemblance with the format of training outputs. As we do not wish to penalize these cases, we use ROUGE-L over all SuperGLUE evaluations.\n(2) In Natural-Instructions evaluation, we find that T k-I nstruct often predicts longer extracts from the input prompt. This is problematic with ROUGE-L in the cases where the extract contains all possible answers, such as in the T k-I nstruct 1B\u2019s prediction: \u201cyes or no\u201d to the prompt whose instruction ends with \u201cPlease answer in the form of yes or no.\u201d. As we encounter this behaviour in a large portion of Natural-Instructions tasks, we evaluate all models on Natural-Instructions for exactmatch accuracy after the normalization of the casing and the removal of non-alphabetic symbols. To make sure that the model is presented with the exact-matching answer option, we exclude from evaluation the tasks where the correct answer is not presented in the task\u2019s instruction. The reference to the list of Natural-Instructions evaluation tasks can be found in Appendix C.4.\nFor the reported evaluations of the Reasoning tasks, we pick from the list of evaluation tasks the\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b39/0b3912f8-4df4-44aa-8472-361685ca70ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: In-context learning of natural concepts for each dataset: While C o AT improves the ability to benefit from reasoning concepts on average (Fig. 3), per-concept evaluation reveals that this ability is not consistently robust.\n</div>\nones concerned with the reasoning task by simply matching the tasks with \u2018reasoning\u2019 in their name, resulting in the collection of 20 evaluation tasks.\n\n# C Further evaluations\n\n# C.1 SuperGLUE evaluations of other models\n\nTable 5 compares the performance over the tasks of SuperGLUE collection (Wang et al., 2019) for C o AT models trained on two tasks of the same (QA) type with in-context learners trained on 35\u2013 1,836 tasks of the comparable size. Despite the\n\n\u201cTrue\u201d\n\u201cInput: I will say that she stole my money. Question: I won\u2019t say that she didn\u2019t steal my money. True, False, or Neither? Prediction: Neither Input: I won\u2019t say that she didn\u2019t steal my money. Question: I will say that she stole my money. True, False, or Neither? Prediction: Neither Input: A rabbi is at this wedding, standing right there standing behind that tree. Question: It\u2019s not the case that there is no rabbi at this wedding; he is right there standing behind that tree. True, False, or Neither? Prediction: True Input: Even after now finding out that it\u2019s animal feed, I won\u2019t ever stop being addicted to Flamin\u2019 Hot Cheetos. Question: Even after now finding out that it\u2019s animal feed, I will never stop being addicted to Flamin\u2019 Hot Cheetos. True, False, or Neither? Prediction:\u201d\n\u201cmass\u201d\n\u201cFacts: a decrease is a kind of change. increase means more. as mass of a planet; of a celestial body increases, the force of gravity on that planet will increase. to change means to become di ff erent. an animal is a kind of living thing. the gravitational force of a planet; of a celestial object does not change the mass of an object on that planet or celestial body. an increase is the opposite of a decrease. an astronaut is a kind of human. massive means great in mass. the Mars Rover is a kind of vehicle. a living thing is a kind of object. Earth is greater in mass than Mars. gravity means gravitational pull; gravitational energy; gravitational force; gravitational attraction. greater means higher; more in value. stay the same means not changing. a moon is a kind of celestial object; body. an increase is a kind of change. Earth is a kind of planet. as the force of gravity increases, the weight of objects will increase. less is similar to decrease. Mars is a kind of planet. Input: An object has a weight of 10 kg on the surface of Earth. If the same object were transported to the surface of Mars, the object would have a weight of 3.8 kg. Which best explains why the weight of the object changed when transported from Earth to Mars? (A) The density of the object is greater on Earth than it is on Mars. (B) The volume of the object is greater on Earth than it is on Mars. (C) Gravitational force is greater on Earth than it is on Mars. (D) Atmospheric pressure is less on Earth than it is on Mars. Prediction: Gravitational force is greater on Earth than it is on Mars [two more examples] Input: When astronauts walked on the Moon, they used weighted boots to help them walk due to the lower gravitational pull. What di ff erence between Earth and the Moon accounts for the di ff erence in gravity? (A) density (B) diameter (C) mass (D) volume. Prediction:\u201d\n\u201cyes\u201d\n\u201cInput: Are Broughtonia and Laeliocattleya both orchids? Hint: use the information from the paragraphs below to answer the question. Otaara, abbreviated Otr. in the horticultural trade, is an intergeneric hybrid of orchids, with \"Brassavola\", \"Broughtonia\",\"Cattleya\", \"Laelia\" and \"Sophronitis\" as parent genera. Paracaleana commonly known as duck orchids, is a genus of flowering plants in the orchid family, Orchidaceae that is found in Australia and New Zealand. Duck orchids have a single leaf and one or a few, dull-coloured, inconspicuous flowers. (...) Prediction: yes [two more examples] Input: Are both Parodia and Thalictrum flowering plants? Hint: use the information from the paragraphs below to answer the question. - Thalictrum ( ) is a genus of 120-200 species of herbaceous perennial flowering plants in the Ranunculaceae (buttercup) family native mostly to temperate regions. Meadow-rue is a common name for plants in this genus. - Parodia is a genus of flowering plants in the cactus family Cactaceae, native to the uplands of Argentina, Peru, Bolivia, Brazil, Colombia and Uruguay. This genus has about 50 species, many of which have been transferred from \"Eriocactus\", \"Notocactus\" and \"Wigginsia\". They range from small globose plants to 1 m tall columnar cacti. All are deeply ribbed and spiny, with single flowers at or near the crown. Some species produce o ff sets at the base. They are popular in cultivation, but must be grown indoors where temperatures fall below 10 degrees. Prediction:\u201d\n\u201cthe clos star from us\u201d\n\u201cInput: Despite what some think, instead around themselves, our planet spins around... Choices: pluto, the moon, the milky way, the sun. Prediction: the sun Input: In a single year, a giant globe will do this to a giant star. Choices: fight, burn, circle, explode. Prediction: circle Input: The earth revolves around... Choices: a heat source, the Milky Way, a neighboring planet, the moon. Prediction: a heat source Input: the central object of our solar system is also... Choices: the smallest object in the solar system, the coldest heavenly body, the farthest star from us, the closest star from us. Prediction:\u201d\n\nGLUE NLI Diag. Double negation\n\nShared facts: {\"Earth is greater in mass than Mars\", \"gravity means gravitational pull; gravitational force; gravitational attraction\",\"as the force of gravity increases, the weight of objects will increase.\"}\n\nHotpotQA\n\nShared relation in reasoning: \u201cX is a genus\u201d\n\nRelation of objects:\"generate\"\n\n\u201cThe sun was rising\n\u201cInput: The soldiers were concealed in the brush. Select the most plausible cause: - They were armed with rifles. - They wore camouflage uniforms. Prediction: They wore camouflage uniforms. Input: The print on the brochure was tiny. Select the most plausible e ff ect: - The man put his glasses on. - The man retrieved a pen from his pocket. Prediction: The man put his glasses on. Input: I excused myself from the group. Select the most plausible cause: - I turned o ff my phone. - My phone rang. Prediction: My phone rang. Input: My body cast a shadow over the grass. Select the most plausible cause: - The sun was rising. - The grass was cut. Prediction:\u201d\n\u201cYes\u201d\n\u201cIndicate with \u2018Yes\u2018 if the given question involves the provided reasoning \u2018Category\u2018. Indicate with \u2018No\u2018, otherwise. We define five categories of temporal reasoning. First: \"event duration\" which is defined as the understanding of how long events last. For example, \"brushing teeth\", usually takes few minutes. Second: \"transient v. stationary\" events. This category is based on the understanding of whether an event will change over time or not. For example, the sentence \"he was born in the U.S.\" contains a stationary event since it will last forever; however, \"he is hungry\" contains a transient event since it will remain true for a short period of time. Third: \"event ordering\" which is the understanding of how events are usually ordered in nature. For example, \"earning money\" usually comes before \"spending money\". The fourth one is \"absolute timepoint\". This category deals with the understanding of when events usually happen. For example, \"going to school\" usually happens during the day (not at 2 A.M). The last category is \"frequency\" which refers to how often an event is likely to be repeated. For example, \"taking showers\" typically occurs 5 times a week, \"going to Saturday market\" usually happens every few weeks / months, etc. Input: Sentence: Jack played basketball after school, after which he was very tired. Question: How long did Jack play basketball? Category: Event Duration. Prediction: Yes Input: Sentence: He was born in China, so he went to the Embassy to apply for a U.S. Visa. Question: How often does he apply a Visa? Category: Frequency. Prediction: Yes Input: Sentence: Jack played basketball after school, after which he was very tired. Question: Was Jack still tired the next day? Category: Event Duration. Prediction: No Input: Sentence: It refers to a woman who is dangerously attractive, and lures men to their downfall with her sexual attractiveness. Question: How long does it take to lure men to their downfall? Category: Event Duration. Prediction:\u201d\n\n4: Examples of evaluation instructions with expected outputs, for selected tasks of SuperGLUE and  Naturaluctions (RQ3). Displayed samples are from CoPA and MCTato Temporal Reasoning tasks, respectively. Note these evaluations, demonstrations are picked randomly, regardless of their concepts.\n\nsignificantly smaller volumes and complexity of the training dataset, C o AT-trained models show competitive results to similar-size or even larger incontext learners of previous work. For instance, the 1-billion-parameter T k-C o AT performs better than the 3-billion T0 in 3 cases (Ax-b, RTE, COPA) and comparably in another 3 cases (WSC, CB, WiC). In comparison with T k instruct of the same size, T k-C o AT-1B outperforms T k instruct in 3 out of 7 unseen tasks (WSC, CB, ReCoRD), and reaches similar scores in most other cases, even in 2 out of 3 tasks that were included in T k instruct \u2019s training mix. Similarly, larger T k-C o AT-3B outperforms T k instruct on 4 of 7 new tasks (Ax-b, WSC, WiC, ReCoRD), but with larger gaps on the others.\n\n# C.2 Natural-Instructions: other task types\n\nFigure 8 evaluates the impact of C o AT\u2019s mechanism on the quality of in-context learning separately on the English and non-English tasks. The figure reveals that C o AT works particularly well for non-English tasks. Our analyses found this is mainly due to the low performance of the baseline on the non-English tasks. We speculate that this can be a consequence of the higher reliance of the\n\nbaseline on token semantics (Section 4.6, RQ2); As our models are fine-tuned on an English-only QA model, such learnt reliance is not applicable in multilingual settings. Figure 9 compares the performance of C o AT models against the models of previous work, separately on the English and non-English tasks. We can see that CoAT is slightly better at the multilingual portion of Natural-Instructions, but the di ff erence is not principal.\n\n# C.3 Per-concept evaluations\n\nFigure 7 evaluates the performance gains of the baseline models (\u00a7 4.2) and C o AT-trained models individually per each of the concepts of the natural datasets. While the C o AT models are able to benefit from concepts the largest in the relative change of quality, they are also not consistent in the ability to benefit from all the concepts. However, as discussed in Section 4.4, this does not imply that C o AT is unable to utilize these concepts.\n\n# C.4 Evaluation tasks and other configurations\n\nSuperGLUE (Wang et al., 2019) consists of the following tasks (as ordered in our Results, \u00a7 4.6): Winogender Schema Diagnostics (AxG) (Rudinger\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f625/f625a58e-4dd7-4bc4-829c-42ad7aab8c53.png\" style=\"width: 50%;\"></div>\n# train tasks\nAxG\nAx-b\nWSC\nCB\nRTE\nWiC\nReCoRD\nBoolQ\nCOPA\nMultiRC\nFlan-1B\n1,836\n84.8\u00b13.9 21.9\u00b14.0 70.7\u00b14.8 92.5\u00b12.8* 92.1\u00b13.0* 69.9\u00b15.1* 38.9\u00b15.2* 92.3\u00b12.7* 97.8\u00b11.5* 88.3\u00b13.2*\nFlan-3B\n1,836\n95.3\u00b13.7 22.0\u00b18.0 80.2\u00b19.2 92.7\u00b16.7* 96.0\u00b14.0* 79.7\u00b18.3* 62.2\u00b19.7* 92.1\u00b15.1* 99.3\u00b11.6* 90.4\u00b16.4*\nTk-Instruct-1B\n1,616\n51.9\u00b14.9 57.2\u00b15.8 49.8\u00b14.9 46.0\u00b15.5\n55.5\u00b14.8\n53.5\u00b15.3 13.1\u00b13.7 63.4\u00b13.4* 76.",
    "paper_type": "method",
    "attri": {
        "background": "Many recent language models (LMs) are capable of in-context learning (ICL), but the reasons for varying quality in ICL performance are not fully understood. Previous methods attributed ICL to model scale, while recent studies suggest that the quality of training data plays a crucial role. This paper explores the potential of concept-dependent training data to enhance ICL performance.",
        "problem": {
            "definition": "The problem addressed is the inconsistency in the ability of language models to effectively perform in-context learning across different tasks and models.",
            "key obstacle": "The main challenge is the reliance on model scale and the quality of training data, which has been insufficiently explored, leading to functional deficiencies in existing in-context learners."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that language models can learn better from training data that includes latent reasoning concepts.",
            "opinion": "The proposed idea involves Concept-aware Training (CoAT), a framework that constructs training scenarios to help models learn to utilize analogical reasoning concepts from demonstrations.",
            "innovation": "CoAT differs from existing approaches by focusing on the selection of training data that emphasizes concept dependencies, rather than relying solely on large-scale training."
        },
        "method": {
            "method name": "Concept-aware Training",
            "method abbreviation": "CoAT",
            "method definition": "CoAT is a framework designed to improve the in-context learning ability of language models by selecting training examples that share latent reasoning concepts.",
            "method description": "CoAT constructs training scenarios that encourage models to learn and apply reasoning concepts from demonstrations, enhancing their in-context learning capabilities.",
            "method steps": [
                "Filter training examples to ensure they share specific reasoning concepts with the predicted sample.",
                "Select demonstrations that are informative yet non-trivial to ensure diverse concept representation."
            ],
            "principle": "The effectiveness of CoAT lies in its ability to guide models in extracting and applying relevant concepts from the training data, which improves their learning and prediction accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using datasets like SuperGLUE and Natural-Instructions, comparing CoAT against traditional instruction tuning methods across various tasks.",
            "evaluation method": "Performance was assessed based on accuracy and ROUGE-L scores, with models evaluated on their ability to learn from concept-sharing demonstrations."
        },
        "conclusion": "The experiments demonstrate that CoAT significantly improves the in-context learning capabilities of language models, enabling them to effectively utilize unseen concepts and outperform traditional instruction tuning methods.",
        "discussion": {
            "advantage": "The key advantage of CoAT is its ability to enhance the robustness of in-context learning by focusing on concept dependencies in training data, leading to better performance on unseen tasks.",
            "limitation": "A limitation of the approach is the dependency on the availability of annotated reasoning concepts in training datasets, which may not always be feasible.",
            "future work": "Future research could explore methods to upscale concept-dependent training data in unsupervised settings and investigate its applicability across different languages."
        },
        "other info": [
            {
                "info1": "The proposed method has shown to be effective even with smaller models trained on limited data.",
                "info2": {
                    "info2.1": "CoAT can improve the performance of models on tasks requiring complex reasoning.",
                    "info2.2": "The framework allows for more efficient training, reducing the need for extensive datasets."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) is influenced by the quality of training data, particularly the inclusion of latent reasoning concepts."
        },
        {
            "section number": "1.2",
            "key information": "The inconsistency in ICL performance across different tasks and models highlights the relevance of this concept within NLP."
        },
        {
            "section number": "3.1",
            "key information": "Concept-aware Training (CoAT) enhances the robustness of in-context learning by focusing on concept dependencies in training data."
        },
        {
            "section number": "3.2",
            "key information": "CoAT constructs training scenarios that encourage models to learn and apply reasoning concepts from demonstrations."
        },
        {
            "section number": "4.1",
            "key information": "The design of training data in CoAT significantly influences the outcomes of in-context learning by ensuring that examples share specific reasoning concepts."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of CoAT is its dependency on the availability of annotated reasoning concepts in training datasets."
        },
        {
            "section number": "7",
            "key information": "CoAT has shown to significantly improve the in-context learning capabilities of language models, enabling better performance on unseen tasks."
        }
    ],
    "similarity_score": 0.7252436320198804,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Concept-aware Data Construction Improves In-context Learning of Language Models.json"
}