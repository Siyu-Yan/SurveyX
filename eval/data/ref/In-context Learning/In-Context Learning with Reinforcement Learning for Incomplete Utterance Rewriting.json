{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2408.13028",
    "title": "In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting",
    "abstract": "In-context learning (ICL) of large language models (LLMs) has attracted increasing attention in the community where LLMs make predictions only based on instructions augmented with a few examples. Existing example selection methods for ICL utilize sparse or dense retrievers and derive effective performance. However, these methods do not utilize direct feedback of LLM to train the retriever and the examples selected can not necessarily improve the analogy ability of LLM. To tackle this, we propose our policy-based reinforcement learning framework for example selection (RLS), which consists of a language model (LM) selector and an LLM generator. The LM selector encodes the candidate examples into dense representations and selects the top-k examples into the demonstration for LLM. The outputs of LLM are adopted to compute the reward and policy gradient to optimize the LM selector. We conduct experiments on different datasets and significantly outperform existing example selection methods. Moreover, our approach shows advantages over supervised finetuning (SFT) models in few shot setting. Further experiments show the balance of abundance and the similarity with the test case of examples is important for ICL performance of LLM.",
    "bib_name": "du2024incontextlearningreinforcementlearning",
    "md_text": "# In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting\n\nHaowei Du, Dongyan Zhao\n\nAbstract\n\nIn-context learning (ICL) of large language models (LLMs) has attracted increasing attention in the community where LLMs make predictions only based on instructions augmented with a few examples. Existing example selection methods for ICL utilize sparse or dense retrievers and derive effective performance. However, these methods do not utilize direct feedback of LLM to train the retriever and the examples selected can not necessarily improve the analogy ability of LLM. To tackle this, we propose our policy-based reinforcement learning framework for example selection (RLS), which consists of a language model (LM) selector and an LLM generator. The LM selector encodes the candidate examples into dense representations and selects the top-k examples into the demonstration for LLM. The outputs of LLM are adopted to compute the reward and policy gradient to optimize the LM selector. We conduct experiments on different datasets and significantly outperform existing example selection methods. Moreover, our approach shows advantages over supervised finetuning (SFT) models in few shot setting. Further experiments show the balance of abundance and the similarity with the test case of examples is important for ICL performance of LLM.\n\n# Introduction\n\nIn recent years, there has been a growing focus on multi-turn dialogue modeling (Choi et al., 2018; Sun et al., 2019; Reddy et al., 2019). One of the primary challenges in this field is the tendency of speakers to employ incomplete utterances, such as co-reference or ellipsis, when referring back to entities or concepts that have been mentioned in the dialogue history. Su et al. (2019) demonstrates that ellipsis and co-reference occur in over 70% of dialogue utterances. To address this phenomenon, the Incomplete Utterance Rewriting (IUR) task, as proposed by (Pan et al., 2019; Elgohary et al., 2019),\n\naims to rephrase an incomplete utterance into a selfcontained utterance that is semantically equivalent and can be comprehended independently, without relying on contextual information. Recent generation-based approaches for IUR tackle this task as a seq2seq problem and gain impressive performances (Pan et al., 2019; Huang et al., 2021; Inoue et al., 2022). With the increasing ability of LLMs, ICL has become a new direction for natural language generation, where LLMs make predictions only depending on contexts augmented by a few examples (demonstration) without weights updating (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023). However, the performance of ICL is sensitive to the selection of in-context examples (Zhao et al., 2021; Liu et al., 2021; Dong et al., 2022). We take a case from TASK dataset in Table 1\nand 2, where the incomplete utterance is \u201cHow about Mediterranean food?\u201d and the omitted part is the postpositive attributive \u201cin expensive price range\u201d. By metrics of sparse retrieval methods like BM25 (Robertson et al., 2009) or dense retrieval methods like PTM (Liu et al., 2022), the example incomplete utterance e 1 as well as its contexts and rewritten utterance should be selected to be in-context examples. However, the performance of LLM for this question with example e 1 drops by 5 ROUGE score compared with example e 2. To directly select the examples that can improve the analogy ability of LLM, we introduce policy based reinforcement learning (RL) into example selection for IUR (RLS). Given a set of candidate examples, we utilize a small-scale language model (LM) to encode the context and incomplete utterance of each example. Intuitively, if a subset of candidate examples leads to increasing performance of LLM for IUR, RLS should assign high scores to them; the more the performance increases, the higher the example scores should be. Therefore, we compute the reward by ICL performance and\n\n<div style=\"text-align: center;\">Utterance\n</div>\nUtterance\nu1\nHello, I am looking for an expensive\nrestaurant that serves fusion food.\nu2\nI \u2019m sorry, there are no fusion restaurants\nlisted in the expensive price range. Would\nyou like to try something else?\nu3\nHow about Mediterranean food?\nu\u2217\n3\nHow about Mediterranean food\nin expensive price range?\ne1\nHow about serving Mediterranean food?\ne\u2217\n1\nHow about a cheap restaurant\nserving Mediterranean food?\ne2\nCan you recommend a restaurant to me?\nI don\u2019t want to spend a lot of money.\ne\u2217\n2\nCan you recommend a restaurant to me\nin the south part of town? I don\u2019t\nwant to spend a lot of money.\nTable 1: One example from CANARD dataset. u 1 u 2 are 2 turns of contextual utterances, u 3 denotes the incomplete utterance, u \u2217 3 denote the golden rewritten utterance, e 1 /e \u2217 1 and e 2 /e \u2217 2 denote two candidate example pairs of incomplete utterance and rewritten utterance which can be prompted to the LLM.\n\nExample\nSparse\nDense\nROUGE\ne1\n\ufffd\n\ufffd\n50.0\ne2\n\ufffd\n\ufffd\n55.6\nTable 2: The Rouge score of ChatGLM by use of different examples in prompts. \u201cSparse\u201d denotes selecting the example by sparse retrieval methods like BM25, \u201cDense\u201d denotes selecting the example by dense representations by PTM.\n\noptimize the LM selector with policy gradient.\nWe conduct experiments on three benchmark datasets in IUR field and compare with existing competitive example selection methods. Our approach outperforms existing methods by about 2 score in CANARD and REWRITE dataset and 10 score in TASK dataset with different metrics including BLEU, ROUGE and F-score.\nOur contributions can be summarized as: 1. We are the first to explore ICL performance of LLM for IUR task and design the effective formulation of demonstration for IUR task. 2.  We introduce policy-based RL into example selection for ICL prompts, which directly utilize the LLM feedback to train the LM selector. 3. Our approach significantly outperforms existing\n\nexample selection methods across sparse retrieval and dense retrieval methods. 4. Our approach shows advantages against SFT models in few shot setting. We explain the improvement comes from the linguistic complexity and abundance, as well as the similarity with the test case of examples.\n\n# 2 Related Work\n\nThere are two main streams of approaches to tackle the task of IUR: edit-based and generationbased. Generation-based models solve this task as a seq2seq problem, which is more relevant to our approach for ICL with LLM. Su et al. (2019) utilize pointer network to respectively predict the prob of tokens in rewritten utterance from contexts or incomplete utterance. Hao et al. (2021) formulate the task as sequence tagging to reduce the search space. Huang et al. (2021) combine a source sequence tagger with an LSTM-based decoder to maintain grammatical correctness. Demonstration selection is crucial to ICL and\nLiu et al. (2021) showed that downstream performance can vary widely depending on the choice of in-context examples. Liu et al. (2022) utilize sentence representations of PTM to select the examples with more cosine similarity. Sorensen et al. (2022) and Gonen et al. (2022) argue that mutual information and perplexity are also valuable selection metrics which do not need labeled examples and specific LLM. Levy et al. (2022) select diverse demonstrations to collectively cover all of the structures required in the outputs. Kim et al. (2022) generate demonstrations for ICL from PLM itself to minimize the reliance on the external demonstration. Rubin et al. (2021) first build an unsupervised retriever like BM25 to recall similar examples as candidates and then construct a supervised retriever to select demonstrations from candidates However, these methods fail to directly select the examples into demonstrations that can improve the analogy ability of LLM for IUR task and some useful examples like in table 1 will be neglected.\n\n# 3 Task Definition\n\nGiven the context C and the incomplete utterance U, we aim to derive the rewritten version R which can be comprehended without the context. The IUR task is to learn a map function f (C, U | \u03b8) = R. ICL with LLM for IUR task needs a set of candidate examples {y 1, y 2, \u00b7 \u00b7 \u00b7, y N}, where y i =\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7493/74937702-c8b9-4a8f-a5a4-10ad09e87a3e.png\" style=\"width: 50%;\"></div>\n(C i, U i, R i), 1 \u2264 i \u2264 n denotes the i-th example and N denotes the number of candidate examples. For a test case x = (c x, u x), we select k  examples from the candidate set and the input to LLM M is the concatenation of the task instruction I, the k demonstration examples, and the test case x. The rewritten utterance of x is generated by M: R x = M [I; (C i 1, U i 1, R i 1); (C i 2, U i 2, R i 2); \u00b7 \u00b7 \u00b7; (C i k, U i k, R i k); (C i x, U i x)],;  denotes concatenation of texts. So the key of ICL to solve IUR task is to select the appropriate examples.\n\n# 4 Methodology\n\nWe approach the example selection as a sequential decision-making problem. State space: the sequence of selected demonstration examples y i 1, y i 2, \u00b7 \u00b7 \u00b7, y i k, where k denotes the number of selected examples. Action space: select the next example given the current selection result and insert the concatenation of its contexts, incomplete utterance and rewritten utterance into demonstrations. Policy: We utilize transformer architecture to model the probability of each example in the candidate set to be selected into the demonstration.\n\n# 4.1 Utterance Encoder\n\nWe make use of BERT (Devlin et al., 2018) to encode the semantic and syntactic information of utterances. For each example in the candidate set and the current test case, the input to PTM is the concatenation of contexts and incomplete utterance, which is separated by \u201c[SEP]\u201d token. The hidden state corresponding to \u201c[CLS]\u201d token is used to\n\nrepresent the case.\n\nS i = BERT ([CLS; c i, SEP, u i])\nS x = BERT ([CLS; c x, SEP, u x]\n\n(1)\n(2)\n\nwhere c i and u i as well as c x and u x  denote the context and the incomplete utterance of i-th candidate example and the current test case. To be consistent with the inference, we do not include the rewritten utterance into representing each case.\n\n# 4.2 Scoring Policy\n\nWith the representation of each case, we model the probability of each candidate case to be selected as in-context examples by metrics of cosine similarity:\n\n(3)\n\n(4)\n\nwhere \u2225. \u2225 denotes L2 norm, N denotes the number of candidate examples. We sample k demonstration examples according to the probability computed.\n\n# 4.3 Policy Iterating\n\nIntuitively, if a subset of candidate examples leads to increasing performance of LLM to rewrite the current incomplete utterance, RLS should assign high scores to them; the more the performance increases, the higher the example scores should be. For each case in training set, we input the demonstration formulated as section 3 containing the examples selected by the current policy into the\n\n<div style=\"text-align: center;\">REWRITE\n</div>\nCANARD\nTASK\nREWRITE\nLanguage\nEnglish\nEnglish\nChinese\n# Train\n32K\n2.2K\n18K\n# Dev\n4K\n0.5K\n2K\n# Test\n6K\nNA\nNA\nCon. len\n85.4\n52.6\n17.7\nCur. len\n7.5\n9.4\n6.5\nRew. len\n11.6\n11.3\n10.5\nTable 3: Statistics of datasets. \u201cCon. len\u201d, \u201cCur. len\u201d, \u201cRew. len\u201d denote the length of contexts, incomplete utterance and rewritten utterance respectively.\n\nLLM. The performance metrics of outputs from the LLM against the golden rewritten utterance are utilized as the rewards of the policy.\nTo maximum the expected rewards of the current policy, we compute the gradient of policy as follows:\n\nwhere K denotes the number of demonstration examples, n denote the size of training set and R (a i,t, s i,t)  denotes the reward computed by inputing the current examples selected s i,t to LLM for i-th case in training set.\nTo reduce the generation time cost of LLM, we simplify the computation of \ufffd K t =1 R (a i,t, s i,t) by replacing with R (a i,K, s i,K). That is, in the process of selecting examples, we do not input the intermediate demonstrations into LLM to derive the reward. Instead, after selecting all the K examples, we input the final demonstration to LLM. To reduce the variance of learning process, we modify the reward by subtracting a baseline score, which will not affect the gradient. In practice, we compute the ICL performance with randomly selecting examples as the baseline score for IUR task. So the\n\nWith the policy gradient computed, we optimize the parameters of policy model defined in section 4.2. We compute the performance on validation set to control the termination of iterating process. In practice, we sample a small subset from the original training dataset to compose the candidate example set C and another disjoint subset to form T to train our RLS algorithm.\n\n# 5 Experiments\n5.1 Datasets and LLM\n\nFollowing Liu et al. (2020), Inoue et al. (2022) and Zhang et al. (2022) we conduct experiments on three benchmark datasets across different languages and domains in IUR field. CANARD dataset (Elgohary et al., 2019) includes English conversational question answering; TASK dataset (Quan et al., 2019) contains task-oriented English dialogues; REWRITE dataset (Su et al., 2019) is composed of open-domain Chinese dialogues. We choose ChatGLM-6B 1 as the LLM f M which does not conduct parameter updating. ChatGLM is a bilingual large language model pretrained by supervised finetuning, instruction tuning and human feedback reinforcement learning, which is suitable for our bilingual datasets.\n\n# 5.2 Metrics\n\nFollowing Liu et al. (2020); Inoue et al. (2022); Zhang et al. (2022) , we utilize BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and F-score to evaluate the IUR task. BLEU and ROUGE focus on the overall quality of the rewritten utterance and F-score concentrate more on words from the context (important words) which are argued to be harder to copy (Pan et al., 2019; Inoue et al., 2022).\n\n# 5.3 Baselines\n\nWe compare our approaches with competitive example selection methods as follows:\n1 https://github.com/THUDM/ChatGLM-6B\n\nModel\nROUGE\nBLEU\nF-score\nRL\nR1\nR2\nB1\nB2\nB3\nB4\nF1\nF2\nF3\nRandom\n52.41\n54.02\n38.74\n48.40\n40.80\n35.08\n29.70\n25.03\n17.65\n14.26\nBM25\n53.92\n56.16\n40.14\n51.03\n43.05\n37.03\n31.44\n30.00\n20.19\n15.62\nKATE\n53.00\n55.24\n39.61\n49.31\n41.60\n35.82\n30.47\n28.99\n19.51\n15.23\nEPR\n54.08\n56.26\n40.23\n51.59\n43.54\n37.51\n31.91\n29.59\n19.96\n15.62\nBSR\n54.28\n56.39\n40.50\n51.99\n44.00\n38.01\n32.44\n29.61\n20.15\n15.90\nOurs\n55.69\n57.55\n42.22\n53.29\n45.55\n39.65\n34.05\n29.52\n20.59\n16.46\n<div style=\"text-align: center;\">ROUGE\n</div>\n4: ICL Evaluations of ChatGLM with 5-shot demonstrations on CANARD dataset. Our approach significantl forms existing example selection methods, where p-values of ROUGE, BLEU and F-score are smaller tha\n\nModel\nTASK\nREWRITE\nROUGE\nBLEU\nROUGE\nBLEU\nRL\nR1\nR2\nB1\nB2\nB3\nRL\nR1\nR2\nB1\nB2\nB3\nRandom\n44.5\n45.8\n31.8\n34.1\n28.6\n25.3\n64.4\n66.8\n54.0\n61.2\n55.3\n49.7\nBM25\n46.2\n47.5\n34.1\n33.1\n27.9\n24.8\n64.6\n67.0\n55.6\n65.7\n60.1\n54.7\nKATE\n45.8\n47.3\n34.4\n33.1\n28.0\n25.0\n63.2\n65.2\n53.2\n61.6\n55.9\n50.2\nEPR\n49.2\n50.4\n37.0\n35.4\n30.1\n27.0\n65.8\n68.1\n56.4\n66.7\n61.1\n55.8\nBSR\n49.7\n51.6\n37.8\n35.2\n29.6\n26.4\n65.3\n68.0\n56.3\n65.8\n60.1\n54.7\nOurs\n57.2\n57.9\n45.1\n43.0\n38.3\n35.2\n66.5\n68.3\n56.8\n67.2\n61.7\n56.4\nTable 5: ICL Evaluations of ChatGLM with 5-shot demonstrations on Task and REWRITE dataset. significantly outperforms existing example selection methods, where p-values of ROUGE, BLEU a smaller than 0.001.\n\n5: ICL Evaluations of ChatGLM with 5-shot demonstrations on Task and REWRITE dataset. Our approach cantly outperforms existing example selection methods, where p-values of ROUGE, BLEU and F-score are\n\nRandom In this baseline, we randomly select the examples from candidate set to formulate the demonstration.\n\nBM25 (Robertson et al., 2009) In this baseline, first we utilize SpaCy NLP tools (Vasiliev, 2020) to stem words in the context and incomplete utterance of each case. Then BM25 method is adopted to estimate the relevance of examples to a given test case. The top-k relevant examples are selected to formulate the demonstration. It belongs to the sparse retrieval methods.\nKATE Liu et al. (2022) make use of SBERT (Reimers and Gurevych, 2019) to select examples which are semantically similar to the test sample and build a kNN-based unsupervised retriever. It belongs to the dense retrieval methods.\nEPR Rubin et al. (2021) assume the unsupervised retriever can act as the guide to the LM retriever and propose a two-stage approach. It first builds an unsupervised retriever (e.g., BM25) to recall surface similar examples as candidates and then constructs a supervised retriever EPR to select demonstrations from candidates. It can be seen as a combination of sparse and dense retrieval methods.\n\nEPR Rubin et al. (2021) assume the unsupervised retriever can act as the guide to the LM retriever and propose a two-stage approach. It first builds an unsupervised retriever (e.g., BM25) to recall surface similar examples as candidates and then constructs a supervised retriever EPR to select demonstrations from candidates. It can be seen as a combination of sparse and dense retrieval methods.\n\nBSR Gupta et al. (2023) propose a novel framework for selecting sets of maximally informative demonstrations for the salient aspects of the test input, e.g., reasoning patterns, entities, etc. Examples selected using this framework are informative about the test input and help the LLM understand and perform the task However, these methods fail to directly utilize the feedback by LLM and the examples selected can not necessarily improve the analogy ability of LLM.\n\n# 5.4 Experimental Details\n\nFollowing Liu et al. (2022) and Rubin et al. (2021) we utilize SentenceBERT (Reimers and Gurevych, 2019) as LM selector for English datasets and bertbase-Chinese for Chinese datasets. The sizes of candidate example set C and training set T are 500. The learning rate is set to be 1e-5. The task instruction in section 3 is designed as \u201cRewrite an incomplete utterance into an utterance which is semantically equivalent but self-contained to be understood without context. The sentence structure and expression should be consistent.\u201d for English dataset and its translated version for Chinese\n\nModel\nROUGE\nBLEU\nF-score\nRL\nR1\nR2\nB1\nB2\nB3\nB4\nF1\nF2\nF3\nOurs (100-100)\n54.22\n56.13\n40.86\n51.08\n43.49\n37.74\n32.30\n28.60\n20.02\n16.08\nOurs (100-500)\n55.08\n56.51\n41.81\n51.77\n44.62\n39.06\n33.73\n27.17\n20.18\n16.59\nOurs (500-100)\n54.12\n56.29\n40.73\n51.49\n43.65\n37.74\n32.22\n29.82\n20.29\n15.90\nOurs (500-500)\n55.69\n57.55\n42.22\n53.29\n45.55\n39.65\n34.05\n30.52\n20.59\n16.46\n<div style=\"text-align: center;\">ons of ChatGLM with different sizes of candidates and training samples on CANARD dataset. s experiments with 500 candidates and 100 training samples, and so on.\n</div>\nModel\nROUGE\nBLEU\nF-score\nRL\nR1\nR2\nB1\nB2\nB3\nB4\nF1\nF2\nF3\nEPR-3\n53.91\n55.92\n40.14\n51.44\n43.47\n37.45\n31.86\n29.15\n20.05\n15.83\nOurs-3\n54.90\n56.01\n41.22\n49.45\n42.80\n37.57\n32.53\n29.60\n20.15\n16.03\nEPR-4\n54.12\n56.27\n40.45\n51.36\n43.40\n37.37\n31.76\n29.64\n20.01\n15.59\nOurs-4\n55.11\n57.29\n41.57\n52.71\n44.81\n38.84\n33.37\n30.92\n21.12\n16.75\nEPR-5\n54.08\n56.26\n40.23\n51.59\n43.54\n37.51\n31.91\n29.59\n19.96\n15.62\nOurs-5\n55.69\n57.55\n42.22\n53.29\n45.55\n39.65\n34.05\n29.52\n20.59\n16.46\nModel\nF1\nF2\nF3\nQUEEN\n20.33\n13.25\n11.59\nOurs\n29.52\n20.59\n16.46\nTable 8: Comparing with SFT model in few-shot setting.\n\ndataset.\n\n# 5.5 Results\n\nIn Table 4, our approach outperforms all the baselines by about 1.2-1.7 ROUGE score, 1.3-2.5 BLEU score, 0.4 F2 score and 0.6 F3 score in CANARD dataset. It demonstrates the efficiency of directly utilizing feedback by LLM and RL policy gradient to train the LM selector. With the examples selected by our method, the performances of LLM are significantly improved. Our model not only improves the overall quality of utterance rewritten, but also captures the important words from the context. Compared with Random selecting examples, both BM25 and KATE baselines derive better performance. It shows the textual similarity captured by sparse retrieval like BM25 and the semantic similarity captured by KATE can help select better examples to prompt the LLM for IUR task. EPR and BSR show an overall better performance compared with the sole sparse or dense retrieval methods. It demonstrates the combination of sparse or dense retrieval can capture both the textual and semantic\n\n<div style=\"text-align: center;\">BLEU\n</div>\nsimilarity between candidate examples and the test case. The examples selected are better prompts to LLM for IUR task. Compared with EPR, BSR behaves a better performance across different metrics. BM25 behaves as the effective supervision for the LM retriever and releasing the constraint can improve the example selection furthermore.\nIn TASK dataset, our approach outperforms all the baselines by about 6.3-7.5 ROUGE score and 7.6-8.2 BLEU score. Compared with CANARD dataset, TASK contains more complex and diverse dialogue topics. It demonstrates the efficiency of utilizing direct LLM feedback to train the LM retriever and select examples to prompt the LLM for IUR task. In REWRTTE dataset, our approach outperforms all the baselines by about 0.6 ROUGE and BLEU score, which shows our stable efficiency with different languages.\n\n# 6 Analysis\n\nIn this part, first we explore our performance with different sizes of candidate set and training set. Then we probe the effect of different numbers of examples in demonstrations. Furthermore, we compare our approach with the SFT method in few shot setting. Finally, we explore the reason why examples selected by our approach can improve the analogy ability of LLM.\n\nModel\nIncomplete\nRewritten\nLength\nPOS\nChunk\nLength\nPOS\nChunk\nBSR\n8.55\n6.29\n2.33\n11.91\n7.78\n3.45\nOurs\n10.81\n6.99\n3.23\n13.36\n7.95\n4.03\n9: Complexity and abundance of example selected. \u201cIncomplete\u201d denotes the metrics of incomplete utter Rewritten\u201d denotes the metrics of rewritten utterance, \u201cLength\u201d, \u201cPOS\u201d, and \u201cChunk\u201d denote utterance length r of POS types and number of text chunks respectively.\n\nModel\nROUGE\nBLEU\nF-score\nRL\nR1\nR2\nB1\nB2\nB3\nB4\nF1\nF2\nF3\nLength\n50.84\n52.13\n37.17\n46.76\n39.41\n33.89\n28.67\n19.95\n14.46\n11.99\nPOS\n52.17\n53.77\n38.69\n47.92\n40.44\n34.80\n29.46\n23.89\n17.09\n13.92\nChunk\n53.68\n55.20\n40.34\n52.09\n44.68\n38.86\n33.29\n24.70\n18.15\n14.90\nOurs\n55.69\n57.55\n42.22\n53.29\n45.55\n39.65\n34.05\n29.52\n20.59\n16.46\nTable 10: ICL Evaluations of ChatGLM with 5-shot demonstrations on CANARD dataset.\n\n# 6.1 Different Sizes of Candidates and Training Samples\n\nIn Table 4, the number of candidates # C  and training samples # T  are 500. In this part, we do further experiments with # C = 100, # T = 100; # C = 100, # T = 500; # C = 500, # T = 100 and freeze other hyperparameters respectively. In Table 6, generally, with more candidates and training samples, our approach will select better examples for the demonstration. With 100 candidates and 100 training samples, our approach beats random selection by about 2.0 ROUGE score, 2.5 BLEU score and 2.0 F-score. It is also comparable to the competitive baseline BSR with 500 candidates and training samples. It shows the efficiency to utilize direct LLM feedback to train the LM retriever. Compared with setting 100-500, our approach outperforms by about 0.7 ROUGE score, 0.8 BLEU score and 0.9 F-score. It demonstrates our ability to select better examples to improve the analogy ability of LLM With more candidates. Compared with setting 500-100, our approach outperforms by about 1.4 ROUGE score, 1.9 BLEU score and 0.2 F-score. It shows more training samples improve the selection of our approach from the candidate set for IUR task.\n\n# 6.2 Different Number of Examples in Demonstration\n\nIn Table 4, we conduct the experiments with 5shot demonstrations. In this part, we do further experiments with 3-shot and 4-shot demonstrations. In Table 7, generally with more examples in the demonstration, our approach improves the\n\nICL performance of LLM for IUR task. Especially, with more demonstration examples, our approach derives more improvement compared with the competitive baseline EPR. It demonstrates the efficiency of our RLS by directly utilizing LLM feedback to train the LM retriever and improve the ICL performance of LLM.\n\n# 6.3 Comparing with SFT Model\n\nIn this part, we compare our approach with the existing state-of-the-art QUEEN (Liu et al., 2020) in IUR field. QUEEN tackles IUR task by finetuning PTM (Devlin et al., 2018) and constructing the word edit matrix. Different from QUEEN , our approach utilizes LM as a proxy to select appropriate examples and parameters of the answer generator ChatGLM are fixed. To keep a fair comparison, we assign the candidate set and training set in our approach as the training set of QUEEN. In Table 8, our approach QUEEN by 9.2 F1 score, 7.3 F2 score and 4.9 F3 score. F-score concentrate on the words from the context, which are argued to be harder to copy (Pan et al., 2019). It shows our efficiency to capture important words from the context to rewrite the incomplete utterance. Considering our RLS approach does not depend on the LLM server (ChatGLM in our experiments), it is promising for our approach to derive better results for IUR task with stronger LLM.\n\n# 6.4 What Examples are Appropriate for IUR\n\nIn this part, we explore the reason why examples selected by our approach serve as better demonstrations for the LLM to solve IUR task.\n\nModel\nROUGE\nBLEU\nF-score\nRL\nR1\nR2\nB1\nB2\nB3\nB4\nF1\nF2\nF3\nRandom\n78.07\n79.34\n66.50\n69.05\n64.15\n60.26\n56.35\n52.86\n43.49\n38.14\nBSR\n80.13\n81.24\n68.78\n73.54\n67.37\n65.42\n59.78\n56.16\n48.36\n44.89\nOurs\n82.77\n84.04\n70.37\n79.07\n73.81\n69.56\n65.47\n62.64\n52.61\n47.30\n<div style=\"text-align: center;\">ROUGE\n</div>\nTable 11: ICL Evaluations of gpt3.5 with 5-shot demonstrations on TASK dataset.\n\nModel\nROUGE\nBLEU\nR1\nR2\nB1\nB2\nB3\nBSR\n51.6\n37.8\n35.2\n29.6\n26.4\nOurs\n57.9\n45.1\n43.0\n38.3\n35.2\nOurs(r)\n57.6\n44.5\n43.3\n38.6\n34.9\nTable 12: ICL Evaluations of example orders with ChatGLM on TASK dataset.\n\nWe assume that other than the textual and semantic similarity of examples with the current test case, the complexity and abundance of examples in the demonstrations also matter for IUR task. We evaluate the complexity and abundance of examples by three metrics of incomplete utterance and rewritten utterance: 1. the length of utterance; 2. the number of Part Of Speech (POS) tagging types (Kumawat and Jain, 2015). 3. the number of text chunks (Ramshaw and Marcus, 1999). We assume the examples are more complex and abundant with longer utterances, more POS tag types and more text chunks. In practice, we adopt SpaCy to do POS tagging and text chunking. In Table 9, the examples selected by our policy-based RL framework have longer utterances, more POS types and more text chunks though without explicit assignment, We argue that the complexity and abundance of examples in the demonstration are important to improve the analogy ability of LLM. What if we select the examples only by metrics of complexity and abundance? To address this issue, we select examples from the candidate set to construct the demonstration by metrics of the length of utterance, the number of POS types, the number of text chunks respectively and conduct the experiments on CANARD dataset. In Table\n10, if selecting the examples only by the metric of number of text chunks, the ICL results will drop by about 2.1 ROUGE score, 0.9 BLEU score, 2.9 F-score on CANARD dataset. It will drop more if selecting the examples only by the metric of utterance length or POS types. It shows only selecting examples by complexity and abundance, the LLM\n\nperformance will not necessarily improve. Balancing the abundance and the similarity with the test case of examples is important to improve the ICL ability of LLM. Our approach attains the balance of complexity and abundance, as well as semantic similarity with the test case without explicit assignment.\n\n# 6.5 Efficiency of Larger LLM\n\nIn this part, we conduct the experiments by applying our approach with gpt3.5 in TASK dataset. In Table 11, our RLS outperforms random selecting examples by about 4.4 ROUGE score, 9.5 BLEU score and 9.4 F-score in TASK dataset. It demonstrates the efficiency of directly utilizing LLM feedback to train the LM retriever with policy-based RL and improve the analogy ability of larger LLM. With the emergence of larger LLM, it is promising for our approach to select appropriate examples to improve the ICL performance of LLM.\n\n# 6.6 Order of Examples\n\nIn this part, we probe the effect of the order of examples in the demonstration. We arrange examples in the demonstration by the order of sampling with Eq. 3  or the reverse order and conduct the experiments on TASK dataset. Table 12  shows if arranging the examples by the reverse order of sampling, our approach shows comparable performance . It demonstrates the stable efficiency of our directly selecting the examples that can improve the analogy ability of LLM by policy gradient.\n\n# 7 Conclusion\n\nExisting example selection methods fail to directly utilize the LLM feedback to choose appropriate examples in the demonstration. We propose a novel and effective example selection framework by directly adopting the LLM feedback to train the LM selector with policy-based RL. Our approach significantly improves the ICL performance of LLM for IUR tasks.\n\nwe propose our novel and effective example selection framework by directly adopting the LLM feedback to train the LM selector with policy-based RL and demonstrate our efficiency on three benchmark datasets in this field. Though only utilizing the LLM ChatGLM-7B and gpt3.5, we will conduct experiments with diverse kinds of LLMs in future research.\n\n# References\n\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. arXiv preprint arXiv:1808.07036.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234.\nAhmed Elgohary, Denis Peskov, and Jordan BoydGraber. 2019. Can you unpack that? learning to rewrite questions-in-context. Can You Unpack That? Learning to Rewrite Questions-in-Context.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. arXiv preprint arXiv:2212.04037.\nShivanshu Gupta, Sameer Singh, and Matt Gardner. 2023. Coverage-based example selection for incontext learning. arXiv preprint arXiv:2305.14907.\nJie Hao, Linfeng Song, Liwei Wang, Kun Xu, Zhaopeng Tu, and Dong Yu. 2021. Rast: Domain-robust dialogue rewriting as sequence tagging. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4913\u20134924.\n\nMengzuo Huang, Feng Li, Wuhe Zou, and Weidong Zhang. 2021. Sarg: A novel semi autoregressive generator for multi-turn incomplete utterance restoration. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 13055\u201313063.\nShumpei Inoue, Tsungwei Liu, Nguyen Hong Son, and Minh-Tien Nguyen. 2022. Enhance incomplete utterance restoration by joint learning token extraction and text generation. arXiv preprint arXiv:2204.03958.\nHyuhng Joon Kim, Hyunsoo Cho, Junyeob Kim, Taeuk Kim, Kang Min Yoo, and Sang-goo Lee. 2022. Self-generated in-context learning: Leveraging autoregressive language models as a demonstration generator. arXiv preprint arXiv:2206.08082.\nDeepika Kumawat and Vinesh Jain. 2015. Pos tagging approaches: A comparison. International Journal of Computer Applications, 118(6).\nItay Levy, Ben Bogin, and Jonathan Berant. 2022. Diverse demonstrations improve in-context compositional generalization. arXiv preprint arXiv:2212.06800.\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning.  Advances in Neural Information Processing Systems, 35:1950\u20131965.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt3? arXiv preprint arXiv:2101.06804.\nQian Liu, Bei Chen, Jian-Guang Lou, Bin Zhou, and Dongmei Zhang. 2020. Incomplete utterance rewriting as semantic segmentation. arXiv preprint arXiv:2009.13166.\nZhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou, and Xiaojiang Liu. 2019. Improving open-domain dialogue systems via multi-turn incomplete utterance restoration. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1824\u20131833.\nKishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\nJun Quan, Deyi Xiong, Bonnie Webber, and Changjian Hu. 2019. Gecor: An end-to-end generative ellipsis and co-reference resolution model for task-oriented dialogue. arXiv preprint arXiv:1909.12086.\n\nLance A Ramshaw and Mitchell P Marcus. 1999. Text chunking using transformation-based learning.  Natural language processing using very large corpora, pages 157\u2013176.\nSiva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge.  Transactions of the Association for Computational Linguistics, 7:249\u2013266.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond.  Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.\nTaylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364.\nHui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Pengwei Hu, Cheng Niu, and Jie Zhou. 2019. Improving multi-turn dialogue modelling with utterance rewriter. arXiv preprint arXiv:1906.07004.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. 2019. Dream: A challenge data set and models for dialogue-based reading comprehension.  Transactions of the Association for Computational Linguistics, 7:217\u2013231.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nYuli Vasiliev. 2020. Natural language processing with Python and spaCy: A practical introduction. No Starch Press.\nYong Zhang, Zhitao Li, Jianzong Wang, Ning Cheng, and Jing Xiao. 2022. Self-attention for incomplete utterance rewriting. In  ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8047\u20138051. IEEE.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In  International Conference on Machine Learning, pages 12697\u201312706. PMLR.\nA Example Appendix\n\nLance A Ramshaw and Mitchell P Marcus. 1999. Text chunking using transformation-based learning.  Natural language processing using very large corpora, pages 157\u2013176.\nSiva Reddy, Danqi Chen, and Christopher D Manning. 2019. Coqa: A conversational question answering challenge.  Transactions of the Association for Computational Linguistics, 7:249\u2013266.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond.  Foundations and Trends\u00ae in Information Retrieval, 3(4):333\u2013389.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633.\nTaylor Sorensen, Joshua Robinson, Christopher Michael Rytting, Alexander Glenn Shaw, Kyle Jeffrey Rogers, Alexia Pauline Delorey, Mahmoud Khalil, Nancy Fulda, and David Wingate. 2022. An information-theoretic approach to prompt engineering without ground truth labels. arXiv preprint arXiv:2203.11364.\nHui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Pengwei Hu, Cheng Niu, and Jie Zhou. 2019. Improving multi-turn dialogue modelling with utterance rewriter. arXiv preprint arXiv:1906.07004.\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. 2019. Dream: A challenge data set and models for dialogue-based reading comprehension.  Transactions of the Association for Computational Linguistics, 7:217\u2013231.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nYuli Vasiliev. 2020. Natural language processing with Python and spaCy: A practical introduction. No Starch Press.\nYong Zhang, Zhitao Li, Jianzong Wang, Ning Cheng, and Jing Xiao. 2022. Self-attention for incomplete utterance rewriting. In  ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8047\u20138051. IEEE.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In  International Conference on Machine Learning, pages 12697\u201312706. PMLR.\nA Example Appendix\n\n",
    "paper_type": "method",
    "attri": {
        "background": "In recent years, there has been a growing focus on multi-turn dialogue modeling, where incomplete utterances often occur due to co-reference or ellipsis. Existing methods for Incomplete Utterance Rewriting (IUR) have shown impressive performances but do not effectively utilize direct feedback from large language models (LLMs) to improve the selection of in-context examples needed for better performance.",
        "problem": {
            "definition": "The paper addresses the challenge of rewriting incomplete utterances into self-contained, semantically equivalent utterances that can be understood independently.",
            "key obstacle": "The main difficulty lies in the selection of in-context examples that can effectively enhance the analogy ability of LLMs, as existing methods do not leverage LLM feedback to guide this selection."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that certain examples can significantly improve the performance of LLMs in rewriting incomplete utterances, suggesting a need for a more effective selection method.",
            "opinion": "The proposed method, called Reinforcement Learning for Example Selection (RLS), utilizes a policy-based reinforcement learning framework to optimize the selection of examples based on LLM feedback.",
            "innovation": "The primary innovation is the introduction of a policy-based RL approach that directly uses LLM outputs as feedback to train the example selector, differing from traditional methods that do not incorporate such feedback."
        },
        "method": {
            "method name": "Reinforcement Learning for Example Selection",
            "method abbreviation": "RLS",
            "method definition": "RLS is a framework that combines a language model selector with an LLM generator to improve the selection of in-context examples for IUR tasks through reinforcement learning.",
            "method description": "RLS optimizes the selection of examples by utilizing the performance feedback from an LLM to enhance the analogy ability during the rewriting process.",
            "method steps": [
                "Encode candidate examples using a language model.",
                "Select the top-k examples based on their encoded representations.",
                "Use the LLM to generate rewritten utterances from the selected examples.",
                "Compute rewards based on the performance of the LLM outputs.",
                "Update the selection policy using the computed rewards."
            ],
            "principle": "The effectiveness of RLS lies in its ability to leverage direct feedback from the LLM, allowing it to adaptively select examples that maximize the performance of the model in rewriting tasks."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three benchmark datasets: CANARD, TASK, and REWRITE, comparing RLS against existing example selection methods and supervised finetuning models.",
            "evaluation method": "Performance was assessed using metrics such as BLEU, ROUGE, and F-score, with comparisons made against baseline methods to evaluate improvements."
        },
        "conclusion": "The experiments demonstrate that RLS significantly outperforms existing example selection methods, highlighting the importance of utilizing LLM feedback in enhancing ICL performance for IUR tasks.",
        "discussion": {
            "advantage": "RLS stands out due to its ability to directly utilize LLM feedback for example selection, leading to improved performance in IUR tasks compared to traditional methods.",
            "limitation": "One limitation is that the method may require careful tuning of the RL parameters to achieve optimal performance, and it might be sensitive to the choice of LLM used.",
            "future work": "Future research could explore the application of RLS with different types of LLMs and investigate further enhancements to the example selection process."
        },
        "other info": {
            "additional notes": "RLS was shown to have advantages in few-shot settings, indicating its potential effectiveness in practical applications where labeled data may be scarce."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the challenge of rewriting incomplete utterances into self-contained, semantically equivalent utterances that can be understood independently."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, called Reinforcement Learning for Example Selection (RLS), utilizes a policy-based reinforcement learning framework to optimize the selection of examples based on LLM feedback."
        },
        {
            "section number": "3.1",
            "key information": "RLS optimizes the selection of examples by utilizing the performance feedback from an LLM to enhance the analogy ability during the rewriting process."
        },
        {
            "section number": "4.1",
            "key information": "RLS stands out due to its ability to directly utilize LLM feedback for example selection, leading to improved performance in Incomplete Utterance Rewriting tasks compared to traditional methods."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is that the method may require careful tuning of the RL parameters to achieve optimal performance, and it might be sensitive to the choice of LLM used."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that RLS significantly outperforms existing example selection methods, highlighting the importance of utilizing LLM feedback in enhancing in-context learning performance for IUR tasks."
        }
    ],
    "similarity_score": 0.7327871067728834,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-Context Learning with Reinforcement Learning for Incomplete Utterance Rewriting.json"
}