{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.18293",
    "title": "1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization",
    "abstract": "Despite the advances in probabilistic model checking, the scalability of the verification methods remains limited. In particular, the state space often becomes extremely large when instantiating parameterized Markov decision processes (MDPs) even with moderate values. Synthesizing policies for such \\emph{huge} MDPs is beyond the reach of available tools. We propose a learning-based approach to obtain a reasonable policy for such huge MDPs.\n  The idea is to generalize optimal policies obtained by model-checking small instances to larger ones using decision-tree learning. Consequently, our method bypasses the need for explicit state-space exploration of large models, providing a practical solution to the state-space explosion problem. We demonstrate the efficacy of our approach by performing extensive experimentation on the relevant models from the quantitative verification benchmark set. The experimental results indicate that our policies perform well, even when the size of the model is orders of magnitude beyond the reach of state-of-the-art analysis tools.",
    "bib_name": "azeem2024123gopolicysynthesisparameterized",
    "md_text": "# 1\u20132\u20133\u2013Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization\u22c6\nMuqsit Azeem1, Debraj Chakraborty3, Sudeep Kanav3, Jan K\u0159et\u00ednsk\u00fd1,3, Mohammadsadegh Mohagheghi2, Stefanie Mohr1, and Maximilian Weininger4\n1 Technical University of Munich, Munich, Germany 2 Vali-e-Asr University of Rafsanjan, Rafsanjan, Iran 3 Masaryk University, Brno, Czech Republic 4 Institute of Science and Technology, Vienna, Austria\nAbstract. Despite the advances in probabilistic model checking, the scalability of the verification methods remains limited. In particular, the state space often becomes extremely large when instantiating parameterized Markov decision processes (MDPs) even with moderate values. Synthesizing policies for such huge MDPs is beyond the reach of available tools. We propose a learning-based approach to obtain a reasonable policy for such huge MDPs. The idea is to generalize optimal policies obtained by model-checking small instances to larger ones using decision-tree learning. Consequently, our method bypasses the need for explicit state-space exploration of large models, providing a practical solution to the state-space explosion problem. We demonstrate the efficacy of our approach by performing extensive experimentation on the relevant models from the quantitative verification benchmark set. The experimental results indicate that our policies perform well, even when the size of the model is orders of magnitude beyond the reach of state-of-the-art analysis tools.\nKeywords: model checking \u00b7 probabilistic verification \u00b7 Markov decision process \u00b7 policy synthesis.\n# 1 Introduction\nMarkov decision processes (MDPs) are the model for combining probabilistic uncertainty and non-determinism. MDPs come with a rich theory and algorithmics developed over several decades with mature verification tools arising 20 years ago [30] and proliferating since then [10]. Despite all this effort, the scalability of the methods is considerably worse than of those used for verification of non-deterministic systems with no probabilities, even for basic problems.\n\u22c6This research was funded in part by the German Research Foundation (DFG) project 427755713 GOPro, the DFG GRK 2428 (ConVeY), and the MUNI Award in Science and Humanities (MUNI/I/1757/2021) of the Grant Agency of Masaryk University.\n\u22c6This research was funded in part by the German Research Foundation (DFG) project 427755713 GOPro, the DFG GRK 2428 (ConVeY), and the MUNI Award in Science and Humanities (MUNI/I/1757/2021) of the Grant Agency of Masaryk University.\nWhat to do about very large models? Researchers have made various attempts to tackle this issue, however, only with limited success. Firstly, prominent techniques which work well in non-probabilistic verification, such as symbolic techniques [30], abstraction [31], and symmetry reduction [17], are harder to apply efficiently in the probabilistic setting. Secondly, \u201cengineering\u201d improvements, such as the use of external storage [21] or parallelization, help by a significant, but principally very limited factor. Thirdly, there is a relaxation of the guarantees on the precision and/or certainty of the result, which we describe in detail below. The result of the analysis is typically a number (called the value), such as the expected reward or the probability to reach a given state, maximized (or minimized) over all resolutions of non-deterministic choices (called policies, strategies, schedulers, adversaries, or controllers in different applications). It is generally accepted that the precise number is not needed and an approximation is sufficient in most settings. Interestingly, until a few years ago [18, 7, 4], typically only the under-approximations were computed for the fundamental (maximization) problems, with no reliable over-approximations (with dual issues for minimization). It is worth noting that over-approximating is inherently harder since reasoning that the value cannot be greater than x involves the claim that all policies induce a lower value. In contrast to this universal quantification, the existential one is sufficient for under-approximating: upon providing a policy, its value forms automatically a lower bound, which is typically easier to compute. Consequently, many best-effort approaches, such as reinforcement learning (RL) [46] and lightweight statistical model checking [9] simply try to find a good policy while giving only empirically good chances to be close to the optimum. This is sufficient in the setting of (i) policy synthesis, where a \u201cgood enough\u201d (close to optimum), but not necessarily optimal, controller is sought, or (ii) bug hunting and falsification, where finding significant counter-examples cheaply is desirable. However, the cal quality of the results relies on certain assumptions of these methods: RL results suffer when the rewards in the model are sparse (e.g., in the case of reachability) and lightweight statistical model checking suffers when near-optimal policies are not abundant. To summarize, synthesizing practically good policies is sufficient in many settings and also the only way when the systems are too large. Yet, when the system is extremely large, the available techniques either run out of resources or yield policies that are far from optimum (and close to random). Examining the structure of large MDPs in standard benchmark sets, e.g. [22], reveals that their huge sizes are typically not due to astronomically large humanwritten code, but rather because the MDPs are parameterized (e.g., by the number of participants in a protocol) and then instantiated with large values. Accordingly, this paper proposes a new approach to scalable policy synthesis for parameterized MDPs. Namely, it produces good policies for arbitrarily large instantiations of parameterized MDPs in particular those beyond the reach of any state-of-the-art tools. We focus on probabilistic reachability\n(i) for simplicity and (ii) because it is a fundamental building block for many other problems.\nOur main idea is to generalize the decisions taken by the optimal policies for the smaller instances: Instead of investigating a huge MDP, we synthesize optimal policies for the given parameterized MDP by instantiating it with small numbers. We then generalize this information and learn a policy that can be applied to any instantiation with an arbitrarily larger number. It is important to note that we generalize the corresponding decisions (i.e., the policy itself), not the values of the states across different parameterizations. Indeed, while the numeric values can differ vastly, the optimal behaviour is often similar in all instantiations. In order to capture this regularity, we thus need a symbolic representation of a policy, which applies to all instantiations. Decision trees (DT) can provide such a representation. Moreover, since they can represent policies explainably [6, 2] and capture the essence of the decisions, not just a list of state-action pairs, they generalize well. As an illustrative task for our \u201cgeneralization\u201d, consider a buggy mutual exclusion protocol with a high number of participants. While finding the bug with many participants may be hard, an exhaustive investigation of the case with two participants may reveal a scenario violating the exclusion. A similar scenario can then also happen with many participants where the choices of the remaining participants may be irrelevant. Consequently, the key decisions in the policy to find bug with two participant may also be used for finding the bug with multiple participants.\nOur contribution can be summarized as follows:\n# Our contribution can be summarized as follows:\n\u2013 We provide a simple and elegant way of computing practically good policies for parameterized MDPs of any size (as long as some instantiations exist that are small enough so that some technique can be applied), in particular also orders of magnitude beyond the reach of any other methods. The method is based on generalizing5 policies via their decision-tree representations. The method scales constantly in the parameter instantiation since it applies available techniques to a fixed number of small base instantiations, and the large instantiation is never explicitly considered. \u2013 We demonstrate the efficacy of the method experimentally on standard benchmarks. In particular, from the practical perspective, we observe that our policies mostly achieve values that are close to the actual optimum. Note that, this in principle cannot be guaranteed for instantiations too large for precise methods to apply, which are exactly of our interest. Nevertheless, the often consistent results on the smaller instantiations convincingly substantiate the expectation that the policies perform well also for the large instantiations.\n5 The nature of our generalization-based policy synthesis is also portrayed by ou quipping title \u201c1\u20132\u20133\u2013Go!\u201d: Find out what works for cases 1, 2, and 3, then \u201cGo! and apply it for arbitrary large values of the parameters.\n\u2013 Finally, comparing to the benchmarks where our policies do not perform so well, we identify aspects of the models indicating where our heuristic generalizes well and where either more tailored or completely different techniques are required. It should be emphasized that we regard the simplicity of our approach rather as an advantage, making it easy to exploit. While there is a body of work on policy representation (via post-processing them), the use of DT to compute policies is very limited (as described in the Related Work below) and, to the best of our knowledge, non-existent for computing/generalizing them for arbitrarily large systems. Altogether, this simple, yet efficient idea deserves to be finally explored.\n\u2013 Finally, comparing to the benchmarks where our policies do not perform so well, we identify aspects of the models indicating where our heuristic gener alizes well and where either more tailored or completely different technique are required.\nIt should be emphasized that we regard the simplicity of our approach rather as an advantage, making it easy to exploit. While there is a body of work on policy representation (via post-processing them), the use of DT to compute policies is very limited (as described in the Related Work below) and, to the best of our knowledge, non-existent for computing/generalizing them for arbitrarily large systems. Altogether, this simple, yet efficient idea deserves to be finally explored.\n# Related work\nSymbolic approaches are widely used as for alleviating the challenges of the state explosion problem [3]. These approaches are based on data structures storing the information of a model compactly. In particular, the multi-terminal version of BDDs (MTBDDs) has been developed for probabilistic model checking [28, 37, 40]. In a sense, our approach is also symbolic, since we represent the policy using a decision tree. This data structure is most suitable for the goal of explainability, as argued in, e.g., [2]. Reduction techniques try to reduce the state space of the model while the smaller model satisfies the same set of properties. A symmetry reduction technique for probabilistic models has been proposed in [32] for systems with several symmetric components. Probabilistic bisimulation is available for MDPs and discrete-time Markov chains (DTMCs) that reduce the original model to the smallest one that satisfies the same set of temporal logic formulae [15]. Considering a subset of temporal logic formulae, more efficient techniques have been proposed in [27] for reducing the model to a smaller one. Applying reduction on a high-level description before constructing the resulting model is available in [45, 36]. Further techniques improving scalability of traditional algorithms include the following. Using secondary storage in an efficient way to keep a sparse representation of a large model has been studied in [21]. Compositional techniques have been developed for the verification of stochastic systems [13, 35]. Prioritizing computation can reduce running times in many case studies by using topological state ordering [34, 11] or learnt prioritizing [7, 39, 29]. All the above techniques help solving larger models, however, only up to a certain limit. In contrast, our approach synthesizes policy that can be applied to arbitrarily large instances. Statistical model checking (SMC) is an alternative solution for approximating the quantitative properties [24, 7, 23] by running a set of simulations on the model to approximate the requested values, while providing a confidence interval for the precision of computed values for discrete and continuous-time\nMarkov chains (DTMCs and CTMCs). This is scalable since the number of samples does not depend on the size of the model. Still, the length of the simulations does. Using SMC for MDPs faces the difficulty of resolving non-determinism. A smart-sampling method has been proposed in [12] that considers a set of random policies, with some of them hopefully approximating the optimal one; however, this method cannot generally provide a confidence interval for the precision of computations [23]. Machine learning within formal verification of MDP has been widely studied for a decade since the seminal [24]. An L\u2217learning approach has been developed in [47] to learn an MDP model efficiently. Neural networks and regression can be used to resolve non-determinism of large MDPs and provide the opportunity of applying SMC for this class of models [43, 16]. Reinforcement learning has also been adapted to the setting of verification with objectives such as reachability [7, 19], but the sparsity of the rewards is still an issue affecting the scalability. Still, prioritizing the subset of the states that has the biggest impact on the value can allow for verifying huge models if such a subset is small and easy to find [7, 29]. Unlike reinforcement learning (RL), which suggests policies for specific models through random exploration, our approach generalizes policies for various instances by computing optimal policies on smaller models and generalizing them. Pyeatt and Howe [42] propose using decision trees to approximate value function for discounted rewards in reinforcement learning. However, we learn a DT that is a valid policy for any instantiations of the parameterized MDP, whereas the DT learned in [42] is applicable only to the model under consideration. Decision trees have been used as a data structure for representing MDP policies [2, 6]. Interestingly, while binary decision diagrams (BDD) may appear to the verification community as a suitable candidate, it has been shown that DT are more appropriate if adequately used [6, 2, 39] due to their ability to handle various predicates and complex relationships, enhancing explainability. Another advantage of DTs is the ability to declare some inputs as uninteresting (\u201cdon\u2019t care\u201d inputs), saving on size via semantics of the controller.\n# 2 Preliminaries\nWe provide basic definitions in Section 2.1, then describe what it means for models to be parameterized and scalable in Section 2.2 and finally recall how decision trees can be used for representing policies in Section 2.3.\n# 2.1 Markov decision processes with a reachability objective\nA probability distribution over a discrete set X is a function \u00b5 : X \u2192[0, 1] where \ufffd x\u2208X \u00b5(X) = 1. We denote the set of all probability distributions over X by\nA probability distribution over a discrete set X is a function \u00b5 : X \u2192[0, 1] where \ufffd x\u2208X \u00b5(X) = 1. We denote the set of all probability distributions over X by D(X).\nA probability distribution over a discrete set X is a function \u00b5 : X \u2192[0, 1] where \ufffd x\u2208X \u00b5(X) = 1. We denote the set of all probability distributions over X by D(X).\nDefinition 1. A (finite) Markov Decision Process (MDP) is a tuple M = (S, A, \u03b4, \u00afs, G) where S is a finite set of states, A is a finite set of actions, overloading A(s) to denote the (non-empty) set of enabled actions for every state s \u2208S, \u03b4 : S \u00d7 A \u2192D(S) is a probabilistic transition function mapping each state s \u2208S and enabled action a \u2208A(s) to a probability distribution over successor states, \u00afs \u2208S is the initial state, and G \u2286S is the set of goal states.\nA Markov chain (MC) can be seen as an MDP where |A(s)| = 1 for all s \u2208S, i.e. a system exhibiting only probabilistic behavior, but no non-determinism. The semantics of an MDP are defined in the usual way by means of policies and paths in the induced Markov chain. An infinite path \u03c1 = s1s2 . . . \u2208S\u03c9 is an infinite sequence of states. A policy is a function \u03c3 : S \u2192D(A) that, intuitively, prescribes for every state which action to play. The policy is called deterministic if in every state it selects a single action surely, otherwise it is randomized. Note that we limit our definition of policies w.l.o.g. to those that are memoryless (history-independent), i.e. those that depend only on the current state, not on a whole path. We denote the i-th state on a path as \u03c1i, the set of all paths as Paths, and the set of all policies as \u03a3. By using a policy \u03c3 to resolve all nondeterministic choices in an MDP M, we obtain a Markov chain M\u03c3 [3, Definition 10.92]. This Markov chain induces a unique probability measure P\u03c3 s over paths starting in state s [3, Definition 10.10]. The reachability objective is included in our definition of MDP in the form of the initial state \u00afs and the set of goal states G. Intuitively, the value V of a reachability objective is the optimal probability to reach some goal state when starting in the initial state; formally V(M) = opt\u03c3\u2208\u03a3P\u03c3 \u00afs [\u2662G], where opt \u2208 {max, min} indicates whether we are trying to reach or avoid the set of goal states and \u2662G = {\u03c1 \u2208Paths | \u2203i.\u03c1i \u2208G} denotes the set of all paths that reach a goal state. One can restrict this optimum to the deterministic memoryless policies [41, Proposition 6.2.1].\n# 2.2 State space structure and scalable models\nFor learning (e.g. of DTs) to be effective, it is important that the state space of MDP is structured, i.e. every state is a tuple of values of state variables. In other words, the state space of the system is not monolithic (e.g. states defined by a simple numbering), but in fact, there are multiple factors defining it, for example, time or protocol state. Each of these factors is represented by a state variable vi with domain Di. Thus, every state s \u2208S is in fact a tuple (v1, v2, . . . , vn), where each vi \u2208Di is the value of a state variable. A parameterized MDP can be described as a variant of standard MDP where certain parameters are not fixed constants but instead can take different values within a parameter space. These parameters can be associated to the state-space of the system (for example, lower or upper bound of a state variable) or transition dynamics (the probabilities can be functions of the parameter). We provide the formal definition below.\n\u2013 \u0398 is the parameter space, \u2013 For each \u03b8 \u2208\u0398, the tuple (S\u03b8, A\u03b8, \u03b4\u03b8, \u00afs\u03b8, G\u03b8) defines an MDP instance, where: \u2022 S\u03b8 is the set of states, \u2022 A\u03b8 is the set of actions, \u2022 \u03b4\u03b8 : S\u03b8 \u00d7 A\u03b8 \u2192D(S\u03b8) is the probabilistic transition function, \u2022 \u00afs\u03b8 \u2208S\u03b8 is the initial state, \u2022 G\u03b8 \u2286S\u03b8 is the set of goal states.\nIn this framework, different parameter values \u03b8 \u2208\u0398 yield different instances of the MDP, and the parameterization can affect the state space, transition dynamics, or both.\nIntuitively, a parameterized MDP can be seen as a family of MDPs where different value of parameter gives different instance of the MDP. In particular, this typically makes the models scalable: by increasing the values of parameters in the model description, one can scale up the size of the state space of the model. MDPs in the PRISM benchmark suite [33] and the quantitative verification benchmark set [22] have these properties of being structured and scalable. Example 1. Consider the MDP in Figure 1. Every state is a tuple (m, x) of two state variables m and x with domains Dm = {0, 1, . . . , k} and Dx = {0, 1, 2}. The state variable m indicates which of the k + 1 blocks we are in, while the state variable x indicates the position inside a block. The block with m = 0 is special: it contains the initial state (0,0), the goal state (0,2), and a sink state (0,1) which cannot reach the goal. All other blocks look as follows: for every m \u2208[1, k], the x = 0 state can choose to continue to x = 1 (action a) or self-loop (action b). For every m \u2208[1, k \u22121], the x = 1 state can go back to x = 0 in the same block (action a) or leave the block (action b). When using b, there is a 50% chance of going to the sink state (0, 1) and a 50% chance to continue to x = 0 in the (m+1)-th block. In the k-th block, the action leaving the block progresses to the goal state. The model is scalable, since the number of blocks k can be increased arbitrarily. This affects both the size of the state space, which is 2k + 3, as well as the maximum reachability probability, which is 0.5k\u22121. We assume the MDPs are defined using high-level modeling languages such as Probmela [3], PRISM [33] or MODEST [20]. In such modeling languages, MDPs are often represented as a composition of multiple identical components, called modules. For example, in a distributed system where multiple processes are interacting or sharing common resources, each process can be described as a separate module. In the context of this paper, we also consider the number of modules as a parameter. Example 2 (Dining philosophers). The dining philosophers problem involves a number of philosophers, seated around a circular table, blessed with infinite\nIntuitively, a parameterized MDP can be seen as a family of MDPs where different value of parameter gives different instance of the MDP. In particular, this typically makes the models scalable: by increasing the values of parameters in the model description, one can scale up the size of the state space of the model. MDPs in the PRISM benchmark suite [33] and the quantitative verification benchmark set [22] have these properties of being structured and scalable. Example 1. Consider the MDP in Figure 1. Every state is a tuple (m, x) of two state variables m and x with domains Dm = {0, 1, . . . , k} and Dx = {0, 1, 2}. The state variable m indicates which of the k + 1 blocks we are in, while the state variable x indicates the position inside a block. The block with m = 0 is special: it contains the initial state (0,0), the goal state (0,2), and a sink state (0,1) which cannot reach the goal. All other blocks look as follows: for every m \u2208[1, k], the x = 0 state can choose to continue to x = 1 (action a) or self-loop (action b). For every m \u2208[1, k \u22121], the x = 1 state can go back to x = 0 in the same block (action a) or leave the block (action b). When using b, there is a 50% chance of going to the sink state (0, 1) and a 50% chance to continue to x = 0 in the (m+1)-th block. In the k-th block, the action leaving the block progresses to the goal state. The model is scalable, since the number of blocks k can be increased arbitrarily. This affects both the size of the state space, which is 2k + 3, as well as the maximum reachability probability, which is 0.5k\u22121.\nWe assume the MDPs are defined using high-level modeling languages such as Probmela [3], PRISM [33] or MODEST [20]. In such modeling languages, MDPs are often represented as a composition of multiple identical components, called modules. For example, in a distributed system where multiple processes are interacting or sharing common resources, each process can be described as a separate module. In the context of this paper, we also consider the number of modules as a parameter. Example 2 (Dining philosophers). The dining philosophers problem involves a number of philosophers, seated around a circular table, blessed with infinite\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f9b/3f9ba693-f90f-4bd0-8ba6-014f7ffe3bd8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: A parameterized, scalable MDP with k+1 blocks, described in Example 1.</div>\navailability of food and things to ponder about. There is a fork placed between each pair of neighbouring philosophers and a philosopher must have both forks in order to eat. They may only pick up one fork at a time and once they finish eating, they place the forks back at the table and return to thinking. This can be described as an MDP where each philosopher is modeled as a separate module. Thus, the number of philosophers is a parameter.\n# 2.3 Decision trees for policy representation\nKnowing that the state space is a product of state-variables, a deterministic policy is a mapping \ufffd i Di \u2192A from tuples of state variables to actions. By viewing the state variables as features and the actions as labels, we can employ machine learning classification techniques such as decision trees , see e.g. [38, Chapter 3], to represent a policy concisely. We refer to [2] for an extensive description of the approach and its advantages. Here, we shortly recall the most relevant definitions in order to formally state our results.\nDefinition 3. A decision tree (DT) T is defined as follows:\n\u2013 T is a rooted full binary tree, meaning every node either is an inner node and has exactly two children or is a leaf node and has no children. \u2013 Every inner node v is associated with a decision predicate \u03b1v which is a boolean function S \u2192{false, true} (or equivalently \ufffd i Di \u2192{false, true}).\nFor a given state s, we use the following recursive procedure to obtain the action \u03c3(s) = a that a DT prescribes: Start at the root. At an inner node, evaluate the decision predicate on the given state s. Depending on whether it evaluates to false or true, recursively continue evaluating on the left or right child, respectively. At a leaf node, return the associated action a. Example 3. Consider again the MDP given in Figure 1. The optimal policy needs to continue towards the goal and not be stuck in any loops. This can be achieved by playing action a in states where x = 0 and action b in all other states. Traditionally, this policy would be represented as a lookup table, storing 2k + 3 state-action pairs explicitly. Instead, we can condense the policy to the DT given in Figure 2b, mimicking the intuitive description of the policy: If x > 0, we play b, otherwise, we play a. Constructing an optimal binary decision tree is an NP-complete problem [26]. Consequently, practical decision tree learning algorithms are based on heuristics. But they tend to work reasonably well. Here, we briefly recall a general framework of learning the decision tree representation of a policy \u03c3 as described in [2]. If the policy suggests same action a for all states (i.e., for all states s, we have \u03c3(s) = a), the tree is just a single leaf node with label a. Otherwise, we split the policy. A predicate \u03c1, defined on state variables, is chosen, and an inner node labeled \u03c1 is created. Then we partition the policy by evaluating the predicate on the state space, and recursively construct one DT for the policy restricted to the states {s \u2208S|\u03c1(s)} where the predicate is true, and one for the policy restricted to the states {s \u2208S|\u00ac\u03c1(s)} where \u03c1 is false. These policies become the children of the inner node with label \u03c1 and the process repeats recursively. The selection of \u201cbest\u201d predicate is done by selecting the one which is able to split the policy as homogeneous as possible. This is determined by optimizing some impurity measure such as Gini impurity [8, Chapter 4] or entropy [44]. As we want the learnt DT to exactly represent the policy, unlike other ML algorithms, we do not stop the learning early based on a stopping criterion. Instead, we overfit on the data. So, the iteration stops when every state in the leaf node of the tree has the same labeling.\nConstructing an optimal binary decision tree is an NP-complete problem [26]. Consequently, practical decision tree learning algorithms are based on heuristics. But they tend to work reasonably well. Here, we briefly recall a general framework of learning the decision tree representation of a policy \u03c3 as described in [2]. If the policy suggests same action a for all states (i.e., for all states s, we have \u03c3(s) = a), the tree is just a single leaf node with label a. Otherwise, we split the policy. A predicate \u03c1, defined on state variables, is chosen, and an inner node labeled \u03c1 is created. Then we partition the policy by evaluating the predicate on the state space, and recursively construct one DT for the policy restricted to the states {s \u2208S|\u03c1(s)} where the predicate is true, and one for the policy restricted to the states {s \u2208S|\u00ac\u03c1(s)} where \u03c1 is false. These policies become the children of the inner node with label \u03c1 and the process repeats recursively. The selection of \u201cbest\u201d predicate is done by selecting the one which is able to split the policy as homogeneous as possible. This is determined by optimizing some impurity measure such as Gini impurity [8, Chapter 4] or entropy [44]. As we want the learnt DT to exactly represent the policy, unlike other ML algorithms, we do not stop the learning early based on a stopping criterion. Instead, we overfit on the data. So, the iteration stops when every state in the leaf node of the tree has the same labeling.\n# 3 Generalizing policies from small problem instances\nIn this section, we develop an approach for obtaining good policies for MDPs that are practically beyond the reach of any available rigorous analysis. Our approach exploits the regularity in structure of the MDPs, therefore, we focus on parameterized MDPs where we expect regularity in the state space. Intuitively, we solve a few small instances (colloquially speaking \u201c1, 2, and 3\u201d) where an optimal policy is easy to compute. Then we generalize these policies by learning a DT from the combined information. The policy represented by this\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3318/331827eb-6e36-41af-9815-04794f448519.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2: (a) MDP instance for m = 1 from Figure 1. (b) The optimal policy represented as a DT as learned by our approach.</div>\nDT is applicable to any instantiation of the parameterized MDP, even the ones that are infeasibly large for any state-of-the-art solver. More concretely, our approach proceeds in three phases, each of which is described in detail in the following subsections. 1. Select the parameters for the small instances (base instances) to learn on. 2. Collect the optimal policies on the base instances. 3. Generalize these policies by learning a DT. Finally, we discuss how to apply the DT to the MDP and evaluate the policy in order to judge its performance. We illustrate every phase on our running example, the MDP in Figure 1.\nFinally, we discuss how to apply the DT to the MDP and evaluate the policy in order to judge its performance. We illustrate every phase on our running example, the MDP in Figure 1.\n# 3.1 Parameter selection\nIn principle, one can use any of the solvable instances as the set of base instances. However, too small instances may not contain enough information to learn a good generalization. Therefore, we select a small set of instances B = {b1, ..., bn}, such that the computed policies of these instances are rich enough to learn a generalized DT (see Section 4.1 for the details how we practically choose this set). This process can be seen as hyper-parameter search. Domain knowledge\nAlgorithm 1 Computing input for the DT learning from small problem in-\nstances.\nInput: A parameterized MDP M and base instances B = {b1, ..., bn}\nOutput: A data set D \u2286SB \u00d7 2AB\nD \u2190\u2205\nfor all bi \u2208B do\nSolve Mbi and get optimal policy \u03c3bi\nfor all s \u2208Sbi do\nif s is reachable in M\n\u03c3bi\nbi\nand s /\u2208Gbi then\nadd (s, \u03c3bi(s)) to D\n\u25b7Add optimal actions\nreturn D;\ncan be very useful to obtain this small set of instances. If not available, we can choose a time budget and solve as many instances as possible in that time. For the remainder of this section, we assume that we are given a set B = {b1, ..., bn} of base instances to learn on. We shall interchangeably use bi for ith instance of the MDP, as well as for the parameters of the ith instance. Hence each bi can be seen as a vector of parameters with its corresponding values. Formally, bi := \u27e8p1=v1, ..., pm=vm\u27e9, where p1, ..., pm are the parameters and v1, ...vm are the values that are assigned to each of the respective parameters to obtain the instance.\nExample 4. Our running example has one parameter k, the number of module In fact, we will see that it suffices to consider B = {b1} where b1 := \u27e8k=1\u27e9, i. only learn on the simplest instance of the MDP as depicted in Figure 2a.\n# 3.2 Collecting policies\nWe collect the optimal decisions from the optimal policies of each of the base instances into a single dataset, later to be used for learning their generalization. The input of the learning algorithms is a data set (possibly a multiset) of samples of (input, output)-pairs. In our case, it is a set of pairs of the structured state and the chosen action. Algorithm 1 describes how to obtain the input function that can be used for the DT learning from a parameterized MDP and a set of parameterizations. Let Mb = (Sb, Ab, \u03b4b, \u00afsb, Gb) be a concrete instance of a parameterized MDP with b := \u27e8p1=v1, ..., pm=vm\u27e9. For a set of base instances B = {b1, ..., bn}, we denote the union of all state spaces as SB := \ufffdn i=1 Sbi, and similarly define AB as the union of all action spaces. Note that when aggregating the information, we exclude states that are not reachable in the Markov Chain (MC) induced by the computed optimal policy, as well as goal states. This reduces the input size for the DT learning which has two advantages: firstly, it speeds up the computation and secondly, it allows the DT to focus on the relevant states. Example 5. For our running example in Example 1, we only consider one base instance. Thus, our data is given by the function for all reachable non-goal\nExample 5. For our running example in Example 1, we only consider one base instance. Thus, our data is given by the function \u03c31 for all reachable non-goal\n# states in the MDP. Concretely, we have pairs of state (0, 0) with action a, th (1, 0) also with a, and (1, 1) with b.\nstates in the MDP. Concretely, we have pairs of state (0, 0) with action a, then (1, 0) also with a, and (1, 1) with b.\nNote that the algorithms are even able to deal with data where the output is non-deterministic (different outputs for the same input within the data set). This corresponds to accommodating permissive policies, i.e. ones that allow multiple actions per state. Thus, we can include all optimal actions from a base instance in the data set. Similarly, we can aggregate the information over multiple base instances by a simple union of the recommended actions over all policies. In such cases, the derived DT would also predict multiple actions. Various determinization approaches (see [1]) exist to get one action in these cases, e.g., determinization by voting picks the action that appears most often in the base instances. Action labels. Usually, actions are internally represented as integer values in the model-checkers, e.g. in STORM [25] and PRISM [30]. A natural choice would be to use these integers as labels for the actions during decision tree learning. However, this creates a problem: When the set of actions varies among different instances, \u201cidentical\u201d action choices are denoted by different integers in each instance. Example 6. Consider the dining philosophers problem in Example 2. For three philosophers, the initial state would have 6 actions. The first 3 are labeled with 0, 1, 2, where an action i represents that the (i + 1)th philosopher thinks. The second 3 actions are labeled with 3, 4, 5, where an action i represents that the (i \u22122)th philosopher eats. If we try to generalize this to an MDP with n > 3 philosophers, the label 3 is now interpreted as the action that the fourth philosopher thinks, not the first philosopher eats as it was in the case of three philosophers. Thus, representing actions only by the index in which they appear can be sufficient if the number of modules does not change, but is problematic in the opposite case.\nNote that the algorithms are even able to deal with data where the output is non-deterministic (different outputs for the same input within the data set). This corresponds to accommodating permissive policies, i.e. ones that allow multiple actions per state. Thus, we can include all optimal actions from a base instance in the data set. Similarly, we can aggregate the information over multiple base instances by a simple union of the recommended actions over all policies. In such cases, the derived DT would also predict multiple actions. Various determinization approaches (see [1]) exist to get one action in these cases, e.g., determinization by voting picks the action that appears most often in the base instances. Action labels. Usually, actions are internally represented as integer values in the model-checkers, e.g. in STORM [25] and PRISM [30]. A natural choice would be to use these integers as labels for the actions during decision tree learning. However, this creates a problem: When the set of actions varies among different instances, \u201cidentical\u201d action choices are denoted by different integers in each instance.\nExample 6. Consider the dining philosophers problem in Example 2. For three philosophers, the initial state would have 6 actions. The first 3 are labeled with 0, 1, 2, where an action i represents that the (i + 1)th philosopher thinks. The second 3 actions are labeled with 3, 4, 5, where an action i represents that the (i \u22122)th philosopher eats. If we try to generalize this to an MDP with n > 3 philosophers, the label 3 is now interpreted as the action that the fourth philosopher thinks, not the first philosopher eats as it was in the case of three philosophers. Thus, representing actions only by the index in which they appear can be sufficient if the number of modules does not change, but is problematic in the opposite case.\nTo overcome this issue, we take advantage of the action-labeling feature in the PRISM language. An action in PRISM is described by a command of the following form: [label] guard -> prob_1 : update_1 +...+ prob_n : update_n. This means that when the condition in the guard is true, update_i happens with probability prob_i. The label of the action is optional (except when the action is a synchronizing action). But, as a simple preprocessing step, we always define the label in each command in the PRISM file. The DT learning then can use these labels instead of the action indices. These labels need to be unique: assigning same label to two actions in two different modules would force the modules to take these actions simultaneously (i.e. to synchronize) changing the structure of the MDP. Also, we only need to define labels for non-synchronizing actions as synchronizing actions already have labels defined that we can use. For example, the problem in Example 6 can be avoided by giving the unique label phil_i_line_j to the action for (i + 1)th philosopher defined by the command at line j in the PRISM file.\n# 3.3 Decision tree learning\nWe use standard DT learning algorithms to learn a DT from the dataset constructed in the Algorithm 1. For predicates to be used, we consider axis-aligned predicates (i.e. predicates of the form x > c where x is a state variable and c \u2208R). The best predicate is selected by calculating the Gini index. Instead of having a stopping criterion, we let the recursive splitting of the dataset happen until no further splitting is possible. The resulting DT generalizes the policy in two ways: 1. The DT is trained using smaller base instances. The same state variables in the DT\u2019s inner node predicates are present in the larger MDP instances, but they can have a larger domain. Despite this difference, the DT would still partition the state space of the larger MDP instances and still recommend actions corresponding to each state. 2. As we are aggregating multiple policies, in our dataset, unlike the learning algorithm described in Section 2.3, we can have a state with more than one suggested actions. The learning algorithm considers them as distinct data-points sharing the same value but different labels. Since the values are the same, there are no predicates that can distinguish them. So these datapoints traverse the same path in the tree until they reach a leaf node. The classification at the leaf node is determined by \u2018majority voting\u2019, the label that appears most often is assigned to the leaf node. This approach helps filter out actions suggested by only a few less generalizing base instances. Example 7. For our example data set D constructed in Example 5, the result of the DT learning is the DT depicted in Figure 2b. This policy is in fact optimal for all k; see Example 3 for an explanation of this. In addition to being optimal, it is also small and perfectly explainable. In contrast, if we are interested in a huge instance of this model, e.g., setting k = 1015, already storing the resulting MDP in the memory in order to compute an optimal policy is challenging or even infeasible for a large enough k. Additionally, the policy produced by state-of-the-art model checkers is represented as a lookup table with as many rows as there are states.\n# 3.4 Applying and evaluating the resulting policy\nOnce we have a decision-tree representation of a policy, we can apply it to MDP instances of arbitrary size. To evaluate a policy, we simply need to compute the value of the MC induced by applying the DT. Since solving MCs is computationally easier than solving MDPs, we can explicitly compute values for larger MDPs (which we could not do otherwise). Nonetheless, one can still scale the parameter to such an extent that the construction of the corresponding MC requires too much time or memory. In such cases, we can use statistical model checking methods [48]. The resulting value is not only a measure for the performance of the DT policy, but also a guaranteed lower bound on the value of the MDP (or an upper bound in the case of minimization).\nSince our approach is based on generalization, the learned DT may, in principle, recommend an action that is not available in that state. In such cases our implementation would choose an action uniformly from the available actions. While this may occur in principle, we have not encountered this situation in any of the experiments we conducted for evaluation.\n# 4 Evaluation\n# 4.1 Experiment Setup\nBenchmark Selection We selected parameterized MDPs with reachability objective from the quantitative verification benchmark set (QVBS) [22]. Models with reward-bounded reachability (e.g., eajs and resource-gathering) were excluded. We also identified trivial model and property combinations where the equation min\u03c3 P\u03c3 \u00afs [\u2662G] = max\u03c3 P\u03c3 \u00afs [\u2662G] holds for the set of goal states G and the initial state \u00afs. In such cases, any valid policy would act as an optimal policy. We have excluded these from our benchmark set. We extended the benchmark set with the Mars Exploration Rovers (mer) case study, which was introduced in [14] and appears frequently in recent literature. This model is interesting because the probability of its property is non-trivial and it is scalable to large parameter values without degenerating into a trivial model. Choice of Base Instances We conducted experiments to observe the effect of the set of base instances on the value produced by the learned policy. We synthesized decision trees from different sets of base instances, increasing the parameter(s) linearly as well as exponentially, and evaluated them on models larger than the base instances. We observed that one or two instances are often already enough to generalize the policy in the considered benchmark set (See Appendix A for the chosen set of base instances used in our experiments). System Configuration The experiments were executed on an AMD EPYC\u2122 7443 server with 48 physical cores, 192 GB RAM, running Ubuntu 22.04.2 LTS operating system with Linux kernel version 5.15.0-83-generic. This powerful server was used to execute many runs in parallel. We assigned 2 cores and 8 GB RAM to each run. For all experiments, we used BenchExec [5], a state of the art benchmarking tool, to isolate the executions and enforce the resource limitations. Implementation Details We implemented our approach as an extension of the probabilistic model checker Storm [25]. Our code is publicly available at: https://github.com/muqsit-azeem/dtstrat-123go-artifact/. Method of Comparison Our aim is to provide a method for policy synthesis for arbitrarily large instances of parameterized MDPs, in particular for MDPs beyond the reach of any available rigorous analysis. Consequently, the optimal value for such an MDP is by definition unknown and optimality becomes not only uncheckable, but also unexpectable\u2014rather, one can hope for values close to the range where the unknown optimum is expected to lie.\nHence a straightforward evaluation is thus beyond reach, and we devise the following ancillary evaluation process. First, we also compare on small benchmarks, although our approach is by no means meant as a competitor of Storm on them. Nonetheless, it gives us the following two details: (i) optimal values for various parameter instantiations, often allowing for a simple extrapolation, and (ii) our relative error for these various parameter instantiations, allowing for another extrapolation. While the performance on small models is irrelevant (of course, exact methods are to be used when feasible), the resulting extrapolations give us some idea how our approach performs in the area of interest. In addition to comparing to the theoretical optimum (obtained by extrapolation), we compare to SMC, which is the key state-of-the-art technique for too large systems, and to randomly chosen policies as a baseline. Technical Description First, to obtain optimal values for each of the MDP instances, we executed all the engines of Storm (sparse, dd, hybrid, dd-tosparse). We executed each run with a CPU time limit of 1 hour and memory limit of 8 GB. We considered the CPU time taken by the fastest engine for each instance. Second, we obtain values by using the state-of-the-art statistical model checker MODES [9], part of the MODEST toolset [20]. The approaches for picking the policies are (i) smart lightweight scheduler sampling (Smart LSS) [12], executed in the default configuration, producing the policy value with confidence bound 0.99 and error bound 0.01; (ii) the uniform policy, which resolves each non-deterministic choice by picking an action uniformly at random, again with confidence bound 0.99 and error bound 0.01; and (iii) an aggregate of 1000 randomly generated deterministic policies, i.e., non-randomizing policies where each non-deterministic choice is resolved by a single action, sampled independently according to the uniform distribution, each evaluated with 1000 simulation runs. Finally, we evaluate our approach by computing the value of the MCs resulting from applying our generalizing DTs. In most of the cases (except 4), we were able to evaluate our policy precisely. In the 4 remaining cases, we used our own implementation of SMC to evaluate the learned DT. For 3 out of these 4, we were able to produce a value with with confidence bound 0.99 and error bound 0.01, and in the remaining one (csma+some_before, N = 8) we had to use the confidence bound 0.95 and error bound 0.05. The key idea of the evaluation is to show how the values (optimal / for our approach / for different random schedulers) evolve with the parameter. We executed all the tools on the MDPs obtained by scaling the value of the parameter. In case of MDPs with a single parameter, we start with the smallest parameter values suggested in the QVBS and then increase it. In cases where there was more than one parameter, we scaled each parameter while fixing the values of the other ones. The values chosen to be fixed were the smallest values for these parameters taken from the QVBS website. Since we could not run experiments for all the parameter values due to resource constraints, we sampled the parameters. (See Table 5 in Appendix B for the concrete parameter values used in our experiments.)\nWe present the results for the parameters that Storm was able to solve within a minute of CPU time, within one hour of CPU time, and an instance that even Storm was not able to solve within an hour of CPU time. Sometimes, parameter scaling does not increase the time required to solve the given MDP. In such cases, we still present several parameter valuations and the corresponding values to assess how the values evolve.\n# 4.2 Results\nTable 1 and Table 2 show the results of our evaluation for the minimizing and maximizing properties, respectively. Each instance refers to a combination of model, property and parameter. The tables show, for each model+property combination, the parameter value and CPU time taken by Storm to solve it (< 1 min, < 60 min, and Beyond in case Storm could not solve the instance in an hour), the values produced by Storm, our approach and the sampling based SMC. The tables report OOR when running out of resource (time or memory). Also, a few MODES runs resulted in a run length exceeded (RLE) error. Some models converge to triviality (i.e., max=min) as we scale the parameter. Zeroconf_dl+deadline_min becomes trivial for higher values of the parameter K than 3, firewire+deadline becomes trivial for deadline > 1300, and csma+some_before becomes trivial when the value of K is more than twice the value of N. Pacman also approaches closer to triviality for higher values of the parameter MAX_STEPS (the horizon). The results show that our approach gives near optimal values for 13 out of 21 cases (the upper halves of Table 1 and Table 2), better than Smart LSS for 2 out of remaining 8 cases, and generally better than random and uniform in remaining cases. There are two instances where random performs better than our approach (pacman for the MAX_STEPS 25, and csma+all_before_max for N=3), see the discussion below.\n# 4.3 Discussion\nAlthough our approach is simple, it performs well in a number of cases. We often generalize from a single instance or two, yielding satisfactory solutions for arbitrarily large instantiations. In a number of cases, we can justifiably extrapolate that the policies are (nearly) optimal for all instances. For instance, consider the two benchmarks of Figure 3. No matter how much the model is scaled up, the value of our policy seems to remain stable. While its (near-)optimality can be proven only up to a certain point (beyond which no ground truth can be known), the apparent stability suggests it is true onwards, too. Note that MODES returns low values as the optimal policies are rather rare. In the sequel, we discuss the scope and the limitations of our approach in details. As discussed earlier, we can divide the parameters in two types. Type 1: Parameters that dictate the number of PRISM-modules. This type of parameter not only changes the structure of the MDP, but also increases the\nValues\nModel+property\nScale\nMODES\n(values of parameters)\nVariable\nTime\nStorm\n1-2-3-Go\nSmart LSS\nUniform\nRandom\nzeroconf_dl+deadline_min\n(N=1000, K=1)\ndl=200\n< 1 min\n5.02 \u00d7 10\u2212207\n2.62 \u00d7 10\u221243\n0\u266f\n0.00\n0.00\ndl=1600 < 60 min\n0\n6.81 \u00d7 10\u221286\n0\n0.00\n0.00\ndl=3200\nBeyond\nOOR\n8.71 \u00d7 10\u2212135\n0\n0\n0.00\nzeroconf_dl+deadline_min\n(N=1000, deadline=10)\nK=2\n< 1 min\n0.34\n0.34\n0.34\n0.34\u266f\n0.34\nK=8\n< 1 min\n1\n1\n1\n1\n1\nfirewire+deadline\n(deadline=200)\ndelay=5\n< 1 min\n0.50\n0.50\n0.56\n0.99\n0.99\ndelay=21 < 1 min\n0.50\n0.50\n1\n1\n1\ndelay=34 < 1 min\n0\n0\n1\n1\n1\ndelay=89 < 1 min\n0\n0\n1\n1\n1\nfirewire+deadline\n(delay=3)\ndl=200\n< 1 min\n0.50\n0.50\n0.50\u266f\n0.98\n0.97\ndl=500\n< 1 min\n0.85\n0.85\n1\n1\n1\ndl=1300\n< 1 min\n1.00\n1.00\n1\n1\n1\ncsma+some_before\n(N=2)\nK=2\n< 1 min\n0.50\n0.50\n0.50\u266f\n0.50\u266f\n0.50\nK=3\n< 1 min\n0.88\n0.88\n0.88\n0.87\u266f\n0.87\u266f\ncsma+some_before\n(fix K=2)\nN=3\n< 1 min\n0.59\n0.59\n0.58\u266f\n0.89\n0.90\nN=5\n< 60 min\n0.21\n0.21\n0.39\n0.79\n0.78\nN=8\n< 60 min\n0.04\n0.03\u2020\n0.51\n0.75\n0.76\nN=13\nBeyond\nOOR\nOOR\n0.56\n0.75\n0.76\nconsensus+c2\n(N=2)\nK=2\n< 1 min\n0.38\n0.47\n0.42\n0.49\n0.49\nK=55\n< 1 min\n0.49\n0.50\nRLE\nRLE\nRLE\nK=144\n< 1 min\n0.49\n0.50\nRLE\nRLE\nRLE\nconsensus+c2\n(K=2)\nN=6\n< 1 min\n0.29\n0.45\n0.49\n0.48\n0.48\nN=7\n< 60 min\n0.29\n0.46\n0.48\n0.49\n0.48\nN=13\nBeyond\nOOR\n0.46\nRLE\n0.49\nRLE\nzeroconf_dl+deadline_min\n(deadline=10, K=1)\nN=1000\n< 1 min\n0.00\n0.00\n0.00\n0.01\u266f\n0.01\nN=8000\n< 1 min\n0.01\n0.04\n0.04\n0.07\n0.06\nN=32000 < 1 min\n0.11\n0.22\n0.21\n0.35\n0.32\npacman+crash\nMS=5\n< 1 min\n0.55\n0.55\n0.55\u266f\n0.55\n0.55\u266f\nMS=25\n< 1 min\n0.55\n0.87\n0.73\n0.93\n0.92\nMS=200 < 60 min\n0.55\n1.00\n1\n1\n1\nMS=300\nBeyond\nOOR\n1.00\n1\n1\n1\nnumber of state variables. Note that when we train a DT from the policies from smaller base instances, the predicates in decision tree would not use the state variables present only in the bigger instances. Even then, as the system is a product of these isomorphic modules, we can think the smaller base instances as projections of the larger instance to the first\nTable 2: The results table for maximizing properties. The values marked by \u2020 were approximated using SMC. We shorten the parameter deadline to dl. OOR means out-of-resources (both time and memory) and RLE means run length\nexceeded.\nValues\nModel+property\nScale\nMODES\n(values of parameters)\nVariable\nTime\nStorm\n1-2-3-Go\nSmart LSS\nUniform\nRandom\nzeroconf_dl+deadline_max\n(N=1000, K=1)\ndl=200\n< 1 min\n0.01\n0.01\n0.00\n0.00\n0.00\ndl=1600 < 60 min\n0.01\n0.01\n0.00\n0.00\n0.00\ndl=3200\nBeyond\nOOR\n0.01\n0.00\n0\n0.00\nzeroconf_dl+deadline_max\n(N=1000, deadline=10)\nK=2\n< 1 min\n0.34\n0.34\n0.34\n0.34\n0.34\nK=8\n< 1 min\n1\n1\n1\n1\n1\nK=32\n< 1 min\n1\n1\n1\n1\n1\nzeroconf_dl+deadline_max\n(deadline=10, K=1)\nN=1000\n< 1 min\n0.02\n0.02\n0.01\n0.01\n0.01\nN=8000\n< 1 min\n0.12\n0.12\n0.10\n0.07\n0.06\nN=32000 < 1 min\n0.49\n0.49\n0.47\n0.35\n0.32\nphilosophers+eat\nN=5\n< 1 min\n1\n1\n1\n1\n0.04\nN=21\n< 60 min\n1\n1\n0.51\n1\n0.00\nN=34\nBeyond\nOOR\n1\n0.49\n1\n0\npnueli-zuck+live\nN=5\n< 1 min\n1\n1\n1\n1\n0.04\nN=21\n< 60 min\n1\n1\n1\n1\n0.00\nN=34\nBeyond\nOOR\n1\n1\n1\n0.00\ncsma+all_before_max\n(N=2)\nK=8\n< 1 min\n1\n1\n1\n1\n1\nK=11\n< 60 min\n1\n1\u2020\n1\n1\n1\nK=13\nBeyond\nOOR\n1\u2020\n1\n1\nOOR\nmer+p1\n(x=0.01)\nn=1000\n< 1 min\n0.20\n0.20\nRLE\nRLE\n0.00\nn=21000 < 60 min\n0.20\n0.20\nRLE\nRLE\n0\nn=55000\nBeyond\nOOR\n0.20\nRLE\nRLE\n0.00\nmer+p1\n(n=10)\nx=0.01\n< 1 min\n0.20\n0.20\nRLE\nRLE\n0.00\nx=0.08\n< 1 min\n0.21\n0.20\nRLE\nRLE\n0.00\nx=0.55\n< 1 min\n0.36\n0.20\nRLE\nRLE\n0.00\nx=0.89\n< 1 min\n0.67\n0.20\nRLE\nRLE\n0.00\nconsensus+disagree\n(K=2)\nN=5\n< 1 min\n0.34\n0.10\n0.05\n0.03\n0.04\nN=7\n< 60 min\n0.38\n0.07\n0.03\n0.04\n0.03\nN=13\nBeyond\nOOR\n0.05\nRLE\n0.03\nRLE\nconsensus+disagree\n(N=2)\nK=2\n< 1 min\n0.11\n0.06\n0.08\n0.03\n0.03\nK=55\n< 1 min\n0.00\n0.00\nRLE\nRLE\nRLE\nK=144\n< 1 min\n0.00\n0.00\nRLE\nRLE\nRLE\ncsma+all_before_max\n(K=2)\nN=3\n< 1 min\n0.86\n0.52\n0.85\n0.03\n0.68\nN=5\n< 60 min\n0.70\n0.05\n0.41\n0.04\n0.24\nN=6\nBeyond\nOOR\n0.01\u2020\n0.21\n0.04\n0.11\nfew state variables. Then, for some interesting properties, our method still gives good policies: cases where there is a generalizing optimal policy that does not depend on the additional modules. For example, consider the csma model describing the CSMA/CD consensus protocol when N stations use a network with a single channel. Each station is\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/19bc/19bc10bb-c87e-4031-a94a-971774e9b958.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) zeroconf_dl + deadline_max</div>\n<div style=\"text-align: center;\">Fig. 3: Robustness of the policy given by 1-2-3-Go!, Storm and MODES for different instances of zeroconf_dl + deadline_max and philosophers + eat. For larger models, while Storm times out and MODES gives sub-optimal policies, 1-2-3-Go! consistently gives better results irrespective of N.</div>\nFig. 3: Robustness of the policy given by 1-2-3-Go!, Storm and MODES for different instances of zeroconf_dl + deadline_max and philosophers + eat. For larger models, while Storm times out and MODES gives sub-optimal policies, 1-2-3-Go! consistently gives better results irrespective of N.\nrepresented by a module in the PRISM file. Now consider two different properties \u201call_before_max\u201d and \u201csome_before\u201d. the first one checks the maximum probability that all stations send the message successfully avoiding a data collision. A policy maximizing successful transmission for all stations, needs to take account of the state variables corresponding to all modules in the PRISM file. Our approach fails with this kind of property. But on the other hand, the second property checks the minimum probability that some station eventually sends the message successfully. Thus, a decision tree generated from the base instances gives a policy that minimizes the collision probability for some station among the first three stations. This would act as an optimal policy (as it optimizes the collision probability for some station) in the larger instances even though the predicates in the DT does not contain state variables related to stations with larger index. Thus, for this property, our approach succeeds in generalizing the optimal policy. Type 2 : Other parameters which can be changed by setting the value externally. Often, this would mean expanding the domain of the state variables (as in the case of Example 1), which increases the size of the state space linearly. Then our approach can still work as we can have an optimal policy that is independent of that specific state variable, or if the newly added states are not relevant. However, this is not always the case: The parameter MAX_STEPS in pacman + crash denotes the number of steps Pac-Man needs to stay safe from the ghosts. Our algorithm fails for this model as the policies across instances cannot be generalized. Indeed, a policy that minimized the probability of crash for K steps does not provide any information about how to stay safe for K\u2032 > K steps. In the case of mer + p1, if we fix n and vary the parameter x, our approach fails to generalize a policy. Changing the value of x does not change the statespace or the structure of the model, but changes the probability values of the transitions, which in turn would change the optimal policy across different in-\n<div style=\"text-align: center;\">(b) philosophers + eat</div>\nstances. For that reason, we cannot construct a generalizing DT as the decision predicates are defined on the state variables and not on the probability value of the transition. Time The approach typically needs less than 5 s to generate a policy, except for zeroconf_dl (see Table 8 and Table 9 in Appendix D). In the case of zeroconf_dl, the time taken is a couple of minutes because we learned from a larger instance, with non-trivial values for all three involved parameters. However, this instance is then so informative that we could later use it for all instances derived by varying different parameters (i.e., varying N and fixing others, varying delay and fixing other, and varying deadline and fixing others, not having to take care of these three families separately).\n# 5 Conclusion\nWe have seen that practically good policies (with values close to the unknown optimum in the sense of the \u201cMethod of Comparison\u201d above) can be generated in a lightweight way even for very large parameterized models, beyond reach of any other methods. In order to synthesize policies for arbitrarily large models, we generalize the policies computed for the smaller instances using (more explainable and thus more generalizable) decision trees, coining the \u201cgeneralizability by explainability\u201d. The generalization is an example of unreliable reasoning, which can contribute to better scalability. On the one hand, the unreliability results in no guarantees that the produced policies are anywhere close to optimum, which, however, often cannot be computed anyway. On the other hand, the values of the policies can be reliably approximated: either numerically with absolute guarantees if the resulting Markov chain is still analyzable (e.g., with partialexploration methods [29]) or with statistical guarantees by SMC on the Markov chain. Consequently, although optimal control policies might be out of reach, we can still produce what we thus coin here as provably good enough policies. Moreover, the consistency of the values over the different instantiations often suggests practical proximity to optimum. A possibly surprising point is the conclusion of our experiments that very few base instances need to be analyzed. Such robustness (together with the robustness across the target instances as seen in Fig. 3) suggests that this generalizability is a deeply inherent property of many models, and thus deserves further investigation and exploitation. In particular, our approach is only the first, generic try to exploit this property, opening the new paradigm. As suggested by our experimental results, more specific heuristics for certain types of systems where parameters play different roles, such as number of modules, number of repetitions, time-outs, etc., offer a desirable direction of future work.\n# References\n1. Ashok, P., Jackermeier, M., Jagtap, P., Kret\u00ednsk\u00fd, J., Weininger, M., Zamani, M.: dtcontrol: decision tree learning algorithms for controller rep-\nresentation. In: Ames, A.D., Seshia, S.A., Deshmukh, J. (eds.) HSCC \u201920: 23rd ACM International Conference on Hybrid Systems: Computation and Control, Sydney, New South Wales, Australia, April 21-24, 2020. pp. 30:1\u201330:2. ACM (2020). https://doi.org/10.1145/3365365.3383468, https://doi.org/10.1145/3365365.3383468 2. Ashok, P., Jackermeier, M., Kret\u00ednsk\u00fd, J., Weinhuber, C., Weininger, M., Yadav, M.: dtcontrol 2.0: Explainable strategy representation via decision tree learning steered by experts. In: TACAS (2). Lecture Notes in Computer Science, vol. 12652, pp. 326\u2013345. Springer (2021) 3. Baier, C., Katoen, J.: Principles of model checking. MIT Press (2008) 4. Baier, C., Klein, J., Leuschner, L., Parker, D., Wunderlich, S.: Ensuring the reliability of your model checker: Interval iteration for markov decision processes. In: CAV (1). Lecture Notes in Computer Science, vol. 10426, pp. 160\u2013180. Springer (2017) 5. Beyer, D., L\u00f6we, S., Wendler, P.: Reliable benchmarking: requirements and solutions. Int. J. Softw. Tools Technol. Transf. 21(1), 1\u201329 (2019). https://doi.org/10.1007/s10009-017-0469-y, https://doi.org/10.1007/s10009-0170469-y 6. Br\u00e1zdil, T., Chatterjee, K., Chmelik, M., Fellner, A., Kret\u00ednsk\u00fd, J.: Counterexample explanation by learning small strategies in markov decision processes. In: Kroening, D., Pasareanu, C.S. (eds.) Computer Aided Verification 27th International Conference, CAV 2015, San Francisco, CA, USA, July 1824, 2015, Proceedings, Part I. Lecture Notes in Computer Science, vol. 9206, pp. 158\u2013177. Springer (2015). https://doi.org/10.1007/978-3-319-21690-4\\_10, https://doi.org/10.1007/978-3-319-21690-4_10 7. Br\u00e1zdil, T., Chatterjee, K., Chmelik, M., Forejt, V., Kret\u00ednsk\u00fd, J., Kwiatkowska, M.Z., Parker, D., Ujma, M.: Verification of markov decision processes using learning algorithms. In: ATVA. Lecture Notes in Computer Science, vol. 8837, pp. 98\u2013114. Springer (2014) 8. Breiman, L.: Classification and Regression Trees. (The Wadsworth statistics / probability series), Wadsworth International Group (1984) 9. Budde, C.E., D\u2019Argenio, P.R., Hartmanns, A., Sedwards, S.: A statistical model checker for nondeterminism and rare events. In: TACAS (2). Lecture Notes in Computer Science, vol. 10806, pp. 340\u2013358. Springer (2018). https://doi.org/10.1007/978-3-319-89963-3\\_20, https://doi.org/10.1007/978-3319-89963-3_20 10. Budde, C.E., Hartmanns, A., Klauck, M., Kret\u00ednsk\u00fd, J., Parker, D., Quatmann, T., Turrini, A., Zhang, Z.: On correctness, precision, and performance in quantitative verification - QComp 2020 competition report. In: ISoLA (4). Lecture Notes in Computer Science, vol. 12479, pp. 216\u2013241. Springer (2020) 11. Ciesinski, F., Baier, C., Gr\u00f6\u00dfer, M., Klein, J.: Reduction techniques for model checking markov decision processes. In: QEST. pp. 45\u201354. IEEE Computer Society (2008) 12. D\u2019Argenio, P.R., Legay, A., Sedwards, S., Traonouez, L.: Smart sampling for lightweight verification of markov decision processes. Int. J. Softw. Tools Technol. Transf. 17(4), 469\u2013484 (2015) 13. Feng, L.: On learning assumptions for compositional verification of probabilistic systems. Ph.D. thesis, University of Oxford, UK (2014) 14. Feng, L., Kwiatkowska, M., Parker, D.: Automated learning of probabilistic assumptions for compositional reasoning. In: Giannakopoulou, D., Orejas, F. (eds.)\nFundamental Approaches to Software Engineering. pp. 2\u201317. Springer Berlin Heidelberg, Berlin, Heidelberg (2011) 15. Groote, J.F., Verduzco, J.R., de Vink, E.P.: An efficient algorithm to determine probabilistic bisimulation. Algorithms 11(9), 131 (2018) 16. Gros, T.P., Hermanns, H., Hoffmann, J., Klauck, M., Steinmetz, M.: Deep statistical model checking. In: FORTE. Lecture Notes in Computer Science, vol. 12136, pp. 96\u2013114. Springer (2020) 17. Gr\u00f6\u00dfer, M., Baier, C.: Partial order reduction for markov decision processes: A survey. In: FMCO. Lecture Notes in Computer Science, vol. 4111, pp. 408\u2013427. Springer (2005) 18. Haddad, S., Monmege, B.: Reachability in mdps: Refining convergence of value iteration. In: RP. Lecture Notes in Computer Science, vol. 8762, pp. 125\u2013137. Springer (2014) 19. Hahn, E.M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., Wojtczak, D.: Omegaregular objectives in model-free reinforcement learning. In: TACAS (1). Lecture Notes in Computer Science, vol. 11427, pp. 395\u2013412. Springer (2019) 20. Hartmanns, A.: MODEST - A unified language for quantitative models. In: FDL. pp. 44\u201351. IEEE (2012), https://ieeexplore.ieee.org/document/6336982/ 21. Hartmanns, A., Hermanns, H.: Explicit model checking of very large MDP using partitioning and secondary storage. In: Finkbeiner, B., Pu, G., Zhang, L. (eds.) Automated Technology for Verification and Analysis - 13th International Symposium, ATVA 2015, Shanghai, China, October 12-15, 2015, Proceedings. Lecture Notes in Computer Science, vol. 9364, pp. 131\u2013147. Springer (2015). https://doi.org/10.1007/978-3-319-24953-7\\_10, https://doi.org/10.1007/978-3-319-24953-7_10 22. Hartmanns, A., Klauck, M., Parker, D., Quatmann, T., Ruijters, E.: The quantitative verification benchmark set. In: TACAS (1). Lecture Notes in Computer Science, vol. 11427, pp. 344\u2013350. Springer (2019). https://doi.org/10.1007/978-3030-17462-0\\_20, https://doi.org/10.1007/978-3-030-17462-0_20 23. Hartmanns, A., Timmer, M.: Sound statistical model checking for MDP using partial order and confluence reduction. Int. J. Softw. Tools Technol. Transf. 17(4), 429\u2013456 (2015) 24. Henriques, D., Martins, J.G., Zuliani, P., Platzer, A., Clarke, E.M.: Statistical model checking for markov decision processes. In: QEST. pp. 84\u201393. IEEE Computer Society (2012) 25. Hensel, C., Junges, S., Katoen, J., Quatmann, T., Volk, M.: The probabilistic model checker storm. Int. J. Softw. Tools Technol. Transf. 24(4), 589\u2013610 (2022). https://doi.org/10.1007/s10009-021-00633-z, https://doi.org/10.1007/s10009-02100633-z 26. Hyafil, L., Rivest, R.L.: Constructing optimal binary decision trees is np-complete. Information Processing Letters 5(1), 15\u201317 (1976). https://doi.org/https://doi.org/10.1016/0020-0190(76)90095-8, https://www.sciencedirect.com/science/article/pii/0020019076900958 27. Kamaleson, N.: Model reduction techniques for probabilistic verification of Markov chains. Ph.D. thesis, University of Birmingham, UK (2018) 28. Klein, J., Baier, C., Chrszon, P., Daum, M., Dubslaff, C., Kl\u00fcppelholz, S., M\u00e4rcker, S., M\u00fcller, D.: Advances in probabilistic model checking with PRISM: variable reordering, quantiles and weak deterministic b\u00fcchi automata. Int. J. Softw. Tools Technol. Transf. 20(2), 179\u2013194 (2018) 29. Kret\u00ednsk\u00fd, J., Meggendorfer, T.: Of cores: A partial-exploration framework for markov decision processes. Log. Methods Comput. Sci. 16(4) (2020)\n30. Kwiatkowska, M.Z., Norman, G., Parker, D.: PRISM: probabilistic symbolic model checker. In: Computer Performance Evaluation / TOOLS. Lecture Notes in Computer Science, vol. 2324, pp. 200\u2013204. Springer (2002) 31. Kwiatkowska, M.Z., Norman, G., Parker, D.: Game-based abstraction for markov decision processes. In: QEST. pp. 157\u2013166. IEEE Computer Society (2006) 32. Kwiatkowska, M.Z., Norman, G., Parker, D.: Symmetry reduction for probabilistic model checking. In: CAV. Lecture Notes in Computer Science, vol. 4144, pp. 234\u2013 248. Springer (2006) 33. Kwiatkowska, M.Z., Norman, G., Parker, D.: The PRISM benchmark suite. In: QEST. pp. 203\u2013204. IEEE Computer Society (2012). https://doi.org/10.1109/QEST.2012.14, https://doi.org/10.1109/QEST.2012.14 34. Kwiatkowska, M.Z., Parker, D., Qu, H.: Incremental quantitative verification for markov decision processes. In: DSN. pp. 359\u2013370. IEEE Compute Society (2011) 35. Li, R., Liu, Y.: Compositional stochastic model checking probabilistic automata via symmetric assume-guarantee rule. In: 2019 IEEE 17th International Conference on Software Engineering Research, Management and Applications (SERA). pp. 110\u2013 115. IEEE (2019) 36. Lomuscio, A., Pirovano, E.: A counter abstraction technique for the verification of probabilistic swarm systems. In: AAMAS. pp. 161\u2013169. International Foundation for Autonomous Agents and Multiagent Systems (2019) 37. Maisonneuve, V.: Automatic heuristic-based generation of mtbdd variable orderings for prism models. internship report (2009) 38. Mitchell, T.: Machine learning, vol. 1. McGraw-hill New York (1997) 39. Mohagheghi, M., Salehi, K.: Machine learning and disk-based methods for qualitative verification of markov decision processes. In: ICTERI Workshops. CEUR Workshop Proceedings, vol. 2732, pp. 74\u201388. CEUR-WS.org (2020) 40. Parker, D.A.: Implementation of symbolic model checking for probabilistic systems. Ph.D. thesis, University of Birmingham, UK (2003) 41. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley Series in Probability and Statistics, Wiley (1994). https://doi.org/10.1002/9780470316887, https://doi.org/10.1002/9780470316887 42. Pyeatt, L.D., Howe, A.E.: Decision tree function approximation in reinforcement learning (1999) 43. Rataj, A., Wozna-Szczesniak, B.: Extrapolation of an optimal policy using statistical probabilistic model checking. Fundam. Informaticae 157(4), 443\u2013461 (2018) 44. Shannon, C.E.: A mathematical theory of communication. The Bell system technical journal 27(3), 379\u2013423 (1948) 45. Smolka, S., Kumar, P., Kahn, D.M., Foster, N., Hsu, J., Kozen, D., Silva, A.: Scalable verification of probabilistic networks. In: PLDI. pp. 190\u2013203. ACM (2019) 46. Sutton, R.S., Barto, A.G.: Introduction to Reinforcement Learning. Cambridge, MA, USA, 1st edn. (1998) 47. Tappler, M., Aichernig, B.K., Bacci, G., Eichlseder, M., Larsen, K.G.: L*-based learning of markov decision processes. In: FM. Lecture Notes in Computer Science, vol. 11800, pp. 651\u2013669. Springer (2019) 48. Younes, H.L.S., Simmons, R.G.: Probabilistic verification of discrete event systems using acceptance sampling. In: CAV. Lecture Notes in Computer Science, vol. 2404, pp. 223\u2013235. Springer (2002). https://doi.org/10.1007/3-540-456570\\_17, https://doi.org/10.1007/3-540-45657-0_17\nAzeem et al.\n# Appendix\nA Choice of base Instances\n# A Choice of base Instances\nTable 3 shows the effect of different choices of base instances for csma + all_before_m We applied the DT-based policy trained on each such set of instances and evaluated on the instance with N = 2 and K = 2. We see that the base instance set gave the best policy.\nour experiments is marked as bold.\nModel instance\nParameters for base instances\nValue\ncsma+all_before_max\n(N=2, K=2)\n{(N = 2, K = 2)}\n0.9159110604\n{(N = 2, K = 2), (N = 3, K = 2)}\n0.919277227\n{(N = 2, K = 2), (N = 2, K = 4), (N = 2, K = 6)}\n0.9159110604\n{(N = 2, K = 2), (N = 2, K = 4)}\n0.9159110604\nTable 4 shows the chosen base instances and some information about all the models and properties in our benchmark.\n# B Parameter Values used in the Experiments\nTable 5 shows the parameter values we sampled for running our experiments. We tried sequences based on Fibonacci series, linear, or exponential based on the intuition we gained during our initial experiments.\n# C Result of simulations using random schedulers\nTable 6 and Table 7 reports out results from running 1000 simulations using randomly generated 1000 independent schedulers. A model and property combination with higher coefficient of variation (ratio of the standard deviation to the mean) would imply that the distribution of policies are more sparse and it is improbable to find a near optimal policy by randomly generating one.\n# D Time comparison\nTable 8 and Table 9 shows the time needed for creating and evaluating the DT on different instances.\n<div style=\"text-align: center;\">Table 4: MDPs in the benchmark set, parameters, choice of base instances</div>\nTable 4: MDPs in the benchmark set, parameters, choice of base instances\nModel instance\nParameters\nParameter value\nfor base instance\nMaximizing Probabilities\nconsensus+disagree\n(N, #modules),\n(K,Var)\n{(N=2,K=8),\n(N=3,K=8)}\ncsma+all_before_max\n(N, #modules),\n(K,Var)\n{(N=2,K=2),\n(N=3,K=2)}\nmer+p1\n(n, Var),\n(x, Var, probability)\n{(n=1,x=0.01)}\nphilosophers+eat\n(N, #modules, number of processes)\n{(N=4)}\nzeroconf_dl+deadline_max\n(deadline, Var),\n(K,Var)\n{(reset=false,\nN=1000,K=2,\ndeadline=50)}\npnueli-zuck+live\n(N, #modules, number of processes)\n{(N=3)}\nMinimizing Probabilities\nconsensus+c2\n(N, #modules),\n(K,Var)\n{(N=2,K=8),\n(N=3,K=8)}\ncsma+some_before\n(N, #modules),\n(K,Var)\n{(N=2,K=2),\n(N=3,K=2)}\nzeroconf_dl+deadline_min\n(deadline, Var),\n(K,Var)\n{(reset=false,\nN=1000,K=2,\ndeadline=50)}\nfirewire+deadline\n(deadline, Var) ,\n(delay, Var)\n{(deadline=200,\ndelay=3)}\npacman+crash\n(MAX_STEPS, Var)\n{(MAX_STEPS=5)}\nTable 5: The parameter values used in the experiments\nModel instance\nFixed parameters\nParameter\nused\nfor scaling\nMinimizing Probabilities\nzeroconf_dl+deadline_min\nN=1000, K=1\ndeadline \u2208{100, 200, 300, 600, 1000, 1600, 2000,\n2400, 2800, 320}\nzeroconf_dl+deadline_min\nN=1000, deadline=10\nK \u2208{1, 2, 3, 4, 8, 16, 32}\nfirewire+deadline\ndeadline=200\ndelay \u2208{3, 5, 8, 13, 21, 34, 55, 89, 144}\nfirewire+deadline\ndelay=3\ndeadline \u2208{200, 300, 500, 800, 1300, 2100, 3400,\n5500, 8900, 14400}\ncsma+some_before\nN=2\nK \u2208{2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 17}\ncsma+some_before\nK=2\nN \u2208{2, 3, 5, 6, 7, 8, 13, 21, 34, 55, 89, 144}\nconsensus+c2\nN=2\nK \u2208{2, 3, 5, 8, 13, 21, 34, 55, 89, 144}\nconsensus+c2\nK=2\nN \u2208{2, 3, 5, 6, 7, 8, 13, 21, 34, 55, 89, 144}\nzeroconf_dl+deadline_min\ndeadline=10, K=1\nN \u2208{1000, 2000, 4000, 8000, 16000, 32000}\npacman+crash\nNA\nMAX_STEPs \u2208{5, 25, 50, 75, 100, 150, 200, 300,\n400, 500}\nMinimizing Probabilities\nzeroconf_dl+deadline_max\nN=1000, K=1\ndeadline \u2208{100, 200, 300, 600, 1000, 1600, 2000,\n2400, 2800, 320}\nzeroconf_dl+deadline_max\nN=1000, deadline=10\nK \u2208{1, 2, 3, 4, 8, 16, 32}\nzeroconf_dl+deadline_max\ndeadline=10, K=1\nN \u2208{1000, 2000, 4000, 8000, 16000, 32000}\nphilosophers+eat\nN \u2208{3, 4, 5, 8, 13, 21, 24, 27, 30, 34, 55, 89, 144}\npnueli-zuck+live\nN \u2208{3, 4, 5, 8, 13, 21, 24, 27, 30, 34, 55, 89, 144}\ncsma+all_before_max\nN=2\nK \u2208{2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 17}\nmer+p1\nx=0.01\nn \u2208{1, 10, 100, 1000, 3000, 8000, 13000, 21000,\n34000, 55000, 89000, 144000}\nmer+p1\nn=10\nx \u2208{0.01, 0.03, 0.05, 0.08, 0.13, 0.21, 0.34, 0.55,\n0.89}\nconsensus+disagree\nK=2\nN \u2208{2, 3, 5, 6, 7, 8, 13, 21, 34, 55, 89, 144}\nconsensus+disagree\nN=2\nK \u2208{2, 3, 5, 8, 13, 21, 34, 55, 89, 144}\ncsma+all_before_max\nK=2\nN \u2208{2, 3, 5, 6, 7, 8, 13, ",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of scalability in the verification of parameterized Markov decision processes (MDPs), which becomes a significant challenge due to the large state space involved in even moderately sized instances. Previous methods have struggled to synthesize policies for such large MDPs, necessitating a new approach that can effectively manage the state-space explosion problem.",
        "problem": {
            "definition": "The core problem is the inability to synthesize effective policies for parameterized MDPs that are instantiated with large values, leading to state spaces that are infeasible for current verification tools to handle.",
            "key obstacle": "The main challenge lies in the existing methods' limitations in scaling, as they either run out of computational resources or produce suboptimal policies when faced with large state spaces."
        },
        "idea": {
            "intuition": "The inspiration for this approach stems from the observation that optimal policies for smaller instances of MDPs can be generalized to larger instances, leveraging the regularities in the decision-making process across different sizes.",
            "opinion": "The proposed idea involves synthesizing policies for small parameterized MDP instances and using decision-tree learning to generalize these policies to larger instances, thus avoiding the need for explicit exploration of large state spaces.",
            "innovation": "This method differs from existing approaches by focusing on generalizing the policies derived from smaller instances rather than directly attempting to solve the large MDPs, which is often computationally prohibitive."
        },
        "method": {
            "method name": "1\u20132\u20133\u2013Go! Policy Synthesis",
            "method abbreviation": "1-2-3-Go!",
            "method definition": "The method synthesizes policies for parameterized MDPs by first solving small instances and then generalizing the optimal policies obtained from these instances using decision-tree representations.",
            "method description": "This approach enables the synthesis of effective policies for arbitrarily large parameterized MDPs without directly analyzing the large models.",
            "method steps": [
                "Select small instances of the parameterized MDP.",
                "Compute optimal policies for these small instances.",
                "Learn a decision tree that generalizes these optimal policies to larger instances."
            ],
            "principle": "The effectiveness of this method lies in its ability to capture the decision-making patterns of optimal policies across different parameterizations, allowing for scalable policy synthesis without the need for exhaustive state space exploration."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved testing the proposed method on standard benchmarks from the quantitative verification benchmark set, focusing on parameterized MDPs with reachability objectives.",
            "evaluation method": "The performance of the synthesized policies was assessed by comparing their values against known optimal values from smaller instances and against other state-of-the-art methods, including statistical model checking."
        },
        "conclusion": "The experiments demonstrated that the proposed method can generate policies that perform well across a range of parameterized MDPs, achieving near-optimal values even for instances that are otherwise intractable for traditional analysis methods.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach are its scalability and the ability to produce effective policies for large parameterized MDPs without the need for direct analysis of the large state spaces.",
            "limitation": "A limitation of the method is that it may not guarantee optimality for all instances, particularly when the structure of the MDP changes significantly with parameter adjustments.",
            "future work": "Future research could explore more tailored heuristics for specific types of parameterized MDPs, improving the generalizability and effectiveness of the synthesized policies."
        },
        "other info": {
            "funding": "This research was funded in part by the German Research Foundation (DFG) project 427755713 GOPro, the DFG GRK 2428 (ConVeY), and the MUNI Award in Science and Humanities (MUNI/I/1757/2021) of the Grant Agency of Masaryk University.",
            "repository": "The implementation of the proposed method is publicly available at: https://github.com/muqsit-azeem/dtstrat-123go-artifact/"
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses scalability in the verification of parameterized Markov decision processes (MDPs), highlighting the significance of understanding cognitive processes in managing complex decision-making scenarios."
        },
        {
            "section number": "2.1",
            "key information": "The core problem defined in the paper is the inability to synthesize effective policies for parameterized MDPs with large state spaces, which is crucial for understanding cognitive processes in problem-solving."
        },
        {
            "section number": "2.3",
            "key information": "The theoretical foundation of the proposed method involves generalizing optimal policies from smaller instances of MDPs, which reflects cognitive strategies in logical reasoning and problem-solving."
        },
        {
            "section number": "3.3",
            "key information": "The proposed method synthesizes policies for parameterized MDPs by analyzing decision-making patterns, illustrating cognitive processes involved in problem-solving and optimization tasks."
        },
        {
            "section number": "5.1",
            "key information": "The paper discusses heuristic approaches to policy synthesis, emphasizing the role of decision-tree learning in developing effective problem-solving strategies for large parameterized MDPs."
        },
        {
            "section number": "7.1",
            "key information": "Understanding the scalability of policy synthesis in parameterized MDPs can have applications in cognitive therapy and educational settings, where decision-making processes are critical."
        }
    ],
    "similarity_score": 0.5062364820960185,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/1-2-3-Go! Policy Synthesis for Parameterized Markov Decision Processes via Decision-Tree Learning and Generalization.json"
}