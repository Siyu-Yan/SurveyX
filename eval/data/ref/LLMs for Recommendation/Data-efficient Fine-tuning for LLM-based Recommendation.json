{
    "from": "google",
    "scholar_id": "R40OGkvuQLcJ",
    "detail_id": null,
    "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
    "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs\u2019 adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLMbased recommendation, aimed at identifying representative samples tailored for LLMs\u2019 few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two primary objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score. Considering",
    "bib_name": "lin2024data",
    "md_text": "# Data-efficient Fine-tuning for LLM-based Recommendation\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce7f/ce7f10a7-57d6-4ef8-9a60-ba799b0420dd.png\" style=\"width: 50%;\"></div>\nTat-Seng Chua dcscts@nus.edu.sg National University of Singapore Singapore\n# ABSTRACT\nLeveraging Large Language Models (LLMs) for recommendation has recently garnered considerable attention, where fine-tuning plays a key role in LLMs\u2019 adaptation. However, the cost of fine-tuning LLMs on rapidly expanding recommendation data limits their practical application. To address this challenge, few-shot fine-tuning offers a promising approach to quickly adapt LLMs to new recommendation data. We propose the task of data pruning for efficient LLMbased recommendation, aimed at identifying representative samples tailored for LLMs\u2019 few-shot fine-tuning. While coreset selection is closely related to the proposed task, existing coreset selection methods often rely on suboptimal heuristic metrics or entail costly optimization on large-scale recommendation data. To tackle these issues, we introduce two primary objectives for the data pruning task in the context of LLM-based recommendation: 1) high accuracy aims to identify the influential samples that can lead to high overall performance; and 2) high efficiency underlines the low costs of the data pruning process. To pursue the two objectives, we propose a novel data pruning method incorporating two scores, namely influence score and effort score, to efficiently identify the influential samples. Particularly, the influence score is introduced to accurately estimate the influence of removing each sample on the overall performance. To achieve low costs of the data pruning process, we employ a small-sized surrogate model to replace LLMs to obtain the influence score. Considering\n\u2217Corresponding author. This work is supported by the CCCD Key Lab of Ministr Culture and Tourism.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR \u201924, July 14\u201318, 2024, Washington, DC, USA \u00a9 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0431-4/24/07 https://doi.org/10.1145/3626772.3657807\nthe potential gap between the surrogate model and LLMs, we further propose an effort score to prioritize some hard samples specifically for LLMs. We instantiate the proposed method on two competitive LLM-based recommender models, and empirical results on three real-world datasets validate the effectiveness of our proposed method. In particular, our method uses only 2% samples to surpass the full data fine-tuning, reducing time costs by 97%.\n# CCS CONCEPTS \u2022 Information systems \u2192Recommender systems.\nKEYWORDS\nData Pruning, LLM-based Recommendation, Efficient Fine-tuning ACM Reference Format: Xinyu Lin, Wenjie Wang\u2217, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \u201924), July 14\u201318, 2024, Washington, DC, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3626772.3657807\n# 1 INTRODUCTION\nLeveraging Large Language Models (LLMs) for recommendation has demonstrated promising efficacy across various tasks, including Click-Through Rate (CTR) prediction [4], sequential recommendation [35], and explainable recommendation [11]. To build LLM-based recommender models, it is crucial to fine-tune LLMs on recommendation data for two primary reasons: 1) there exists a significant gap between previous LLMs\u2019 tuning tasks and the recommendation tasks [4], and 2) the rapid and continuous update of recommendation data necessitates frequent fine-tuning of LLMs [38]. For example, there are approximately 160 million new videos and 942 billion interactions emerging on TikTok per day1. Thus, frequent fine-tuning is imperative to incorporate up-todate item information and enhance user behavior comprehension.\n1https://www.tiktok.com/transparency/.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/febf/febfed2a-fa51-4c55-b024-37926dff3b8a.png\" style=\"width: 50%;\"></div>\nModel\nGPU (GiB)\nTime\nBIGRec\n18.60 \u21e54GPU\n36.87h\nSASRec\n1.61 \u21e51GPU\n0.45h\n% Red\n97.84%\n98.78%\n.\nFigure 2: The comparison of training costs between an LLM (BIGRec) and a surrogate model (SASRec)3. (b) The comparison of training costs  between an LLM (BIGRec) and a  surrogate  model  (SASRec).  The  statistics are based on NVIDIA RTX  A5000 on Games. \n<div style=\"text-align: center;\">(a) Few-shot Performance. (a) Few-shot performance on  MicroLens-50K. (a) Effect of ! w.r.t. Recall</div>\nFigure 1: image model (SASRec). To overcome the above issues, we summarize two principal objecFigure 1: (a) reveals that BIGRec achieves remarkable performance with only hundreds of samples. (b) shows the low costs of surrogate models.\ntives for data pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses on selecting the samples that can lead to low empirical risk; and 2) high e\uffffciency, which emphasizes the low costs of the data pruning process, i.e., eliminating the dependency of well-trained LLMs on the full data. Nevertheless, pursuing the two objectives faces two challenges: \u2022 To achieve high accuracy, it is essential to measure the in\uffffuence of removing each training sample on the empirical risk. However, assessing the in\uffffuence of all samples is costly, as it requires the leaving-one-out retraining for each sample [39]. \u2022 To achieve high e\uffffciency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs (see Table ??). However, there exists a gap between LLMs and surrogate models, attributable to their divergent capabilities in learning user behaviors (refer to Figure 4). As such, in\uffffuential samples selected by surrogate models might deviate from the ones on LLMs, potentially hurting the adaptation of LLMs. To address the challenges, we propose a novel Data pruning method, to E\uffffciently identify the in\uffffuentiAl samples for LLMbased Recommender \uffffne-tuning (shorted as DEALRec). DEALRec leverages two scores, namely in\uffffuence score and e\uffffort score, to identify the in\uffffuential samples. The influence score is formulated to estimate the in\uffffuence of removing each training sample on the empirical risk. It is calculated by extending the in\uffffuence function [16] via chain rules and second-order optimization techniques [25]. To e\uffffciently calculate the in\uffffuence score for all samples, DEALRec employs a simple yet e\uffffective symmetric property to accelerate the calculation, requiring only the estimation once for all samples (cf. Section 3.1). Thereafter, DEALRec uses a traditional recommender model as a surrogate model to obtain the in\uffffuence score and introduces the e\uffffort score to mitigate the gap between the surrogate model and LLMs. The e\uffffort score is obtained by calculating the gradient norm of a sample loss w.r.t. the parameters of LLMs, intuitively measuring the e\uffffort of LLMs to \ufffft a speci\uffffc sample. By regularizing the in\uffffuence score with the e\uffffort score, DEALRec identi\uffffes the in\uffffuential samples that encompass both the representativeness of the full data and the signi\uffffcance to LLMs. We instantiate DEALRec on two LLM-based recommender models and conduct extensive experiments on three real-world datasets, validating the superiority of DEALRec in terms of both e\uffffciency and accuracy. The code and datasets are available BIGRec achieves remarkable perform on Games However, fine-tuning LLMs on large-scale recommendation data demands substantial computational resources and time costs [26], thereby diminishing the practicality of LLM-based recommender models in real-world applications. As such, it is essential to enhance the fine-tuning efficiency of LLM-based recommender models. Fortunately, the rich world knowledge encoded in LLMs offers a promising solution for efficient fine-tuning: few-shot fine-tuning. Previous studies have uncovered that LLMs have the potential to quickly adapt to recommendation tasks by fine-tuning on randomly sampled few-shot data [3, 4, 27] (Figure 1(a)), significantly reducing training time and computational costs. Despite its efficiency, randomly sampled data may lack sufficient representativeness to enable LLMs to effectively comprehend new items and user behaviors. To combat this issue, we introduce the task of data pruning for efficient LLM-based recommendation, which aims to identify representative samples tailored for LLMs\u2019 few-shot finetuning. A closely related literature to this data pruning task is coreset selection [13]. It tries to select a small but representative subset from the full data, aiming to achieve comparable performance. Existing coreset selection methods generally fall into two categories2: 1) Heuristic methods select hard or diverse samples based on predefined metrics [30, 34, 49]. Such heuristic methods do not estimate the impact of selected samples on empirical risk, possibly leading to suboptimal coreset selection. 2) Optimization-based methods mainly optimize the selection of subsets to minimize the empirical risk [5, 50]. However, these methods are inapplicable to large-scale recommendation datasets due to the complex and costly bi-level or discrete optimization problem [17]. Worse still, both heuristic and optimization-based methods rely on the model well-trained by the full data to select the coreset, e.g., calculating pre-defined scores or optimizing the data subset based on the well-trained model (cf. Section 2). As such, it is infeasible to directly apply these methods for LLM-based recommendation because of the high training costs of LLMs on the large-scale full recommendation data. To overcome the above issues, we summarize two principal objectives for data pruning in the context of LLM-based recommendation: 1) high accuracy, which focuses on selecting the samples that can lead to low empirical risk; and 2) high efficiency, which emphasizes the low costs of the data pruning process, i.e., eliminating the dependency of well-trained LLMs on the full data. Nevertheless, pursuing the two objectives faces two challenges:\nof both e\uffffciency and accuracy. The code and datasets are a 2More detailed related work is discussed and compared in Section 4 and 5.\nIn summary, this work o\uffffers three major contributions: \u2022 We introduce a data pruning task to identify the in\uffffuential samples tailored for e\uffffcient LLM-based recommender \uffffnetuning, unlocking the remarkable potential of applying LLMbased recommender models to real-world platforms. \u2022 We propose a novel data pruning method to discover the in\uffffuential samples for LLM-based recommendation, which e\uffffectively and e\uffffciently assesses the in\uffffuence of removing a sample on empirical risk. \u2022 We conduct extensive experiments on three real-world datasets, demonstrating the e\uffffectiveness of DEALRec in achieving both high e\uffffciency and accuracy. \u2022 To achieve high accuracy, it is essential to measure the influence of removing each training sample on the empirical risk. However, assessing the influence of all samples is costly, as it requires the leaving-one-out retraining for each sample [43]. \u2022 To achieve high efficiency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs (see Figure 1(b)). However, there exists a gap between LLMs and surrogate models, attributable to their divergent capabilities in learning user behaviors (refer to Figure 3). As such, influential samples selected by surrogate models might deviate from the ones on LLMs, potentially hurting the adaptation of LLMs.\nIn summary, this work o\uffffers three major contributions:  We introduce a data pruning task to identify the in\uffffuential samples tailored for e\uffffcient LLM-based recommender \uffffnetuning, unlocking the remarkable potential of applying LLMbased recommender models to real-world platforms.  We propose a novel data pruning method to discover the in\uffffuential samples for LLM-based recommendation, which e\uffffectively and e\uffffciently assesses the in\uffffuence of removing a sample on empirical risk.  We conduct extensive experiments on three real-world datasets, demonstrating the e\uffffectiveness of DEALRec in achieving both high e\uffffciency and accuracy. \u2022 To achieve high accuracy, it is essential to measure the influence of removing each training sample on the empirical risk. However, assessing the influence of all samples is costly, as it requires the leaving-one-out retraining for each sample [43]. \u2022 To achieve high efficiency, one possible solution is to train a surrogate model for sample selection, e.g., using a small-sized traditional recommender model, which can drastically reduce the GPU memory usage and the training time compared to LLMs (see Figure 1(b)). However, there exists a gap between LLMs and surrogate models, attributable to their divergent capabilities in learning user behaviors (refer to Figure 3). As such, influential samples selected by surrogate models might deviate from the ones on LLMs, potentially hurting the adaptation of LLMs.\n2 TASK FORMULATION In this section, we \uffffrst introduce LLM-based recommender models and uncover the challenge of real-world applicability. Thereafter, we formulate the task of data pruning for LLM-based recommendation and compare the related work on coreset selection. \u2022 LLM-based recommender models. To leverage the competent capabilities of LLMs, LLM-based recommendation typically utilize powerful LLMs directly as the recommender models. Since LLMs are not particularly trained on the recommendation data, \uffffne-tuning is the necessary and key step for LLMs to learn the item knowledge and understand user behavior. Let U and I denote the sets of users and items, respectively. We present each training sample, i.e., user sequence, as B = (G,~), where G = [81,82, . . .,8|G |] is the user\u2019s historical interactions in chronological order, and ~ is the next interacted item of the user4, where {81, . . .,8|G |,~} \u21e2I. Formally, given the user sequences of the training set D = {BD|D 2 U}, the target is to \uffffne-tune an LLM for recommendation tasks. The learnable parameters (q 2 \u03a6) of an LLM is optimized by minimizing the negative log-likelihood of the next interacted item~ conditioned on input G: min q2\u03a6{L!!\" q = \u2212 |~| \u2019 C=1 log%q (~C |~<C,G)}, (1) where ~C denotes the C-th token of ~, and ~<C represents the token sequence preceding ~C. While \uffffne-tuning LLMs has demonstrated e\uffffectiveness in recommendation tasks [30], its practical application is hindered by the high resource costs required by LLMs and the continuous in\uffffux of new recommendation data [35]. Hence, it is essential to enhance the e\uffffciency of LLM-based recommender \uffffne-tuning. \u2022 Data pruning for e\uffffcient LLM-based recommendation. To achieve e\uffffcient LLM-based recommendation, a promising approach is to reduce the costs by few-shot \uffffne-tuning with randomly selected samples [4]. Nevertheless, the random samples might lose some crucial information for LLMs to acquire the latest information on user behavior or items, e.g., trending items. In this with only hundreds of samples  To address the challenges, we propose a novel Data pruning method, to Efficiently identify the influentiAl samples for LLMbased Recommender fine-tuning (shorted as DEALRec). DEALRec leverages two scores, namely influence score and effort score, to identify the influential samples. The influence score is formulated to estimate the influence of removing each training sample on the empirical risk. It is calculated by extending the influence function [15] via chain rules and second-order optimization techniques [24]. To efficiently calculate the influence score for all samples, DEALRec employs a simple yet effective symmetric property to accelerate the calculation, requiring only the estimation once for all samples (cf. Section 3.1). Thereafter, DEALRec uses a traditional recommender model as a surrogate model to obtain the influence score and introduces the effort score to mitigate the gap between the surrogate model and LLMs. The effort score is obtained by calculating the gradient norm of a sample loss w.r.t. the parameters of LLMs, intuitively measuring the effort of LLMs to fit a specific sample. By regularizing the influence score with the effort score, DEALRec identifies the influential samples that encompass both the representativeness of the full data and the significance to LLMs. We instantiate DEALRec on two LLM-based recommender models and conduct extensive experiments on three real-world datasets, validating the superiority of DEALRec in terms of both efficiency and accuracy. The code and datasets are available at https://github.com/Linxyhaha/DEALRec. In summary, this work offers three major contributions: \u2022 We introduce a data pruning task to identify the influential samples tailored for efficient LLM-based recommender finetuning, unlocking the remarkable potential of applying LLMbased recommender models to real-world platforms. \u2022 We propose a novel data pruning method to discover the influential samples for LLM-based recommendation, which effectively and efficiently assesses the influence of removing a sample on empirical risk. \u2022 We conduct extensive experiments on three real-world datasets, demonstrating the effectiveness of DEALRec in achieving both high efficiency and accuracy.\n# ight, we introduce the task of data pru ecommendation, which aims to ide 2 TASK FORMULATION\nrecommendation, which aims to identify a set of representative samples particularly for LLMs\u2019 few-shot \uffffne-tuning. Formally, given all training samples D = {BD|D 2 U}, the target of data 4Our main focus lies in sequential recommendation, which holds notable practical In this section, we first introduce LLM-based recommender models and uncover the challenge of real-world applicability. Thereafter, we formulate the task of data pruning for LLM-based recommendation and compare the related work on coreset selection.\n\u2022 LLM-based recommender models. To leverage the competent capabilities of LLMs, LLM-based recommendation typically utilize powerful LLMs directly as the recommender models. Since LLMs are not particularly trained on the recommendation data, fine-tuning is the necessary and key step for LLMs to learn the item knowledge and understand user behavior. Let U and I denote the sets of users and items, respectively. We present each training sample, i.e., user sequence, as \ud835\udc60= (\ud835\udc65,\ud835\udc66), where \ud835\udc65= [\ud835\udc561,\ud835\udc562, . . . ,\ud835\udc56|\ud835\udc65|] is the user\u2019s historical interactions in chronological order, and \ud835\udc66 is the next interacted item of the user3, where {\ud835\udc561, . . . ,\ud835\udc56|\ud835\udc65|,\ud835\udc66} \u2282I. Formally, given the user sequences of the training set D = {\ud835\udc60\ud835\udc62|\ud835\udc62\u2208 U}, the target is to fine-tune an LLM for recommendation tasks. The learnable parameters (\ud835\udf19\u2208\u03a6) of an LLM is optimized by minimizing the negative log-likelihood of the next interacted item\ud835\udc66conditioned on input \ud835\udc65:\n\u2211\ufe01 where \ud835\udc66\ud835\udc61denotes the \ud835\udc61-th token of \ud835\udc66, and \ud835\udc66<\ud835\udc61represents the token sequence preceding \ud835\udc66\ud835\udc61. While fine-tuning LLMs has demonstrated effectiveness in recommendation tasks [29], its practical application is hindered by the high resource costs required by LLMs and the continuous influx of new recommendation data [38]. Hence, it is essential to enhance the efficiency of LLM-based recommender fine-tuning.\n# \u2022 Data pruning for efficient LLM-based recommendation. To achieve efficient LLM-based recommendation, a promising\n\u2022 To achieve efficient LLM-based recommendation, a promising approach is to reduce the costs by few-shot fine-tuning with randomly selected samples [4]. Nevertheless, the random samples might lose some crucial information for LLMs to acquire the latest information on user behavior or items, e.g., trending items. In this light, we introduce the task of data pruning for efficient LLM-based recommendation, which aims to identify a set of representative samples particularly for LLMs\u2019 few-shot fine-tuning. Formally, given all training samples D = {\ud835\udc60\ud835\udc62|\ud835\udc62\u2208U}, the target of data pruning is to select a subset S \u2282D, such that the LLMs trained on the subset S can yield good performance on the testing set. The size of S is controlled by the given selection ratio \ud835\udc5f, i.e., |S| = \ud835\udc5f|D|.\n(2)\nwhere L(\u00b7) is the loss function of the task, e.g., image classification [16] or CTR prediction [14], and \ud835\udc3b(\u00b7) denotes the heuristic strategy such as selecting samples with larger prediction entropy [7], or clustering the samples based on the sample representations [6]. However, this group of methods designs the strategy \ud835\udc3b(\u00b7) intuitively and fails to explicitly consider the influence of a sample on the empirical risk. This might lead to suboptimal selection, thereby declining the performance of the model trained by the selected subset.\n3Our main focus lies in sequential recommendation, which holds notable practical significance by intricately considering the temporal aspect in real-world scenarios.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a8b/9a8b1c64-c3ef-428f-9359-4968c34adc9c.png\" style=\"width: 50%;\"></div>\nFigure 2: Overview of DEALRec. DEALRec first trains a surrogate model on the full training samples. Subsequently, it calculates the influence score, which is then regularized by the effort score, to identify influential samples.\n2) Optimization-based methods [5, 22, 23, 48] mainly utilize bilevel optimization techniques to learn the best subset chosen for training:\nBesides, there is also some work that employs discrete optimization problems based on the empirical minimizer \u02c6\ud835\udf03in Eq. (2). Nevertheless, they struggle to be applied to large-scale datasets e.g., recommendation data, due to the complex solving of the optimization problem [17].\n# 3 DEALREC\nTo pursue efficient LLM-based recommendation, we propose a novel data pruning method DEALRec, which involves two key components, i.e., the influence score to estimate the influence on empirical risk, and the effort score as a regularization to mitigate the gap between surrogate model and LLMs. The overview of our method is presented in Figure 2.\n# 3.1 Influence Score\nTo achieve good overall performance with the model trained on the pruned dataset S, the key lies in the ability to assess the influence on the empirical risk, i.e., overall performance, caused by removing a sample in training. However, simply assessing the the influence by removing each sample is impractical, because it requires brute force leaving-one-out-retraining for \ud835\udc5b= |D| times. To overcome this challenge, we propose an efficient approximation of the influence for all samples by extending influence on parameter change (i.e., a classic result from influence function [24]) via chain rule and secondorder optimization techniques. We further utilize the symmetric property to speed up the calculation of the influence score.\n\u2022 Influence on parameter change. To estimate the influence on empirical risk for each sample, we first start with the classic result [28] from research on influence function [8], which gives us the estimation of the parameter change caused by upweighting a sample \ud835\udc60for training. Considering a training sample \ud835\udc60is upweighted by a small \ud835\udf16, the empirical minimizer can be rewritten as: \u2211\ufe01\n(4)\nAccording to [28], the influence of upweighting a sample \ud835\udc60on the parameter change is then given as: \ufffd\n(5)\n\ufffd\ufffd\ufffd where \ud835\udc3b\u02c6\ud835\udf03 = 1 \ud835\udc5b \ufffd \ud835\udc60\ud835\udc56\u2208D \u22072 \ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03) is the Hessian and positive definite by assumption, Iparam(\ud835\udc60) \u2208R\ud835\udc5a, and \ud835\udc5ais the number of parameters. Notably, assigning \u22121 \ud835\udc5bto \ud835\udf16is equivalent to removing the sample \ud835\udc60from training. As such, the parameter change of removing a training sample \ud835\udc60can be linearly approximated as:\n(6)\nwhere \u02c6\ud835\udf03\u2212\ud835\udc60= arg min\ud835\udf03\u2208\u0398 \ufffd \ud835\udc60\ud835\udc56\u2208D,\ud835\udc60\ud835\udc56\u2260\ud835\udc60L(\ud835\udc60\ud835\udc56,\ud835\udf03). Based on Eq. (6), an intuitive approach to assess the sample influence for model training is to utilize the L2 norm of a sample\u2019s influence on parameter change or an additional discrete optimization problem as proposed in [50]. Nevertheless, large parameter changes do not necessarily lead to performance improvements. Besides, calculating Eq. (6) for all training samples can be computationally costly [17] and is infeasible for recommendation data. To alleviate the issues, we propose an efficient approximation for the influence of removing a sample on the empirical risk. \u2022 Influence on empirical risk. Based on the parameter change obtained via the influence function, we can then estimate the influence of upweighting a training sample \ud835\udc60by a small \ud835\udf16on the loss of an arbitrary sample \ud835\udc60\u2032: \ufffd\nSimilarly, the influence of removing a training sample \ud835\udc60on the loss of an arbitrary sample \ud835\udc60\u2032 can be linearly approximated as:\n(8)\nWe can then obtain the influence of removing a sample \ud835\udc60on the empirical risk (i.e., influence score) by \ufffd \ufffd \ufffd \ufffd\n(9)\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd However, it is non-trivial to directly obtain \ud835\udc3b\u22121 \u02c6\ud835\udf03 as forming and inverting \ud835\udc3b\u02c6\ud835\udf03= 1 \ud835\udc5b \ufffd \ud835\udc60\ud835\udc56\u2208D \u22072 \ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03) requires O(\ud835\udc5b\ud835\udc5a2 + \ud835\udc5a3) with\nAlgorithm 1 Procedure of HVP Estimation\nInput: Original training dataset D, parameters of a well-trained model \u02c6\ud835\udf03,\niteration number \ud835\udc47.\n1: Compute \ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03) for \u2200\ud835\udc56\u2208{1, . . . ,\ud835\udc5b}.\n2: Initialize \u02dc\ud835\udc3b\u22121\n0\n\ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n= \ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03).\n3: for all \ud835\udc61\u2208{1, . . . ,\ud835\udc47} do\n4:\nRandomly sample a training sample \ud835\udc60\ud835\udc61\u2208D;\n5:\nCalculate \u22072\n\ud835\udf03L(\ud835\udc60\ud835\udc61) as the unbiased estimator of \ud835\udc3b;\n6:\n\u02dc\ud835\udc3b\u22121\n\ud835\udc61\n\ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n\u2190\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)+\n\ufffd\n\ud835\udc3c\u2212\u22072\n\ud835\udf03L(\ud835\udc60\ud835\udc61)\n\ufffd\u02dc\ud835\udc3b\u22121\n\ud835\udc61\u22121\n\ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n;\n\u22b2Eq. (10)\n7:\n\u02dc\ud835\udc3b\u22121 \ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n\u2190\u02dc\ud835\udc3b\u22121\n\ud835\udc47\n\ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n.\nOutput: Unbiased estimation \u02dc\ud835\udc3b\u22121 \ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\n.\n\ud835\udc5btraining samples and \ud835\udf03\u2208R\ud835\udc5a. This results in cumbersome calculation of influence scores for all training samples.\n\ud835\udc5btraining samples and \ud835\udf03\u2208R\ud835\udc5a. This results in cumbersome calculation of influence scores for all training samples. \u2022 Efficient estimation of influence score. To achieve efficient computation of influence score, we utilize stochastic-based HessianVector Products (HVP) [1] to efficiently approximate \ud835\udc3b\u22121 \u02c6\ud835\udf03\u2207\ud835\udf03L(\ud835\udc60, \u02c6\ud835\udf03). The idea of stochastic-based HVP estimation is to iteratively obtain an unbiased estimator of \ud835\udc3b\u02c6\ud835\udf03and approach the unbiased estimation of HVP, i.e., \ud835\udc3b\u22121 \u02c6\ud835\udf03\u2207\ud835\udf03L(\ud835\udc60, \u02c6\ud835\udf03). Specifically, we omit the \u02c6\ud835\udf03subscript for clarity and write the first \ud835\udc57terms in Taylor expansion of \ud835\udc3b\u22121 as \ud835\udc3b\u22121 \ud835\udc57 def = \ufffd\ud835\udc57 \ud835\udc56=0(\ud835\udc3c\u2212\ud835\udc3b)\ud835\udc56, which can be further rewritten recursively as \ud835\udc3b\u22121 \ud835\udc57 = \ud835\udc3c+ (\ud835\udc3c\u2212\ud835\udc3b)\ud835\udc3b\u22121 \ud835\udc57\u22121. From the validity of the Taylor expansion, we have \ud835\udc3b\u22121 \ud835\udc57 \u2192\ud835\udc3b\u22121 as \ud835\udc57\u2192\u221e. Thereafter, denoting \u2207\ud835\udf03L(\ud835\udc60, \u02c6\ud835\udf03) as \ud835\udc63, the update iteration for the estimated \ud835\udc3b\u22121 \u02c6\ud835\udf03\u2207\ud835\udf03L(\ud835\udc60, \u02c6\ud835\udf03) at step \ud835\udc61 can be written as: \ufffd \ufffd\n(10)\n\ufffd \ufffd where\ud835\udc60\ud835\udc61is a training sample randomly drawn from D, and \u22072 \ud835\udf03L(\ud835\udc60\ud835\udc61) is an unbiased estimator of the \ud835\udc3bat step \ud835\udc61for fast-to-compute HVP [24]. Despite that stochastic-based HVP can alleviate the computation burdens of the estimation, calculating the influence score for each sample is still costly due to the independent \ud835\udc5b estimations of \ud835\udc3b\u22121 \u02c6\ud835\udf03\u2207\ud835\udf03L(\ud835\udc60, \u02c6\ud835\udf03) for each \ud835\udc60\u2208D (refer to Eq. (9)). To further enhance the efficiency of acquiring influence scores for all samples, we use symmetric property to rewrite Eq. (9) into: \ufffd \ufffd\n(11)\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd The reformulation is based on the assumption that L(\u00b7) has continuous second-order derivatives, which is consistent with the assumption for influence function [24], leading to the fact that \ud835\udc3b\u22121 \u02c6\ud835\udf03 is symmetric. Since \ud835\udc3b\u22121 \u02c6\ud835\udf03 \ufffd\ufffd \ud835\udc561 \ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03) \ufffd \u2208R\ud835\udc5ais a constant vector for any sample \ud835\udc60\u2208D, we can efficiently obtain influence scores for all samples by only applying HVP estimation once for \ud835\udc3b\u22121 \u02c6\ud835\udf03 \ufffd\ufffd \ud835\udc561 \ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03) \ufffd . The detailed HVP estimation process is illustrated in Algorithm 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6186/618646a9-597e-4cd6-ab1a-8e1e640c0e8a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: (a) depicts the different learning ability due to the prior knowledge in LLMs. (b) presents the distributions of effort scores of LLM and surrogate model on Games dataset4.</div>\n# (a) Depicts the some users are easi (b) Presents the distributions of dis surrogate model, showing the diffe 3.2 Gap Regularization\nsurrogate model, showing the different learning ability of the two  models.  As shown in Eq. (11), assessing the influence score of a sample requires the optimized parameters \u02c6\ud835\udf03well-trained over all training samples D. Nevertheless, this poses challenges for LLM-based recommender models due to the continuous influx of large-scale new data in real-world scenarios. In this light, we propose to utilize a surrogate model to replace the LLMs and introduce an effort score as a gap regularization to complement the learning ability gap between LLMs and the surrogate models.\n# \u2022 Surrogate model. To reduce the costs, we propose utili surrogate model, a small-sized traditional recommender \n\u2022 Surrogate model. To reduce the costs, we propose utilizing a surrogate model, e.g., a small-sized traditional recommender model, to compute the influence scores. Nevertheless, since LLMs acquire rich world knowledge during the pre-training stage, they intricately possess different learning abilities compared to the surrogate model (Figure 3(a)). Therefore, the influential samples on LLMs might deviate from the ones for LLMs.\n\u2022 Effort score. To compensate for the gap, we introduce the effort score, which aims to capture significant samples particularly for LLMs. Specifically, we define the effort score of a sample, i.e., a user sequence, \ud835\udc60as:\nwhere \ud835\udf19is the learnable parameters of LLMs5. Intuitively, it measures the learning effort of LLMs to fit a specific user sequence, and a larger score indicates a harder sample for LLMs to learn. To elaborate, Eq. (12) measures the change in the model parameters, which can be interpreted as the discrepancy from the current knowledge encoded in LLMs\u2019 parameters to the latest item knowledge or user behavior. As such, the effort score can emphasize significant samples particularly for LLMs, supplementing the different learning ability of the surrogate model (Figure 3(b)). \u2022 Overall score. By injecting the signals of LLMs\u2019 learning ability into the calculation of influence score, we can obtain the final score of each user sequence for LLM-based recommender fine-tuning: \ufffd\u2211\ufe01 \ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dea6/dea6fb86-38b8-468b-8822-e22109a66313.png\" style=\"width: 50%;\"></div>\n\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd 4We obtain the effort scores for surrogate model by calculating the gradient norm of the parameters of the surrogate model (Eq. (12)). 5The learnable parameters can be either the whole parameters of LLMs or the learnable parameters from parameter-efficient training, e.g., LoRA [19].\nAlgorithm 2 Procedure of DEALRec\nInput: Original training dataset D, randomly initialized parameters of\nsurrogate model \ud835\udf03, pre-trained parameters of LLM \ud835\udf19.\n1: \u02c6\ud835\udf03= arg min\ud835\udf03\u2208\u0398\n1\n\ud835\udc5b\n\ufffd\n\ud835\udc60\ud835\udc56\u2208D L(\ud835\udc60\ud835\udc56,\ud835\udf03).\n2: Obtain estimated \ud835\udc3b\u22121 \ufffd\ufffd\n\ud835\udc561\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)\n\ufffd\nvia HVP estimation.\n3: for all \ud835\udc56\u2208{1, . . . ,\ud835\udc5b} do\n4:\n\ud835\udc3c\ud835\udc60\ud835\udc56=\n1\n\ud835\udc5b2 \u2207\ud835\udf03L(\ud835\udc60\ud835\udc56, \u02c6\ud835\udf03)T\ud835\udc3b\u22121\n\u02c6\ud835\udf03\n\ufffd\ufffd\n\ud835\udc571\n\ud835\udc5b\u2207\ud835\udf03L(\ud835\udc60\ud835\udc57, \u02c6\ud835\udf03)\n\ufffd\n+\n\ud835\udf06\u2225\u2207\ud835\udf19L\ud835\udc3f\ud835\udc3f\ud835\udc40(\ud835\udc60\ud835\udc56) \u22252; \u22b2Eq. (13)\n5: G = {\ud835\udc3a1, . . . ,\ud835\udc3a\ud835\udc3e} \u2190Split training samples D into \ud835\udc3egroups\naccording to the final score \ud835\udc3c\ud835\udc60with even range width.\n6: S \u2190\u2205, \ud835\udc35\u2190\u230a\ud835\udc5f|D|\n\ud835\udc3e\u230b.\n7: while G \u2260\u2205do\n8:\n\ud835\udc58\u2217= arg min\ud835\udc58|\ud835\udc3a\ud835\udc58|;\n9:\nS\ud835\udc58\u2217\u2190randomly select min{\ud835\udc35, |\ud835\udc3a\ud835\udc58\u2217|} samples from \ud835\udc3a\ud835\udc58\u2217;\n10:\nS \u2190S \u222aS\ud835\udc58\u2217; G \u2190G \\ {\ud835\udc3a\ud835\udc58\u2217};\n11:\n\ud835\udc35\u2190\u230a\ud835\udc5f|D|\u2212|S|\n|G|\n\u230b;\n\u22b2Update sampling budget\nOutput: Selected samples S for few-shot fine-tuning.\nwhere \ud835\udf06is a hyper-parameter to balance the strength of the gap regularization. Notably, the gap regularization would suppress the easy samples with smaller effort scores while emphasizing the samples that are more difficult to learn, i.e., larger effort scores. Intuitively, DEALRec identifies the influential samples with two key considerations: 1) the influence score focuses on selecting the representative samples from the full dataset, capturing collaborative filtering information for low empirical risk; and 2) the effort score highlights the non-trivial samples that are significant to the learning of LLMs. The effectiveness of the two scores is empirically validated in Section 4.3.1.\n# 3.3 Few-shot Fine-tuning\nBased on the final influential score obtained via Eq. (13), we can select a subset of data S for LLMs\u2019 few-shot fine-tuning, given an expected selection ratio \ud835\udc5f.\n\u2022 Few-shot data coverage. A straightforward approach is to select the data greedily, i.e., rank the samples based on the overall scores, and then select the top-\ud835\udc5fpercentage of the training data. However, greedily selecting the samples with higher scores might result in very similar samples with low data coverage, which leads to: 1) Inadequacy of samples from other areas, thus hurting the bounded empirical risk [57] and lowering the overall performance (cf. Section 4.2). 2) Poor utilization of training samples because of the redundant samples with similar patterns, thereby causing suboptimal selection for few-shot fine-tuning.\n# \u2022 Coverage-enhanced sample selection. To address th issues, we follow [57] to select the users based on the \n\u2022 issues, we follow [57] to select the users based on the idea of stratified sampling. The core idea is to maintain the budget for the samples in different areas of training distribution, such that the data coverage will be improved to ensure a high-probability bound for the empirical risk (refer to [57] for detailed proof). In detail, we first divide the samples into \ud835\udc3egroups according to their overall scores. We then iteratively sample \ud835\udc5b\ud835\udc60user sequences from the group with the fewest samples and discard that group after sampling, where \ud835\udc5b\ud835\udc60is the average sampling budget for all groups\n<div style=\"text-align: center;\">Table 1: Statistics of the three datasets.</div>\nTable 1: Statistics of the three datasets.\nDatasets\n# Users\n# Items\n# Interactions\nDensity\nGames\n49,156\n17,332\n342,329\n0.04%\nMicroLens-50K\n49,887\n19,217\n359,048\n0.04%\nBook\n88,263\n86,272\n5,303,707\n0.07%\nand is initialized with \u230a\ud835\udc5f| D| \ud835\udc3e\u230b. If the group size is smaller than the average sampling budget, we select all users from this group and update the average sampling budget for the remaining groups (see Algorithm 2). Based on the selected few-shot samples S, we optimize the learnable parameters (\ud835\udf19\u2208\u03a6) of LLMs: \u2211\ufe01\nBased on the selected few-shot samples S, we optimize the learnable parameters (\ud835\udf19\u2208\u03a6) of LLMs: \u2211\ufe01\n\u2022 Instantiation. To instantiate DEALRec on LLM-based recommender models, we first employ a surrogate model to train on original training samples D and calculate the influence score for all samples via Eq. (11), where the L(\u00b7) can be any form of the loss function from the surrogate model, e.g., BPR [37]. We then obtain the effort score for LLMs via Eq. (12), where \ud835\udf19can be the learnable parameters from any backend LLM-based recommender models. Eventually, we apply the stratified sampling to select the samples for LLMs\u2019 few-shot fine-tuning. The detailed data pruning process of DEALRec is demonstrated in Algorithm 2.\n# 4 EXPERIMENT\nWe conduct extensive experiments on three real-world datasets to answer the following research questions: \u2022 RQ1: How does our proposed DEALRec perform compared to the coreset selection baselines for LLM-based recommendation and the models trained with full data? \u2022 RQ2: How do the different components of DEALRec (i.e., influence score, gap regularization, and stratified sampling) affect the performance, and is DEALRec generalizable to different surrogate models? \u2022 RQ3: How does DEALRec perform under different selection ratios and how does DEALRec improve the overall performance?\n# 4.1 Experimental Settings\n4.1.1 Datasets. We conduct experiments on three real-world recommendation datasets: 1) Games is from the Amazon review datasets6, which covers interactions between users and video games with rich textual features. 2) MicroLens-50K7 is a newly released micro-video recommendation dataset [33]. It contains 50\ud835\udc58users\u2019 interactions with micro-videos and their associated multimodal features. 3) Book is also from Amazon review datasets, containing users\u2019 interactions with extensive books. For Games and Book, we follow previous work and discard the interactions with the ratings < 4. For the three datasets, we sort all user-item interactions according to the global timestamps, and then split the interactions into training, validation, and testing sets with the ratio of 8:1:1. Besides, we consider two different fine-tuning settings as follows:\n6https://jmcauley.ucsd.edu/data/amazon/. 7https://github.com/westlake-repl/MicroLens/.\n1) Few-shot fine-tuning fine-tunes LLM-based recommender models with limited samples at a fixed size, e.g., 1024-shot, obtained via different data pruning methods. 2) Full fine-tuning utilizes all samples to fine-tune LLM-based recommender models without data pruning.\n4.1.2 Baselines. We compare DEALRec with the random sampling and several competitive coreset selection methods, including difficulty-based methods and diversity-based methods: 1) Random obtains the data subset via random sampling, which is a popular and strong baseline in data-efficient training [13]. 2) GraNd [34] is a representative coreset selection method that selects the difficult samples with larger gradient norms during training. 3) EL2N [34] proposes to select the difficult samples with larger errors between the labels and the prediction from the model trained by the original dataset. 4) CCS [57] is a competitive method that selects the samples considering both high data coverage and sample importance. We use EL2N as the importance metric for CCS. 5) TF-DCon [49] is a recently proposed data pruning method for content-based recommendation, which clusters the user sequences based on the user representations obtained from both well-trained recommender models and LLMs for selection. 6) RecRanker [30] proposes a sampling strategy to select high-quality user sequences. It selects the users with more interactions for better user modeling and utilizes a cluster-based sampling strategy to enhance user diversity. We do not perform optimization-based methods for comparison because of the inapplicability of complex bi-level or discrete optimization for LLMs on large-scale recommendation data (cf. Section 2). We instantiate our proposed DEALRec and all baselines on two competitive backend LLM-based recommender models: 1) BIGRec [3] utilizes the item title to present the user sequence for recommendation generation; 2) TIGER [35] learns extra tokens from item features to present items, and then converts the user sequence into the sequence of the new item token for next-item generation. \u2022 Evaluation. We employ the widely used metrics Recall@\ud835\udc3eand NDCG@\ud835\udc3eto evaluate the models [18], with \ud835\udc3eset to 10 and 20 for Games, and \ud835\udc3e= 20 and 50 for MicroLens-50K and Book8.\n4.1.3 Implementation. As for the two backend LLM-based recommender models, we follow the original settings in their paper for implementation. We employ LLaMA-7B for BIGRec and transformer-based architecture for TIGER as in their paper [35]. All fine-tuning experiments are conducted on four NVIDIA RTX A5000 GPUs. Besides, we adopt the parameter-efficient fine-tuning technique LoRA [19] to fine-tune BIGRec and fully fine-tune the parameters of TIGER. We utilize SASRec [20], a representative sequential recommender model, as the surrogate model in DEALRec. We set the iteration number \ud835\udc47for HVP estimation at 5000, and search the regularization strength \ud835\udf06in {0.1, 0.3, 0.5, 1.0, 2.0}. For cluster-based methods, the number of clusters \ud835\udc3eis explored in {25, 50, 75}. As for the coreset selection methods that require the training of LLMs, we consider a feasible implementation [7] by executing them on the same surrogate model as DEALRec.\n8We report metrics@20 and @50 because of the challenging modeling of user behavior on book and micro-video recommendations, where the temporal shifts of user interests and the item feature is stronger and thus more difficult to capture [45, 46].\n<div style=\"text-align: center;\">Table 2: Overall performance comparison between the baselines and DEALRec instantiated on two competitive LLM-based recommender models on three datasets. For each backend model, the bold results highlight the best results while the second-best ones are underlined. \u2217implies the improvements over the second-best results are statistically significant (\ud835\udc5d-value < 0.01) under one-sample t-tests. We run all experiments for 3 times with different random seeds and report the averaged results.</div>\nGames\nMicroLens-50K\nBook\n1024-shot (\ud835\udc93=2%)\n1024-shot (\ud835\udc93=2%)\n1024-shot (\ud835\udc93=1%)\nMethods\nR@10\nR@20\nN@10\nN@20\nR@20\nR@50\nN@20\nN@50\nR@20\nR@50\nN@20\nN@50\nTF-DCon\n0.0102\n0.0157\n0.0062\n0.0078\n0.0066\n0.0099\n0.0027\n0.0034\n0.0104\n0.0144\n0.0083\n0.0092\nRecRanker\n0.0112\n0.0166\n0.0074\n0.0090\n0.0024\n0.0042\n0.0011\n0.0014\n0.0108\n0.0145\n0.0090\n0.0097\nCCS\n0.0164\n0.0246\n0.0097\n0.0122\n0.0096\n0.0131\n0.0041\n0.0049\n0.0110\n0.0145\n0.0088\n0.0096\nGraNd\n0.0158\n0.0250\n0.0098\n0.0125\n0.0014\n0.0032\n0.0006\n0.0010\n0.0102\n0.0136\n0.0080\n0.0087\nEL2N\n0.0154\n0.0256\n0.0098\n0.0128\n0.0096\n0.0045\n0.0041\n0.0016\n0.0107\n0.0149\n0.0085\n0.0094\nRandom\n0.0163\n0.0241\n0.0100\n0.0122\n0.0108\n0.0151\n0.0044\n0.0054\n0.0099\n0.0134\n0.0083\n0.0090\nBIGRec\nDEALRec\n0.0181*\n0.0276*\n0.0115*\n0.0142*\n0.0124*\n0.0160*\n0.0055*\n0.0064*\n0.0117*\n0.0155*\n0.0096*\n0.0104*\nTF-DCon\n0.0051\n0.0074\n0.0033\n0.0040\n0.0006\n0.0057\n0.0002\n0.0013\n0.0028\n0.0051\n0.0020\n0.0027\nRecRanker\n0.0028\n0.0045\n0.0019\n0.0024\n0.0043\n0.0064\n0.0011\n0.0014\n0.0027\n0.0052\n0.0018\n0.0025\nCCS\n0.0050\n0.0084\n0.0031\n0.0041\n0.0026\n0.0061\n0.0010\n0.0013\n0.0026\n0.0048\n0.0018\n0.0024\nGraNd\n0.0042\n0.0053\n0.0027\n0.0030\n0.0006\n0.0014\n0.0003\n0.0005\n0.0008\n0.0020\n0.0006\n0.0010\nEL2N\n0.0034\n0.0048\n0.0024\n0.0029\n0.0011\n0.0016\n0.0004\n0.0004\n0.0005\n0.0015\n0.0004\n0.0007\nRandom\n0.0062\n0.0102\n0.0039\n0.0051\n0.0037\n0.0059\n0.0011\n0.0014\n0.0033\n0.0066\n0.0022\n0.0031\nTIGER\nDEALRec\n0.0074*\n0.0114*\n0.0062*\n0.0074*\n0.0058*\n0.0076*\n0.0020*\n0.0020*\n0.0039*\n0.0076*\n0.0026*\n0.0037*\n<div style=\"text-align: center;\">Table 3: Performance comparison between DEALRec under 1024-shot fine-tuning and the full fine-tuning of the BIGRec in terms of both accuracy and time costs. \u201c%Improve.\u201d denotes the relative improvement achieved by DEALRec compared to the full fine-tuning. Models are trained for 50 epochs with the early stopping strategy.</div>\nGames\nMicroLens-50K\nBook\nR@10\u2191\nR@20\u2191\nN@10\u2191\nN@20\u2191\nTime\u2193\nR@20\u2191\nR@50\u2191\nN@20\u2191\nN@50\u2191\nTime\u2193\nR@20\u2191\nR@50\u2191\nN@20\u2191\nN@50\u2191\nTime\u2193\nFull\n0.0169\n0.0233\n0.0102\n0.0120\n36.87h\n0.0081\n0.0136\n0.0038\n0.0053\n66.64h\n0.0076\n0.0108\n0.0060\n0.0068\n84.77h\nDEALRec\n0.0181\n0.0276\n0.0115\n0.0142\n1.67h\n0.0124\n0.0160\n0.0055\n0.0064\n1.23h\n0.0117\n0.0155\n0.0096\n0.0104\n1.93h\n% Improve.\n7.10%\n18.45%\n12.75%\n18.33%\n-95.47%\n53.09%\n17.65%\n44.74%\n20.75%\n-98.15%\n53.95%\n43.52%\n60.00%\n52.94%\n-97.72%\n# 4.2 Overall Performance (RQ1)\nThe results of the baselines and DEALRec with two competitive backend LLM-based recommender models on three datasets under few-shot fine-tuning (1024 samples) are presented in Table 2, from which we have the following observations:\n\u2022 All methods with BIGRec typically yield better performance than those with TIGER, which is attributed to two reasons: 1) BIGRec employs a larger LLM (i.e., LLaMA-7B) compared to TIGER, thereby benefiting from the stronger generalization ability of large-sized LLMs [27]; and 2) BIGRec leverages item titles to present the user sequence, leading to better utilization of world knowledge in LLMs. In contrast, TIGER learns extra item tokens for LLMs. This might result in cold-start item issues since only limited item tokens are learned while others are maintained randomly initialized under the few-shot fine-tuning setting. \u2022 Among all coreset selection baselines, difficulty-based (GraNd, EL2N) methods generally perform better than diversity-based methods (TF-DCon, RecRanker). This is reasonable since diversity-based methods merely heuristically encourage selecting users with divergent preference, which lacks the assessments of their contributions to the model training. In contrast, GraNd and EL2N use pre-defined metrics to measure the sample difficulty and select the samples with larger scores, which encourages selecting the samples that are more informative for models\u2019 optimization. Besides, CCS improves EL2N in most cases, as\nit maintains easy samples for selection, thus compensating the knowledge of recommendation data from high-density areas. \u2022 Another interesting observation is that random sampling yields competitive performance or even outperforms other coreset selection methods in some cases, which might attributed to two possible reasons: 1) Uniformly selected user sequences preserve high coverage of the original training distribution compared to other baselines, which ensures a high probability of guaranteed bound for low empirical risk [57]. This observation is also consistent with the findings in [13]. 2) The inferior performance of some coreset selection methods also might be caused by the implementation settings (Section 4.1.3), where they may suffer from the learning ability gap between the surrogate model and LLMs. (cf. Section 3.2). \u2022 DEALRec significantly outperforms all coreset selection methods across the three datasets. The consistent performance improvements on both backend models validate the superiority of DEALRec in identifying influential samples for LLMs\u2019 adaptation to the recommendation data. The superior performance is attributed to: 1) the accurate and efficient estimation of the influence on empirical risk, i.e., overall performance by removing a sample in training; and 2) the gap regularization based on the effort score to penalize the easy samples for LLMs. By emphasizing the non-trivial samples specifically for LLMs, gap regularization alleviates the learning ability gap between the surrogate model and the LLMs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3aa4/3aa46608-d0b1-449d-bd1e-59478877d474.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Ablation study of the influence score, effort score, and coverage-enhanced sample selection strategy.</div>\nTable 4: Performance comparison between DEALRec with different surrogate models and the BIGRec under full training. \u201cTime\u201d presents the time costs for training the surrogate model on a single NVIDIA RTX A5000.\nR@10\u2191\nR@20\u2191\nN@10\u2191\nN@20\u2191\nTime\u2193\nFull\n0.0169\n0.0233\n0.0102\n0.0120\n/\nBERT4Rec\n0.0175\n0.0258\n0.0103\n0.0128\n0.76h\nSASRec\n0.0181\n0.0276\n0.0115\n0.0142\n0.45h\nDCRec\n0.0211\n0.0283\n0.0117\n0.0137\n0.61h\n\u2022 Comparison with full fine-tuning. We further compare DEALRec with BIGRec under full training w.r.t. accuracy and efficiency, as presented in Table 3. We can find that: 1) DEALRec achieves higher performance compared to the model trained by the full data, indicating the effectiveness of DEALRec for high accuracy. The inferior performance of BIGRec under full training also implies that not all user sequences are informative for model training, or even harmful to the training, e.g., false negative interactions. This has also been observed in CTR prediction [48] and has been discussed in [2] from the view of data redundancy. 2) DEALRec significantly reduces the time costs for LLMs\u2019 fine-tuning (97.11% reduction of fine-tuning costs on average). With the remarkably declined training costs, DEALRec has the potential to facilitate real-world applications of LLM-based recommender models.\n# 4.3 In-depth Analysis\n4.3.1 Ablation Study (RQ2). To study the effectiveness of each component of DEALRec, i.e., influence score, effort score, and coverage-enhanced sample selection strategy, we separately remove the Influence Score (IS) and effort score \ud835\udeff\ud835\udc60, referred to as \u201cw/o IS\u201d and \u201cw/o \ud835\udeff\ud835\udc60\u201d, respectively. Besides, we replace the coverage-enhanced sample selection strategy by greedily selecting the samples with higher scores, denoted as \u201cGreedy\u201d. From the results presented in Figure 4, we can observe that: removing either the influence score or effort score will cause performance drops. This validates the effectiveness of 1) the assessment of overall performance change caused by removing samples from training; 2) additional signals of learning ability captured from LLMs as regularization, alleviating the gap between the surrogate model and the LLMs. Moreover, simply selecting the samples with higher overall scores might weaken the learning of distinct user behaviors and item knowledge (inferior performance of \u201cGreedy\u201d), as discussed in Section 3.3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bdfb/bdfbe5bc-cc83-455f-b3ed-c62d0fa387c8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance of DEALRec with different selection ratio \ud835\udc5fw.r.t. accuracy and efficiency on Games.</div>\n4.3.2 Robustness on different surrogate model (RQ2). To further assess the generalization ability of DEALRec on different surrogate models, we employ three representative sequential recommender models, i.e., BERT4Rec [41], SASRec [20], and DCRec [51] as the surrogate models, respectively. From the results in Table 4, we can find that: 1) DEALRec with the three surrogate models consistently outperforms BIGRec under full finetuning. This demonstrates the strong robustness of DEALRec on different surrogate models. 2) Different surrogate models cause some fluctuations in accuracy. This is reasonable because different model architectures express user behavior and item knowledge differently, possibly resulting in varied selected samples which will affect the performance. 3) SASRec exhibits the least time costs for training and achieves competitive performance among the three surrogate models. Therefore, SASRec could be a good choice of surrogate model for DEALRec in real-world deployments.\nof selection ratio \ud835\udc5fon DEALRec on both accuracy and efficiency, we vary the ratio \ud835\udc5ffrom 0.2% (128-shot) to 4% (4096-shot) and present the results in Figure 5. It is observed that: 1) The recommendation accuracy rapidly improves as the number of selected samples increases from 0.2% to 1%, surpassing the full training when \ud835\udc5f= 1%. Besides, if we continuously increase the selection ratio from 2% to 4%, the benefits from additional samples gradually diminish and only minor improvements in accuracy are observed. We suspect that the gap between and the recommendation data mainly resides in a small subset of the representative user behaviors, which is what DEALRec aims to identify. 2) Meanwhile, although the time costs for fine-tuning LLMs gradually increase because of additional samples, the cost reduction compared to the full training still reaches over 94%. 3) Empirically, setting \ud835\udc5f= 1% is recommended to achieve comparable performance to full fine-tuning and low costs.\nachieves superior overall performance, we test DEALRec over user sequences of different difficulties. Specifically, we calculate the loss of each user sequence via the model trained by randomly selected few-shot samples; we then divide the users into three groups according to their loss values, from the easier samples with smaller loss (Group 1) to the harder samples with larger loss (Group 3). The results of each group of DEALRec and Random on Games are presented in Figure 6. We can find that 1) the performance of both DEALRec and Random gradually declines from Group 1 to Group 3, because users with larger loss are more difficult to predict. Nevertheless, 2) DEALRec consistently outperforms Random in\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5575/55753e42-c8fb-4375-8760-e459f344388f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Performance of DEALRec over easy to difficult samples (Group 1 to Group 3).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66e1/66e118e1-e196-451a-8321-edc530773f5c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance of DEALRec with different \ud835\udf06.</div>\neach group, which validates the effectiveness of DEALRec in considering the influence on overall performance.\n4.3.5 Effect of regularization strength \ud835\udf40. We vary \ud835\udf06from 0.1 to 2 for DEALRec and evaluate the performance as in Figure 7. From the figures, we can find that: 1) As we incrementally increase the value of \ud835\udf06, the overall trend of accuracy has been observed to be generally improved. This is due to the gap between the surrogate model and LLMs as discussed in Section 3.2, emphasizing the necessity to regularize the influence score to be aligned with the learning ability of the LLMs. 2) However, blindly pursuing larger lambda is not necessarily beneficial. We should carefully balance between the performance-driven influential samples from the surrogate model and the difficult samples for the LLMs.\n# 5 RELATED WORK 5.1 LLM-based Recommendation\n# 5 RELATED WORK\n# 5.1 LLM-based Recommendation\nLeveraging LLMs for recommendation has gained remarkable attention recently [36, 52], showcasing their potential across various recommendation tasks [4, 12, 27]. Some early studies explore the recommendation ability of powerful LLMs through in-context-learning ability [9, 42]. Nevertheless, the performance of LLMs is limited without extra fine-tuning over the domain-specific recommendation data [4]. To fully leverage the potential of LLMs for recommendation, a series of work studies various fine-tuning strategies tailored for recommendation tasks [12, 26, 31, 32, 53, 54]. However, fine-tuning LLMs requires extensive computational resources and time costs, thus hindering real-world applications. Therefore, it is crucial to enhance the fine-tuning efficiency of LLMbased recommender models. In this work, we propose the task of data pruning for efficient LLM-based recommendation, aiming to identify representative samples for LLMs\u2019 few-shot fine-tuning.\n# 5.2 Coreset Selection\nCoreset selection has been widely studied in both traditional machine learning and deep learning [47, 50], benefiting many downstream tasks such as data-efficient learning [44], neural architecture search [40], and active learning [39]. It aims to select a small but representative subset from the full data that can lead to comparable model performance. Previous work mainly falls into two groups: 1) Heuristic methods [7, 10, 44] typically assume difficult or diverse samples are informative for model training. 2) Optimization-based methods [21, 25, 50] leverages the bi-level or discrete optimization techniques to optimize the data subset that can minimize the empirical risk. However, heuristic methods might be suboptimal since they overlook the impact of selected samples on empirical risk. And optimization-based methods fail to be applied to LLM-based recommendation due to the cumbersome calculation for complex optimization. Furthermore, previous methods usually rely on the training of the model on full data for selection, which is infeasible for LLM-based recommendation (cf. Section 2). \u2022 Data Condensation [56] is another potential solution to achieve data-efficient training. However, it is intrinsically different from our proposed task of data pruning. While it aims to synthesize a small but informative dataset [55], our task targets to identify existing samples that are representative. Besides, previous work mainly works for continuous data, which is inapplicable to LLMbased recommendation [48]. TF-DCon [49] is recently proposed for content-based recommendation and we compare it in Section 4.2.\n# 6 CONCLUSION\nIn this work, we proposed the task of data pruning for efficient LLM-based recommendation, which aims to identify representative samples tailored for LLMs\u2019 few-shot fine-tuning. Furthermore, we posited two objectives for this data pruning task: 1) high accuracy targets to select the samples that can lead to low empirical risk; and 2) high efficiency strives to consume low costs for the data pruning process. To this end, we proposed a novel data pruning method, namely DEALRec, to efficiently identify the influential samples with two scores. 1) The influence score is formulated to estimate the influence of sample removal on empirical risk, which is extended from the influence function and is accelerated through the symmetric property. 2) We introduced a small-sized surrogate model to calculate the influence score efficiently and proposed the effort score to bridge the gap between the surrogate model and LLMs. Empirical results validate the effectiveness of DEALRec in achieving both high efficiency and high accuracy. This work proposes a data pruning task for LLM fine-tuning, opening up a new research direction for efficient LLM-based recommendation and leaving many promising future directions for future work. 1) It is worthwhile to apply DEALRec to more LLM-based recommender models on more cross-domain datasets, improving fine-tuning performance with limited resources. 2) Due to the limited context window length of LLMs, it is promising to select the informative interacted items in users\u2019 interaction sequences for LLMs\u2019 fine-tuning. 3) Enhancing the inference efficiency of LLM-based recommender models is also a crucial problem for their real-world deployments.\n# REFERENCES\n[1] Naman Agarwal, Brian Bullins, and Elad Hazan. 2016. Second-order stochastic optimization in linear time. stat 1050 (2016), 15. [2] Sharat Agarwal, Himanshu Arora, Saket Anand, and Chetan Arora. 2020. Contextual diversity for active learning. In ECCV. Springer, 137\u2013153. [3] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023. A bi-step grounding paradigm for large language models in recommendation systems. arXiv:2308.08434. [4] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In RecSys. ACM. [5] Zal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause. 2020. Coresets via bilevel optimization for continual learning and streaming. NeurIPS 33 (2020), 14879\u2013 14890. [6] Chengliang Chai, Jiayi Wang, Nan Tang, Ye Yuan, Jiabin Liu, Yuhao Deng, and Guoren Wang. 2023. Efficient coreset selection with cluster-based methods. In KDD. ACM, 167\u2013178. [7] C Coleman, C Yeh, S Mussmann, B Mirzasoleiman, P Bailis, P Liang, J Leskovec, and M Zaharia. 2020. Selection via Proxy: Efficient Data Selection for Deep Learning. In ICLR. [8] R Dennis Cook. 1977. Detection of influential observation in linear regression. Technometrics 19, 1 (1977), 15\u201318. [9] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpt\u2019s capabilities in recommender systems. In RecSys. ACM, 1126\u20131132. [10] Vitaly Feldman and Chiyuan Zhang. 2020. What neural networks memorize and why: Discovering the long tail via influence estimation. NeurIPS 33 (2020), 2881\u20132891. [11] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv:2303.14524. [12] Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023. An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In CIKM. 4595\u20134601. [13] Chengcheng Guo, Bo Zhao, and Yanbing Bai. 2022. Deepcore: A comprehensive library for coreset selection in deep learning. In DEXA. Springer, 181\u2013195. [14] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: a factorization-machine based neural network for CTR prediction. In IJCAI. 1725\u20131731. [15] Frank R Hampel. 1974. The influence curve and its role in robust estimation. Journal of the american statistical association 69, 346 (1974), 383\u2013393. [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. IEEE, 770\u2013778. [17] Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. 2023. Large-scale Dataset Pruning with Dynamic Uncertainty. arXiv:2306.05175. [18] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In SIGIR. 639\u2013648. [19] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv:2106.09685. [20] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In ICDM. IEEE, 197\u2013206. [21] Krishnateja Killamsetty, Sivasubramanian Durga, Ganesh Ramakrishnan, Abir De, and Rishabh Iyer. 2021. Grad-match: Gradient matching based data subset selection for efficient deep model training. In ICML. PMLR, 5464\u20135474. [22] Krishnateja Killamsetty, Durga Sivasubramanian, Ganesh Ramakrishnan, and Rishabh Iyer. 2021. Glister: Generalization based data subset selection for efficient and robust learning. In AAAI, Vol. 35. 8110\u20138118. [23] Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. 2021. Retrieve: Coreset selection for efficient and robust semi-supervised learning. NeurIPS 34 (2021), 14488\u201314501. [24] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In ICML. PMLR, 1885\u20131894. [25] Suraj Kothawade, Vishal Kaushal, Ganesh Ramakrishnan, Jeff Bilmes, and Rishabh Iyer. 2022. PRISM: A Unified Framework of Parameterized Submodular Information Measures for Targeted Data Subset Selection and Summarization. In AAAI. [26] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. In CIKM. 1348\u20131357. [27] Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. 2023. A multi-facet paradigm to bridge large language model and recommendation. arXiv:2310.06491. [28] Robert F Ling. 1984. Residuals and influence in regression. [29] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models. In WSDM. ACM. [30] Sichun Luo, Bowei He, Haohan Zhao, Yinya Huang, Aojun Zhou, Zongpeng Li, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2023. RecRanker:\nInstruction Tuning Large Language Model as Ranker for Top-k Recommendation. arXiv:2312.16018. [31] Zheqi Lv, Wenqiao Zhang, Zhengyu Chen, Shengyu Zhang, and Kun Kuang. 2024. Intelligent Model Update Strategy for Sequential Recommendation. In WWW. ACM. [32] Zheqi Lv, Wenqiao Zhang, Shengyu Zhang, Kun Kuang, Feng Wang, Yongwei Wang, Zhengyu Chen, Tao Shen, Hongxia Yang, Beng Chin Ooi, et al. 2023. DUET: A Tuning-Free Device-Cloud Collaborative Parameters Generation Framework for Efficient Device Model Generalization. In WWW. ACM, 3077\u20133085. [33] Yongxin Ni, Yu Cheng, Xiangyan Liu, Junchen Fu, Youhua Li, Xiangnan He, Yongfeng Zhang, and Fajie Yuan. 2023. A Content-Driven Micro-Video Recommendation Dataset at Scale. arXiv:2309.15379 (2023). [34] Mansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. 2021. Deep learning on a data diet: Finding important examples early in training. NeurIPS 34, 20596\u201320607. [35] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al. 2023. Recommender Systems with Generative Retrieval. In NeurIPS. Curran Associates, Inc. [36] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In WWW. ACM. [37] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In UAI. AUAI Press, 452\u2013461. [38] Noveen Sachdeva, Mehak Dhaliwal, Carole-Jean Wu, and Julian McAuley. 2022. Infinite recommendation networks: a data-centric approach. NeurIPS 35, 31292\u2013 31305. [39] Ozan Sener and Silvio Savarese. 2018. Active learning for convolutional neural networks: A core-set approach. (2018). [40] Jae-hun Shim, Kyeongbo Kong, and Suk-Ju Kang. 2021. Core-set sampling for efficient neural architecture search. arXiv:2107.06869. [41] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In CIKM. 1441\u20131450. [42] Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, and Zhaochun Ren. 2023. Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent. In EMNLP. ACL, 14918\u201314937. [43] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan Wang, and Xiaojuan Qi. 2023. Data Pruning via Moving-one-Sample-out. arXiv:2310.14664. [44] Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio, and Geoffrey J Gordon. 2018. An empirical study of example forgetting during deep neural network learning. arXiv:1812.05159. [45] Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yunshan Ma, and Tat-Seng Chua. 2023. Causal Disentangled Recommendation Against User Preference Shifts. TOIS (2023). [46] Wenjie Wang, Xinyu Lin, Liuhui Wang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2023. Equivariant Learning for Out-of-Distribution Cold-start Recommendation. In MM. 903\u2013914. [47] Kai Wei, Rishabh Iyer, and Jeff Bilmes. 2015. Submodularity in data subset selection and active learning. In ICML. PMLR, 1954\u20131963. [48] Jiahao Wu, Wenqi Fan, Shengcai Liu, Qijiong Liu, Rui He, Qing Li, and Ke Tang. 2023. Dataset condensation for recommendation. arXiv:2310.01038. [49] Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, and Ke Tang. 2023. Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation. arXiv:2310.09874. [50] Shuo Yang, Zeke Xie, Hanyu Peng, Min Xu, Mingming Sun, and Ping Li. 2023. Dataset pruning: reducing training data by examining generalization influence. (2023). [51] Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang, Da Luo, and Kangyi Lin. 2023. Debiased Contrastive Learning for Sequential Recommendation. In WWW. 1063\u20131073. [52] Honglei Zhang, He Liu, Haoxuan Li, and Yidong Li. 2024. TransFR: Transferable Federated Recommendation with Pre-trained Language Models. arXiv:2402.01124. [53] Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023. LightFR: Lightweight federated recommendation with privacy-preserving matrix factorization. TOIS 41, 4 (2023), 1\u201328. [54] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv:2305.07001. [55] Bo Zhao and Hakan Bilen. 2023. Dataset condensation with distribution matching. In WACV. IEEE, 6514\u20136523. [56] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. 2020. Dataset Condensation with Gradient Matching. In ICLR. [57] Haizhong Zheng, Rui Liu, Fan Lai, and Atul Prakash. 2022. Coverage-centric Coreset Selection for High Pruning Rates. In ICLR.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of efficiently fine-tuning Large Language Models (LLMs) for recommendation systems, highlighting the limitations of existing methods that require extensive computational resources and time due to the rapid influx of recommendation data. The authors propose a novel data pruning method to enhance the efficiency of LLM-based recommendation by identifying representative samples for few-shot fine-tuning.",
        "problem": {
            "definition": "The problem is the high resource costs associated with fine-tuning LLMs on large-scale recommendation data, which limits their practical application.",
            "key obstacle": "Existing coreset selection methods are either based on suboptimal heuristic metrics or require costly optimization, making them impractical for large-scale recommendation datasets."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for efficient adaptation of LLMs to new recommendation data through few-shot fine-tuning, which can be achieved by selecting a representative subset of data.",
            "opinion": "The proposed method, DEALRec, aims to identify influential samples that enhance the performance of LLMs while minimizing the costs of the data pruning process.",
            "innovation": "DEALRec introduces a dual scoring system (influence score and effort score) to effectively select samples, addressing the limitations of existing methods."
        },
        "method": {
            "method name": "Data Pruning for Efficient LLM-based Recommendation",
            "method abbreviation": "DEALRec",
            "method definition": "DEALRec is a data pruning method that identifies influential samples for LLM-based recommendation by utilizing influence and effort scores.",
            "method description": "DEALRec efficiently selects samples for few-shot fine-tuning of LLMs, significantly reducing training costs while maintaining high accuracy.",
            "method steps": [
                "Train a surrogate model on the full dataset.",
                "Calculate the influence score for each sample based on its impact on empirical risk.",
                "Compute the effort score to account for the learning gap between the surrogate model and LLMs.",
                "Select a representative subset of samples based on the combined scores."
            ],
            "principle": "The method leverages the influence of sample removal on overall performance and the learning effort required by LLMs, ensuring that selected samples are both representative and significant."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on three real-world datasets: Games, MicroLens-50K, and Book, using two LLM-based recommender models (BIGRec and TIGER) and comparing with various coreset selection baselines.",
            "evaluation method": "Performance was assessed using metrics such as Recall@K and NDCG@K, with results averaged over multiple runs to ensure statistical significance."
        },
        "conclusion": "The proposed DEALRec method demonstrates significant improvements in both efficiency and accuracy for LLM-based recommendation, achieving higher performance with only a fraction of the training data compared to full fine-tuning.",
        "discussion": {
            "advantage": "DEALRec significantly reduces training time and computational costs while maintaining or improving model performance, making LLMs more practical for real-world applications.",
            "limitation": "The method may still face challenges when dealing with extremely large datasets or highly dynamic data environments where user behavior changes rapidly.",
            "future work": "Future research could explore applying DEALRec to other LLM-based models, enhancing inference efficiency, and adapting the method to continuously evolving recommendation data."
        },
        "other info": {
            "code_url": "https://github.com/Linxyhaha/DEALRec",
            "datasets": {
                "Games": {
                    "users": 49156,
                    "items": 17332,
                    "interactions": 342329,
                    "density": "0.04%"
                },
                "MicroLens-50K": {
                    "users": 49887,
                    "items": 19217,
                    "interactions": 359048,
                    "density": "0.04%"
                },
                "Book": {
                    "users": 88263,
                    "items": 86272,
                    "interactions": 5303707,
                    "density": "0.07%"
                }
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The paper addresses the challenge of efficiently fine-tuning Large Language Models (LLMs) for recommendation systems, highlighting the limitations of existing methods that require extensive computational resources and time."
        },
        {
            "section number": "4.2",
            "key information": "The proposed method, DEALRec, aims to identify influential samples that enhance the performance of LLMs while minimizing the costs of the data pruning process."
        },
        {
            "section number": "4.3",
            "key information": "DEALRec introduces a dual scoring system (influence score and effort score) to effectively select samples, addressing the limitations of existing methods."
        },
        {
            "section number": "10.1",
            "key information": "The problem is the high resource costs associated with fine-tuning LLMs on large-scale recommendation data, which limits their practical application."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore applying DEALRec to other LLM-based models, enhancing inference efficiency, and adapting the method to continuously evolving recommendation data."
        },
        {
            "section number": "3.2",
            "key information": "DEALRec efficiently selects samples for few-shot fine-tuning of LLMs, significantly reducing training costs while maintaining high accuracy."
        }
    ],
    "similarity_score": 0.7969552349829142,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce7f/ce7f10a7-57d6-4ef8-9a60-ba799b0420dd.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/febf/febfed2a-fa51-4c55-b024-37926dff3b8a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9a8b/9a8b1c64-c3ef-428f-9359-4968c34adc9c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6186/618646a9-597e-4cd6-ab1a-8e1e640c0e8a.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dea6/dea6fb86-38b8-468b-8822-e22109a66313.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3aa4/3aa46608-d0b1-449d-bd1e-59478877d474.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bdfb/bdfbe5bc-cc83-455f-b3ed-c62d0fa387c8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5575/55753e42-c8fb-4375-8760-e459f344388f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66e1/66e118e1-e196-451a-8321-edc530773f5c.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Data-efficient Fine-tuning for LLM-based Recommendation.json"
}