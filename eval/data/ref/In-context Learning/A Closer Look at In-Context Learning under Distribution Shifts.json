{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.16704",
    "title": "A Closer Look at In-Context Learning under Distribution Shifts",
    "abstract": "In-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. However, under severe distribution shifts, both models' in-context learning abilities diminish.",
    "bib_name": "ahuja2023closerlookincontextlearning",
    "md_text": "# A Closer Look at In-Context Learning under Distribution Shifts\nKartik Ahuja\u2217 FAIR (Meta AI) David Lopez-Paz FAIR (Meta AI)\nAbstract\nIn-context learning, a capability that enables a model to learn from input examples on the fly without necessitating weight updates, is a defining characteristic of large language models. In this work, we follow the setting proposed in (Garg et al., 2022) to better understand the generality and limitations of in-context learning from the lens of the simple yet fundamental task of linear regression. The key question we aim to address is: Are transformers more adept than some natural and simpler architectures at performing in-context learning under varying distribution shifts? To compare transformers, we propose to use a simple architecture based on set-based Multi-Layer Perceptrons (MLPs). We find that both transformers and set-based MLPs exhibit in-context learning under in-distribution evaluations, but transformers more closely emulate the performance of ordinary least squares (OLS). Transformers also display better resilience to mild distribution shifts, where set-based MLPs falter. However, under severe distribution shifts, both models\u2019 in-context learning abilities diminish.\n# 1 Introduction\nTransformers (Vaswani et al., 2017) form the backbone of modern large language models (LLMs) including the likes of GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI, 2023). These LLMs demonstrate remarkable capabilities, such as in-context learning and natural language-based algorithmic reasoning. However, we are only beginning to understand the origins, limitations, and generality of these capabilities, which is essential for developing safe and reliable LLMs. In-context learning (ICL) refers to a model\u2019s capability to acquire knowledge on the fly from examples provided at test time without requiring any weight updates. This ability is especially useful when the model has to adapt to new tasks from a few demonstrations in the test prompt, for example, adapting a model to drive in a new region with few demonstrations. Understanding ICL for LLMs such as GPT-3 trained on raw text data is particularly challenging. In Garg et al. (2022), the authors propose an insightful training setup, which abstracts away the raw nature of text data. In their work, transformer models from GPT-2 family are trained on prompts comprising of input, label demonstrations and shown to emulate the ordinary least squares (OLS) algorithm. Certain natural questions arise at this point. What specifics of the transformer are responsible for the emergence of this behvavior? Can simpler architectures exhibit the same capabilities? How resilient is ICL to distribution shifts? These are the questions that motivate our work. To compare with transformers, we propose a natural baseline that is based on set-based MLPs Zaheer et al. (2017); Lopez-Paz et al. (2017) that exploit the permutation-invariant nature of the task. Depending on the distribution of test prompts, we categorize in-context learning into in-distribution ICL (ID-ICL) and out-of-distribution ICL (OOD-ICL). Under ID-ICL, the train\ndistribution of the prompt is identical to the test distribution of the prompt. Under OOD-ICL, the test distribution of prompt sequence is different from the train distribution. When evaluating OOD-ICL, we are particularly interested in the case when the test distribution of prompts is centered on the tail of the train distribution of prompts. We summarize our key contributions below. \u2022 First, we derive conditions under which the the optimal model that predicts the label for the current query based on the prompt coincide with the OLS or ridge regression. These are based on known arguments, yet it is important to provide them for completeness. \u2022 Despite set-based MLPs being particularly suited for this permutation-invariant task, we find that transformers (GPT-2 family) exhibit better ID-ICL abilities. \u2022 Under mild distribution shifts, we find that transformers degrade more gracefully than set-based MLPs. Under more severe distribution shifts, both transformers and set-based MLPs do not exhibit ICL abilities. \u2022 ID-ICL performance is not predictive of OOD-ICL performance for both architecture choices. Moving forward, several questions need to be answered. Why are transformers better than set-based MLPs at ICL? How can we improve the OOD-ICL abilities of these architectures?\n# 2 Related Works\nRecent studies have offered intriguing insights into in-context learning (ICL). Olsson et al. (2022) propose that the formation of \u201cinduction heads\u201d, which allow models to copy in-context information, is key to ICL. Building on Garg et al. (2022)\u2019s work, several researchers Aky\u00fcrek et al. (2022); von Oswald et al. (2022); Dai et al. (2022) demonstrated that transformer model\u2019s ability to implicitly execute gradient descent steps during inference could also be central to ICL, supporting their claims with empirical evidence. Li et al. (2023) explore this setup further by analyzing generalization bounds for the learnability of algorithms. Lastly, Xie et al. (2021) focus on data sampled from hidden Markov model and interpret in-context learning through the lens of implicit Bayesian inference. They go on to provide conditions under which models can perform ICL even when prompts have low probability under the training distribution. Chan et al. (2022) studied the impact of inductive bias of pretraining the model on ICL. The authors showed that pretrained transformers exhibit rule-based generalization, while those trained from scratch use exemplar-based generalization, i.e., leverage information from the examples provided in-context to carry out ICL. Kirsch et al. (2022) find that among factors determining the inductive bias of the model, state-size is a more crucial parameter than the model size for ICL abilities. More recently, Wei et al. (2023) showed that model size can be a crucial parameter as well. In particular, they show that sufficiently large models such as PaLM-540B are capable of overriding semantic priors if needed, while smaller counterparts are unable to do so.\n# 3 In-context Learning under Distribution Shifts\nWe start with some standard notation. Inputs and labels are denoted as x \u2208Rd and y \u2208R respectively. Each prompt p is a sequence of independent and identically distributed (i.i.d.) input label pairs, denoted as p = {(xi, yi)}k i=1. Each prompt p is sampled independently as follows\nf \u223cPf, xi \u223cPx, \u03b5i \u2208P\u03b5, \u2200i \u2208{1, \u00b7 \u00b7 \u00b7 , k}, yi \u2190f(xi) + \u03b5i, \u2200i \u2208{1, \u00b7 \u00b7 \u00b7 , k},\n(1)\nwhere the labeling function f, which is fixed for the entire prompt p, is sampled from a distribution Pf, inputs xi are sampled independently from Px, yi is generated by adding some noise \u03b5i to the labeling function\u2019s output f(xi). For the prompt p, we define its prefix as pj = ((x1, y1), (x2, y2), \u00b7 \u00b7 \u00b7 , xj), where j \u2208{1, \u00b7 \u00b7 \u00b7 , k}. Define the support of prefix pj as Pj. Define the risk for model M as R(M) = \ufffdk j=1 E \ufffd \u2113 \ufffd M(pj), yj \ufffd\ufffd , where \u2113is the loss, M(pj) looks at the prefixes pj and makes the prediction, the loss is computed w.r.t the true label yj, E[\u00b7] is the expectation over the joint distribution of (pj, yj). We want to find a model that minimizes the risk R(M) i.e.,\nFor the results to follow, we make some standard regularity assumptions that we state as follows The probability measure associated with pj is absolutely continuous w.r.t Lebesgue measure. The conditional expectation and variance exists, i.e., |E[yj|pj]| < \u221eand Var[yj|pj] < \u221efor all pj \u2208Pj. Lemma 1. If \u2113is the square loss, then the solution to equation (2) satisfies, M\u2217(pj) = E[yj|pj], almost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}.\nWhile the above lemma is stated for square loss, an equivalent statement holds for cross-entrop loss. We now turn to our case study, i.e., linear labeling functions f. Each prompt p is sampled a follows\n\u03b2 \u223cN(0, \u03a3), where \u03a3 \u2208Rd\u00d7d is invertible xi \u223cPx, \u03b5i \u223cN(0, \u03c32), \u2200i \u2208{1, \u00b7 \u00b7 \u00b7 , k} yi \u2190\u03b2\u22a4xi + \u03b5i, \u2200i \u2208{1, \u00b7 \u00b7 \u00b7 , k}\nwhere \u03b2 is drawn from a normal distribution with mean zero and covariance \u03a3 and noise \u03b5i is sampled from a normal distribution with mean zero and variance \u03c32. We break down prefix pj into a matrix Xj \u2208R(j\u22121)\u00d7d and vector yj \u2208Rj\u22121 that stacks the first j \u22121 xi\u2019s and yi\u2019s observed in the prompt up to query xj. The tuple (Xj, yj, xj) captures all the relevant information from pj for predicting yj. Since p1 has no inputs to look at in the past, we set X1, y1 to zero. To better understand the notation, consider the following example, p = {(x1, y1), (x2, y2), (x3, y3)}. Prefix p3 = {(x1, y1), (x2, y2), x3}, X3 = \ufffdx1 x2 \ufffd , y3 = \ufffdy1 y2 \ufffd . Next, we derive the optimal models M\u2217(pj) for the data distribution in equation (3). The theorems derived below follows from standard results on linear regression (See Dicker (2016); Richards et al. (2021)). We still state and derive these for completeness. Theorem 1. If \u2113is the square loss and prompt generation follows equation (3), then the optimal model from equation (2) satisfies,\nalmost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}. If \u03a3 is identity, then the above solution coincides with ridge regression (Hoerl and Kennard, 1970) using \u03c32 as the ridge penalty. We now study the noiseless setting. To analyze the noiseless case, we will look at the ridge solutions in the limit of \u03c3 going to zero.\n(2)\n(3)\nTheorem 2. If \u2113is the square loss and prompt generation follows equation (3) with \u03a3 as identity, 1 hen in the limit of \u03c3 \u21920 the optimal model from equation (2) satisfies\nalmost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}, where X+ j is the Moore-Penrose pseudo-inverse of Xj. In the above results (Lemma 1, Theorem 1, and Theorem 2) we do not use the fact that inputs xi\u2019s are drawn independently. In Theorem 1, and Theorem 2, we assumed that \u03b2 is drawn from a normal distribution. For distributions beyond normal, we now argue that if we restrict the search space of models, then the same results continue to hold.\nM\u2217(pj) = x\u22a4 j (X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121X\u22a4 j yj\nalmost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}.\nSo far, we have characterized different conditions under which the optimal model emulates the OLS or the ridge regression on the support of training distribution of the prompts. The study by Garg et al. (2022) demonstrated that transformers, when trained with sufficient data, can emulate OLS regression. Theorem 1, 2 suggest that sufficiently high capacity models (that can handle input data of varying lengths) trained on sufficient amount of data should behave as well as transformers on the prompts sampled from the same distribution as the train distribution. We test this hypothesis in the experiments section. Outside the support of the training distribution of prompts, performance is not guaranteed to be good, and it depends on the inductive biases \u2013 architecture, optimizer, and the loss function. Our experiments will examine the bias from the architecture. We now propose a natural architecture for the task in question. A natural baseline for the above task We revisit the data generation in equation (1) and parametrize the labeling function. Say the labeling process now is yi \u2190f(xi, \u03b2) + \u03b5i, where \u03b2 is sampled from some distribution. E[yi|xi, \u03b2] = f(xi, \u03b2). Our model will first estimate \u03b2 from the given set of samples Xj, yj. The estimation of \u03b2 does not depend on the order of inputs and thus estimation should be invariant w.r.t. to the order of inputs. Further, we want to work with architectures that are capable of handling inputs of variable length. For this purpose, the most natural architecture are the ones that accept sets as inputs. We revisit the Theorem 2 in Zaheer et al. (2017). The theorem states\nTheorem. Zaheer et al. (2017) A function operating on a set A having elements from a countable universe is a valid set function iff it can be expressed as \u03c1 \ufffd\ufffd ai\u2208A \u03d5(ai) \ufffd .\n\ufffd\ufffd \ufffd The aforementioned theorem is stated for elements from a countable universe, with its extension to uncountable sets provided in Zaheer et al. (2017), albeit for fixed-length sets. Since functions of the form \u03c1 \ufffd\ufffd ai\u2208A \u03d5(ai) \ufffd are uninversal representers of set-based functions we use them as the basis for our architecture. We pick both \u03c1 and \u03d5 as Multilayer Perceptrons (MLPs), and we use these to\nestimate the parameter \u03b2. The output from these MLPs is then input into another MLP together with the query xj. The final architecture takes the form \u03c8 \ufffd \u03c1 \ufffd 1 j\u22121 \ufffdj\u22121 i=1 \u03d5(xi, yi) \ufffd , xj \ufffd , where (xi, yi) are input, label pairs seen up to xj. To manage sequences of variable length, we incorporate a normalization term 1 j\u22121. Consider the noisy label scenario that we studied in Theorem 2, where the optimal model is defined by x\u22a4 j (X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121X\u22a4 j yj. Here, \u03c1 \ufffd 1 j\u22121 \ufffdj\u22121 i=1 \u03d5(xi, yi) \ufffd aims to output the best estimate for \u03b2, which is \u02c6\u03b2(Xj, yj) = (X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121X\u22a4 j yj; note how \u02c6\u03b2(Xj, yj) is permutation-invariant. As per (Zaheer et al., 2017), sufficiently expressive \u03c1 and \u03d5 should be capable of expressing \u02c6\u03b2(Xj, yj). The final MLP, \u03c8, must approximate a linear map. Next, we delve into the distribution shifts we consider and their underlying rationale. Distribution shifts for ICL. In both regression and classification problems, the concept of covariate shift (Shimodaira, 2000) is well-understood. Covariate shift refers to the situation where the distribution of the input features, denoted as Px, changes between training and testing phases, but the conditional distribution of the target variable given the features remains invariant. This idea can be applied to the prompts p. When the distribution over prompts changes, but the conditional distribution of the target variable (or response) given the prompt remains invariant, this is referred to as \u201ccovariate shift over prompts\u201d. This is a particularly important setting to test, as it helps us understand the model\u2019s ability to learn from novel types of prompts or demonstrations at test time. Consider two examples that leverage equation (3) as the underlying data generation process Suppose at train time, we generate prompt sequences with inputs xi\u2019s that are mostly positive and then test on prompts comprised of negative inputs. If between train and test we do not alter the label generation process, then this setting qualifies as covariate shift over prompts. On the other hand, consider the setting, where the only difference from train to test is that during label generation at test time is noisy. In this case, the prompt distribution changes but the conditional distribution of the target conditional on the prompt also changes (E[y|p] at train time is the OLS solution and at test time it is the ridge regression solution). As a result, this type of shift does not qualify as covariate shift over prompts. We want to remark that the difference between two models that perfectly minimize the expected loss in equation (2) is not apparent under all types of covariate shifts but those that put much more weight on input sequences that are very low probability at train time. This is one aspect in which our choice of distribution shifts differs from Garg et al. (2022).\n# 4 Experiments\nIn this section, we experiment with the set-based MLPs detailed earlier and transformers from Garg et al. (2022). We generate data in line with the equation (3). The inputs x\u2032 is at train time are sampled from N(0, Id), where Id is the d dimensional identity matrix, and at test time they are sampled from N(\u00b5, I). In one case, we set \u00b5 = 2 \u00b7 1 and refer to it as a mild distribution shift, and in another case we set \u00b5 = 4 \u00b7 1 as severe distribution shift, where 1 is a d dimensional vector of all ones. The results are presented for d = 10. The covariance of \u03b2, i.e., \u03a3 is identity. We present results for both noiseless labels and noisy labels with \u03c32 = 1. For the set-based MLPs, which we refer to as MLP-set, we compare the performance of MLP-set under varying depths, {4, 5, 10, 17, 26} (indexed from 0 to 4 in the increasing order of depth). The width was same for all the layers at 500. We trained the MLP-set model with the Adam optimizer and a learning rate of 0.001 except for the case of depth 26, where we had to lower the learning rate to 0.0001 to\nenable learning. We used ReLU activations and batch norm between any two hidden layers. For training the transformer model, we adopt the same architecture used in (Garg et al., 2022), which belongs to the GPT-2 family, and we include performances at two depths - 12 (Transformer 1) and 16 (Transformer 2).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/37eb/37eb6f63-6941-4793-8cb5-ee42a0e18098.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Comparison of MLP-set and transformers for noiseless setting, i.e., \u03c3 = 0. a) ID-ICL (\u00b5 = 0), b) OOD-ICL (Mild distribution shift with \u00b5 = 2 \u00b7 1), c) OOD-ICL (Severe distribution shift with \u00b5 = 4 \u00b7 1).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ab7b/ab7b202e-adfd-4637-b293-6269dfde7b24.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Comparison of MLP-set and transformers for noisy setting, i.e., \u03c3 = 1. a) ID-ICL (\u00b5 = 0) b) OOD-ICL (Mild distribution shift with \u00b5 = 2 \u00b7 1), c) OOD-ICL (Severe distribution shift with \u00b5 = 4 \u00b7 1).</div>\nThis research reveals that transformers outperform natural baselines in approximating OLS and ridge regression algorithms under mild distribution shifts. The question remains, why are transformers superior? Further investigation is required to theorize why transformers when optimized with familiar optimizers like stochastic gradient descent (SGD), can achieve better approximations of algorithms than set-based MLPs. Additionally, it\u2019s crucial to explore if these comparisons hold up for a broader set of algorithms (beyond OLS), architectures (beyond set-based MLPs), and understand why. Some important steps towards these inquiries have been made by Liu et al. (2022).\n# References\n# A Appendix\nLemma. [Restatement of Lemma 1] If \u2113is the square loss, then the solution to equation (2) satisfies, M\u2217(pj) = E[yj|pj], almost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}.\nProof. We write\nObserve that Rj(M) \u2265Epj \ufffd Var[yj|pj] \ufffd and thus R(M) \u2265\ufffdk j=1 Epj \ufffd Var[yj|pj] \ufffd . If M\u2217is a minimizer of R(M), then it also has to minimize Rj(M). If that were not the case, then M\u2217 could be strictly improved by replacing M\u2217for the jth query with the better model, thus leading to a contradiction. Consider the model \u02dc M(pj) = E[yj|pj] for all pj \u2208Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}. This model \u02dc M minimizes R(M) and each Rj( \u02dc M). Observe that Rj( \u02dc M) = Epj \ufffd Var[yj|pj] \ufffd . Therefore, for any minima M\u2217, Rj(M\u2217) = Epj \ufffd Var[yj|pj] \ufffd . From equation (4), we obtain that Epj \ufffd\ufffd M\u2217(pj) \u2212E[yj|pj] \ufffd2 \ufffd = 0. From Theorem 1.6.6 in Ash and Dol\u00e9ans-Dade (2000), it follows that M\u2217(pj) = E[yj|pj] almost everywhere in Pj.\nTheorem. [Restatement of Theorem 1.] If \u2113is the square loss and prompt generation follows equation (3), then the optimal model from equation (2) satisfies,\nalmost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}.\nProof. From Lemma 1, we know that M\u2217(pj) = E[yj|pj] almost everywhere in Pj. We now simplify E[yj|pj] for the data generation provided in equation (3). We follow standard steps of computing the posterior in Bayesian linear regression to obtain the posterior of \u03b2 conditioned on prefix pj\n\nwhere \u02dc\u00b5 = \u02dc\u03a3X\u22a4 j yj and \u02dc\u03a3 = (X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121. Therefore, \u03b2 conditioned on pj is a Gaussian distribution with mean \u02dc\u00b5 and covariance \u02dc\u03a3. Recall\nFrom the linearity of expectation and the expression above for the posterior, it follows E[yj|pj] = E[yj|Xj, yj, xj] = E[\u03b2\u22a4xj|Xj, yj, xj] = \u02dc\u00b5\u22a4xj\nThis completes the proof.\nTheorem. [Restatement of Theorem 2] If \u2113is the square loss and prompt generation follows equation (3) with \u03a3 as identity, then in the limit of \u03c3 \u21920 the optimal model from equation (2) satisfies\nalmost everywhere in Pj, \u2200j \u2208{1, \u00b7 \u00b7 \u00b7 , k}, where X+ j is the Moore-Penrose pseudo-inverse of Xj.\nProof. For clarity, in this case we make the dependence of M\u2217(pj) on \u03c3 explicit and instead write it as M\u2217(pj, \u03c3) We calculate the limit of the ridge regression predictor as \u03c3 goes to zero. We obtain\nIn the simplification above, we used \u03a3 is identity and also used the standard limit definition of Moore-Penrose pseudo-inverse Albert (1972).\nImplications for Theorem 2 when \u03a3 is not identity Now consider the more general case when \u03a3 is not identity. In this case, suppose the inverse of X\u22a4 j Xj exists, which can happen when the rank of X\u22a4 j Xj is d. In this case, lim\u03c3\u21920(X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121X\u22a4 j = X+ j . To see why this is the case, observe that the map M\u2217(pj, \u03c3) is well defined for all \u03c3 including that at zero and it is also continuous in \u03c3. If the inverse of X\u22a4 j Xj does not exist, then the limit may not converge to the Moore-Penrose pseudo-inverse. Consider the following example.\nThe lim\u03c3\u21920(X\u22a4 j Xj + \u03c32\u03a3\u22121)\u22121X\u22a4 j \u0338= X+ j .\n\n\nTheorem. [Restatement of Theorem 3] Suppose \u2113is the square loss, \u03b2\u2019s and xi\u2019s are drawn from an arbitrary distribution with a finite mean and invertible covariance, rest of the prompt generation follows equation (3). In this setting, the solution to equation (2) under Constraint 1 satisfies\nProof. Recall that R(M) = \ufffd j Rj(M), where Rj(M) = E \ufffd (M(pj) \u2212yj)2\ufffd . Let us simplify one of the terms Rj(M).\n\ufffd Suppose the covariance of xj is \u039b. We write \u039b 1 2 to denote the symmetric positive definite square root of \u039b (Such a square root always exists, see Theorem 3 in 2). We use this to simplify the above expression in equation (7) as follows\nIn the last simplification above, we use the fact that yj = Xj\u03b2 + \u03b5j, where Xj \u2208R(j\u22121)\u00d7d stacks first j \u22121 xi\u2019s and \u03b5j \u2208Rj\u22121 stacks first j \u22121 \u03b5i\u2019s, and that each component of noise is independent and zero mean. Define \u03981 = E[X\u22a4 j m(Xj)\u22a4\u039bf(Xj)Xj] and \u03982 = m(Xj)\u22a4\u039bm(Xj). Since Xj is independent of \u03b2 the above expression simplifies to\n(7)\n(8)\n(9)\n(10)\n(11)\nefine \u0393 = X\u22a4 j m(Xj)\u22a4\u039b. Since Xj is independent of \u03b2 the above expression simplifies to\nFrom the above simplifications it is clear that the loss depends on prior on \u03b2 through its mean and covariance only. Therefore, if we use a Gaussian prior with same mean and covariance we obtain the same loss. As a result, we can assume that prior is Gaussian with same mean and covariance and leverage the previous result, i.e., Theorem 1. This completes the proof.\n(12)\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The paper addresses the understanding of in-context learning (ICL) capabilities of large language models (LLMs), particularly transformers, and their limitations under distribution shifts. This is crucial for developing reliable LLMs as the current state of research has only begun to explore these capabilities.",
            "purpose of benchmark": "The benchmark is intended to compare the performance of transformers against simpler architectures like set-based Multi-Layer Perceptrons (MLPs) in the context of ICL under varying distribution shifts."
        },
        "problem": {
            "definition": "The benchmark is designed to address the problem of evaluating how well different models, specifically transformers and set-based MLPs, perform ICL tasks under both in-distribution and out-of-distribution scenarios.",
            "key obstacle": "Existing benchmarks do not adequately assess the resilience of models to distribution shifts, particularly the performance degradation under severe shifts."
        },
        "idea": {
            "intuition": "The idea for the benchmark stems from the observation that while transformers are powerful in ICL, simpler architectures may not exhibit the same capabilities, prompting a comparative analysis.",
            "opinion": "The authors believe that understanding the conditions under which transformers outperform simpler models is vital for advancing the field of ICL and improving model reliability.",
            "innovation": "This benchmark innovatively categorizes ICL into in-distribution and out-of-distribution scenarios, providing a structured way to evaluate model performance under varying conditions.",
            "benchmark abbreviation": "ICL-Bench"
        },
        "dataset": {
            "source": "The dataset is generated synthetically based on a defined process that involves sampling input-label pairs from specified distributions.",
            "desc": "The dataset consists of prompts that are sequences of independent and identically distributed input-label pairs, suitable for evaluating ICL capabilities.",
            "content": "The types of data included are numerical input-label pairs used for regression tasks.",
            "size": "10,000",
            "domain": "Linear Regression",
            "task format": "In-context Learning"
        },
        "metrics": {
            "metric name": "Mean Squared Error (MSE)",
            "aspect": "Accuracy of model predictions",
            "principle": "MSE is chosen as it quantifies the average squared difference between predicted and actual values, making it a standard metric for regression tasks.",
            "procedure": "Model performance is evaluated by calculating MSE over the predicted outputs compared to the true labels across different distribution scenarios."
        },
        "experiments": {
            "model": "The models tested include transformers from the GPT-2 family and set-based MLPs.",
            "procedure": "Models were trained on a dataset generated from specified distributions, with varying noise levels and distribution shifts applied during testing.",
            "result": "Transformers outperformed set-based MLPs under mild distribution shifts, but both models showed diminished ICL abilities under severe shifts.",
            "variability": "Variability was accounted for by conducting multiple trials with different subsets of the dataset and varying experimental conditions."
        },
        "conclusion": "The experiments indicate that while transformers excel under certain conditions, both transformer and MLP architectures struggle with severe distribution shifts, highlighting the need for further research into improving ICL capabilities.",
        "discussion": {
            "advantage": "The benchmark provides a structured approach to evaluate ICL capabilities across different architectures, contributing to the understanding of model performance under distribution shifts.",
            "limitation": "A limitation of the benchmark is that it primarily focuses on linear regression tasks, which may not generalize to more complex scenarios.",
            "future work": "Future research should explore the ICL capabilities of a broader range of architectures and algorithms beyond those tested in this benchmark."
        },
        "other info": {
            "additional details": {
                "experiment settings": "Experiments were conducted with both noiseless and noisy label conditions.",
                "model parameters": "MLP-set model depths varied from 4 to 26, and transformers were tested at depths of 12 and 16."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the understanding of in-context learning (ICL) capabilities of large language models (LLMs), particularly transformers, and their limitations under distribution shifts."
        },
        {
            "section number": "1.2",
            "key information": "Understanding the conditions under which transformers outperform simpler models is vital for advancing the field of ICL and improving model reliability."
        },
        {
            "section number": "2",
            "key information": "The benchmark is designed to address the problem of evaluating how well different models, specifically transformers and set-based MLPs, perform ICL tasks under both in-distribution and out-of-distribution scenarios."
        },
        {
            "section number": "3",
            "key information": "The benchmark innovatively categorizes ICL into in-distribution and out-of-distribution scenarios, providing a structured way to evaluate model performance under varying conditions."
        },
        {
            "section number": "5",
            "key information": "The dataset consists of prompts that are sequences of independent and identically distributed input-label pairs, suitable for evaluating ICL capabilities."
        },
        {
            "section number": "6.1",
            "key information": "Both transformer and MLP architectures struggle with severe distribution shifts, highlighting the need for further research into improving ICL capabilities."
        },
        {
            "section number": "6.4",
            "key information": "A limitation of the benchmark is that it primarily focuses on linear regression tasks, which may not generalize to more complex scenarios."
        }
    ],
    "similarity_score": 0.73616926258198,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/A Closer Look at In-Context Learning under Distribution Shifts.json"
}