{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2304.07575",
    "title": "What Makes Good In-context Demonstrations for Code Intelligence Tasks with LLMs?",
    "abstract": "Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively",
    "bib_name": "gao2023makesgoodincontextdemonstrations",
    "md_text": "# What Makes Good In-context Demonstrations for Code Intelligence Tasks with LLMs?\nShuzheng Gao1\u2020, Xin-Cheng Wen1, Cuiyun Gao1\u2217, Wenxuan Wang2, Hongyu Zhang3, Michael R. Lyu2 1 School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China 2 Department of Computer Science and Engineering, The Chinese University of Hong Kong, China 3 School of Big Data and Software Engineering, Chongqing University, China zgao98@gmail.com, xiamenwxc@foxmail.com, gaocuiyun@hit.edu.cn, hyzhang@cqu.edu.cn, {wxwang,lyu}@cse.cuhk.edu.h\nAbstract\u2014Pre-trained models of source code have gained widespread popularity in many code intelligence tasks. Recently, with the scaling of the model and corpus size, large language models have shown the ability of in-context learning (ICL). ICL employs task instructions and a few examples as demonstrations, and then inputs the demonstrations to the language models for making predictions. This new learning paradigm is training-free and has shown impressive performance in various natural language processing and code intelligence tasks. However, the performance of ICL heavily relies on the quality of demonstrations, e.g., the selected examples. It is important to systematically investigate how to construct a good demonstration for code-related tasks. In this paper, we empirically explore the impact of three key factors on the performance of ICL in code intelligence tasks: the selection, order, and number of demonstration examples. We conduct extensive experiments on three code intelligence tasks including code summarization, bug fixing, and program synthesis. Our experimental results demonstrate that all the above three factors dramatically impact the performance of ICL in code intelligence tasks. Additionally, we summarize our findings and provide takeaway suggestions on how to construct effective demonstrations, taking into account these three perspectives. We also show that a carefully-designed demonstration based on our findings can lead to substantial improvements over widely-used demonstration construction methods, e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing, and program synthesis, respectively.\n# I. INTRODUCTION\nRecently, there has been an increasing focus on code intelligence research, aiming at reducing the burden on software developers and enhancing programming productivity [1], [2]. With the large-scale open-source code corpora and the progress of deep learning techniques, various neural source code models have been developed and have achieved stateof-the-art performance on a variety of code intelligence tasks including code summarization [3], bug fixing [4], and program synthesis [5]. In recent years, the advent of pre-training techniques has significantly advanced progress in this area. For instance, CodeBERT [6], a BERT-based model pre-trained on\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce87/ce872e28-4b31-40c9-8c67-76d8c61b2081.png\" style=\"width: 50%;\"></div>\nFig. 1: An example of in-context learning on code summarization task.\nboth natural and programming language data, has demonstrated promising performance in various code intelligence tasks [4], [7]. Other subsequent pre-trained code models such as PLBART [8] and CodeT5 [9] further achieve much improvement over CodeBERT. However, the size and training data of the above models are limited, which may hinder the models from achieving their potential [10]. In these years, we have witnessed explosive growth in the size of pretrained models. Various billion-level large language models are proposed such as GPT-3 [11] and PALM-E [12]. For instance, the size of the pre-trained model PALM-E [12] (562B) in 2023 is over two thousand times larger than the largest model BERT [13] (223M) in 2018. As the size of language models and training data continues to increase, large language models (LLMs) demonstrate various emergent abilities. One such ability is in-context learning (ICL) [11], [14], which allows models to learn from just a few examples within a specific context. As shown in Fig. 1, ICL utilizes a demonstration including task instructions and a few examples to describe the task, which are then concatenated with a query question to form an input for the language model to make predictions. The most significant difference between ICL and traditional tuning methods such as finetuning [6] is that it is training-free and does not need parameter updates. The training paradigm enables ICL to be directly used upon any LLMs and significantly reduces the training costs of adapting models to new tasks [11]. Recent studies show that ICL has achieved impressive results in various\n2) The order of demonstration examples has a large impact on the performance of ICL. In most cases, placing similar samples at the end of a prompt achieves better results. 3) Increasing the number of demonstration examples can be beneficial for ICL, provided that the examples are not cut off due to the input length limitation of LLMs. Careful attention should be paid to this issue, as the length of code is generally longer than natural language. We also show that a carefully-designed demonstration based on the achieved findings can lead to substantial improvements over the widely-used demonstration construction methods [17], [19], [22], e.g., improving BLEU-4, EM, and EM by at least 9.90%, 175.96%, and 50.81% on code summarization, bug fixing and program synthesis, respectively. Contributions. In summary, the main contributions of this work are as follows: 1) To the best of our knowledge, this paper represents the first systematic study on how to construct effective demonstrations for code intelligence tasks. 2) Our comprehensive exploration of demonstration design highlights a range of findings for improving ICL\u2019s performance in code intelligence tasks. 3) We discuss the implications of our findings for researchers and developers and future work for code intelligence tasks in the era of large language models.\n# A. Large Language Models\nLLMs have become a ubiquitous part of Natural Language Processing (NLP) due to their exceptional performance [11], [23]. These models typically follow the Transformer [24] architecture and are trained on large-scale corpora using selfsupervised objectives such as masked language modeling [13]. The size of LLMs has increased significantly in the past few years. For example, the parameters of recent LLMs like GPT3 [11] and PALM-E [12] are over one hundred billion. Apart from the LLMs for general purposes, there are also LLMs with billion-level parameters trained on code corpora, such as AlphaCode [25], and Codex [2]. The OpenAI\u2019s Codex is a large pre-trained code model that is capable of powering Copilot. AlphaCode [25] is a 41-billion-large model trained for generating code in programming competitions like Codeforces. Recently, LLMs like ChatGPT [26] and GPT-4 [23] have also shown impressive performance in many code intelligence tasks. Apart from proposing new LLMs, how to effectively leverage them has also become an important research topic. A prevalent method is to fine-tune the model and update its parameters on downstream datasets [13]. Recently, promptbased fine-tuning has been proposed, which aims to convert the training objective of downstream tasks into a similar form as the pre-training stage [27], [28]. Considering the cost of tuning the whole model, various Parameter Efficient Tuning methods have been proposed, such as Adapter [29], Lora [30], and prefix tuning [31]. These methods keep most of the parameters in the model frozen and only tune a small portion of them.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/180d/180d371f-6b2d-4dda-89f0-b8f71dfa3533.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> 3. Number of demonstration examples</div>\nFig. 2: Illustration of design space of in-context demonstrations.\n# B. In-context Learning\nTuning a large pre-trained model can be expensive and impractical for researchers, especially when limited fine-tuned data is available for certain tasks. ICL offers a new alternative that uses language models to perform downstream tasks without requiring parameter updates [11], [14]. It leverages a demonstration in the prompt to help the model learn the inputoutput mapping of the task. This new paradigm has achieved impressive results in various tasks such as logic reasoning and program repair [15], [17], [19]. Specifically, as shown in Fig. 1, ICL employs N demonstration examples {(x1, y1), (x2, y2), ..., (xN, yN)} and further reconstructs them into reconstructed examples {(x\u2032 1, y\u2032 1), (x\u2032 2, y\u2032 2), ..., (x\u2032 N, y\u2032 N)} by natural language instructions and prompt template, where xi, yi, x\u2032 i, y\u2032 i are the input, output, reconstructed input, and reconstructed output, respectively. Typically, The value of N is relatively small, i.e., fewer than 50 samples, which is significantly smaller than the size of the training set in previous fine-tuned methods [6], [9]. This setting is referred to as few-shot in-context learning. Specially, when the value of N is zero, it is called the zeroshot in-context learning setting. Then, ICL concatenates the reconstructed demonstration examples d1 to dN literally into demonstration D = x\u2032 1 \u2225y\u2032 1 \u2225x\u2032 2 \u2225y\u2032 2 \u2225... \u2225x\u2032 N \u2225y\u2032 N, and further adds the test sample at the end to construct the input prompt P = D \u2225x\u2032 test, where \u2225denotes the literal concatenation operation. This prompt is finally fed into the language model for predicting the label ytest for test samples. Previous studies in NLP have shown that the performance of ICL is strongly dependent on the quality of the demonstration. For example, Liu et al. [20] show that selecting demonstration examples with higher similarity or increasing the number of demonstration examples can improve ICL\u2019s performance. The\nresults in [21] show that the order of demonstration examples also has a large impact on the results. Following previous studies, we summarize three key factors to consider when designing a demonstration for ICL: the selection, ordering, and number of demonstration examples, as shown in Fig. 2. We would like to further clarify that there are two types of demonstration in ICL: task-level demonstration and instancelevel demonstration [32], [33]. The task-level demonstration uses the same demonstration examples for all test samples and does not take the difference of each test sample into consideration, while the instance-level demonstration selects different demonstration examples for different test samples. Although instance-level demonstrations generally perform better than task-level demonstrations, it requires a labeled training set in advance for retrieval. The task-level demonstration is more flexible as it can be used in scenarios where very few data are labeled, or no labeled data are available by selecting few representative data for human labeling [33]. In this paper, we investigate both the task-level and instance-level demonstration construction methods for code intelligence tasks.\n# III. EXPERIMENTAL EVALUATION\nWe design experiments to investigate the impact of the selection, ordering, and number of demonstrations on ICL for code intelligence tasks. Our research aims to answer the following questions: RQ1: What kind of selection methods are helpful for ICL in code intelligence tasks? RQ2: How should demonstration examples be arranged for ICL in code intelligence tasks? RQ3: How does the number of demonstration examples in a prompt impact the performance of ICL in code intelligence tasks? RQ4: How is the generalizability of our findings?\nIn RQ1, we aim at verifying whether selecting similar and diverse demonstration examples is helpful. Besides, we also compare different retrieval methods to analyze the impact of different similarity measurement methods for ICL. RQ2 aims at investigating the influence of ordering methods by comparing random ordering with similarity-based ordering. In RQ3, we want to explore whether increasing the number of examples could bring better performance for ICL. In RQ4, we evaluate whether the findings achieved in RQ1-RQ3 are also applicable to different LLMs for verifying the generalizability of the findings.\n# B. Evaluation tasks\nWe conduct experiments on three popular code intelligence tasks: code summarization, bug fixing, and program synthesis. 1) Code Summarization: Code summarization, also known as code comment generation, aims to generate useful comments automatically for a given code snippet [7]. Recent work mainly formulates it as a sequence-to-sequence neural machine translation (NMT) task and involves pre-trained techniques to achieve better performance [9], [34].\n<div style=\"text-align: center;\">TABLE I: Statistics of the benchmark datasets.</div>\nTask\nDatasets\nTrain\nDev\nTest\nCode Summarization\nCSN-Java\n164,923\n5,183\n10,955\nTLC\n69,708\n8,714\n6,489\nBug Fixing\nB2Fsmall\n46,628\n5,828\n5,831\nB2Fmedium\n53,324\n6,542\n6,538\nProgram Synthesis\nCoNaLa\n2,389\n-\n500\nDatasets. To evaluate the performance of code summarization, we use two widely-used datasets: CodeSearchNet (CSN) [35] and TLCodeSum (TLC) [7]. CSN is a largescale source code dataset mined from open-source GitHub repositories. It contains code summarization data in six programming languages, i.e., Java, Go, JavaScript, PHP, Python, and Ruby. The dataset is split into training, validation, and test sets in the proportion of 8:1:1. In this study, considering our time and resource limitation, we use the Java portion of the filtered CSN dataset in CodeBERT [6], which contains 181,061 samples across the training, validation, and test sets for evaluation. TLC has 87,136 code-comment pairs crawled from 9,732 open-source Java projects on GitHub with at least 20 stars. The code snippets are all at the method level and the comments of corresponding Java methods are considered as code summaries. The portion of training, validation, and test set is also 8:1:1. As reported in previous work, there are duplicated data in the training and test set. Therefore, we follow previous work [36] and remove the duplicated data, and finally get a test set with 6,489 samples. Metrics. We use three widely-adopted metrics for code summarization evaluation: BLEU-4 [37], ROUGE-L [38] and METEOR [39] for evaluation. These metrics evaluate the similarity between generated summaries and ground-truth summaries and are widely used in code summarization [3], [36], [40]. 2) Bug Fixing: Bug fixing is the task of automatically fixing bugs in the given code snippet. It helps software developers find and fix software errors [4], [41]. Datasets. The dataset for bug fixing is B2F which is collected by Tufano et al. [4] from bug-fixing commits in GitHub. We use the multi-model version proposed in MODIT [42] for experiments as it contains both the code changes and the fix instruction. The model is given both the buggy code and natural language fix guidance to predict the fixed code. We follow their original setting to split the dataset into two parts B2Fmedium and B2Fsmall based on the length of code tokens (the code length of B2Fmedium is between 50 and 100 tokens and that of B2Fsmall is below 50 tokens). Metrics. We follow previous work [43] and use Exact Match (EM) and BLEU-4 for both datasets. 3) Program Synthesis: Program synthesis is the task of generating source code based on the given natural language description. It provides practical assistance to developers and enhances their productivity [2]. Datasets. For program synthesis, we use the CoNaLa [44] dataset for evaluation. This dataset consists of 2,889 \u27e8intent, code\u27e9pairs mined from Stack Overflow in Python. We directly\n<div style=\"text-align: center;\">TABLE II: Prompt template for each task. Here text in the form of {#xxx} will be filled in actual inputs from the dataset.</div>\n {}\nTask\nTemplate\nCode Summarization\nGenerate comment (summarization) for this code\n[input] {#code} [output] {#comment}\nBug Fixing\nFix the bug according to the guidance [input]\n{#buggy code} <s> {#instruction} [output] {#fixed code}\nProgram Synthesis\nGenerate code based on the requirement\n[input] {#requirement}[output] {#code}\nuse the original partition of the dataset, which includes 2,389 samples for training and 500 samples for testing. Metrics. We follow previous work [43] and evaluate the performance of program synthesis with four metrics including Exact Match (EM), CodeBLEU (CB), Syntax Match (SM), and Dataflow Match (DM). EM measures whether the code generated by the model is identical to the goal code. CB [45] is a modified version of BLEU designed specifically for code, which leverages syntax and semantic information such as Abstract Syntax Tree (AST) and data flow to measure the similarity of two code snippets. SM and DM are two components that calculate the matching subtrees and data flow edges\u2019 proportion, respectively.\n# C. Implementation\nWe utilize the OpenAI Codex (code-davinci-002) API [2] in our paper for all experiments in the first three RQs. In RQ4, we further use the API of GPT-3.5 (text-davinci-003) [11] and ChatGPT (gpt-3.5-turbo) [26] for experiments. As for the hyperparameters of the APIs, following the previous work [46], [47], we set the temperature to 0 to get the deterministic output. The frequency penalty and presence penalty are also set to 0. The input length limitation of Codex, GPT-3.5, and ChatGPT is 8,001, 4,096, and 4,097 tokens, respectively. Hence we cut off the input code of each demonstration example to 8001 N+1, 4096 N+1, and 4097 N+1 tokens, respectively, where N represents the number of demonstration examples. Empirically, it took approximately 6 hours to evaluate 1,000 examples for Codex. To avoid excessive time costs, we randomly sample a small test set (2,000 samples) for each dataset with over 2,000 test samples. We use four examples in the demonstration in RQ1 and RQ2, and further discuss the impact of the number of demonstration examples in RQ3. The templates used in this study are shown in Table II. We also show some examples in our GitHub repository1. We conduct all the experiments on a server with 2 Nvidia RTX 3090 GPUs. The GPUs are used in the dense retrieval process.\n1) Experimental design: We first explore the impact of demonstration selection methods on ICL for code-related tasks. To provide a comprehensive study, we adopt different\n<div style=\"text-align: center;\">TABLE III: Experimental results of different demonstration selection methods on Code Summarization. \u201cAvg\u201d and \u201cCV\u201d deno the average results and Coefficient of Variation over three different orders, respectively.</div>\nApproach\nCode Summarization\nCSN\nTLC\nBLEU-4\nROUGE-L\nMETEOR\nBLEU-4\nROUGE-L\nMETEOR\nAvg\nCV\nAvg\nCV\nAvg\nCV\nAvg\nCV\nAvg\nCV\nAvg\nCV\nTask-level Demonstration\nRandom\n19.64\n1.44\n35.46\n1.88\n15.30\n1.54\n17.29\n0.71\n34.28\n0.61\n12.48\n0.67\nKmeansRND\n20.71\n0.82\n38.03\n0.44\n16.34\n0.83\n17.91\n1.19\n35.69\n1.60\n13.48\n0.91\nInstance-level Demonstration\nBM-25\n22.35\n0.46\n38.31\n0.56\n17.01\n0.78\n36.96\n0.84\n51.42\n0.79\n24.22\n0.99\nSBERT\n22.27\n0.23\n38.39\n0.42\n16.91\n0.22\n36.42\n0.61\n50.47\n0.40\n23.86\n0.68\nUniXcoder\n22.11\n0.61\n38.23\n0.53\n16.81\n0.23\n36.77\n0.52\n51.11\n0.29\n24.08\n0.79\nCoCoSoDa\n21.92\n0.46\n37.85\n0.22\n16.78\n0.24\n36.91\n0.69\n50.69\n0.53\n24.08\n0.39\nOracle (BM-25)\n27.69\n0.43\n46.17\n0.14\n20.26\n0.22\n43.16\n0.15\n59.17\n0.09\n28.09\n0.16\nTABLE IV: Experimental results of different demonstration selection methods on Bug Fixing.\n<div style=\"text-align: center;\">TABLE IV: Experimental results of different demonstration selection methods on Bug Fixing.</div>\nApproach\nBug Fixing\nB2Fmedium\nB2Fsmall\nBLEU-4\nEM\nBLEU-4\nEM\nAvg\nCV\nAvg\nCV\nAvg\nCV\nAvg\nCV\nTask-level Demonstration\nRandom\n86.96\n0.16\n7.26\n16.18\n71.18\n0.56\n9.95\n6.33\nKmeansRND\n86.91\n0.17\n9.03\n5.45\n72.89\n1.36\n10.37\n3.86\nInstance-level Demonstration\nBM-25\n88.05\n0.09\n21.85\n1.78\n77.54\n0.13\n30.45\n0.96\nSBERT\n87.98\n0.06\n19.00\n2.88\n76.26\n0.16\n26.15\n0.87\nUniXcoder\n87.87\n0.09\n19.14\n2.00\n77.52\n0.07\n29.93\n0.51\nCoCoSoDa\n87.73\n0.07\n19.23\n0.74\n76.45\n0.07\n27.40\n1.04\nkinds of demonstration selection methods for the three code intelligence tasks. For task-level demonstration, we need to select a group of demonstration examples for the whole test set, as illustrated in Section II-B. To explore the influence of different in-context demonstration examples on the performance of ICL, we randomly select three groups of demonstration examples from the training set, and evaluate their performance on different tasks, denoted as Random. Besides, we further investigate whether improving the diversity of demonstration examples is beneficial to ICL. We select the demonstration examples by first dividing the whole samples into N clusters and then randomly selecting one sample from each cluster, namely KmeansRND. Specifically, we use UniXcoder [48] for vectorization and use the K-means++ algorithm [49] for clustering, where K is set to N that represents the number of demonstration example. Similar to Random, we also investigate the performance of different groups of examples for KmeansRND and conduct the selection process three times, resulting in three groups of demonstration examples. For instance-level demonstration, we need to select examples for each test sample, as illustrated in Section II-B. Following [20], we formulate the selection process as a retrieval problem and compare the performance of different retrievalbased methods including: 1) BM-25: BM-25 is a classic sparse retrieval method in the information retrieval field. It has also been widely used in many code intelligence models [50], [51]. 2) SBERT: SBERT [52] is a popular sentence modeling\nmethod and has been widely used in text retrieval [52], [53]. Specifically, in this paper, we use the version that is further trained on the code-related dataset to obtain code and text representations [54]. 3) UniXcoder: UniXcoder [48] is a unified cross-modal pre-trained model that is pre-trained with three sequence modeling tasks and two contrastive learning-based tasks. It shows promising performance on zero-shot code-tocode search. 4) CoCoSoDa: CoCoSoDa [55] is a state-of-the-art code search model that utilizes contrastive learning for code and text representation learning.\nFor BM-25, we implement with the gensim package [56] by retrieving samples with the highest similarity from the training set. For dense retrieval methods, we directly use these pre-trained models in the replication packages released by the authors without further tuning. Based on the code/text representations output by the pre-trained models, we select the training samples presenting the highest cosine similarities with the test sample. We also follow the previous work [32] and create a method called Oracle, which selects demonstration examples by calculating the similarity between the output of the test sample and the output of all training set examples. The Oracle method is usually regarded as an upper bound of the performance, considering that the output of the test sample is not available in practice. The retrieval process in Oracle is implemented by BM-25, since BM-25 shows the best performance compared with other dense retrieval methods as shown in Table III-V. To avoid the influence of different orders of demonstration examples, we run each experiment three times with different orders and report the average results on each metric. Besides, we further evaluate the sensitivity of each method to different orders by Coefficient of Variation (CV) [57]. The CV is calculated by \u03c3/\u00b5, where \u03c3 is the standard deviation and \u00b5 is the mean. A lower CV indicates smaller data variation. It takes the magnitude of data into account and has been widely used to measure the data dispersion in many fields such as economics and software engineering [57], [58]. 2) Analysis: We present the experimental results in Table III-V. For each metric, we report the average results over\n<div style=\"text-align: center;\">TABLE V: Experimental results of different demonstration selection methods on Program Synthesis.</div>\nApproach\nProgram Synthesis\nCB\nSM\nDM\nEM\nAvg\nCV\nAvg\nCV\nAvg\nCV\nAvg\nCV\nTask-level Demonstration\nRandom\n28.36\n1.30\n44.37\n0.83\n39.70\n1.33\n16.00\n1.60\nKmeansRND\n28.03\n1.47\n44.41\n0.54\n37.31\n1.54\n17.03\n1.06\nInstance-level Demonstration\nBM-25\n30.37\n0.91\n46.22\n0.84\n40.75\n1.06\n18.53\n0.50\nSBERT\n29.08\n0.70\n44.91\n0.31\n39.81\n3.01\n16.13\n2.54\nUniXcoder\n28.96\n0.50\n43.93\n0.67\n37.96\n1.12\n16.00\n3.53\nCoCoSoDa\n29.42\n0.82\n44.62\n0.70\n40.91\n1.12\n16.30\n0.86\nthree random orders and CV which measures their sensitivity to different orders. In Fig. 3, we show the distribution of results with different groups of examples for Random and KmeansRND. Diversity of examples is beneficial for task-level demonstration. As can be seen in Table III-V and Fig. 3, by comparing the results on Random and KmeansRND, we can find that in most cases improving the diversity of task-level demonstrations can not only improve the average performance of ICL but also reduce the fluctuation brought by different groups of examples. For example, as shown in Table III, comparing the results of code summarization on CSN, the average improvements of KmeansRND over Random are 5.45%, 7.25%, and 6.80% with respect to BLEU-4, ROUGE-L, and METEOR, respectively. Besides, we can also find that the performance of different in-context demonstration examples of Random varies a lot, and improving the diversity of selected examples can reduce this variation in general. For example, as shown in Fig. 3 (a), the gap between the best and worst BLEU4 score of Random is about 2.5 while that of KmeansRND is only about 0.6. This indicates that improving the diversity of selected demonstration examples is beneficial for building task-level demonstration.\nFinding 1: Diversity of examples is helpful for the demonstration selection of ICL. It can help improve overall performance and lead to a more stable prediction regarding different groups of examples.\nBM-25 is a simple and effective method for instancelevel demonstration. By comparing the results of different instance-level demonstration methods, we can find that the simple BM-25 method can achieve comparable or even better performance than other dense retrieval methods on demonstration selection in ICL. For example, the average EM of BM-25 on Program Synthesis is 18.53, which outperforms three strong dense retrieval methods SBERT, UniXcoder, and CoCoSoDa by 14.88%, 15.81%, and 13.68%, respectively. This result indicates that BM-25 serves as an effective baseline approach and could be taken into account in future studies of demonstration selection for code intelligence tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a4cf/a4cf1cf4-1fbf-481c-aca9-f68f4d5e7d1f.png\" style=\"width: 50%;\"></div>\nFig. 3: Comparison of the performance distribution of Random and KmeansRND regarding different groups of examples on three tasks.\nFinding 2: The retrieval methods for demonstration selection can impact the performance of ICL, among which BM-25 is a simple and effective method.\nInstance-level demonstration outperforms task-level demonstration greatly. As shown in Table III-V, we can find that instance-level demonstration can achieve much better performance in all tasks. Specifically, the instance-level selection methods improve the best task-level demonstration\u2019s exact match results by at least 141.97% and 193.64% on B2Fmedium and B2Fsmall, respectively. These results indicate that selecting similar demonstration examples specifically for each test sample can benefit ICL in code intelligence tasks a lot. The task-level demonstration is more sensitive to the order than the instance-level demonstration. By comparing the CV of task-level demonstration and instance-level demonstration, we can find that the performance of instancelevel demonstration is generally more stable than task-level demonstration regarding different example orders. Specifically, as shown in Table IV, the CV of BLEU-4 of task-level demonstration KmeansRND to the order is 0.17 and 1.36 on two bug fixing datasets, which is much larger than that of instance-level demonstration methods (e.g., 0.09 and 0.13 for BM-25, respectively). This indicates that selecting examples by similarity is more robust to the changes in the demonstration order and we should carefully arrange the order of\n<div style=\"text-align: center;\">TABLE VI: Experimental results of different demonstration ordering methods.</div>\nApproach\nCode Summarization (CSN)\nBug Fix (B2Fsmall)\nProgram Synthesis (CoNaLa)\nBLEU-4\nROUGE-L\nMETEOR\nBLEU-4\nEM\nCB\nSM\nDM\nEM\nRandom\nRandom\n20.46\n36.71\n16.17\n72.40\n9.52\n27.72\n44.46\n37.53\n15.53\nSimilarity\n21.04\n37.86\n16.26\n72.02\n9.93\n28.47\n44.87\n37.79\n16.00\nReverse Similarity\n19.78\n33.71\n15.64\n71.44\n9.02\n27.62\n44.48\n37.96\n15.20\nKmeansRND\nRandom\n20.67\n37.64\n15.97\n72.29\n8.60\n26.64\n42.97\n37.24\n16.87\nSimilarity\n20.69\n37.62\n16.05\n72.90\n10.15\n27.20\n42.97\n36.93\n16.40\nReverse Similarity\n20.55\n37.43\n16.20\n72.05\n9.78\n27.09\n43.74\n37.19\n16.60\nBM-25\nRandom\n22.35\n38.31\n17.01\n77.54\n30.45\n30.37\n46.22\n40.75\n18.53\nSimilarity\n22.23\n38.12\n17.01\n77.76\n30.95\n30.83\n46.41\n41.33\n17.60\nReverse Similarity\n22.13\n38.26\n16.91\n77.60\n29.80\n30.01\n45.72\n39.60\n18.20\n<div style=\"text-align: center;\">demonstration examples when using task-level demonstration</div>\nFinding 3: Compared with task-level demonstration,\ninstance-level demonstrations can achieve much better\nperformance and are generally more robust to the\nchanges in the demonstration order.\nApart from the above, we can also observe in Table III that the best demonstration selection method BM-25 still has a large gap with the Oracle. This indicates that these retrieval methods may fail to select semantic similar examples and there exists a large space for further improvement concerning the demonstration selection method for code intelligence tasks.\n# B. RQ2: Demonstration Order\n1) Experimental setup: In RQ1, we have found that the order of demonstration examples impacts the performance of ICL on code intelligence tasks, especially on task-level demonstration. Therefore, in this section, we explore how to better arrange the demonstration examples in ICL. Inspired by the finding that the task-level demonstration is more sensitive to the example order than the instance-level demonstration, we suppose that the order of similarities between each demonstration example and test sample plays an important role in ICL. To verify this, in this RQ, we compare random order with two basic ordering methods, i.e., Similarity and Reverse Similarity. In the Similarity method, we compare the similarity of each example with test sample and the example with a higher similarity will be placed closer to the test sample. On the contrary, for the Reverse Similarity method, the demonstration examples will be placed in descending order according to their similarity to the test sample. We experiment with three demonstration selection methods here. As illustrated in RQ1, since the order arrangement is important for task-level demonstration, we use both the Random and KmeansRND for experiments. As for instance-level demonstration, we conduct experiments on BM-25, since it shows the best performance among all the instance-level demonstration selection methods. 2) Analysis: From the results in Table VI, we can find that placing the demonstration examples in ascending order based on their similarity to the test sample performs generally better than the reverse. Specifically, Similarity consistently outperforms Reverse Similarity on code summarization and\nbug fixing by at least 0.45% and 0.21% with respect to BLEU4 and EM, respectively. By further comparing all the results together, we can observe that similarity achieves the best performance in most cases. Specifically, it achieves the best performance in 62.96% (17/27) metrics and tasks. However, we can also observe that there are some cases in which both Similarity and Reverse Similarity perform worse than the average results of using random order, indicating that more complex demonstration ordering methods can be explored by the future work.\nFinding 4: The different orders of demonstration examples can impact the performance of ICL. Arranging the demonstration examples based on their similarity to the test sample in ascending order can achieve relatively better results in most cases.\n# C. RQ3: The Number of Demonstration Examples\n1) Experimental setup: In this section, we investigate whether the increase in the number of examples will improve the performance of ICL on code intelligence tasks. We vary the number of demonstration examples from 1 to 64. We use BM-25 and Similarity as demonstration selection and demonstration ordering methods, respectively, based on the above findings. 2) Analysis: As shown in Fig. 4, we can find that the performance of ICL on all the tasks increases with the number of demonstration examples at first. However, when the number of examples is above 16, the results on different tasks show different trends. For example, for bug fixing, the performance achieves the peak when the number of demonstration examples is 32 and suffers from a significant drop when further increasing the number to 64. As for program synthesis, the performance keeps increasing and tends to be stable when the number exceeds 32. We believe that the different trends are caused by the truncation problem [59], [60]. As illustrated in Section III-C, when increasing the number of examples, the length of the whole demonstration will increase and the examples might be cut off to avoid exceeding the length limitation of LLMs. Specifically, for the B2Fsmall dataset, all the examples are complete without cutting off when the number of examples is below 32. However, when the number becomes 32, 2.33% demonstration examples are cut off. When\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a9fe/a9fef646-7d0b-47bb-b747-8093479c7c61.png\" style=\"width: 50%;\"></div>\nTABLE VII: Experiments of generalization of findings on GPT3.5 and ChatGPT.\nApproach\nCB\nEM\nSelection\nOrder\nAvg\nCV\nAvg\nCV\nGPT-3.5\nRandom\nRandom\n26.60\n3.01\n12.32\n4.73\nKmeansRND\nRandom\n28.26\n1.93\n13.60\n1.65\nUniXcoder\nRandom\n30.06\n0.53\n13.73\n1.13\nBM-25\nRandom\n30.81\n1.05\n14.40\n1.81\nBM-25\nSimilarity\n30.69\n0.00\n15.20\n0.00\nChatGPT\nRandom\nRandom\n28.17\n1.98\n11.88\n4.24\nKmeansRND\nRandom\n28.25\n2.31\n12.92\n1.78\nUniXcoder\nRandom\n29.33\n1.85\n14.32\n2.87\nBM-25\nRandom\n28.95\n5.75\n13.47\n1.82\nBM-25\nSimilarity\n30.03\n0.00\n14.20\n0.00\nfurther increasing the number to 64, the truncation problem happens on over 80% examples and 44.32% characters in those examples are discarded, resulting in a dramatic performance degradation. Since the length of samples in CSN and B2Fsmall datasets is much larger than that of the CoNaLa dataset, i.e., 557, 492, 101 characters per sample for CSN, B2Fsmall, and CoNaLa, respectively, the truncation problem does not appear on program synthesis even though the number grows to 64. Therefore, balancing the number of examples and the ensuing truncation problem is important for ICL. Since the code is generally much longer than natural language [35], the truncation problem is easier to appear in code intelligence tasks. Besides, more examples will also lead to a larger cost of using external API and the inference time [61]. A smaller number of examples may be more appropriate for code intelligence tasks. From the results (Fig. 4), we can also find that the performance with four demonstration examples is good enough, achieving 96.48%, 97.80%, and 94.80% of the best performance on the three tasks with respect to EM, BLEU-4, and CodeBLEU, respectively. Therefore, considering the above trade-off, using four examples in the demonstration is a good choice for code intelligence tasks.\nfurther increasing the number to 64, the truncation problem happens on over 80% examples and 44.32% characters in those examples are discarded, resulting in a dramatic performance degradation. Since the length of samples in CSN and B2Fsmall datasets is much larger than that of the CoNaLa dataset, i.e., 557, 492, 101 characters per sample for CSN, B2Fsmall, and CoNaLa, respectively, the truncation problem does not appear on program synthesis even though the number grows to 64. Therefore, balancing the number of examples and the ensuing truncation problem is important for ICL.\nSince the code is generally much longer than natural language [35], the truncation problem is easier to appear in code intelligence tasks. Besides, more examples will also lead to a larger cost of using external API and the inference time [61]. A smaller number of examples may be more appropriate for code intelligence tasks. From the results (Fig. 4), we can also find that the performance with four demonstration examples is good enough, achieving 96.48%, 97.80%, and 94.80% of the best performance on the three tasks with respect to EM, BLEU-4, and CodeBLEU, respectively. Therefore, considering the above trade-off, using four examples in the demonstration is a good choice for code intelligence tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e3fc/e3fc9903-8bb6-4db3-bdec-333b9e0816d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Comparison of the performance distribution of Random and KmeansRND regarding different groups of examples on GPT-3.5 and ChatGPT.</div>\nFinding 5: More demonstration examples in the prompt will not always lead to better performance considering the truncation problem. To save costs, it is suggested that four examples are used in the demonstration.\n# D. RQ4: The Generalization of Findings\n1) Experimental setup: In this section, we evaluate the generalization of our findings on different LLMs. Apart from Codex, we experiment on two other LLMs including GPT3.5 [11] and ChatGPT [26]. To validate the finding 1-4, we experiment with the following combinations of demonstration selection and ordering methods: Random+Random, KmeansRND+Random, UniXcoder+Random, BM-25+Random, and BM-25+Similarity. As for the finding 5 in RQ3, we use BM-25+Similarity as the selection and ordering method and vary the number of demonstration examples from 1 to 128 to validate whether the truncation will lead to performance degradation. Due to the cost limit, we choose the program synthesis task for evaluation. We also measure how much improvement could our findings bring by comparing the performance of ICL with a carefully-\n<div style=\"text-align: center;\">TABLE VIII: Comparison of different demonstration construction methods on three LLMs.</div>\nApproach\nCode Summarization (CSN)\nBug Fix (B2Fsmall)\nProgram Synthesis (CoNaLa)\nBLEU-4\nROUGE-L\nMETEOR\nBLEU-4\nEM\nCB\nSM\nDM\nEM\nCodex\nZero-shot\n1.82\n4.27\n4.19\n34.65\n1.43\n8.71\n9.26\n23.81\n0.20\nBaseline demonstration\n17.37\n32.04\n13.43\n69.07\n9.70\n27.54\n44.56\n37.07\n14.20\nCarefully-designed demonstration\n22.73\n39.52\n17.35\n77.54\n32.25\n32.07\n48.03\n42.88\n21.40\nGPT-3.5\nZero-shot\n6.34\n15.05\n14.08\n2.81\n0.15\n0.06\n0.26\n0.00\n0.20\nBaseline demonstration\n14.55\n21.53\n13.81\n62.87\n9.15\n26.36\n36.94\n41.67\n10.00\nCarefully-designed demonstration\n15.99\n26.78\n16.70\n71.70\n25.25\n30.69\n43.95\n44.78\n15.20\nChatGPT\nZero-shot\n3.63\n11.40\n13.16\n2.32\n0.05\n25.70\n37.64\n54.44\n3.40\nBaseline demonstration\n10.76\n20.02\n14.83\n41.57\n4.60\n27.62\n41.83\n46.85\n9.40\nCarefully-designed demonstration\n11.90\n23.31\n16.93\n53.92\n18.15\n30.03\n45.04\n44.26\n14.20\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c077/c0777842-dc10-4c06-a66f-28c294a3a60a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: Experimental results of different number of demonstration examples on GPT-3.5 and ChatGPT.</div>\ndesigned demonstration, ICL with the widely-used demonstration construction method [17], [19], [22], and zero-shot ICL. In the carefully-designed demonstration, we use BM25 and Similarity as demonstration selection and ordering methods and employ four demonstration examples; while for the widely-used baseline demonstration construction method, we use the settings in previous work [17], [19], [22] and randomly select two demonstration examples from the training set with random order. As for zero-shot ICL, as illustrated in section II-B, no demonstration example is used and the model predicts only based on the instruction. 2) Analysis: We present the average results and CV of GPT3.5 and ChatGPT in Table VII. In Fig. 5 and Fig. 6, we present the performance distribution of different groups of examples and the impact of the number of examples on these two LLMs, respectively. The comparison of different demonstrations is shown in Table VIII. Due to the space limitation, we only present the performance on EM and CB and the results on other metrics can be found in our replication package. From these results, we can observe that our findings can also be applied to GPT-3.5 and ChatGPT. As shown in Table VII and Fig. 6, we can observe that KmeansRND+Random not only outperforms Random+Random on the average results, but also has a more stable prediction distribution regarding different groups of examples. Taking GPT-3.5 as an example, KmeansRND+Random improves Random+Random by 6.24% and 10.39% with respect to CB and EM, respectively. This indicates that diversity is also beneficial for the demonstration construction of these two models (finding 1). Similarly, by comparing BM25+Random and UniXcoder+Random, we can also find that\nBM-25 can achieve similar performance and even outperforms UniXcoder on GPT-3.5 by 2.50% and 4.88% with respect to CB and EM, respectively. This shows that BM-25 is also a simple and effective demonstration selection method in these two models (finding 2). Besides, on GPT-3.5 and ChatGPT, instance-level demonstrations also consistently outperform task-level demonstrations and achieve lower CV to different orders in general. It indicates that selecting demonstration examples by similarity is also beneficial for these two LLMs (finding 3). As for the impact of example order, we can also find that BM-25+Similarity consistently improves BM25+Random on all metrics and LLMs, e.g., improving the average EM by 5.56% and 5.42% on GPT-3.5 and ChatGPT, respectively (finding 4). As for the impact of numbers, we can observe similar trends on GPT-3.5 and ChatGPT in Fig. 6, the EM first increases with the number of demonstration examples. As the number further increases to 128, 25.05% examples suffer from the truncation problem, resulting in a sudden degradation (finding 5). Table VIII shows the comparison of different demonstrations. We can also observe that the performance of zero-shot ICL is very poor on all tasks, which indicates the importance of using demonstration examples to guide the LLM to understand the task. Besides, by comparing the performance of the carefully-designed demonstration with the baseline demonstration, we can find that ChatGPT with a carefully-designed demonstration outperforms the baseline demonstration by at least 10.59%, 294.57%, and 51.06% on code summarization, bug fixing, and program synthesis with respect to BLEU-4, EM, and EM, respectively. The results indicate the importance of constructing a good demonstration, and the generalizability of the findings.\n# V. DISCUSSION\n# A. Implications of Findings\nIn this section, we discuss the implications of our work for researchers and developers. Researchers: Our research demonstrates that the performance of few-shot in-context learning is highly dependent on the design of demonstrations. With well-constructed demonstrations, ICL can achieve much better performance. Our experimental results also show potential research directions in the era of LLM and ICL for the code intelligence community. Specifically:\n\u2022 As shown in the results of RQ1, current state-of-the-art code retrieval models still have a large gap with the Oracle, indicating that these models fail to select examples with the highest semantic similarities. Therefore, effective code representation models for zero-shot code-to-code search are worth studying. Besides, designing example selection strategies based on the prior knowledge of each task or the properties of source code are also interesting directions that are worth exploring. \u2022 Placing similar examples in the back of all examples leads to relatively better performance than random and reverse placings. However, such improvement is not consistent. Therefore, how to automatically design a better ordering method for code intelligence tasks needs to be further investigated. \u2022 Different from natural language text, the length of a code snippet is often much longer. This limits the number of examples in the prompt and could bring large computation and time costs for LLMs. Therefore, incorporating program slicing and reduction techniques into ICL to reduce the costs is worth investigating. Developers: In-context learning is a paradigm that allows for learning from a few examples in the prompt without requiring parameter updates. This new approach has also fascinated the language-model-as-a-service community. Our findings indicate that the selection, order, and number of demonstration examples have significant impacts on the performance of ICL for code intelligence tasks. Based on our findings, we conclude the following insights and takeaways for developers to use LLM in their work: \u2022 Including demonstration examples in the prompt, which help the model understand the task and guide the output format. \u2022 Using a retrieval method to select demonstration examples when a labeled training set is available. For the retrieval methods, consider using BM-25 as it is a simple yet effective method. \u2022 Improving the diversity of task-level demonstration examples with clustering to obtain more accurate and stable predictions. \u2022 When arranging the order of demonstration examples, placing similar samples at the end of the list is a good choice in most cases. \u2022 Using as many demonstration examples as possible, but be mindful of the maximum length limitation to avoid truncation issues. To save costs, it is also suggested that four examples are used in the demonstration.\n# B. Threats to Validity\nWe identify three main threats to validity of our study: 1) Potential data leakage. In this paper, we conduct experiments by using the API of OpenAI Codex, GPT3.5, and ChatGPT. However, since they are closed-source models, their parameters and training sets are not publicly available, which raises concerns about potential data leakage. Specifically, there is a possibility that the model has\nalready been trained on the test set and merely memorizes the results instead of predicting them. However, we can observe from our experiments that the model\u2019s performance in a zero-shot setting is catastrophic, indicating a low probability of direct memorization of the dataset. Moreover, all experiments in our paper were conducted using these models and we use the relative performance improvement to measure the effectiveness of different demonstration construction strategies. Therefore, the findings of our paper remain convincing. 2) The selection of tasks. In this study, we investigate constructions of the demonstration on representative three tasks including code summarization, bug fixing, and program synthesis. These tasks cover different types such as Code \u2192Text, Code+Text \u2192Code, and Text \u2192Code. Hence, we believe the finding of our paper can generalize to a wide arrange of code intelligent tasks. In the future, we plan to conduct experiments on other types of tasks such as Code \u2192Class tasks (e.g., vulnerability detection) and Code \u2192Code tasks (e.g., code translation). 3) The selection of models. In this paper, we select three LLMs for experiments. Nonetheless, there are other LLMs available, such as CodeGen [62] and CodeGeeX [63]. In the future, we plan to conduct experiments on a broader range of LLMs to verify the generalizability of our findings. 4) The selection of languages. For each task, we select one popular dataset for evaluation. The datasets of three tasks only contain two programming languages, i.e., Java and Python. In the future, we will validate the effectiveness of demonstration construction methods in other languages.\n# VI. RELATED WORK\n# A. Pre-trained Models of Code\nRecently, with the development of pre-trained techniques, the pre-trained models of code have been widely used and achieved state-of-the-art performance on various software engineering tasks. One such model is CodeBERT [6], which is an encoder-only pre-trained model on six programming languages with two self-supervised tasks. Another model, CodeT5 [9] is an encoder-decoder pre-trained model following the same architecture as T5. CodeGPT [64] is a decoder-only model that pre-trains on programming languages dataset and has the same architecture as GPT-2. PLBART [8] uses denoising sequenceto-sequence pretraining for both program understanding and generation purposes. UniXCoder [48] involves multi-modal contrastive learning and cross-modal generation objective to learn the representation of code fragments. Apart from these smaller pre-trained models in academic circles, many pre-trained code models with much larger sizes have been proposed in the industry in recent years. Codex [2] is a large code pre-trained model proposed by OpenAI that supports the service of Copilot. In addition to Codex, the models recently released by OpenAI, such as ChatGPT [26] and GPT-4 [23], are also pre-trained on source code data\nand demonstrate impressive programming abilities. AlphaCode [25] is trained for generating code for programming competitions like Codeforces, using 715G data and 41B parameters. CodeGen [62] is a large pre-trained model for multi-turn program synthesis with more than 16B parameters, while CodeGeeX [63] is a recently proposed open-source multilingual code generation model with 13B parameters.\nLarge language models have revolutionized natural language processing (NLP) in recent years. Based on large pre-training data and model sizes, LLMs show impressive emergent abilities that have not been observed in small models [10]. Brown et al. [11] first show that GPT-3 has the ability to learn from a few examples in the context without parameter update. Liu et al. [20] first explore selecting the closest neighbors as the in-context examples. Recently, Levy et al. [65] propose to improve the diversity of in-context examples and achieve better performance on NLP compositional generalization tasks. Lu et al. [21] find that the order of in-context examples has a large impact on the performance and propose two methods LocalE and GlobalE based on the entropy. Recently, a series of work [15], [66] focus on the complex reasoning tasks and propose chain-of-thought prompt by guiding the model to output its reasoning path. In addition to NLP, there has been increasing interest in applying in-context learning to code intelligence tasks [17], [19], [22], [47], [67], [68]. For example, Xia et al. [17] evaluate the effectiveness of LLMs on program repair. Nashid et al. [47] propose to use the BM-25 to retrieve similar examples and construct the demonstrations for assert generation and program repair. However, these works mainly focus on the evaluation of LLMs on one or two tasks and do not discuss the construction of in-context demonstrations in-depth. In contrast, our work aims at conducting a systematic study of designing better demonstrations for ICL in code intelligence tasks.\nVII. CONCLUSION AND FUTURE WORK\nIn this paper, we experimentally investigate the impact of different demonstration selection methods, different demonstration ordering methods, and the number of demonstration examples on the performance of in-context learning for code intelligence tasks. Our research demonstrates that a carefully-designed demonstration for ICL outperforms simpler demonstrations a lot. We summarize our findings and provide suggestions to help researchers and developers construct better demonstrations for code intelligence tasks. In the future, we will explore more aspects of source code on the performance of in-context learning such as the quality of the code and the naturalness of the code. Additionally, we will also further verify our findings on other large language models. Our source code and full experimental results are available at https://github.com/shuzhenggao/ICL4code.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The increasing focus on code intelligence research aims to reduce the burden on software developers and enhance programming productivity. With the advent of large-scale open-source code corpora and deep learning techniques, various neural source code models have achieved state-of-the-art performance on multiple code intelligence tasks. However, the performance of in-context learning (ICL) heavily relies on the quality of the demonstrations, necessitating a systematic investigation into constructing effective demonstrations for code-related tasks.",
            "purpose of benchmark": "The benchmark is intended for comparing different models and evaluating the effectiveness of various demonstration construction methods in improving ICL performance for code intelligence tasks."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of how to construct effective demonstrations for code intelligence tasks, specifically focusing on the selection, order, and number of demonstration examples.",
            "key obstacle": "Existing benchmarks do not adequately address the variability in performance due to the quality of demonstrations, and there is a lack of systematic exploration in this area."
        },
        "idea": {
            "intuition": "The inspiration for this benchmark stems from the observation that the quality of demonstrations significantly affects the performance of ICL in code intelligence tasks.",
            "opinion": "The authors emphasize the importance of this benchmark in guiding researchers and developers to improve ICL performance through better demonstration design.",
            "innovation": "This benchmark differs from previous ones by systematically exploring the impact of demonstration selection, ordering, and the number of examples on ICL performance, providing empirical evidence and actionable insights.",
            "benchmark abbreviation": "ICL4Code"
        },
        "dataset": {
            "source": "The dataset was created from existing code intelligence tasks, including code summarization, bug fixing, and program synthesis, using publicly available code repositories.",
            "desc": "The benchmark includes datasets with various sizes and distributions tailored to each task, ensuring a comprehensive evaluation of ICL performance.",
            "content": "The dataset includes code snippets and their corresponding comments, bug fixes, and natural language descriptions, facilitating the evaluation of ICL across different tasks.",
            "size": "181,061",
            "domain": "Code Summarization",
            "task format": "Code \u2192 Text"
        },
        "metrics": {
            "metric name": "BLEU-4, EM",
            "aspect": "Accuracy",
            "principle": "The selected metrics evaluate the similarity between generated outputs and ground-truth outputs, providing a quantitative measure of model performance.",
            "procedure": "Model performance is evaluated by comparing the generated outputs against the ground-truth using the specified metrics, with statistical significance tests applied to assess the results."
        },
        "experiments": {
            "model": "The models tested include state-of-the-art code intelligence models such as Codex and GPT-3.5.",
            "procedure": "The experiments involved varying demonstration selection methods, ordering methods, and the number of examples, with a focus on empirical evaluation across three code intelligence tasks.",
            "result": "The results indicate substantial improvements in performance with well-constructed demonstrations, with specific metrics showing increases of 9.90%, 175.96%, and 50.81% across different tasks.",
            "variability": "Variability was accounted for by running multiple trials with different subsets of the dataset and employing statistical measures to evaluate stability."
        },
        "conclusion": "The key findings demonstrate that carefully-designed demonstrations significantly enhance ICL performance in code intelligence tasks, providing valuable insights for future research and development.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by providing a systematic approach to demonstration construction, leading to improved model performance and stability.",
            "limitation": "Potential limitations include the focus on specific tasks and programming languages, which may not generalize across all code intelligence applications.",
            "future work": "Future research should explore additional aspects of demonstration construction and validate findings across a broader range of tasks and programming languages."
        },
        "other info": {
            "source code": "https://github.com/shuzhenggao/ICL4code",
            "additional findings": {
                "finding1": "Diversity of examples is beneficial for demonstration selection.",
                "finding2": "BM-25 is an effective instance-level demonstration selection method.",
                "finding3": "Instance-level demonstrations outperform task-level demonstrations.",
                "finding4": "Ordering demonstrations based on similarity improves performance.",
                "finding5": "Four demonstration examples strike a good balance between performance and truncation issues."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark addresses the problem of how to construct effective demonstrations for code intelligence tasks, specifically focusing on the selection, order, and number of demonstration examples."
        },
        {
            "section number": "1.2",
            "key information": "The increasing focus on code intelligence research aims to reduce the burden on software developers and enhance programming productivity."
        },
        {
            "section number": "3.3",
            "key information": "The results indicate substantial improvements in performance with well-constructed demonstrations, with specific metrics showing increases of 9.90%, 175.96%, and 50.81% across different tasks."
        },
        {
            "section number": "4.1",
            "key information": "The authors emphasize the importance of this benchmark in guiding researchers and developers to improve ICL performance through better demonstration design."
        },
        {
            "section number": "6.1",
            "key information": "Potential limitations include the focus on specific tasks and programming languages, which may not generalize across all code intelligence applications."
        },
        {
            "section number": "6.4",
            "key information": "Future research should explore additional aspects of demonstration construction and validate findings across a broader range of tasks and programming languages."
        }
    ],
    "similarity_score": 0.7057553718839233,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/What Makes Good In-context Demonstrations for Code Intelligence Tasks with LLMs_.json"
}