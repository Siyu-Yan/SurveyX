{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.17447",
    "title": "A Large Language Model Approach to Educational Survey Feedback Analysis",
    "abstract": "This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve humanlevel performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs\u2019 chain-ofthought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.",
    "bib_name": "parker2024largelanguagemodelapproach",
    "md_text": "ARTICLE\n# A Large Language Model Approach to Educational Survey Feedback Analysis\nMichael J. Parker1  \u00b7 Caitlin Anderson1 \u00b7 Claire Stone2 \u00b7 YeaRim Oh2\nAccepted: 8 June 2024  \u00a9 The Author(s) 2024\n# Abstract\nThis paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve humanlevel performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs\u2019 chain-ofthought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.\nKeywords Large language model \u00b7 Survey analysis \u00b7 GPT-4 \u00b7 GPT-3.5 \u00b7 ChatGPT \u00b7  Qualitative methodology\nExtended author information available on the last page of the arti\n\n# Introduction\nSurveys in education have long been used for course evaluation and structured  evaluation of teaching (SET) (Diaz et al., 2022; Dommeyer et al., 2004). The history of education surveys has seen examination of the pros and cons, with some  noting the subjectivity of students\u2019 perspectives and others pointing to the difficulties in gathering responses unbiased by low response rates and the challenges  with analyzing qualitative survey responses in an objective way (McGourty et al.,  2002; Shah & Pabel, 2019; Spooren et al., 2013; Stowell et al., 2012; Wallace  et al., 2019; Wongsurawat, 2011). However, evidence has supported the validity of surveys, including qualitative and mixed methods approaches, for guiding changes to teaching practice and course design (Ferren & Aylesworth, 2001;  Johnson & Onwuegbuzie, 2004; Lattuca & Domagal-Goldman, 2007; Marks  et al., 2017; McKeachie, 1997; Mentkowski, 1991; Spooren et al., 2013). Common use cases for end-of-course evaluations (the type we focus on in this  paper) include improving teaching and learning outcomes, measuring course quality, evaluating teachers, and informing decision-making (Diaz et al., 2022; Flod\u00e9n,  2017; Marsh & Roche, 1993; Moss & Hendry, 2002; Schulz et al., 2014). Marsh  and Roche (1993) found that university teachers receiving feedback and evaluations  improved significantly more than a control group. Teachers can use feedback to target specific areas for improvement, with the potential to lead to structural course  changes (Flod\u00e9n, 2017; Schulz et al., 2014). Flod\u00e9n (2017) observed positive effects  of student feedback, tending to push teaching towards more interactive formats like  seminars and group work rather than lectures. Education administrators have used  feedback for quality assurance and to drive strategic decision-making (Ferren &  Aylesworth, 2001; Marginson & Considine, 2000; Mazzarol et al., 2003). The scope  is not limited to higher education. Student feedback surveys have played an increasing role in teacher development and evaluation in K-12 in recent years (Schulz et al.,  2014). A large study found student surveys to be reliable and predictive of a teacher\u2019s ability to improve student achievement (Kane et al., 2013). While quantitative feedback may have advantages for simplicity of analysis,  the qualitative feedback gathered through student comments has additional value.  In a higher education setting, Alhija and Fresko (2009) examined written student  comments from end-of-course evaluations. They found that qualitative comments  focused on the course, the instructor, and the context, capturing information not  found in quantitative ratings. Comments covered unique aspects and provided more  specific feedback on the strengths and weaknesses of a course. They concluded that  qualitative feedback can provide a more comprehensive view of a course\u2019s teaching. Shah and Pabel (2019) used qualitative student feedback comments to compare  the experiences of online and in-person students at their institution. Based on their  insights, they conclude that universities must analyze qualitative student comments,  not just quantitative ratings, to truly understand and enhance the student experience, especially given the growth in online education. The importance of qualitative  feedback has been examined across online, in-person, and blended or hybrid format  courses (Aldeman & Branoff, 2021; Alhija & Fresko, 2009; Onan, 2021a).\nDespite the utility of qualitative student feedback, significant challenges remain in putting its usage into practice (Richardson, 2005). Shah and Pabel (2019) note the success in use of quantitative data but point to limited prior progress in the analysis and practical use of qualitative feedback. Approaches for analysis have included traditional, manual approaches such as thematic analysis that rely on manual effort of annotating and coding survey responses (Braun & Clarke, 2006; Riger & Sigurvinsdottir, 2016). Manually coding and analyzing large volumes of open-ended survey responses or student feedback comments is extremely time-consuming and labor-intensive and may not always provide actionable suggestions for improvement (Shaik et al., 2023; Mattimoe et al., 2021; Nanda et al., 2021; Shah & Pabel, 2019). Maintaining consistent coding across large educational datasets can be challenging when done manually, especially if multiple researchers are involved (Shaik et al., 2023; Mattimoe et al., 2021). Employing crowdworkers, for example via Amazon\u2019s Mechanical Turk platform, reduces the cost and time of manual annotation of qualitative data but does not solve some of the other issues. The quality of results may vary, particularly in cases where some degree of domain expertise is needed (Gilardi et al., 2023; Rother et al., 2021). Qualitative annotation tasks can be inherently subjective, leading to disagreements among crowdworkers and low inter-annotator agreement (Pradhan et al., 2022; Rother et al., 2021). In addition, a recent study (Veselovsky et al., 2023) provided evidence that a substantial fraction of crowdworkers used generative AI (Large Language Models, or LLMs) to assist with a summarization task, leading to a mix of results from humans and LLMs and raising doubt that crowdworkers will continue to be a reliable source of human annotations. More recently, automated methods for analysis of qualitative data have relied on a variety of machine learning models, (Deepa et al., 2019; Onan, 2021b; Smith & Humphreys, 2006; Zhang et al., 2020). Machine learning approaches (discussed further below)\u2014including unsupervised semantic mapping, topic modeling, and using of different forms of neural networks\u2014have been applied to a range of tasks like clustering, summarization, entity extraction, and sentiment analysis (Gottipati et al., 2018; Hamzah et al., 2020; Nanda et al., 2021; Patil et al., 2019; Shaik et al., 2023). These approaches have shown promise in aiding analysis, but often require conditions that make their use less feasible to most educators, such as the need for significant technical resources, fine-tuning of models on volumes of pre-existing labeled data, use of separate models for the natural language processing tasks involved (impeding broader analyses), or the need for use of specialized software (Fan et al., 2015; Gottipati et al., 2018; Orescanin et al., 2023; Pyasi et al., 2018; Smith & Humphreys, 2006). These models have therefore generally been the domain of research, with a gap in widely accessible, practical methods for qualitative analysis of end-ofcourse surveys. Large language models with generative AI capabilities have become more widely available, capable, and accessible (easier-to-use) in the last one to two years, but are underexplored in terms of use in survey analysis versus more specialized machine learning models. LLMs have the potential to circumvent many of the problems associated with specialized machine learning approaches and potentially democratize access to high quality qualitative survey analysis. For example, the ability to\n1 3\nuse such models through natural language instructions, via simple web interfaces, or at scale through application programming interfaces (APIs), lessens the need for dedicated machines or expensive software. However, a thorough analysis of the feasibility and evaluation of the quality of LLMs\u2019 results has not been performed to establish reliability and rigor in common qualitative survey analysis tasks. Our main research question was: are large language models are at a stage where they can be effectively used across the broad range of tasks that are part of survey analysis? To answer this main question, we propose the following related research questions:\nuse such models through natural language instructions, via simple web interfaces,  or at scale through application programming interfaces (APIs), lessens the need for  dedicated machines or expensive software. However, a thorough analysis of the feasibility and evaluation of the quality of LLMs\u2019 results has not been performed to  establish reliability and rigor in common qualitative survey analysis tasks. Our main  research question was: are large language models are at a stage where they can be  effectively used across the broad range of tasks that are part of survey analysis? To  answer this main question, we propose the following related research questions: \u2022 Research question 1 (RQ1): Can LLMs be used to perform multiple unstructured text analysis tasks on educational survey responses, including multi-label  classification, multi-class classification, binary classification, extraction, inductive thematic analysis, and sentiment analysis? \u2022 Research question 2 (RQ2): Can LLMs\u2019 chain-of-thought (a demonstration of  the intermediate steps of how they arrive at their answers) be captured to provide  a degree of transparency that may help foster confidence in real world usage?  Can we demonstrate examples that show the potential for this use case? \u2022 Research question 3 (RQ3): Is a zero-shot approach (not needing to provide  hand-labeled examples) across all tasks, a scenario that mimics many real-world  practical use cases in the education setting, capable of achieving performance  comparable to human annotation?\n\u2022 Research question 1 (RQ1): Can LLMs be used to perform multiple unstructured text analysis tasks on educational survey responses, including multi-label  classification, multi-class classification, binary classification, extraction, inductive thematic analysis, and sentiment analysis? \u2022 Research question 2 (RQ2): Can LLMs\u2019 chain-of-thought (a demonstration of  the intermediate steps of how they arrive at their answers) be captured to provide  a degree of transparency that may help foster confidence in real world usage?  Can we demonstrate examples that show the potential for this use case? \u2022 Research question 3 (RQ3): Is a zero-shot approach (not needing to provide  hand-labeled examples) across all tasks, a scenario that mimics many real-world  practical use cases in the education setting, capable of achieving performance  comparable to human annotation?\nAs part of the evaluation process, we also developed a set of classification cat egories that can be applied to a variety of course types (online, hybrid, or in-person) and which are amenable to customization depending on specific requirements.\n# Background\n# Types of Tasks Associated with Analysis of Unstructured Survey Data\nAnalyzing survey textual responses to explore the high-level goals of educational stakeholders requires chaining together natural language processing (NLP) tasks in the form of workflows. Such workflows can be implemented with NLP tasks, including classification, extraction, and sentiment analysis, that form composable building blocks for similar workflows. Classification of comments may be single-label (binary or multi-class, the latter involving classifying into one of a set of tags) or multi-label (classification of each comment with one or more of a set of tags). The tags (also called labels, classes, or categories) are frequently custom-chosen, reflecting the goals of a particular analysis (Go\u0161tautait\u0117 & Sakalauskas, 2022). Often those doing the analysis have a specific objective or goal focus that they are investigating (e.g., suggestions for improvement), and text extraction is a useful technique for this purpose. Sentiment analysis can be used to lend nuance and insight to the quantitative ratings that are gathered through Likert scales or \u201cstar\u201d ratings (Gottipati et al., 2017; Nitin et al., 2015). A high-level breakdown of objectives and NLP tasks is shown in Table 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2912/2912052f-1ec0-4651-b875-87ea603e17f8.png\" style=\"width: 50%;\"></div>\nFor discussion on the background of automated means of qualitative survey analysis, it is helpful to divide methods into those developed prior to the broad availability of the most recent generative AI versus those that make use of the recent generative AI advances in the form of LLMs. It is a potentially useful simplification to think of the pre-generative AI approaches as \u201clanguage in, numbers out\u201d and the later generative AI approaches as \u201clanguage in, language out\u201d in terms of how one interacts with the models. These approaches are distinguished below.\n# rning Approaches and Challenges in Analyzing Edu\n# Previous Machine Learning Approaches and Challenges in Analyzing Education Feedback\nFor feature extraction from text, techniques like TF-IDF, and Word2Vec have been  applied for short text classification and sentiment analysis (Deepa et al., 2019;  Devlin et al., 2018; Onan, 2021a; Zhang et al., 2020). Topic modeling using latent  semantic analysis or latent Dirichlet allocation has been useful for discovering  themes and trends in collections of student feedback (Cunningham-Nelson et al.,  2019; Perez-Encinas & Rodriguez-Pomeda, 2018; Unankard & Nadee, 2020). For  evaluating text, sentiment analysis techniques like CNN and Bi-LSTM models  have been used to classify student evaluations (Sindhu et al., 2019; Sutoyo et al.,  2021). Overall, these techniques have shown utility for gaining insights from student  feedback. With the advent of recent machine learning (ML) techniques, great strides have  been made in dealing with unstructured text. BERT (Bidirectional Encoder Representations from Transformers, Devlin et al., 2018) and related models allow for  transformation of text passages into numerical formats (high dimensional dense vectors called embeddings) that are then amenable to classification via conventional ML  methods such as logistic regression. Good results have been achieved in certain contexts using such models (Meidinger & A\u00dfenmacher, 2021). Despite such advances,  challenges remain that present obstacles to routine use of such models in practice. Specialized ML models often require a \u201cfine-tuning\u201d process using labeled data  (data that human annotators have classified) to best adapt to a specific use case.  Depending on the amount of human labeling needed, this aspect may provide a stumbling block based on the time and effort involved. Although there are many examples  of labeled datasets (Papers With Code datasets, n.d.; Hugging Face datasets, n.d.;  Kastrati et al., 2020a, b), real-world use cases often rely on custom labels for which  there is no pre-existing labeled data for fine-tuning. Even supposing such fine-tuning  takes place, there are additional barriers to practical use of this technology. One such barrier is that multiple distinct AI models may be needed, depending on  the range of tasks. The model that is suitable for classification may not be the same  one that performs text extraction, and each model may need its own fine-tuning or  adaptation. Even for a core task like classification, there are a number of challenges. Difficulty of classification increases in situations where multiple labels may concurrently  be assigned to the same survey comment, often leading to a degree of inter-rater disagreement even among highly-skilled human annotators who have high familiarity \nwith the domain. Other challenges include data imbalance, multi-topic comments,  and domain-specific terminology (Edalati et al., 2022; Shaik et al., 2022). In classifying unstructured textual feedback, data imbalance exists when the  labels chosen are not attributable in equal proportions across a dataset; some labels  may be comparatively rare. If there are few examples of particular labels, this scarcity can create difficulties in training machine learning models that classify new  comments. If human labeling is being used as ground truth, rarity of certain labels  may require labeling a larger set of feedback to enable training an ML classifier.  Techniques addressing data imbalance include synthesizing new training examples  for the minority class through data augmentation or by oversampling the minority  class instances through techniques like SMOTE (Kennedy et al., 2024). Other methods involve modifying the learning algorithms to assign higher misclassification  costs to minority class examples, such that the model parameters are affected more  by rare class examples, and ensemble methods trained on resampled versions of the  data (Johnson & Khoshgoftaar, 2019; Shah & Ali, 2023). Another challenge is that of multi-topic comments. Depending on how feedback is collected and how open-ended the survey questions are, students may provide feedback that encompasses multiple topics (for example, \u201cI found the quizzes  incredibly difficult, but the teacher was great and I felt I got what I paid for. If I had  had more time to complete the course, this would have been even better.\u201d). Such  multi-topic comments present a challenge for ML techniques based on embeddings  (dense vector representations) derived from models such as BERT (or BERT related,  such as Sentence-BERT, Reimers & Gurevych, 2019), given that the embedding of  a comment is related to the comment\u2019s semantic meaning. A comment with multiple topics may have an embedding that doesn\u2019t adequately localize to the semantic \u201cneighborhood\u201d of any of the individual topics associated with that comment,  decreasing the performance of downstream classifiers. Use of context-specific, specialized terms in the text data, known as domain-specific language, can also decrease the performance of ML techniques. Deep learning  models like BERT that perform feature selection by creating embeddings have been  pre-trained on a large corpus of text, usually publicly accessible and mostly from the  internet. Depending on the pre-training, terms specific to a specialized domain such  as immunology or biomedical engineering may not have been seen during training,  or seen only in very limited quantities. In those cases, the pre-trained model cannot  adequately capture the semantics of such terms via its embeddings, again impacting the performance of downstream applications such as classification and clustering that may rely on those embeddings. For example, Lee et al. (2020) discuss how  the original BERT was pre-trained on general domain corpora like Wikipedia and  BookCorpus, which lack sufficient technical biomedical vocabulary and writing  styles. This leads to poor performance on biomedical NLP tasks. Gu et al. (2021)  analyze how continual pre-training of BERT on biomedical corpora like PubMed  abstracts and PMC full-text articles significantly improves performance on downstream biomedical tasks compared to the general BERT model. The key reasons  cited for why BERT models may struggle with domain-specific language are the  lack of domain-specific vocabulary, writing conventions, and linguistic patterns  in the original BERT pre-training corpus, which leads to poor representations for \n1 3\ntechnical terminology and jargon when applied to domain tasks without additional in-domain pre-training. In sentiment analysis, pre-trained sentiment analysis models may not adapt well to settings where it is important to take into account the context. For example, in analyzing comments from biomedical science courses that cover cancer as a topic, learners\u2019 comments may include the words \u2018cancer\u2019 or \u2018oncology\u2019 or \u2018tumor\u2019, simply as referring to parts of the curriculum. These comments may end up being classified as negative even by a state-of-the-art existing model, given that discussions of cancers and tumors in many training datasets (often from internet settings) may be in the context of negative emotions being expressed. Finally, a common challenge is that of lack of interpretability of results coming from specialized machine learning models (Hassija et al., 2024). Although there has been significant work on approaches like visualizing factors that contribute to a neural network-based model\u2019s predictions, complex models may still be viewed as \u201cblack boxes\u201d by downstream users in areas like education, with this perception potentially inhibiting usage.\n# LLM Background and Related Research\nEducation feedback analysis seeks to extract insights from open-ended written responses, such as student surveys or teacher evaluations, and automated techniques can be seen as a particular application of the broader field of natural language processing (Shaik et al., 2022). The introduction of transformer-based neural network architectures in 2017 led to an explosion of new AI models for NLP with increasing capabilities. BERT (mentioned above) was developed shortly thereafter (2018), with multiple related models (e.g., RoBERTa) being further developed over the last five years, with effectiveness at various NLP tasks that often exceeded those of pretransformer models. Such models have been applied to a wide range of tasks, both with fine-tuning and without. Large language models are neural networks based on transformer architectures, including not only those in the BERT lineage but also other models such as GPT2, GPT-3, T5, and many others, with tremendous scale in terms of the number of model parameters (billions and sometimes trillions) and the internet scale volume of text on which they are trained (billions or even trillions of tokens, with tokens being numerical representations of words or parts of words). BERT (the large variant) has approximately 345 million parameters and was trained on about 3.3 billion words; in comparison, GPT-3 has 175 billion parameters and was trained on approximately 500 billion tokens (approximately 375 billion words). Models like GPT-3.5 and GPT-4 are proprietary, and the number of parameters and the amount of training data are unknown, although there are estimates that GPT-4 uses approximately 1.8 trillion parameters and was trained on approximately 13 trillion tokens, with GPT3.5 somewhere in between GPT-3 and GPT-4 (Schreiner, 2023). It is important as well to distinguish between BERT, along with related models like RoBERTa and SentenceTransformers, and generative models like GPT-3.5 and GPT4. While all of these are transformer models (a type of neural network architecture),\nBERT is known as an \u201cencoder-only\u201d model, more suitable for feature extraction, clustering, and a variety of use cases making use of the resulting embeddings (numerical representations of language, discussed above). The GPT models and other recent  generative models are \u201cdecoder-only, auto-regressive\u201d models, with different strengths,  including the ability to generate complex text. These models are also trained on an  order of magnitude more text, which lends to their capabilities as well. These generative AI models have the capability to do tasks like summarization,  translation, and generation of high-quality text output. As their scale has grown, the  range of tasks of which they have shown to be capable has increased, along with a  level of performance that has surprised many. With the recent popularization and  wider spread availability of LLMs, in part due to ChatGPT, with its underlying  GPT-3.5 and GPT-4 models, as well as other LLMs like Claude (Anthropic), Command (Cohere), Bard/Gemini (Google), Llama (Meta), and a range of open-source  models, interest has grown in applying these to use cases like analysis of short text  comments such as are seen in Tweets (T\u00f6rnberg, 2023), customer feedback, and survey feedback (Jansen et al., 2023; Masala et al., 2021). Multiple recent studies have examined using ChatGPT for text annotation and  classification tasks, with mixed results based on variations in prompts, datasets,  parameters, and complexity of tasks. Reiss (2023) focused on sensitivity to the  prompts and parameters used in classification, in the context of performing classification on a German dataset. Pangakis et al. (2023) argues that researchers using  LLMs for annotation must validate against human annotation to show that LLMs are  effective for particular tasks and types of datasets, given that there is variation in the  quality of prompts, the complexity of the data, and the difficulty of the tasks. Other  studies (Gilardi et al., 2023; Huang et al., 2023) demonstrate the potential for ChatGPT to perform text annotation or provide natural language explanations at levels  approaching or matching those of humans. Despite explorations like those mentioned above, research to date has not focused  on the feasibility and quality of LLMs\u2019 results in performing a broad array of common qualitative education survey analysis tasks, leaving a gap that we focus on in  this study. For example, a review published in 2024 focusing on natural language  processing of students\u2019 feedback to instructors makes no mention of studies using  LLMs for this purpose (Sunar & Khalid, 2024). Prior work has primarily focused on  the use of encoder models like BERT for their clustering and feature extraction capabilities and have not explored the current generation of decoder-only auto-regressive  models like the GPT models mentioned above. Our contribution is to explore and  evaluate the capabilities of the most recent generation of LLMs for educational survey feedback analysis.\n# Methods\n# Survey Data Used for Evaluation\n2,500 survey responses (625 for each of four open-ended questions) were selected  at random from a larger set of survey responses (50,525 total responses as of April \n1 3\n# 1 3\n16, 2023, when data collection was performed, with 14,359 responses for Q1 (see below), 13,654 responses for Q2 (see below), 12,825 responses for Q3 (see below), and 9,687 responses for Q4 (see below)) received as end-of-course feedback on a range of online biomedical science courses. The courses included 365 course iterations total across 15 different courses (e.g. Pharmacology, Genetics, Immunology, etc.). An additional 2,000 survey comments were chosen as a development set that could be used for LLM prompt tuning. The courses all use a single, uniform endof-course survey. In addition to quantitative ratings (e.g., net promoter scores) and optional demographic data, the survey included open-ended text responses to four questions/directives:\n\u2013 \u201cPlease describe the best parts of this course.\u201d [Q1] \u2013 \u201cWhat parts of the experience enhanced your learning of the concepts most?\u201d [Q2] \u2013 \u201cWhat can we do to improve this course?\u201d [Q3] \u2013 \u201cPlease provide any further suggestions, comments, or ideas you have for thi series.\u201d [Q4]\n# \u2013 \u201cWhat parts of the experience enhanced your learning of the concepts most?\u201d [Q2]\nOn average, learners answered approximately two of the four questions. The shortest responses containing content were one word, and the longest responses were several paragraphs. Example survey responses are shown in Table 2. Survey responses were collected via Qualtrics, with minor processing with Pandas 2.0.1 for elimination of leading and trailing white space and automated removal of responses with no content (NA or None variants). Survey responses were inspected manually and via named entity recognition (NER), running locally, to ensure that no private or sensitive information was transmitted to publicly available LLMs.\n# Development of Course Tagging System\nWe spent considerable time developing and testing a set of labels that would work well not only for online courses like those that the survey responses in this paper were a part of, but also other types of educational offerings. The motivation for the choice of labels was based on the functional areas of the team creating and delivering the courses. This team had separate functions (sub-teams) for curriculum/ teaching, logistics and operations (e.g. enrollment and technical aspects of course delivery), creative media/visual asset development (art and video creation), technology (learning management system and platform development and maintenance), administration/leadership (analysis of feedback for improvement and future feature/ course requests), and support (solving learner issues and handling inquiries). While most course teams at universities or online learning providers may not have all these functions separately, they are generally present in some fashion even if a single individual (for example, an instructor) covers multiple functions. Tags were therefore designed to cover functional areas and enable identification of feedback of interest for each team\u2019s quality improvement and planning processes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d25/4d25934f-4323-4181-8ab2-11689b6ae455.png\" style=\"width: 50%;\"></div>\nThe label development process started with a much larger set of labels (71 total). Given that each survey response could cover multiple topics, the task was to assign as many labels to each survey response as were applicable (a multi-label classification task). The four authors (all of whom have been involved in either course development or delivery for multiple years and can be considered domain experts in the course resources and processes) each labeled a test set of 2,000 survey responses (from the same educational program overall, but distinct from the set of 2,500 comments ultimately labeled), with resulting relatively low inter-rater agreement. Based on this experiment, tag categories were combined to arrive at a much smaller set of generalizable tags (see Table 3). These were still applied in a multi-label classification approach, with each survey response potentially receiving multiple labels. For example, the example comment mentioned earlier (\u201cI found the quizzes incredibly difficult, but the teacher was great and I felt I got what I paid for. If I had had more time to complete the course, this would have been even better.\u201d) might simultaneously receive labels of \u201cassessment\u201d, \u201cteaching\u201d, and \u201ccourse logistics and fit\u201d. In addition, best practices were followed to ensure generalizability (University of Wisconsin\u2014Madison, n.d.; UC Berkeley Center for Teaching & Learning, n.d.; Brennan & Williams, 2004; Medina et al., 2019). A one to three sentence description of each tag was created to provide guidance so that tags could be applied appropriately in testing rounds. The intent is also that others can adapt these same tags by modifying the description portion for their own\n<div style=\"text-align: center;\">Table 3  Final tags and descriptions</div>\nFinal tags and descriptions\nTag\nDescription\nCourse logistics and fit\ncourse delivery (policy, support), cost, difficulty, time commitment, \ngrading, credit, schedule, user fit, access, background (e.g., prereqs and \nappropriateness of course level)\nCurriculum\ncourse content, curriculum, specific topics, course structure. This focuses \non the content and the pedagogical structure of the content, including \nflow and organization. This also includes applied material such as clinical \ncases and case studies. Includes references to pre-recorded discussions \nbetween experts or between a doctor and a patient. Includes specific sug-\ngestions for additional courses or content\nTeaching modality\nvideo, visual, interactive, animation, step-by-step, deep dive, background \nbuilder (the format rather than the content/topic)\nTeaching\ninstructors, quality of teaching and explanations\nAssessment\nquizzes, exams\nResources\nnote taking tools, study guides, notepads, readings. Includes other potential \nstatic resources like downloadable video transcripts\nPeer and teacher interaction\nincludes chances for the student to interact with another person in the \ncourse (teacher or student). This includes discussion forums, teacher-\nstudent or student\u2013student interactions. Includes requests for live sessions \nwith teachers or live office hours\nOther\ncatch-all for the rarer aspects that we\u2019ll encounter and also the \u2018na\u2019, \u2018thank \nyou\u2019, etc. comments that don\u2019t really belong in the above bins. Also for \nsufficiently general comments like \u2018all the course was terrific\u2019 that can\u2019t \nbe narrowed down to one of the other categories\npurposes. The same descriptions that served as context for the human annotators were also used in the prompts for the LLMs in the multi-label classification task as a form of deductive thematic analysis. We then iteratively tested the new, much smaller set of tags on several sets of 100 survey responses, with all four authors independently tagging the same entries, followed by examination of inter-rater agreement. This yielded good results. With this set of tags, we then independently labeled 2,500 survey responses, and evaluated inter-rater agreement using Jaccard similarity coefficient between pairs of raters and averaged across all pairs of raters. Jaccard similarity was chosen because it is one of the most common metrics for multi-label classification. Other common multi-label classification metrics (e.g., Hamming distance) were also calculated and are provided in the appendix (see discussion of metrics below).\n# LLM Processing\nAll LLM tasks were performed via calls to the OpenAI API endpoints. GPT-3.5 (model: gpt-3.5-turbo-0301) and GPT-4 (model: gpt-4\u20130314) were used for the multi-label classification task; all other tasks described used GPT-3.5 (model: gpt3.5-turbo-0613) and GPT-4 (model: gpt-4\u20130613). All tests were run with calls to the models\u2019 asynchronous endpoints and used a temperature of 0 with other parameters set to their default values, other than the functions parameter and the function_call parameter, which were set to specify the applicable function schema and the function name where applicable. \u201cFunction calling\u201d, a capability specific to these models, was used to generate the JSON structured output for all tasks. Prompts used (see Electronic Supplement) involved function schemas, as well as the system and user messages to the model. For the LLM approach to the multi-label classification task, the multi-class classification task for extracted excerpts, the binary classification task, and the sentiment analysis task, zero-shot chain-of-thought (CoT) prompting was used (where a model is prompted to reason step-by-step but without examples of such reasoning provided) (Kojima et al., 2022; Wei et al., 2022). In addition to use of CoT enhancing the accuracy of the model output, the reasoning was included in the output to allow for error analysis and prompt tuning, as well as to allow inspection of the model\u2019s reasoning, something potentially helpful for those using the results in practice. For sentiment analysis, we had the LLM output a sentiment classification based on the possible categories \u2018negative\u2019, \u2018slightly negative\u2019, \u2018neutral\u2019, \u2018slightly positive\u2019, and \u2018positive\u2019, along with its reasoning. For the LLM approach to inductive thematic analysis of survey responses, a twostep approach was used. The first step involved prompting the LLM to derive themes representing feedback from multiple students and summarize the themes. This step was run in parallel on batches of survey responses that would fit within the model\u2019s context window. The second step involved prompting the LLM to coalesce derived themes based on similarity to arrive at a final set of themes and descriptions. These steps could be considered analogous to part of the human inductive thematic analysis qualitative analysis workflow (Braun & Clarke, 2006).\n1 3\n1) Zero-shot CoT\u2014This technique involves asking the model to think step-by-step to arrive at a correct result and to provide its detailed reasoning. In the absence of providing examples of CoT reasoning in the prompt, this type of prompting is categorized as zero-shot. 2) Prompt tuning via inspection of CoT reasoning\u2014In testing, error analysis was supplemented with inspection of CoT reasoning to help discern where prompts might need refinement. As prompts were updated, we observed corresponding changes in the output and the stated reasoning, with improvement in the development set metrics. 3) Additional descriptive context for labels\u2014Given that there was no fine-tuning  to allow the model to learn the appropriate context and meaning of labels, we added context to prompts in the form of definitions for each label and the types of elements for which each label applied. 4) Additional context through injection of the survey questions into the prompt\u2014 Inclusion of additional context, such as the survey question that a given comment  is in reply to, may improve the performance of LLMs and was used in this study. 5) Use of function calling for reliable structured output\u2014This technique is specific to the GPT-3.5 and GPT-4 models, for which models from the June 2023 checkpoint (0613) onward have been fine-tuned to enable structured output (e.g., JSON) when provided with information about a function schema that could be called with the output. 6) Memetic proxy, also known as the persona pattern (Reynolds & McDonell, 2021; White et al., 2023)\u2014Asking the LLM to act as a certain persona, for example as an expert in survey analysis tasks, has been described as another way to improve results, potentially by helping the model access a portion of its memory that holds higher quality examples of the task at hand.\n1) Zero-shot CoT\u2014This technique involves asking the model to think step-by-step to arrive at a correct result and to provide its detailed reasoning. In the absence of providing examples of CoT reasoning in the prompt, this type of prompting is categorized as zero-shot. 2) Prompt tuning via inspection of CoT reasoning\u2014In testing, error analysis was supplemented with inspection of CoT reasoning to help discern where prompts might need refinement. As prompts were updated, we observed corresponding changes in the output and the stated reasoning, with improvement in the development set metrics. 3) Additional descriptive context for labels\u2014Given that there was no fine-tuning to allow the model to learn the appropriate context and meaning of labels, we added context to prompts in the form of definitions for each label and the types of elements for which each label applied. 4) Additional context through injection of the survey questions into the prompt\u2014 Inclusion of additional context, such as the survey question that a given comment is in reply to, may improve the performance of LLMs and was used in this study. 5) Use of function calling for reliable structured output\u2014This technique is specific to the GPT-3.5 and GPT-4 models, for which models from the June 2023 checkpoint (0613) onward have been fine-tuned to enable structured output (e.g., JSON) when provided with information about a function schema that could be called with the output. 6) Memetic proxy, also known as the persona pattern (Reynolds & McDonell, 2021; White et al., 2023)\u2014Asking the LLM to act as a certain persona, for example as an expert in survey analysis tasks, has been described as another way to improve results, potentially by helping the model access a portion of its memory that holds higher quality examples of the task at hand.\n# Other Models\nIn addition to comparison to human ground truth labels, for multi-label classification, comparison was made to SetFit (Tunstall et al., 2022), a SentenceTransformers finetuning approach based on Sentence-BERT and requiring very little labeled data; for sentiment analysis, comparison was made to a publicly available RoBERTabased model trained on 124 M Tweets. Both are encoder-only models (distinct from generative AI models like GPT-4 and GPT-3.5), with the embeddings used in finetuning to enable classification. These comparisons provide some context for the LLMs\u2019 performance relative to recent specialized models.\n# Evaluation Metrics\nScikit-learn 1.2.0 was used for statistical tests, along with numpy 1.23.5 and Pandas 2.0.1 for data analysis. Weights & Biases was used for tracking of model evaluation results.\nFor the multi-label classification task, model results were compared to the  human ground truth labels. Two ways were used to arrive at ground truth labels  aggregating results from multiple annotators: 1) using consensus rows: only the  subset of survey responses (dataset rows) where all four annotators had majority  agreement on all selected tags were kept; and 2) using consensus labels: all survey responses were kept but only labels with majority agreement were chosen as  selected. To fine-tune the SetFit model, we used a portion of each ground truth dataset  (the first 20 examples for each label). Those examples were omitted from the test  set, leaving 2,359 rows in the consensus labels test set and 1,489 rows in the consensus rows test set. For each of the above scenarios, model results for multi-label classification  were evaluated against aggregated human annotator results via the following metrics: 1) Jaccard similarity coefficient, comparing the model against aggregated  human results for each row (survey response) and then averaged over all rows; 2)  average precision per tag; 3) average recall per tag; 4) macro average precision,  recall, and F1 score across all tags; 5) micro average precision, recall, and F1  score across all tags; 6) Hamming loss; and 7) subset accuracy. For the binary classification task, accuracy, precision, recall, and F1 score  were calculated, comparing the model results to one expert human annotator. For the extraction task, extracted excerpts were evaluated by GPT-4 using a  rubric created specifically for this task, examining performance on multiple  aspects of performance, including the presence of excerpts that were not exact  quotes from the original (part of the original extraction instructions), the completeness of capturing relevant excerpts, the presence of excerpts irrelevant to the  initial goal focus, the inclusion of relevant context from the original comment,  and several others. The results were also evaluated by human annotation to determine the presence of hallucinations (excerpts that were substantial changes from  the original survey responses, rather than just changes in punctuation, spelling, or  capitalization), with the percent of the total number of excerpts representing hallucinations being reported. For the inductive thematic analysis task, there is not an accepted evaluation  method given that this is a complex, compound task, and evaluation consisted of  inspecting the derived themes and descriptions as well as inspecting the results of  the associated multi-label classification step. The sentiment analysis results of GPT-3.5 and GPT-4 were compared to those  of a RoBERTa sentiment classifier trained on ~ 124 million tweets (Hugging Face  cardiffnlp/twitter-roberta-base-sentiment-latest, 2022; Loureiro et al., 2022), as well  as to results from a human annotator, with accuracy, precision, recall, and F1 scores  reported for the prediction of sentiment as negative, neutral, or positive. Comparison  was made by grouping \u2018negative\u2019 and \u2018slightly negative\u2019 into a single class, keeping \u2018neutral\u2019 as its own class, and grouping \u2018positive\u2019 and \u2018slightly positive\u2019 into a  single class to allow for comparison across sentiment analysis methods. The RoBERTa classifier produced a dictionary with negative, neutral, and positive classes,  with probabilities summing to 1.0. The class with the maximum probability score  was chosen as the label for comparison to the human annotations.\n1 3\n# Results\nIn this section, we organize the results in relation to the three research questions. The first research question pertains to whether LLMs can perform multiple text analysis tasks on unstructured survey responses, including multi-label classification, multiclass classification, binary classification, extraction, inductive thematic analysis, and sentiment analysis. The section below on the approach to LLM workflows answers this research question. The second research question explores whether LLMs\u2019 chainof-thought (a demonstration of the intermediate steps of how they arrive at their answers) can be captured to provide a degree of transparency that may help foster confidence in real world usage. In the section below on chain-of-thought reasoning, we demonstrate examples that show the potential for this use case. The final research question asks whether a zero-shot approach (not needing to provide hand-labeled examples) across all tasks can achieve performance comparable to human annotation. The section below on individual NLP task evaluations answers this research question. For the examples, we use GPT-4 as the LLM; the evaluations compare GPT-4 and GPT-3.5 as well as the other models used.\n# Approach to LLM Workflows\nThe main types of workflows demonstrated support the goals shown in Table 1 of 1) high-level analysis, in which the desire is to understand the main categories and areas of emphasis across all student feedback, or 2) more focused analysis, e.g., answering specific questions about a particular aspect of a course. In both cases, quantification of results is a consideration, which is supported by classification tasks. For initial, high-level analysis across the entire set of survey comments, we demonstrate two approaches: 1) inductive thematic analysis, a \u201cbottom-up\u201d approach supporting the use case where no predetermined labels (areas of interest) have been defined, similar to topic modeling, and 2) multi-label classification using predefined labels, a \u201ctop-down\u201d approach, also referred to as deductive thematic analysis. When categories of interest are known in advance, multi-label classification is an appropriate first step, binning survey responses into relevant categories that provide a sense of the type of feedback learners are providing. These categories also provide groupings of comments for further focused analysis (e.g., via extraction), as well as allow for quantification based on the number of comments labeled with each category. For focused analysis, in which there is a specific question or goal for the analysis, not necessarily known in advance, we demonstrate extraction as a key step, followed by either a classification step or thematic analysis. To provide output for further downstream analysis and quantification, multi-class classification can be used as a step, as demonstrated here with the generalizable set of labels used in this study, or with an adapted or fully customized version for one\u2019s own use case. This step is shown used after extraction, given that short excerpts are more likely to be adequately classified with a single label versus multi-sentence comments. The output of\nother forms of classification (binary or multi-label) also lends itself well to quantification of results. Sentiment analysis was applied as a final step for workflows where finding positive or negative excerpts was of interest, as demonstrated in the example related to the level of difficulty of the course. Although the full model responses were in JSON format, only the relevant output text is shown for brevity and clarity.\n# Workflow Examples\n# Example 1\u2014High\u2011Level Analysis by Inductive Thematic Analysis (\u201cBottom\u2011Up\u201d  Approach)\n# Example 1\u2014High\u2011Level Analysis by Inductive Thematic Analysis (\u201cBottom\u2011Up\u201d \nA workflow for finding and summarizing the main themes (ideas expressed by multiple students) of survey responses is shown in Fig. 1, and consists of three LLM steps: 1) themes are first derived and summarized for batches of comments, each of which is sized to fit within the context window of the model used; 2) comments are classified using the derived themes; and 3) sets of themes from these batches are coalesced to arrive at a final set of themes. Additionally, label counts are aggregated from the themes that were combined. In qualitative research, steps 1 and 3 are called inductive thematic analysis; this is similar to topic modeling, in that themes are inductively derived from comments. In general, depending on the input size (context window) for the model used (8 K tokens in this example) and the number of comments being analyzed, dividing into batches and coalescing the themes from each batch may be unnecessary. Results for running this process on the 625 comments from Q1 (\u2018Please describe the best parts of this course\u2019) are shown in Fig. 1. The number of comments that the LLM identified as corresponding to each theme is shown, along with the theme titles and descriptions.\n# Example 2\u2014High\u2011Level Analysis by Categorizing Student Comments (\u201c Approach)\n# Level Analysis by Categorizing Student Comments (\u201c\nMulti-label classification of survey responses, using the set of predetermined labels developed for this study (Table 3) was run on the 625 comments from Q1 (\u2018Please describe the best parts of this course\u2019) and results are shown in Fig. 2. The categorized comments can be used for analysis (for example, comparing the categorization of responses to \u2018Please describe the best parts of this course\u2019 to the categorization of responses to \u2018What can we do to improve this course?\u2019) or as a starting point for further downstream tasks.\n# Example 3\u2014Finding Suggestions for Improvement\nA workflow for finding and quantifying suggestions for course improvement is shown in Fig. 3, and consists of extraction of relevant excerpts, followed by multi-clas\n1 3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6de1/6de11bd8-abcc-4791-aa49-802b41c77237.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5524/5524c2a4-d689-40eb-8a16-b8890fc9122f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0768/076818d4-9482-4a23-a078-e0c2c0fac475.png\" style=\"width: 50%;\"></div>\nclassification, based on the labels in Table 3, to facilitate quantification as well as routing of comments to the appropriate stakeholders. Excerpts resulting from the extraction step were assumed to be focused enough that they could each be categorized with a single class from among the pre-existing labels in Table 3. Results for several representative real comments from the larger set of survey comments are shown in Fig. 3. The model\u2019s CoT reasoning for each step is shown elsewhere, but is omitted here for clarity.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b52/9b5209fd-0704-444f-943f-1599efc5acb1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2  Multi-label classification of student comments (results shown using GPT-4)</div>\n# Example 4\u2014What Other Content or Topics Were Students Interested in Seeing  Covered?\n# mple 4\u2014What Other Content or Topics Were Students Interested in Seeing \nA common goal in analyzing student feedback is to better understand the gaps in course content, in order to decide whether to develop additional material or even new courses. To see if this type of information could reliably be derived from survey responses, we focused on responses to relevant survey questions (Q3 and Q4) for immunology courses with the workflow shown in Fig. 4. Results for several representative real comments are shown. First, just the portions containing new content or topic area suggestions are extracted from the survey responses. Content suggestion themes are then derived and summarized from the excerpts; this is done in batches if they cannot be fit within a single prompt to the LLM (i.e., if there are too many excerpts to fit in the model\u2019s maximum context size). Multi-class classification is performed on the excerpts with the themes from each batch. If thematic analysis is done in batches, sets of themes from these batches are then coalesced to arrive at a final set of content themes. The results suggest that GPT-4 is capable of finding content suggestions despite many being specific to the biomedical domain. This may be due to the volume and diversity of the model\u2019s pre-training data (although this training mixture has not been disclosed). Immunology is used as an example, but the workflow is not specific to the type of course.\n# ple 5\u2014What Feedback Did Students Give About The Teaching\n# Example 5\u2014What Feedback Did Students Give About The Teaching  and Explanations?\nFeedback about teachers and the quality of teaching and explanations in a course is a frequent objective of academic course surveys. Here, we show a workflow where multi-label classification has already been run as an initial step in high-level analysis, and we use the results of that classification as our initial filter to focus on the identified subset of comments related to teaching (corresponding directly to one of\n1 3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1e3/d1e34489-4b71-45c3-89ed-86ea7a10279c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3  Finding suggestions for improvement from student comments (results shown using GPT-4)</div>\nthe pre-existing labels), with extraction used to further narrow the output of analysis. The workflow, shown in Fig. 5, consisted of multi-label classification, using the pre-existing labels developed (Table 3) followed by extraction of relevant excerpts from the comments that were classified into the \u2018teaching\u2019 category (9% of total comments). If multi-label classification hadn\u2019t previously been run, extraction could have been performed on the broader group of comments as the initial step. For our dataset, which includes numerous multi-topic comments, the extraction step was used to further filter the information to only content related to the goal. Results for several representative real comments (de-identified in pre-processing) from the larger set of survey comments are shown in Fig. 5, including one where the model improperly filtered out the comment despite it containing a reference to the quality\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca3b/ca3ba45f-a996-42c1-bfb5-15b4878400e3.png\" style=\"width: 50%;\"></div>\nof explanations. An error such as the one shown could be considered somewhat subtle and highlights the need with zero-shot prompting of LLMs for clear specification  of the goal of the extraction.\n# Chain\u2011of\u2011Thought Reasoning\nThe prompts for binary classification, multi-label classification, multi-class classification, sentiment analysis, and evaluation of extraction results all used zeroshot chain-of-thought (CoT) to enhance the quality of the results while maintaining the zero-shot conditions of this study. The CoT reasoning was included in the\n1 3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/efd7/efd7750a-4025-44be-b8d9-1d86d5d5f8d4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5  Feedback about teaching and explanations (results shown using GPT-4). The red \u2018x\u2019 indicates an error by the model</div>\nstructured output, allowing for inspection. Only the reasoning from GPT-4 was consistently reliable, and examples are shown here. Example results for binary and multi-class classification tasks are shown in Fig. 6 and Fig. 7, and reasoning for sentiment analysis is also shown in Fig. 7. The reasoning, inspected manually over several hundred comments, is consistent with the classification results and appears to provide logical justification that is grounded in the contextual information (e.g., labels and descriptions) included as part of the prompts (see Electronic Supplement). This suggests that the CoT reasoning from GPT-4 meets a threshold of consistency and logic that allows for potential downstream use cases such as prompt tuning and insight into reasoning for end-users. Potential benefits and caveats of such uses are explored in the Discussion. Evaluation of the extraction task used a custom LLM evaluation (see Electronic Supplement), developed for this study. In order to refine the evaluation to align results with human preferences, we inspected the CoT reasoning along with the structured eval results for the separate development set of survey responses and made modifications to the evaluation prompts in an iterative fashion. An example of the CoT output for GPT-4 is shown in Fig. 8. As prompts were altered based on human review, the eval results changed in a consistent fashion, suggesting that GPT-4 provided CoT reasoning may be useful in refining LLM evaluations.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa20/aa20248c-bd58-4187-9c06-b92cd8bd807d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6  Examples of GPT-4 CoT reasoning for binary classification and multi-class classification related to the task of finding suggestions for improvement</div>\nFig. 6  Examples of GPT-4 CoT reasoning for binary classification and multi-class classification related to the task of finding suggestions for improvement\n# Individual NLP Task Evaluations\nTo better assess the reliability of workflows such as those shown in the examples and to answer Research Question 3, we evaluated the individual tasks, including multi-label classification, binary classification, extraction, and sentiment analysis.\n# Multi\u2011Label Classification Metrics\nThe difficulty of multi-label classification tasks varies widely (Meidinger & A\u00dfenmacher, 2021), depending on the content to which the labels are being applied, the  design of the labels (for example, the clarity of their specification and the potential  for overlap), and the number of labels. To put the LLM results in context, we show  the inter-rater agreement for application of the eight-label set (Table 3) to our dataset and also compare the LLM results to SetFit, another classification technique.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a09a/a09adb84-cdec-48c5-8a84-80c1347d7e74.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7  Examples of GPT-4 CoT reasoning for multi-label classification, binary classification, and sentiment analysis related to the task of finding how students felt about the level of difficulty of the course</div>\nInter\u2011rater Agreement 1,413 (57%) of 2,500 rows had all 4 human raters in agreement across all selected labels and 1,572 (63%) had majority (3 of 4) agreement on all selected labels. The average Jaccard similarity coefficient including all 2,500 rows (averaged across the six unique pairings of the four human raters for all rows) was 81.24% (Table 4), suggesting that this was a challenging task even for expert human annotators who developed the custom label set in close collaboration. GPT-4 agreement with human annotators is shown; the average across all pairings including GPT-4 was 80.60%.\nLLM and SetFit Evaluation In addition to evaluating the GPT models, we also per formed multi-label classification using SetFit (Tables 5 and 6).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/773e/773ed9ee-bca9-4c39-8503-977ce6935dae.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8  Example of GPT-4 CoT reasoning for extraction evaluation</div>\n<div style=\"text-align: center;\">Table 4  Inter-rater Jaccard similarity coefficients, including human annotators and GPT-4 as anothe rater/annotator (human pairs average = 81.24%; all pairs average = 80.60%)</div>\nrater/annotator (human pairs average = 81.24%; all pairs average = 80.60%)\nAnnotator1\nAnnotator2\nGPT-4\nAnnotator3\nAnnotator4\nAnnotator1\n-\n81.27\n80.18\n83.37\n82.35\nAnnotator2\n81.27\n-\n79.40\n80.84\n78.42\nGPT-4\n80.18\n79.40\n-\n80.74\n78.22\nAnnotator3\n83.37\n80.84\n80.74\n-\n81.18\nAnnotator4\n82.35\n78.42\n78.22\n81.18\n-\nrows for SetFit)\nMacro average\nMicro average\nJaccard\nAverage precision\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nGPT-4\n92.97\n93.91\n89.88\n90.59\n89.78\n93.66\n93.26\n93.46\nGPT-3.5\n72.61\n74.79\n69.34\n82.18\n72.63\n72.36\n84.48\n77.96\nSetFit\n73.86\n78.01\n84.37\n57.59\n66.85\n91.92\n71.43\n80.39\nFor the consensus rows evaluation, the zero-shot results for GPT-4 are similar to  what might be expected of fine-tuned classifiers (Meidinger & A\u00dfenmacher, 2021).  The other models have strengths and weaknesses, with SetFit having relatively high  precision and lower recall, and GPT-3.5 following the converse pattern. The overall  results for SetFit and GPT-3.5, focusing on Jaccard coefficient and F1 scores, are  similar. The results emphasize 1) the fact that fine-tuning is desirable when feasible, \n1 3\nTable 6  Evaluation on all rows using consensus labels (2500 rows for LLMs, 2359 rows for SetFit)\nMacro average\nMicro average\nJaccard\nAverage precision\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nGPT-4\n80.17\n81.53\n73.91\n88.38\n79.69\n78.32\n89.70\n83.63\nGPT-3.5\n63.00\n65.18\n60.42\n79.79\n65.75\n60.31\n83.45\n70.02\nSetFit\n62.72\n67.52\n73.22\n53.08\n59.61\n79.40\n65.14\n71.57\napproaching the performance of powerful LLMs like GPT-3.5 even with a few-shot  fine-tuning approach; and 2) the quality of the zero-shot performance of GPT-4.\n# Binary Classification Metrics\n1,250 comments (randomly taken from the original selected 2,500 comments, evenly distributed across Q1 through Q4) were classified based on the question \u201cdoes this comment contain suggestions for improvement?\u201d, with the model assigning a \u2018yes\u2019 or \u2018no\u2019 to each comment, and results were compared against one expert human annotator\u2014as the gold standard\u2014assigning \u2018yes\u2019 or \u2018no\u2019 to each comment. Binary classification could be considered the simplest of the evaluated NLP tasks, and both LLM models exhibited good performance (Table 7).\n# Extraction Evaluation\nUsing \u2018suggestions for improvement\u2019 as an example target of extraction, comments were first classified via GPT-4 as containing the target or not (see binary classification task above). Of the 1,250 comments, 716 were labeled by binary classification as containing suggestions for improvement. These comments were then run through extraction to find the individual excerpts. The quality of the extraction was then scored by the following method, employing the concept of LLM-as-a-judge (Huang et al., 2024) where custom LLM-based evaluations are used for cases where standardized evaluation methods don\u2019t exist or are inadequate. For each comment\u2019s excerpt(s), GPT-4 was used to apply a custom evaluation rubric with nine questions. Only GPT-4 was capable of applying the evaluation reliably and was therefore used. An example of one of the rubric questions was \u201cDid the program extract any irrelevant excerpts? (yes or no)\u201d (see the extraction evaluation prompt in the Electronic Supplement for all questions in the rubric). That question is abbreviated as the \u201cIrrelevant Excerpts\u201d category in the table below, showing the percentage of \u201cyes\u201d answers. Given that each of the rubric\nModel\nAccuracy\nPrecision\nRecall\nF1\nGPT-4\n95.20\n96.20\n95.39\n95.79\nGPT-3.5\n90.16\n89.01\n93.35\n91.14\nTable 7  Binary classification task performance\nTable 8  Error rate (%) of \nextraction for \u2018suggestions for \nimprovement\u2019 from comments \nclassified as containing \n\u2018suggestions for improvement\u2019 \n(all metrics from rubric \nand human annotation for \nhallucinations)\nMetric\nGPT-4\nGPT-3.5\nMissed Excerpts\n2.37\n7.82\nAmbiguous Excerpts\n4.61\n4.75\nMissed Existing Context\n0.28\n0.84\nIrrelevant Excerpts\n0.14\n0.84\nIrrelevant Context\n0.00\n0.14\nImplied Goal Focus\n3.07\n2.79\nNon-Quotes\n0.00\n6.01\nNon-Contiguous Excerpts\n0.00\n0.14\nRedundant Excerpts\n0.28\n2.79\nHallucinations\n0.00\n3.91\n<div style=\"text-align: center;\">9  Classification of comments as negative, positive, or neutral rela</div>\nClassification of comments as negative, positive, or neutral relative to human annotator\nModel\nAccuracy\nPrecision (macro)\nRecall (macro)\nF1 (macro)\nGPT-4\n80.86\n82.65\n80.28\n80.78\nGPT-3.5\n65.17\n73.68\n66.44\n64.88\ntwitter-roberta-base-\nsentiment-latest\n66.69\n71.38\n64.86\n61.10\nquestions had a yes/no answer, with a \u201cyes\u201d answer indicating a failure of the aspect of the model\u2019s extraction based on that question, the error rate or percentage of failures for each aspect could be determined. The extracted excerpts were also examined by a human annotator to determine the percentage of the 716 rows that contained hallucinations in the excerpts, as defined by substantial edits or complete fabrication of additional language not present in the original comment. That percentage is shown in the last row of Table 8, with the other rows\u2019 results representing the scores (error rates) determined by GPT-4 judging the extraction results from GPT-4 and GPT-3.5. The GPT-4 model included some ambiguous excerpts; however, those were most commonly due to lack of context in the comment itself, rather than the model failing to extract that context. GPT-4 followed directions very closely, and its results did not contain hallucinations. In contrast, the output of GPT-3.5 contained hallucinations at a rate of about 4% and edits to comments at a rate of about 6%. GPT-3.5 also missed relevant excerpts significantly more frequently than GPT-4. Additional prompt tuning may reduce the rate of these errors; nonetheless, the results suggest that a degree of caution should be applied in using GPT-3.5 for extraction.\n# Sentiment Analysis Metrics\nUsing GPT-4 and GPT-3.5, comments related to course suggestions and improvement (Q3 and Q4) were classified as \u2018negative\u2019, \u2018slightly negative\u2019, \u2018neutral\u2019,  \u2018slightly positive\u2019, or \u2018positive\u2019. Table 9 shows accuracy, and macro precision, recall,  and F1 scores for three models; comparison was made by grouping \u2018negative\u2019 and \n1 3\n\u2018slightly negative\u2019 into a single negative class, keeping \u2018neutral\u2019 as its own class, and grouping \u2018positive\u2019 and \u2018slightly positive\u2019 into a single positive class. GPT-4 is substantially better on each metric than the other models; however, the results are lower than what has been seen for fine-tuned models on in-domain datasets, indicating that the sentiment expressed in student course feedback may differ from the range of sentiment expressed in the internet training data of these models. The negative class was the most challenging for all models, suggesting that negative course feedback may differ significantly from negative internet feedback.\n# LLM Cost and Time\nThe cost of using the OpenAI APIs for GPT-4 and GPT-3.5 depends on the number of prompt tokens and number of completion tokens. For the final prompts and tasks used in this study, the average price of running 100 comments is shown in Table 10 for each model for different tasks (cost as of June 2023, when the results were run). The tasks include the CoT reasoning in the completion (output), significantly increasing the number of completion tokens. These provide an approximate gauge given that comments vary in length. Total API cost for this study including prompt tuning was approximately $300. The time for model calls for GPT-4, the slower of the OpenAI models, was approximately 10 s for running 100 comments in parallel for most tasks listed. For the extraction evaluation, it took approximately 1 min to run 100 comments in parallel. For batches, sleep intervals were also incorporated to stay conservatively within maximum token rates. A small percentage of API calls received errors and automatic retries were used after wait intervals.\n# Discussion\nAnalysis of education feedback, in the form of unstructured data from survey responses, is a staple for improvement of courses (Diaz et al., 2022; Flod\u00e9n, 2017; Marsh & Roche, 1993; Moss & Hendry, 2002; Schulz et al., 2014). However, this task can be time-consuming, costly, and imprecise, hampering the ability for educators and other stakeholders to make decisions based on insights from the data (Shaik\nTask\nGPT-4\nGPT-3.5\nBinary classification\n$0.93\n$0.04\nMulti-label classification\n$2.63\n$0.12\nMulti-class classification\n$2.13\n$0.10\nText extraction\n$1.10\n$0.05\nText extraction evaluation\n$3.01\n$0.13\nSentiment analysis\n$1.17\n$0.05\nInductive thematic analysis\n$0.13\n$0.006\nTable 10  Cost per 100  comments for GPT-4 and GPT-3.5\net al., 2023; Mattimoe et al., 2021; Nanda et al., 2021; Shah & Pabel, 2019). Large  language models with generative AI capabilities have become widely accessible  recently but remain underexplored in analysis of qualitative data from educational  feedback surveys. Our research questions focused on whether these new tools can 1) be successfully  used perform a wide variety of natural language processing tasks on survey results  (RQ1); 2) offer a degree of transparency based on capturing chain-of-thought intermediate output (RQ2); and 3) perform at a level comparable to human performance  across all tasks without needing to be provided with hand-labeled examples (RQ3). We were able to create reliable, reproducible workflows that put together multiple  analysis tasks, running them in minutes on more than one thousand survey responses.  The results demonstrate that these workflows can provide insight into a variety of  questions that may be asked by educators, including finding suggestions for improvement, identifying themes in students\u2019 feedback, and quantifying such results through  classification, including multi-label classification. The zero-shot approach (no handlabeled examples, other than for evaluation metrics) provides flexibility; in other  words, tasks can rapidly be adjusted through changes in the LLM prompts, and new  tasks (for example, using extraction to find information about a different focus) can  be added without need for model fine-tuning or labeling new examples. We show that chain-of-thought prompting, which was used to increase accuracy, may  also provide insight into the model\u2019s reasoning or trajectory. It is possible that the LLM  is imitating plausible reasoning rather than providing insight into how it actually arrived  at its answer; however, this distinction may be immaterial given that 1) GPT-4\u2019s reasoning was logical and highly consistent with the results, displaying elements of causal reasoning; and 2) when prompts were changed, reasoning results changed accordingly. This  has been discussed in recent work; GPT-4 has been shown to score highly on evaluations  of causal reasoning (K\u0131c\u0131man et al., 2023). In Peng et al., 2023, GPT-4 was used for evaluation of other LLMs and was able to provide consistent scores as well as detailed explanations for those scores. While specialized non-LLM models can provide signals like  confidence scores in individual classes, they lack more detailed explanations of results;  we believe that seeing a version of logical reasoning behind complex output can foster  confidence and reduce the perception of these models as black boxes. Furthermore, it is  important to consider that having human annotators reliably provide consistent, logical  justification for each annotation is prohibitive for datasets of any appreciable scale. The evaluation metrics for each individual task show a human or near-human level  of performance across a range of tasks for GPT-4. GPT-3.5 does not reach this level of  accuracy. Our tasks and dataset were drawn from real-world data and actual use cases,  with some of the tasks (e.g., multi-label classification) proving challenging even for  expert annotators. The human-like level of GPT-4 can be seen in examples of the reasoning results as well (Figs. 6, 7, and 8). In addition, it outperformed label-efficient  fine-tuned classifiers like SetFit. For workflows that chain together two or more NLP  tasks, like those examined in this study, it is important that the performance on each  task is reliable enough such that errors do not accrue in the process of obtaining a final  result. It is likely that at the time of publication there are other models (e.g., the latest  version of Claude or the top-performing open-source models) that perform at similar or  higher levels than the version of GPT-4 used in this study.\n1 3\nOverall, we found that large language models are at a stage where they can be effectively used across the broad range of tasks that are part of survey analysis. Use of LLMs  for this purpose has implications for education. These implications include:\n1) the potential to democratize access to high quality qualitative survey analysis. The paradigm of \u201clanguage in, language out\u201d allows non-machine learning experts to create workflows that can handle a range of tasks. 2) a drastic reduction in the time and effort involved for challenging survey analysis tasks, shortening the feedback loop. In many cases where there is a high volume of survey responses, the effort and expertise necessary to arrive at data-driven results may otherwise be prohibitive. The goal is to be able to analyze unstructured text survey data as easily as one might analyze quantitative results like those from Likert scales. 3) making educators and education leaders more ambitious in terms of the questions they can answer based on student feedback. For example, doing thematic analysis on a volume of survey responses may have been infeasible previously.\n1) the potential to democratize access to high quality qualitative survey analysis. The paradigm of \u201clanguage in, language out\u201d allows non-machine learning experts to  create workflows that can handle a range of tasks. 2) a drastic reduction in the time and effort involved for challenging survey analysis tasks, shortening the feedback loop. In many cases where there is a high volume of survey responses, the effort and expertise necessary to arrive at data-driven  results may otherwise be prohibitive. The goal is to be able to analyze unstructured text survey data as easily as one might analyze quantitative results like those from Likert scales. 3) making educators and education leaders more ambitious in terms of the questions they can answer based on student feedback. For example, doing thematic analysis  on a volume of survey responses may have been infeasible previously.\nWe look forward to people using these new tools to effectively learn from studen feedback.\nWe look forward to people using these new tools to effectively l feedback.\n# Limitations\nThe data used in this study was from a specific domain (online biomedical science courses) and was in English. Other domains and languages were not tested. LLM results were highly prompt-dependent, and others may achieve even more accurate results than those we have shown. Even within the most capable models, we observed that prompting techniques and prompt tuning made a significant difference. There is considerable literature on effective methods of prompting. There is an interplay of prompting techniques with the behavior of instruction-tuned models in a way that may or may not fully elicit the capabilities of each model, with prompts being seen as a form of hyperparameter to the model and with responses changing depending on updates to model training (Chen et al., 2023). Other than the comparison to SetFit and to the RoBERTa sentiment analysis model, we limited our exploration to recent OpenAI models; future work may expand this to include other models such as the most capable proprietary models (e.g., Claude and Gemini), and the most capable open-source models.\n# Concluding Remarks\nWhile LLM analysis approaches are being used in other fields, like customer reviews and user feedback (Morbidoni, 2023; Abdali et al., 2023), there has not yet been rigorous demonstration of their utility and accuracy in student feedback surveys. Our contributions are to demonstrate the viability of using LLMs for\nthis purpose, and to perform a thorough analysis of the feasibility and evaluation of the quality of LLMs\u2019 results to show reliability in common qualitative survey analysis tasks. Future research could incorporate additional prompting techniques to improve capabilities and accuracy. For example, self-consistency (Wang et al., 2022), reflection (iterative self-refinement, Madaan et al., 2023), and few-shot learning have been studied and shown to be reliable means of improving performance on difficult tasks; these were out-of-scope for the zero-shot premise of this article but are worth exploring. In addition, the ability to compose survey analysis workflows is also amenable to the use of agents (Shen et al., 2023; Weng, 2023; Yao et al., 2022). An educator or other stakeholder analyzing survey feedback should be able to state a goal or intent to an LLM-powered agent, with the agent picking and running tasks as a chain to get the desired analysis. Such an agent could also incorporate non-LLM tools, for example if a fine-tuned model is available that excels on a given task and is well-matched to the dataset at hand. Ideally, users of such tools should be able to operate by stating intent rather than tuning prompts or fine-tuning specialized models. We look forward to future progress in exploring the capabilities of these new tools; the models will continue to improve but even in their current state, they can be powerful tools for survey analysis.\n# Appendix\n# Additional Metrics for Multi\u2011Label Classification\n# nsensus Rows\u20141572 Rows Dataset (1489 for SetFit)\nConsensus Rows\u20141572 Rows Dataset (1489 for SetFit)\nPrecision, recall, and F1 score are shown for each tag in multi-label classification  for the consensus rows condition, along with macro averages for each metric, in  Table 11 for GPT-4. Hamming loss and subset accuracy are shown in Table 12.\nTag\nPrecision\nRecall\nF1 Score\nCourse logistics and fit\n89.83\n64.63\n75.18\nCurriculum\n95.89\n91.48\n93.64\nTeaching modality\n97.78\n96.70\n97.24\nTeaching\n76.04\n91.25\n82.95\nAssessment\n97.50\n93.41\n95.41\nResources\n92.31\n97.30\n94.74\nPeer and teacher interaction\n77.08\n92.50\n84.09\nOther\n92.60\n97.47\n94.97\nMacro average\n89.88\n90.59\n89.78\nTable 11  Individual label scores for multi-label classification  with GPT-4, consensus rows\nModel\nHamming loss\nSubset accuracy\nGPT-4\n0.0194\n0.8849\nGPT-3.5\n0.0710\n0.5948\nSetFit\n0.0508\n0.6797\nTable 12  Hamming loss and  subset accuracy for multi-label classification, consensus rows\nTable 12  Hamming loss and  subset accuracy for multi-label  classification, consensus rows\nTag\nPrecision\nRecall\nF1 Score\nCourse logistics and fit\n74.81\n57.71\n65.16\nCurriculum\n82.86\n86.38\n84.58\nTeaching modality\n78.35\n95.94\n86.26\nTeaching\n56.70\n87.59\n68.83\nAssessment\n87.36\n94.82\n90.94\nResources\n70.95\n95.49\n81.41\nPeer and teacher interaction\n60.58\n94.03\n73.68\nOther\n79.64\n95.10\n86.68\nMacro average\n73.91\n88.38\n79.69\nTable 13  Individual label scores for multi-label classification  with GPT-4, consensus labels\nTable 13  Individual label scores  for multi-label classification  with GPT-4, consensus labels\nModel\nHamming loss\nSubset accuracy\nGPT-4\n0.0503\n0.7168\nGPT-3.5\n0.10235\n0.4628\nSetFit\n0.07238\n0.5774\nTable 14  Hamming loss and  subset accuracy for multi-label  classification, consensus labels\n# onsensus Labels\u20142500 Rows Dataset (2359 for SetF\nPrecision, recall, and F1 score are shown for each tag in multi-label classification for the consensus labels condition, along with macro averages for each metric, in Table 13 for GPT-4. Hamming loss and subset accuracy are shown in Table 14. Supplementary Information The online version contains supplementary material available at https://\u200bdoi. org/\u200b10.\u200b1007/\u200bs40593-\u200b024-\u200b00414-0. Acknowledgements We wish to thank members of the HMX team for their contributions to creating and administering the surveys used in this study. Author Contributions Conceptualization, methodology, software, analysis, and drafting of the manuscript were performed by Michael J. Parker. Development of labels, annotation/labeling for multi-class classification, and refinement of the manuscript were performed by all authors (equal contributions). Funding This research received no external funding.\nPrecision, recall, and F1 score are shown for each tag in multi-label classification  for the consensus labels condition, along with macro averages for each metric, in  Table 13 for GPT-4. Hamming loss and subset accuracy are shown in Table 14. Supplementary Information The online version contains supplementary material available at https://\u200bdoi.\u200b org/\u200b10.\u200b1007/\u200bs40593-\u200b024-\u200b00414-0. Acknowledgements We wish to thank members of the HMX team for their contributions to creating and  administering the surveys used in this study. Author Contributions Conceptualization, methodology, software, analysis, and drafting of the manuscript were performed by Michael J. Parker. Development of labels, annotation/labeling for multi-class  classification, and refinement of the manuscript were performed by all authors (equal contributions). Funding This research received no external funding.\nPrecision, recall, and F1 score are shown for each tag in multi-label classification for the consensus labels condition, along with macro averages for each metric, in Table 13 for GPT-4. Hamming loss and subset accuracy are shown in Table 14.\nSupplementary Information The online version contains supplementary material available at https://\u200bdoi. org/\u200b10.\u200b1007/\u200bs40593-\u200b024-\u200b00414-0. Acknowledgements We wish to thank members of the HMX team for their contributions to creating and administering the surveys used in this study. Author Contributions Conceptualization, methodology, software, analysis, and drafting of the manuscript were performed by Michael J. Parker. Development of labels, annotation/labeling for multi-class classification, and refinement of the manuscript were performed by all authors (equal contributions). Funding This research received no external funding.\nAvailability of Data and Materials The prompts and the function schemas used in this study are shared in supplementary material (Electronic Supplements 1 and 2). To help preserve the anonymity of students and of feedback about teachers, the survey responses are not included in an open-access repository. The data may be provided upon request to the authors and approval of the university research ethics board.\n# Declarations\nEthics Approval and Consent to Participate This study was determined not to be human subjects rese by the Harvard Medical School Office of Human Research Administration.\nCompeting Interests The authors have no relevant financial or non-financial interests to disclose.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License,  which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as  you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article  are included in the article\u2019s Creative Commons licence, unless indicated otherwise in a credit line to the  material. If material is not included in the article\u2019s Creative Commons licence and your intended use is  not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission  directly from the copyright holder. To view a copy of this licence, visit http://\u200bcreat\u200biveco\u200bmmons.\u200borg/\u200blicen\u200b ses/\u200bby/4.\u200b0/.\n# References\nAbdali, S., Parikh, A., Lim, S. & Kiciman, E. (2023). Extracting self-consistent causal insights from  users feedback with LLMs and in-context learning. In arXiv [cs.AI]. arXiv. Retrieved April 5, 2024,  from http://\u200barxiv.\u200borg/\u200babs/\u200b2312.\u200b06820 Aldeman, M., & Branoff, T. J. (2021). Impact of course modality on student course evaluations. Paper  presented at 2021 ASEE Virtual Annual Conference Content Access, Virtual Online. Retrieved  August 21, 2023, from https://\u200bpeer.\u200basee.\u200borg/\u200b37275.\u200bpdf Alhija, F.N.-A., & Fresko, B. (2009). Student evaluation of instruction: What can be learned from students\u2019 written comments? Studies in Educational Evaluation, 35(1), 37\u201344. https://\u200bdoi.\u200borg/\u200b10.\u200b 1016/j.\u200bstued\u200buc.\u200b2009.\u200b01.\u200b002 Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychology, 3(2), 77\u2013101. https://\u200bdoi.\u200borg/\u200b10.\u200b1191/\u200b14780\u200b88706\u200bqp063\u200boa Brennan, J., & Williams, R. (2004). Collecting and using student feedback. A guide to good practice.  Learning and Teaching Support Network. Retrieved August 21, 2023, from https://\u200bwww.\u200badvan\u200bce-\u200b he.\u200bac.\u200buk/\u200bknowl\u200bedge-\u200bhub/\u200bcolle\u200bcting-\u200band-\u200busing-\u200bstude\u200bnt-\u200bfeedb\u200back-\u200bguide-\u200bgood-\u200bpract\u200bice cardiffnlp/twitter-roberta-base-sentiment-latest. (2022). Retrieved August 21, 2023, from https://\u200bhuggi\u200b ngface.\u200bco/\u200bcardi\u200bffnlp/\u200btwitt\u200ber-\u200brober\u200bta-\u200bbase-\u200bsenti\u200bment-\u200blatest. Chen, L., Zaharia, M., & Zou, J. (2023). How is ChatGPT\u2019s behavior changing over time? arXiv [cs.CL].  Retrieved August 21, 2023, from https://\u200barxiv.\u200borg/\u200babs/\u200b2307.\u200b09009 Cunningham-Nelson, S., Baktashmotlagh, M., & Boles, W. (2019). Visualizing student opinion through text  analysis. IEEE Transactions on Education, 62(4), 305\u2013311. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bTE.\u200b2019.\u200b29243\u200b85 Deepa, D., Raaji, & Tamilarasi, A. (2019). Sentiment analysis using feature extraction and dictionarybased approaches. In 2019 Third International conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), 786\u2013790. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/I-\u200bSMAC4\u200b7947.\u200b2019.\u200b90324\u200b56 Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional  transformers for language understanding. arXiv [cs.CL]. Retrieved August 21, 2023, from http://\u200b arxiv.\u200borg/\u200babs/\u200b1810.\u200b04805 Diaz, N. P., Walker, J. P., Rocconi, L. M., Morrow, J. A., Skolits, G. J., Osborne, J. D., & Parlier, T. R.  (2022). Faculty use of end-of-course evaluations. International Journal of Teaching and Learning in  Higher Education, 33(3), 285\u2013297.\nAbdali, S., Parikh, A., Lim, S. & Kiciman, E. (2023). Extracting self-consistent causal insights from users feedback with LLMs and in-context learning. In arXiv [cs.AI]. arXiv. Retrieved April 5, 2024, from http://\u200barxiv.\u200borg/\u200babs/\u200b2312.\u200b06820 Aldeman, M., & Branoff, T. J. (2021). Impact of course modality on student course evaluations. Paper presented at 2021 ASEE Virtual Annual Conference Content Access, Virtual Online. Retrieved August 21, 2023, from https://\u200bpeer.\u200basee.\u200borg/\u200b37275.\u200bpdf Alhija, F.N.-A., & Fresko, B. (2009). Student evaluation of instruction: What can be learned from students\u2019 written comments? Studies in Educational Evaluation, 35(1), 37\u201344. https://\u200bdoi.\u200borg/\u200b10. 1016/j.\u200bstued\u200buc.\u200b2009.\u200b01.\u200b002 Braun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychology, 3(2), 77\u2013101. https://\u200bdoi.\u200borg/\u200b10.\u200b1191/\u200b14780\u200b88706\u200bqp063\u200boa Brennan, J., & Williams, R. (2004). Collecting and using student feedback. A guide to good practice. Learning and Teaching Support Network. Retrieved August 21, 2023, from https://\u200bwww.\u200badvan\u200bcehe.\u200bac.\u200buk/\u200bknowl\u200bedge-\u200bhub/\u200bcolle\u200bcting-\u200band-\u200busing-\u200bstude\u200bnt-\u200bfeedb\u200back-\u200bguide-\u200bgood-\u200bpract\u200bice cardiffnlp/twitter-roberta-base-sentiment-latest. (2022). Retrieved August 21, 2023, from https://\u200bhuggi ngface.\u200bco/\u200bcardi\u200bffnlp/\u200btwitt\u200ber-\u200brober\u200bta-\u200bbase-\u200bsenti\u200bment-\u200blatest. Chen, L., Zaharia, M., & Zou, J. (2023). How is ChatGPT\u2019s behavior changing over time? arXiv [cs.CL]. Retrieved August 21, 2023, from https://\u200barxiv.\u200borg/\u200babs/\u200b2307.\u200b09009 Cunningham-Nelson, S., Baktashmotlagh, M., & Boles, W. (2019). Visualizing student opinion through text analysis. IEEE Transactions on Education, 62(4), 305\u2013311. https://\u200bdoi.\u200borg/\u200b10.\u200b1109/\u200bTE.\u200b2019.\u200b29243\u200b85 Deepa, D., Raaji, & Tamilarasi, A. (2019). Sent",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Surveys in education have long been used for course evaluation and structured evaluation of teaching. However, qualitative survey responses are often time-consuming to analyze manually, and existing methods lack efficiency and reliability, particularly in educational settings where labeled data is scarce.",
            "purpose of benchmark": "The benchmark aims to evaluate the effectiveness of large language models (LLMs) like GPT-4 and GPT-3.5 in analyzing educational survey feedback, facilitating comparison across different models and methodologies."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of efficiently extracting insights from large volumes of qualitative survey responses in education, specifically focusing on tasks like classification, extraction, thematic analysis, and sentiment analysis.",
            "key obstacle": "Existing benchmarks and methods for qualitative analysis often require extensive manual coding, are prone to subjectivity, and may not adapt well to the specific contexts of educational feedback."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need for more accessible and efficient methods of analyzing qualitative educational feedback, leveraging the capabilities of LLMs to automate and enhance analysis processes.",
            "opinion": "The authors believe that the benchmark is crucial for advancing the field of educational feedback analysis, as it demonstrates the potential of LLMs to improve the quality and efficiency of insights derived from qualitative data.",
            "innovation": "This benchmark introduces a novel approach by utilizing LLMs in a zero-shot context, allowing for analysis without the need for extensive labeled datasets, which is a significant departure from traditional methods.",
            "benchmark abbreviation": "LLM-EFA"
        },
        "dataset": {
            "source": "The dataset consists of 2,500 end-of-course survey comments collected from biomedical science courses, randomly selected from a larger pool of 50,525 responses.",
            "desc": "The dataset is structured around four open-ended questions related to course feedback, allowing for a diverse range of qualitative insights.",
            "content": "The dataset includes textual responses from students, which are analyzed for themes, sentiments, and specific feedback regarding course content and teaching effectiveness.",
            "size": "2,500",
            "domain": "Biomedical Education",
            "task format": "Qualitative Feedback Analysis"
        },
        "metrics": {
            "metric name": "Jaccard Similarity, F1 Score",
            "aspect": "Model performance in classification and extraction tasks.",
            "principle": "The metrics were selected to evaluate the accuracy and reliability of model outputs in comparison to human annotations, ensuring a comprehensive assessment of performance.",
            "procedure": "Model outputs were compared to aggregated human annotations using the Jaccard similarity coefficient and F1 scores across multiple classification tasks."
        },
        "experiments": {
            "model": "GPT-4 and GPT-3.5 were tested as state-of-the-art models in the benchmark.",
            "procedure": "Models were evaluated through a series of NLP tasks including multi-label classification, sentiment analysis, and thematic analysis, utilizing a zero-shot approach without fine-tuning.",
            "result": "The results indicated that GPT-4 achieved near-human performance in various tasks, outperforming GPT-3.5 and showing high consistency in reasoning.",
            "variability": "Variability in results was accounted for by conducting multiple trials and comparing outputs across different subsets of the dataset."
        },
        "conclusion": "The experiments demonstrated that LLMs can effectively analyze educational survey feedback, offering insights comparable to human analysis while significantly reducing the time and effort required for qualitative analysis.",
        "discussion": {
            "advantage": "The benchmark provides a reliable and efficient method for analyzing qualitative data, democratizing access to high-quality educational feedback analysis tools.",
            "limitation": "The benchmark is limited to specific domains (biomedical education) and may not generalize well to other fields or languages.",
            "future work": "Future research should explore the application of LLMs in diverse educational contexts and investigate advanced prompting techniques to further enhance performance."
        },
        "other info": {
            "API cost": "$300 for using OpenAI APIs for analysis tasks.",
            "time efficiency": "The time to process 100 comments was approximately 10 seconds for most tasks."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark aims to evaluate the effectiveness of large language models (LLMs) like GPT-4 and GPT-3.5 in analyzing educational survey feedback, facilitating comparison across different models and methodologies."
        },
        {
            "section number": "2.3",
            "key information": "The benchmark introduces a novel approach by utilizing LLMs in a zero-shot context, allowing for analysis without the need for extensive labeled datasets, which is a significant departure from traditional methods."
        },
        {
            "section number": "4.1",
            "key information": "The experiments demonstrated that LLMs can effectively analyze educational survey feedback, offering insights comparable to human analysis while significantly reducing the time and effort required for qualitative analysis."
        },
        {
            "section number": "5.3",
            "key information": "Existing benchmarks and methods for qualitative analysis often require extensive manual coding, are prone to subjectivity, and may not adapt well to the specific contexts of educational feedback."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore the application of LLMs in diverse educational contexts and investigate advanced prompting techniques to further enhance performance."
        }
    ],
    "similarity_score": 0.7381167770451181,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/A Large Language Model Approach to Educational Survey Feedback Analysis.json"
}