{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1711.02879",
    "title": "LatentPoison - Adversarial Attacks On The Latent Space",
    "abstract": "Robustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.",
    "bib_name": "creswell2017latentpoisonadversarialattacks",
    "md_text": "# LATENTPOISON \u2013 ADVERSARIAL ATTACKS ON THE LATENT SPACE\n, UK Anil A. Bharath Dept. of Bioengineering Imperial College London, London, UK a.bharath@imperial.ac.uk\nAntonia Creswell Dept. of Bioengineering Imperial College London, London, UK ac2211@ic.ac.uk\nBiswa Sengupta Noah\u2019s Ark Lab (Huawei Technologies UK) Imperial College London, London, UK b.sengupta@imperial.ac.uk\n 8 Nov 2017\n# ABSTRACT\nRobustness and security of machine learning (ML) systems are intertwined, wherein a non-robust ML system (classifiers, regressors, etc.) can be subject to attacks using a wide variety of exploits. With the advent of scalable deep learning methodologies, a lot of emphasis has been put on the robustness of supervised, unsupervised and reinforcement learning algorithms. Here, we study the robustness of the latent space of a deep variational autoencoder (dVAE), an unsupervised generative framework, to show that it is indeed possible to perturb the latent space, flip the class predictions and keep the classification probability approximately equal before and after an attack. This means that an agent that looks at the outputs of a decoder would remain oblivious to an attack.\n# INTRODUCTION\nThe ability to encode data reliably is essential for many tasks including image compression, data retrieval and communication. As data is transmitted between communication channels, error detection and correction is often employed to deduce the presence of erroneous bits (Peterson & Weldon, 1972). The source of such errors can be a result of imperfection in the transmitter, channel or in the receiver. Often times, such errors can be deliberate where a man-in-middle attack (Desmedt, 2011; Conti et al., 2016) can result in deleterious erasure of information, yet to the receiver, it may end up as appearing untampered (Kos et al., 2017). In deep learning, we are able to learn an encoding process using unsupervised learning such as in autoencoders (AE) (Kingma & Welling, 2013); however, we are less able to design methods for checking whether encodings have been tampered with. Therefore, there are two facets of this problem \u2013 the first, is to come up with methodologies of tampering with the models and second, is to detect the adversarial breach. In what follows, we will concentrate only on the first problem by presenting a method for tampering autoencoders. An autoencoder has two components: the encoder maps the input to a latent space, while the decoder maps the latent space to the requisite output. A vanilla autoencoder can, therefore, be used to compress the input to a lower dimensional latent (or feature) space. Other forms of autoencoder include the denoising AE (Vincent et al., 2010) that recovers an undistorted input from a partially corrupted input; the compressive AE (Theis et al., 2017) designed for image compression and the variational AE (Kingma & Welling, 2013) that assumes that the data is generated from a directed graphical model with the encoder operationalized to learn the posterior distribution of the latent space. Autoencoders have wide use in data analytics, computer vision, natural language processing, etc. We propose an attack that targets the latent encodings of autoencoders, such that if an attack is successful the output of an autoencoder will have a different semantic meaning to the input. Formally, we consider an autoencoder consisting of an encoder and decoder model designed to reconstruct an input data sample such that the label information associated with the input data is maintained. For\nAnil A. Bharath Dept. of Bioengineering Imperial College London, London, UK a.bharath@imperial.ac.uk\nexample, consider a dataset of images, x with the labels, y = {0, 1}, and an encoder, E : x \u2192z and a decoder, D : z \u2192x where z is a latent encoding for x. If the encoder and decoder are operating normally, the label of the reconstructed data sample, \u02c6\u02c6y = class(D(E(x))) should be the same as the label of the input data sample, where class(\u00b7) is the soft output of a binary classifier. In this paper, we focus on learning an attack transformation, T \u25e6z, such that if z is the latent encoding for a data sample, x, with label 0, T \u25e6z is the latent encoding for a data sample with label 1. The attack is designed to flip the label of the original input and change its content. Note that the same T is applied to each encoding and is not specific to either the input data sample or the encoding, it is only dependent on the label of the input data sample. The success of an attack may be measured in three ways: 1. The number of elements in the latent encoding, changed by the attack process should be small. If the encoding has a particular length, changing multiple elements may make the attack more detectable. 2. When a decoder is applied to tampered encodings, the decoded data samples should be indistinguishable from other decoded data samples that have not been tampered with. 3. Decoded tampered-encodings should be classified with opposite label to the original (untampered) data sample.\n1. The number of elements in the latent encoding, changed by the attack process should be small. If the encoding has a particular length, changing multiple elements may make the attack more detectable. 2. When a decoder is applied to tampered encodings, the decoded data samples should be indistinguishable from other decoded data samples that have not been tampered with. 3. Decoded tampered-encodings should be classified with opposite label to the original (untampered) data sample.\nOur contribution lies in studying transforms with these properties. Experimentally, we find that optimizing for requirement (1) may implicitly encourage requirement (2). Crucially, in contrast to previous work (Goodfellow et al., 2014), our approach does not require knowledge of the model (here a VAE) parameters; we need access only to the encodings and the output of a classifier, making our approach more practical (Papernot et al., 2017). Finally, we owe the success of this attack method primarily to the near-linear structure of the VAE latent space (Kingma & Welling, 2013) \u2013 which our attack exploits.\n# 2 COMPARISON TO PREVIOUS WORK\nSecurity in deep learning algorithms is an emerging area of research. Much focus has gone into the construction of adversarial data examples, inputs that are modified such that they cause deep learning algorithms to fail. Previous work, designing adversarial images, has focused on perturbing input data samples such that a classifier miss classifies adversarial examples (Goodfellow et al., 2014). The perturbation is intended to be so small that a human cannot detect the difference between the original data samples, and its adversarial version. Goodfellow et al. (Goodfellow et al., 2014) propose adding a perturbation proportional to sign(\u2207xJ(\u03b8, x, y)) where J is the cost function used to train a classifier (that is being attacked), \u03b8 are the parameters of that classifier, and x and y the data and label pair, respectively. This type of attack requires the attacker to have high-level access to the classifiers\u2019 parameters and cost function. An alternative approach that does not require the adversary to have access to the model parameters, is presented by Papernot et al. (Papernot et al., 2017) who propose a more practical approach, requiring only the classifier output and knowledge of the encoding size. Our adversary has similar, practical requirements. Our approach, is thus tangential to the previous work on adversarial images for classification. We focus on a man-in-middle form of attack (Diffie & Hellman, 1976): rather than launching an attack on data samples, we launch an attack on an intermediate encoding such that a message being sent from a sender is different to the message received by a receiver. Similar to previous work, we do not want the attack on the encoding to be detectable, but in contrast to previous work (Goodfellow et al., 2014; Papernot et al., 2017), we wish for the message \u2013 in this example the images \u2013 to be detectably changed, while still being consistent with other non-tampered messages. Our work is more similar to that of Kos et al. (Kos et al., 2017) \u2013 in the sense that they propose attacking variational autoencoders in a similar sender-receiver framework. Their goal is to perform an attack on inputs to an autoencoder such that output of the autoencoder belongs to a different class to the input. For example, an image of the digit 8 is encoded, but following an attack, the decoded image is of the digit 7 (Kos et al., 2017). While the overall goal is very similar, their approach is very different since they focus on perturbing images \u2013 while we perturb latent encodings. This difference is illustrated in Figure 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/309f/309f25d5-c949-444c-b77d-0666ae2470d0.png\" style=\"width: 50%;\"></div>\nFigure 1: Comparison of our work to previous work. In both works, the sender sends an image belonging to one class and the receiver receives an image belonging to a different class, \u201csmile\u201d or \u201cno smile\u201d. In our work, we design an attack on the latent encoding, in previous work (Kos et al., 2017), they perform an attack on the input image.\nFinally, most previous work (Goodfellow et al., 2014; Papernot et al., 2017; Kos et al., 2017) requires the calculation of a different perturbation for each adversarial example. Rather, in our approach, we learn a single (additive) adversarial perturbation that may be applied to almost any encoding to launch a successful attack. This makes our approach more practical for larger scale attacks.\n# 3 METHOD\nIn this section, we describe how we train a VAE and how we learn the adversarial transform that we apply to the latent encoding.\n# 3.1 PROBLEM SETUP\nConsider a dataset, D consisting of labeled binary examples, {xi, yi}N i=1, for yi \u2208{0, 1}. To perform the mappings between data samples, x, and corresponding latent samples, z, we learn an encoding process, q\u03c6(z|x), and a decoding process, p\u03b8(x|z), which correspond to an encoding and decoding function E\u03c6(\u00b7) and D\u03b8(\u00b7) respectively. \u03c6 and \u03b8, parameterize the encoder and decoder, respectively. Our objective is to learn an adversarial transform, \u02c6T such that class(x) \u0338= class( \u02c6T \u25e6 x), where, \u02c6T , is constrained under an Lp norm. Here, class(\u00b7) is the soft output of a binary classifier. Rather than applying an adversarial transformation (Moosavi-Dezfooli et al., 2016), \u02c6T directly to the data, x, we propose performing the adversarial transform T on the latent representation, T \u25e6z. We learn a transform, T with z = E\u03c6(x) subject to class(D\u03c6(T \u25e6z)) \u0338= class(D\u03c6(z))1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f257/f2574f94-d7bb-4591-886a-32095615b814.png\" style=\"width: 50%;\"></div>\nWe consider three methods of attack, and compare two approaches for regularizing T . The thr attack methods that we consider are as follows:\n1Note than in the case where class labels are binary, this is equivalent to: learning a T such tha class(D\u03c6(T \u25e6z)) = 1 \u2212class(D\u03c6(z)).\n3. A Poisoning+Class attack: We consider an attack during VAE training, where the VAE is trained not only to reconstruct samples but to produce reconstructions that have low classification error. This, in turn, encourages the VAE to have a discriminative internal representation, possibly making it more vulnerable to attack. We learn T at the same time.\nADDITIVE PERTURBATION (z + \u2206Z)\nHere, we consider T \u25e6z = z + \u2206z. There are several options for the form that \u2206z may take. In the first case, \u2206z may be a constant. We may learn a single transform to flip an image with label 0 to an image with label 1, and another for moving in the opposite direction. On the other hand, we may learn a single \u2206z and apply \u2212\u2206z to move in one direction and +\u2206z to move in the other. The advantage of using a constant \u2206z is that at the attack time the adversarial perturbation has already been pre-computed, making it easier to attack multiple times. There is a further advantage to using only a single \u2206z because the attacker need only learn a single vector to tamper with (almost) all of the encodings. Alternatively, \u2206z may be a function of any combination of variables x, y, z, however, this may require the attacker to learn an attack online \u2013 rather than having a precomputed attack that may be deployed easily. In this paper, we are interested in exploring the case where we learn a single, constant \u2206z. We also consider a multiplicative perturbation. However, we reserve explanation of this for the Appendix (Section 7).\n3.2 LOSS FUNCTIONS\nHere, we consider the cost functions used to train a VAE and learn T . The VAE is trained to reconstruct an input, x, while also minimizing a Kullback-Leibler (KL)-divergence between a chosen prior distribution, p(z) and the distribution of encoded data samples. The parameters of the VAE are learned by minimizing, Jvae = BCE(x, \u02c6x) + \u03b1KL[q\u03c6(z|x)||p(z)], where BCE is the binary cross-entropy and \u03b1 is the regularization parameter. A classifier may be learned by minimizing Jclass = BCE(y, \u02c6y). An additional cost function for training the VAE may be the classification loss on reconstructed data samples, BCE(y, \u02c6\u02c6y). This is similar to an approach used by Chen et al. (Chen et al., 2016) to synthesize class specific data samples. Finally, to learn the attack transform, T we minimize, Jz = BCE((1 \u2212y), \u02c7y) + Lp(T ), for the case above (Section 3.1) we have Lp(T ) = ||\u2206z||p. This allows us to learn a transform on a latent encoding, that results in a label flip in the decoded image. Minimizing the Lp-norm for p = {1, 2}, encourages the transform to target a minimal number of units of z. Specifically, using p = 1 should encourage the perturbation vector to be sparse (Donoho, 2006). When \u2206z is sparse, this means that only a few elements of z may be changed. Such minimal perturbations reduce the likelihood that the attack is detected.\n# 3.3 EVALUATION METHOD\nThe goal for the attacker is to tamper with the encoding such that the label of the decoded sample is flipped. For example, if the label was 1 initially, following a successful attack, the label should be 0. Rather than assigning binary labels to samples, our classifier outputs values between [0, 1] where 0 or 1 suggests that the classifier is highly certain that a data sample belongs to either class 0 or class 1, while a classifier output of 0.5 means that the classifier is unsure which class the sample belongs to. When an attack is successful, we expect a classifier to predict the class of the reconstructed image with high certainty. Further, for an attack to be undetectable, we would expect a classifier to predict the label of a reconstructed, un-tampered data sample with almost the same certainty as a tampered one. Formally, we may evaluate the quality of an attack by measuring |\u03f5| such that 2:\nBased purely on the classification loss, in the case where \u03f5 = 0, the encodings that have been tampered with would be indistinguishable from those that had not. An attack may be considered 2 We assume.\nBased purely on the classification loss, in the case where \u03f5 = 0, the encodings that have been tampered with would be indistinguishable from those that had not. An attack may be considered\n We assume class(x) = class(\u02c6x).\nundetectable if |\u03f5| is small. Typically, |\u03f5| may be related to the standard deviation in classification results. To calculate epsilon we make two practical alterations. The first is that our classifier outputs values [0, 1], which do not necessarily correspond to probabilities, but may in some respect capture the confidence of a single classification. Using the output of the classifier, we compute confidence scores, where 0 corresponds to low confidence and 1 to high confidence. For a sample whose true label is 1, the confidence is taken to be the output of the classifier. For a sample whose true label is 0, the confidence is taken to be (1 \u2212class(\u00b7)), where class(\u00b7) is the output of the classifier. The second, is that if the classifier is more confident when classifying one class compared to the other, it does not make sense to compare class(x) to class( \u02c6T \u25e6x). Rather, we compare:\nT \u25e6 where xy=0 and xy=1 are a data samples with true labels 0 and 1 respectively. zy=0 and zy=1 are encodings of data samples xy=0 and xy=1, respectively. We measure the performance of all attacks using the same classifier, so that we may compare attack types more easily. As a consequence, we are also able to show that the attack is partially agnostic to the classifier, provided that the classifier is trained to perform a similar task. We discuss an additional probabilistic evaluation method in Section 6.4 of the Appendix.\nT \u25e6 ere xy=0 and xy=1 are a data samples with true labels 0 and 1 respectively. zy=0 and zy=1 are codings of data samples xy=0 and xy=1, respectively.\nan additional probabilistic evaluation method in Section 6.4 of the Appen\n# 4 EXPERIMENTS AND RESULTS\nWe compare 3 methods of attack using 2 different types of regularization on \u2206z \u2013 totaling 6 experiments. The three methods of attack are listed in Section 3 and the two types of regularization are the L1-norm and the L2-norm. We show qualitative results for only two examples in the main text and reserve the rest for the appendix. We provide a quantitative analysis in the form of confidence score (discussed in Section 3.3) for all 6 attack types.\n# 4.1 DATASET\nExperiments are performed on the CelebA dataset consisting of 200k colour images of faces, of which 100 are reserved for testing. The samples are of size 64 \u00d7 64, and we do not crop the images. Each image is assigned a binary label, 1 for smiling and 0 for not smiling.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/86b2/86b2171a-f070-40e3-a59e-fa9a2891a8e5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0c16/0c16763d-540a-4261-be05-994648ab1557.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/88d3/88d3499a-a974-4c1d-b30f-2cde871221a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Original (Smile)</div>\n<div style=\"text-align: center;\">Figure 3: The dataset. Snippet of (test)images with two labels \u2013 smiling and non-smiling.</div>\nIn this section, we focus on adversaries that have been trained using L2 regularization. Figure 4 shows the results of an adversarial attack, where the adversary is learned for a pre-trained VAE, which was trained without label information. We expected this to be a more challenging form of attack since the VAE would not have been trained with any discriminative label information \u2013 making\n<div style=\"text-align: center;\">(b) Original (No Smile)</div>\nit less likely to learn features specifically for \u201csmile\u201d and \u201cnot smile\u201d. Visual examples of decoded tampered and non-tampered encodings are shown in Figure 4. Figure 4(a) shows reconstructed images of people smiling, while (b) shows similar faces, but without smiles (attacked). Similarly, Figure 4(c) shows reconstructed images of people that are not smiling, while (d) shows similar faces smiling (attacked). In most cases, the success of the attack is obvious. Quantitative results in Table 1 show several important results. In all cases, the decoded tamperedencodings are classified with high confidence. This is higher than the classifier on either the original image or the reconstructed ones. This suggests that the adversarial attack is successful as tampering with the encoding. By only evaluating the attacks by the confidence, it appears that all adversaries perform similarly well for all attack types. However, it is important to consider the difference between the confidence of reconstructed samples and the samples whose encoding was tampered with. Since the attacker aims to directly optimize the classification score, it is no surprise that affected samples have higher confidence score. It does, however, make the attack potentially more detectable. From this perspective, the more successful attacks would be those whose difference between confidence scores is small (see Section 3.3). For this particular set of attacks, the most stealthy would be switching from \u201cno smile\u201d to \u201csmile\u201d attacking a VAE trained using label information. We may expect a VAE trained with label information to be a particularly good target as it is already trained to learn discriminative features. We also notice that it is easier for the attacker to move in the direction from \u201cno smile\u201d to \u201csmile\u201d than the reverse. The reason for this may be related to the slight bias in the classification results. However, this may also stem from the subjective labelling problem. Some of the faces in Figure (a) that belong to the \u201csmile\u201d class are not clearly smiling. Both the qualitative results in Figure 4 and the quantitative results in Table 1 indicate successful attack strategies. Further, visual results are shown in the Appendix for the other attack methods, and images showing the pixel-wise difference between reconstructions and attacked samples are also shown (Figure 11) to highlight the effects of T . Table 1: Confidence scores (p = 2) for additive perturbation attacks of types: Independent, Poisoning, Poisoning+Class.\nTable 1: Confidence scores (p = 2) for additive perturbation attacks of types: Independent, Poison ing, Poisoning+Class.\nIndependent\nPoisoning\nPoisoning+Class\nData Samples\nOriginal smile\n0.80\n0.80\n0.80\nSmile reconstruction\n0.79\n0.88\n0.86\nNo smile \u2192Smile\n0.98\n0.98\n0.97\nOriginal no smile\n0.93\n0.93\n0.93\nNo smile reconstruction\n0.85\n0.87\n0.96\nSmile \u2192No smile\n0.95\n0.96\n0.96\n# 4.3 USING (z + \u2206z) WITH L1 REGULARIZATION\nIn this section, we look at results for attacks using L1 regularization on the encoding. L1 regularization is intended to encourage sparsity in \u2206z, targeting only a few units of the encoding. In Figure 10 in the appendix, we show that L1 regularization does indeed lead to a more sparse \u2206z being learned. In Figure 5, we show visual results of an adversarial attack, with the original reconstructions on the left and the reconstructions for tampered encodings on the right. We show examples of all 3 types of attack, with L1 regularization in the appendix. The attack appears to be successful in all cases. We visualize the pixel-wise change between reconstructions of encodings and tampered encodings in Figure 11 of the appendix. Note that our results are not \u201ccherry picked\u201d, but simply chosen randomly.\nTable 2 shows confidence values for each type of attack when using L1 regularization on \u2206z. In all cases, the confidence values for the samples which were attacked is higher than both reconstructed samples and original data samples. This is likely to be because the adversary is picking a perturbation that directly optimises the classification score. It is, however, important to remember that the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f781/f781f7c9-8718-47bf-99f0-c4eedde2048f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 4: Under L2 regularization Random examples of decoded latents with and without additive perturbation in an Independent attack. Smile (a) and (d), no smile (b) and (c).</div>\n<div style=\"text-align: center;\">Figure 4: Under L2 regularization Random examples of decoded latents with and without additive perturbation in an Independent attack. Smile (a) and (d), no smile (b) and (c). Table 2: Confidence scores (p=1) for additive perturbation attacks of types: Independent, Poisoning, Poisoning+Class</div>\nTable 2: Confidence scores (p=1) for additive perturbation attacks of types: Independent, Poisoning, Poisoning+Class\nIndependent\nPoisoning\nPoisoning+Class\nData Samples\nOriginal smile\n0.80\n0.80\n0.80\nSmile reconstruction\n0.87\n0.78\n0.80\nNo smile \u2192smile\n0.94\n0.98\n0.98\nOriginal no smile\n0.93\n0.93\n0.93\nNo smile reconstruction\n0.87\n0.91\n0.89\nSmile \u2192No smile\n0.96\n0.91\n0.96\nclassifier used to evaluate the attack is the same for all attacks and not the same one used for trainin the adversary.\nAs before, if there is a clear difference in confidence score between the reconstructed data samples and the decoded tampered-encodings, it will be obvious that an attack has taken place. If we consider the difference between these scores, then the most stealthy attacks are those learning the \u2206z at the same time as learning the VAE to switch between \u201cno smile\u201d and \u201csmile\u201d. Similarly, with the results obtained with L2 regularization on \u2206z, the more successful attack \u2013 in terms of stealth \u2013 is to go from \u201cno smile\u201d to \u201csmile\u201d for all attack types.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b26/2b26314d-10fd-4c17-a859-1a7f57b5144c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 5: Under L1 regularization. Random examples of decoded latents with and without additive perturbation in a Poison+Class attack. Smile (a) and (d), no smile (b) and (c).</div>\nIn this paper, we propose the idea of latent poisoning \u2013 an efficient methodology for an adversarial attack i.e., by structured modification of the latent space of a variational autoencoder. Both additive and multiplicative perturbation, with sparse and dense structure, show that it is indeed possible to flip the predictive class with minimum changes to the latent code. Our experiments show that additive perturbations are easier to operationalize than the multiplicative transformation of the latent space. It is likely that additive perturbations have reasonable performance because of the near-linear structure of the latent space. It has been shown that given two images and their corresponding points in latent space, it is possible to linearly interpolate between samples in latent space to synthesize intermediate images that transit smoothly between the two initial images (Kingma & Welling, 2013; Radford et al., 2015). If the two images were drawn from each of the binary classes, and a smooth interpolation existed between them, this would mean that additive perturbation in the latent space, along this vector, would allow movement of samples from one class to the other. How can we counter such a poisoning of the latent space? It might be helpful to look into the predictive probability and its uncertainty on outputs from an autoencoder. If the uncertainty is above a threshold value, an attack may be detected. Detection via predictive probability and its uncertainty, as well as alternative methods, such as inspection of the latent encoding, become even more difficult when the attacker has altered the latent distribution minimally (under a norm). Given the prevalence of machine learning algorithms, the robustness of such algorithms is increasingly becoming important (McDaniel et al., 2016; Abadi et al., 2017), possibly at par with reporting test error of such systems.\n<div style=\"text-align: center;\">(d) Latent Space Attack (no smile \u2192smile)</div>\n# REFERENCES\nMart\u00b4\u0131n Abadi, \u00b4Ulfar Erlingsson, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Nicolas Papernot, Kunal Talwar, and Li Zhang. On the protection of private information in machine learning systems: Two recent approches. In Computer Security Foundations Symposium (CSF), 2017 IEEE 30th, pp. 1\u20136. IEEE, 2017. Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in Neural Information Processing Systems, pp. 2172\u20132180, 2016. Mauro Conti, Nicola Dragoni, and Viktor Lesyk. A survey of man in the middle attacks. IEEE Communications Surveys & Tutorials, 18(3):2027\u20132051, 2016. Yvo Desmedt. Man-in-the-middle attack. In Encyclopedia of cryptography and security, pp. 759\u2013 759. Springer, 2011. Whitfield Diffie and Martin Hellman. New directions in cryptography. IEEE transactions on Information Theory, 22(6):644\u2013654, 1976. D. L. Donoho. Compressed sensing. IEEE Trans. Inf. Theor., 52(4):1289\u20131306, April 2006. ISSN 0018-9448. Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2013. Jernej Kos, Ian Fischer, and Dawn Song. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832, 2017. Patrick McDaniel, Nicolas Papernot, and Z Berkay Celik. Machine learning in adversarial settings. IEEE Security & Privacy, 14(3):68\u201372, 2016. Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. CoRR, abs/1610.08401, 2016. Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pp. 506\u2013519. ACM, 2017. William Wesley Peterson and Edward J Weldon. Error-correcting codes. MIT press, 1972. Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. In International Conference on Learning Representations (ICLR) 2016, arXiv preprint arXiv:1511.06434, 2015. URL https://arxiv. org/pdf/1511.06434.pdf. Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Husz\u00b4ar. Lossy image compression with compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017. Florian Tram`er, Alexey Kurakin, Nicolas Papernot, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017. Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371\u20133408, 2010.\n# 6 APPENDIX\n# 6.1 SAMPLES WITH AND WITH OUT LABEL SWITCH\nIn the main body of the text, we showed received images for the case where an attack has taken place for two types of attack. In this section, we show the remaining examples.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8949/894901ff-3d08-4f4b-8a70-be3aaa3b761a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 6: Under L1 regularization. Random examples of decoded latents with and without additive perturbation in an Independent attack. Smile (a) and (d), no smile (b) and (c).</div>\n<div style=\"text-align: center;\">Figure 6: Under L1 regularization. Random examples of decoded latents with and without additive perturbation in an Independent attack. Smile (a) and (d), no smile (b) and (c).</div>\n6.2 COMPARE USING |\u2206z|1 WITH |\u2206z|2\nIn this section, we compose Tables of values and figures to compare the 3 different attacks for the 2 different regularization methods.\n# 6.3 ENTROPY OF PERTURBATION\nWe expect that using L1 regularization will give more sparse perturbations, \u2206z than using L2 regularization. In Figure 10, we show the effect of the regularization term for each attack type: (1) learning a \u2206z for a pre-trained VAE, (2) learning a \u2206z while training a VAE and (3) learning a \u2206z while training a VAE and using class information to train the VAE. It is clear from Figure 10 that using L1 regularization does indeed result in a more sparse \u2206z.\n6.4 CAN WE USE KNOWLEDGE OF THE PRIOR TO DETECT AN ADVERSARIAL ATTACK?\nFigure 10 provides information about the magnitude of the adversarial perturbations. Here, we consider how knowledge of the magnitude of the perturbations, may allow us to understand the probability of an attack being detected. We consider an approach to individually test each element of a latent encoding to see if we can determine whether an attack has taken place. We refer to a single element of the perturbation \u2206z, as \u03b4z and consider whether we can detect perturbation to a single element in isolation from the other elements in the encoding. In a variational autoencoder, the distribution of encoded data samples is trained to belong to a chosen prior distribution \u2013 in this case a Gaussian. Assuming that the autoencoder is trained well, we may say that the distribution of encoded data samples is Gaussian. Further, we assume that each element in the encoding is drawn independently from the Gaussian distribution. From this, we know that c.99.5% each individual encoding value lies between \u22122.807\u03c3 and 2.807\u03c3 where sigma is the\n<div style=\"text-align: center;\">(d) Latent Space Attack (no smile \u2192smile)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f29/2f29efef-e554-4894-9934-a4fe4cba64f0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 7: Under L2 regularization. Random examples of decoded latents with and without additive perturbation in a poisoning attack. Smile (a) and (d), no smile (b) and (c).</div>\n<div style=\"text-align: center;\">Figure 7: Under L2 regularization. Random examples of decoded latents with and without addit perturbation in a poisoning attack. Smile (a) and (d), no smile (b) and (c).</div>\nstandard deviation of the Gaussian distribution. This means that approximately 1/200 3 elements lie outside this interval. In our case \u03c3 = 1. Any addition to samples from Gaussian distribution results in a shift of the distribution. For an adversarial attack involving the additive perturbation of \u03b4z on a single unit of \u2206z, we may calculate the probability that a single element in a tampered encoding lies outside the range [\u22122.807, 2.807]. The formula for this is given by:\n\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd where erf(\u00b7) is the error function. Note that P99.5%(1) = 0.04, P99.5%(2) = 0.2 and P99.5%(5 0.98.\n\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd where erf(\u00b7) is the error function. Note that P99.5%(1) = 0.04, P99.5%(2) = 0.2 and P99.5%(5 0.98.\nWe may use this to evaluate our attack processes and may also be used to further regularize our models to ensure that the probability of being detected is less than a chosen threshold. Looking at Figure 10 we can see that only attacks in (a) and (b) using L2 regularization are likely to be undetectable according to the criteria above, assuming that the encoded data samples follow a Gaussian distribution.\n# 6.5 THE EPSILON GAP\nHere, we compare the \u03f5-gap (described in Section 3.3) for each type of attack, using each type of regularization. We expected that using L1 regularization would encourage minimal change to the encoding needed to make a switch between labels. Therefore we might expect this to influence the epsilon value. However, for a sparse \u2206z to have the desired properties we also require the structure of the latent space to be sparse. Since we did not enforce any sparsity constraint on the latent encoding when training the VAE, sparsity on the latent samples is not guaranteed. Therefore,\n3our latent encoding is of size 200, however the choice of a 99.5% is fairly arbitrary and may be chosen more precisely depending on application.\n<div style=\"text-align: center;\">(d) Latent Space Attack (no smile \u2192smile)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/026a/026abe43-1d8c-4f56-955d-ac8a8f009872.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 8: Under L1 regularization. Random examples of decoded latents with and without additive perturbation in an poisoning attack. Smile (a) and (d), no smile (b) and (c).</div>\n<div style=\"text-align: center;\">Figure 8: Under L1 regularization. Random examples of decoded latents with and without additive perturbation in an poisoning attack. Smile (a) and (d), no smile (b) and (c).</div>\nalthough it is useful to learn sparse encodings to facilitate the speed of the attack (minimal number of changes to the encoding), it does not clearly affect the overall quality of the attack.\n<div style=\"text-align: center;\">Table 3: Epsilon gap values</div>\nSamples\n\u2212\u2206z\n+\u2206z\np=1\np=2\np=1\np=2\nLearn \u2206z & Independent\n0.07\n0.19\n0.09\n0.10\nLearn \u2206z & Poisoning jointly\n0.20\n0.10\n0.00\n0.09\nLearn \u2206z & Poisoning+Class\n0.18\n0.11\n0.07\n0.00\n# 6.6 THE EFFECT OF \u2206z ON \u2206x\n<div style=\"text-align: center;\">6.6 THE EFFECT OF \u2206z ON \u2206x</div>\nIn Figure 11 we show the difference between the reconstructed data samples and decoded tamperedencodings. These images highlight the effect of the adversarial perturbation \u2013 applied to the latent space \u2013 in the data space.\n(d) Latent Space Attack (no smile \u2192smile)\n<div style=\"text-align: center;\">(d) Latent Space Attack (no smile \u2192smile)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5510/551099f3-84c9-4820-9d91-117b08dc3293.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) No Smile Reconstructions</div>\n<div style=\"text-align: center;\">Figure 9: Under L2 regularization. Random examples of decoded latents with and without additive perturbation in an poisoning+Class attack. Smile (a) and (d), no smile (b) and (c).</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c15/8c1521ad-69c2-4201-9037-7d654b014734.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) p=1: Independent</div>\n<div style=\"text-align: center;\">Figure 10: Visualization of the values of each element in the learned \u2206z in an additive perturbation attack. The x-axis corresponds to units in \u2206z and the y-axis to the values that each unit takes. This figure demonstrates the effect of L1 and L2 regularization on the sparsity of the \u2206z learned.</div>\n<div style=\"text-align: center;\">(d) Latent Space Attack (no smile \u2192smile)</div>\n<div style=\"text-align: center;\">(f) p=1: poisoning+Class</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9edc/9edcc840-98db-4078-99d1-c03f4a6f6ac6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) p=1: poisoning+Class</div>\n<div style=\"text-align: center;\">Figure 11: Difference image between decoded encodings (reconstructions) and decoded tamperedencodings for additive perturbation attacks. Here we show only results in the direction of +\u2206z.</div>\n<div style=\"text-align: center;\">(f) p=2: poisoning+Class</div>\nFor both the encoder, decoder and classifier we use an architecture similar to that used by Radford et al. Radford et al. (2015). We weight the KL-divergence in the VAE loss by \u03b1 = 0.1 and we train the model using Adam with a learning rate of 2e \u22124 however training was not sensitive to this parameter \u2013 training with a learning rate of 1e \u22123 also worked. For details of network architectures and training please refer to our code, available at: https://github.com/ToniCreswell/Adversarial-Attack-On-Latent-Space.\nMULTIPLICATIVE PERTURBATION z \u00b7 (1 + \u2206z)\nTo formulate a multiplicative perturbation, we require that the element(s) that encode smile or no smile have different signs for each class. We may then learn a multiplicative mask, where most of the values are ones, and one or a few values are negative. The values may not be positive. If the values are positive then signs in the encoding cannot be switched and no label swap may take place. In this formulation, we cannot guarantee that the encoding will take the desired form. From preliminary experiments, we see that faces classified as \u201csmiling\u201d often appear to be smiling more intensely after the transform. This is likely to be because the autoencoder considered the image to be a person not smiling in the first place. In our formulation, we use a single \u2206z to which we apply Lp regularization to. The transform is then z(1 + \u2206z). Note that it does not make sense to have a formulation for each direction i.e. z(1\u2212\u2206z) for the other direction; if the encoding for opposite samples has opposite signs a negative \u2206z is sufficient to provide a transform in both directions. For multiplicative transforms, the perturbations do not appear to perform as well as for the additive approach. This might be a reflection of the near-linear structure of the latent space learned by the autoencoder. An adversary applying an additive perturbation is able to target the near-linear structure, while an adversary applying a multiplicative perturbation implies much stronger assumptions on the structure of the latent space \u2013 which apparently do not hold for all variational autoencoders.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the problem of adversarial attacks on the latent space of deep variational autoencoders (dVAEs), emphasizing the need for robustness in machine learning systems. Previous methods focused on perturbing input data to misclassify them but lacked approaches that directly target the latent space, necessitating a new method for effective attacks.",
        "problem": {
            "definition": "The paper aims to solve the issue of how to effectively tamper with the latent encodings of autoencoders to change the classification of the outputs while maintaining the appearance of the original inputs.",
            "key obstacle": "The main challenge is to perform these attacks in a manner that minimizes the detectability of changes in the latent space, ensuring that the outputs remain similar to the original inputs."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that the latent space of dVAEs has a near-linear structure, which can be exploited to create effective adversarial attacks without requiring knowledge of model parameters.",
            "opinion": "The proposed idea involves learning an adversarial transformation that can flip the class predictions of the latent encodings, allowing for targeted manipulation of outputs while keeping the changes subtle.",
            "innovation": "This method differs from previous approaches by not requiring access to the model parameters, focusing instead on the latent encodings and the outputs of a classifier, making it more practical for real-world applications."
        },
        "method": {
            "method name": "Latent Poisoning",
            "method abbreviation": "LP",
            "method definition": "Latent Poisoning is defined as an adversarial attack method that applies transformations to the latent space of a variational autoencoder to change the classification of the output.",
            "method description": "The core of the method involves applying learned transformations to the latent encodings to achieve label flips in the reconstructed outputs.",
            "method steps": [
                "Train a variational autoencoder (VAE) on a dataset.",
                "Learn an adversarial transformation that operates on the latent space.",
                "Apply the transformation to latent encodings during inference to flip class predictions."
            ],
            "principle": "The effectiveness of this method lies in the near-linear structure of the VAE latent space, which allows for smooth transitions between classes through minimal changes to the latent representations."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the CelebA dataset, which contains 200,000 color images of faces with binary labels for smiling and not smiling. The evaluation involved comparing three attack methods with two types of regularization (L1 and L2).",
            "evaluation method": "The performance of the method was assessed by measuring the confidence scores of the classifier on both tampered and original samples, ensuring that tampered samples were classified with high certainty while remaining indistinguishable from original samples."
        },
        "conclusion": "The experiments demonstrated that the proposed Latent Poisoning method is effective in flipping class predictions with minimal changes to the latent encodings, highlighting the potential vulnerabilities of autoencoders to adversarial attacks.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include its practicality, as it does not require access to model parameters, and its ability to achieve high confidence in misclassification with minimal perturbations.",
            "limitation": "A limitation of the method is that it may still be detectable if the difference in confidence scores between tampered and original samples is too significant, potentially revealing the presence of an attack.",
            "future work": "Future research could explore enhancing the stealthiness of attacks, developing detection mechanisms for adversarial manipulations, and investigating the robustness of different autoencoder architectures against such attacks."
        },
        "other info": {
            "info1": "The method leverages both additive and multiplicative perturbations.",
            "info2": {
                "info2.1": "The CelebA dataset was used for all experiments.",
                "info2.2": "The architecture used for the VAE was similar to that of Radford et al. (2015)."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "5.2",
            "key information": "Adversarial attacks on the latent space of deep variational autoencoders (dVAEs) can significantly affect the robustness of machine learning models."
        },
        {
            "section number": "5.3",
            "key information": "The proposed Latent Poisoning method applies transformations to the latent space of a variational autoencoder to change the classification of the output, enhancing robustness against adversarial attacks."
        },
        {
            "section number": "3.4",
            "key information": "The Latent Poisoning method focuses on the latent encodings and the outputs of a classifier, making it a deep learning technique for OOD detection."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the Latent Poisoning method is that it may be detectable if the difference in confidence scores between tampered and original samples is too significant."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore enhancing the stealthiness of attacks and developing detection mechanisms for adversarial manipulations."
        }
    ],
    "similarity_score": 0.6487751833768921,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/LatentPoison - Adversarial Attacks On The Latent Space.json"
}