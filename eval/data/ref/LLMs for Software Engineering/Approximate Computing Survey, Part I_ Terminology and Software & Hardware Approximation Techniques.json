{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2307.11124",
    "title": "Approximate Computing Survey, Part I: Terminology and Software & Hardware Approximation Techniques",
    "abstract": "The rapid growth of demanding applications in domains applying multimedia processing and machine learning has marked a new era for edge and cloud computing. These applications involve massive data and compute-intensive tasks, and thus, typical computing paradigms in embedded systems and data centers are stressed to meet the worldwide demand for high performance. Concurrently, the landscape of the semiconductor field in the last 15 years has constituted power as a first-class design concern. As a result, the community of computing systems is forced to find alternative design approaches to facilitate high-performance and/or power-efficient computing. Among the examined solutions, Approximate Computing has attracted an ever-increasing interest, with research works applying approximations across the entire traditional computing stack, i.e., at software, hardware, and architectural levels. Over the last decade, there is a plethora of approximation techniques in software (programs, frameworks, compilers, runtimes, languages), hardware (circuits, accelerators), and architectures (processors, memories). The current article is Part I of our comprehensive survey on Approximate Computing, and it reviews its motivation, terminology and principles, as well it classifies and presents the technical details of the state-of-the-art software and hardware approximation techniques.",
    "bib_name": "leon2023approximatecomputingsurveyi",
    "md_text": "# Approximate Computing Survey, Part I: Terminology and Software & Hardware Approximation Techniques\nVASILEIOS LEON, National Technical University of Athens, Greece MUHAMMAD ABDULLAH HANIF, New York University Abu Dhabi, UA GIORGOS ARMENIAKOS, National Technical University of Athens, Greec XUN JIAO, Villanova University, USA MUHAMMAD SHAFIQUE, New York University Abu Dhabi, UAE KIAMAL PEKMESTZI, National Technical University of Athens, Greece DIMITRIOS SOUDRIS, National Technical University of Athens, Greece\nMUHAMMAD SHAFIQUE, New York University Abu Dhabi, UAE KIAMAL PEKMESTZI, National Technical University of Athens, Greece DIMITRIOS SOUDRIS, National Technical University of Athens, Greece\nThe rapid growth of demanding applications in domains applying multimedia processing and machine learning has marked a new era for edge and cloud computing. These applications involve massive data and computeintensive tasks, and thus, typical computing paradigms in embedded systems and data centers are stressed to meet the worldwide demand for high performance. Concurrently, the landscape of the semiconductor field in the last 15 years has constituted power as a first-class design concern. As a result, the community of computing systems is forced to find alternative design approaches to facilitate high-performance and/or power-efficient computing. Among the examined solutions, Approximate Computing has attracted an ever-increasing interest, with research works applying approximations across the entire traditional computing stack, i.e., at software, hardware, and architectural levels. Over the last decade, there is a plethora of approximation techniques in software (programs, frameworks, compilers, runtimes, languages), hardware (circuits, accelerators), and architectures (processors, memories). The current article is Part I of our comprehensive survey on Approximate Computing, and it reviews its motivation, terminology and principles, as well it classifies and presents the technical details of the state-of-the-art software and hardware approximation techniques.\n# CCS Concepts: \u2022 General and reference \u2192Surveys and overviews; \u2022 Software and its e Software notations and tools; \u2022 Hardware \u2192Integrated circuits; \u2022 Computer systems \u2192Architectures.\nAdditional Key Words and Phrases: Inexact Computing, Approximation Method, Approximate Programming Approximation Framework, Approximate Processor, Approximate Memory, Approximate Circuit, Approximate Arithmetic, Error Resilience, Accuracy, Review, Classification\nVasileios Leon, Muhammad Abdullah Hanif, Giorgos Armeniakos, Xun Jiao, Muhammad Shafique, Kiamal Pekmestzi, and Dimitrios Soudris. 2023. Approximate Computing Survey, Part I: Terminology and Software & Hardware Approximation Techniques. ACM Computing Surveys.\nAuthors\u2019 addresses: Vasileios Leon, National Technical University of Athens, Athens 15780, Greece; Muhammad Abdullah Hanif, New York University Abu Dhabi, Abu Dhabi 129188, UAE; Giorgos Armeniakos, National Technical University of Athens, Athens 15780, Greece; Xun Jiao, Villanova University, Villanova, Pennsylvania 19085, USA; Muhammad Shafique New York University Abu Dhabi, Abu Dhabi 129188, UAE; Kiamal Pekmestzi, National Technical University of Athens Athens 15780, Greece; Dimitrios Soudris, National Technical University of Athens, Athens 15780, Greece.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2023 Association for Computing Machinery. 0360-0300/2023/7-ART $15.00 https://doi.org/10.1145/XXXXXXX.XXXXXX\nACM Computing Surveys, Under Review. July 2023.\n# 1 INTRODUCTION\nThe proliferation of emerging technologies such as Artificial Intelligence (AI), Machine Learning (ML), Digital Signal Processing (DSP), big data analytics, cloud computing and Internet of Things (IoT) is driving the growing demand for computational power and storage requirements. The International Data Corporation (IDC) reported that the global data sphere is expected to grow from 33 zettabytes (2018) to 175 zettabytes by 2025 with a Compound Annual Growth Rate (CAGR) of 61% [41], highlighting the pressing need for more efficient computing solutions. This problem is intensified, especially when considering resource-restricted systems and/or battery-driven devices, such as smartphones and wearables [16]. Historically, the industry of computing systems was driven for more than 40 years by two fundamental principles: Moore\u2019s Law [130] and Dennard\u2019s Law [46]. Today, even though the number of transistors integrated per area is still increasing (Moore\u2019s Law), the supply voltage cannot be scaled according to Dennard\u2019s Law, and thus, the power density is increased. The end of Dennard\u2019s scaling combined with other factors (e.g., the cooling technology and the natural limits of silicon) led us to the \u201cDark Silicon\u201d era [48]. In this era, the power efficiency is a critical issue for computing systems, either they are placed at the edge (embedded systems) or on the cloud (data centers). Concurrently, the compute-intensive workloads of novel AI/ML and DSP applications challenge their deployment in terms of performance (speed). As a result, the industry of computing systems is forced to find new design/computing approaches that will improve the power efficiency while providing the desired performance. I: Need for Low-Power/Energy Computing. With the continuous shrinking of the transistor size into deep nanometer regime, the power/energy consumption has become a critical issue and a top priority to consider in the design of computing systems. Actually, with the current trend, scientists have predicted that by the year 2040 computers will need more electricity than the world\u2019s energy resources can generate, unless radical improvements are made in the computer design [143]. The ever-increasing deployment of IoT devices [52, 67], the exploding \u201cBig Data\u201d from all kinds of sources (e.g., videos and images), and the growth of supporting cyberinfrastructure such as data centers, are all exaggerating this situation. This challenge is present in computing devices of all sizes \u2014 from low-power edge devices to high-performance data centers. For example, for mobile/edge devices used intensively in IoT endpoints, low-power/energy computing must be achieved to increase the battery life. On the other hand, data centers must achieve low-power/energy to reduce the costs in electricity and cooling, and achieve reliable operations. In certain applications (e.g., autonomous driving), the high power consumption could lead to an increased temperature of the computing chips, which will adversely affect the reliability of the chips and cause severe consequences. Recently, the increasing workload (in terms of data size and computational demands) from the AI, ML, and DSP domains are all worsening the issue of power/energy consumption. II: Need for Accelerated Computing. Many practical application domains, such as autonomous driving, robotics, and space, require real-time processing of data streams. However, the very nature of existing powerful algorithms pose significant challenges to the hardware implementations. A specific example is the recent massive deployment of AI/ML methods with millions of parameters, like in the case of Deep Neural Networks (DNNs). Typically speaking, the execution of DNN models requires a huge amount of computing operations, such as additions/multiplications and transformations, as well as intensive memory accesses, which may lead to a significant delay in processing data streams. This can cause compromised quality of results, especially in resourceconstrained edge devices that have real-time latency requirements.\nACM Computing Surveys, Under Review. July 2023.\nHow Can Approximate Computing Help? As Dennard\u2019s Law expired in the mid-2000s and Moore\u2019s Law is declining, the transistor scaling is increasingly less effective in improving performance, energy efficiency, and robustness. Therefore, alternative computing paradigms are urgently needed as we look to the future of the computing industry. Approximate Computing (AxC) has recently arisen as a promising candidate for resolving this challenge due to its success in many compute-intensive applications (e.g., image processing, object classification, and bio-signal analysis). Such applications show an inherent error tolerance, i.e., they do not require completely accurate computations for delivering acceptable output quality. For example, in image processing, a few pixel drops do not affect how images are perceived by human eyes; AI/ML may not need precise model parameters to get accurate results in classification and detection; communication systems are resilient against occasional noise. This paves the way for new optimization opportunities. By introducing a new design dimension \u2013 \u201caccuracy\u201d \u2013 to the overall design optimization, it is possible to trade off accuracy and lead to less power used, less time consumed, and fewer computing resources required. This leads to the promising novel design paradigm of Approximate Computing. Some examples of accuracy/quality metrics are peak signal-to-noise ratio (multimedia applications), relative difference (numerical analysis), and classification accuracy (machine learning). The idea of leveraging imprecise computation for improved design dates back several decades in real-time system scheduling, where imprecise computation was used to enhance its dependability [104]. Another related research field is Fault Tolerance, which seeks to continue to provide the required functionality despite occasional failures by hiding the errors [141]. Compared to this field, Approximate Computing \u201cintentionally\u201d seeks to design imperfect hardware and software systems (induce errors) for improved performance and/or power/area efficiency. Specifically, researchers have built approximate integrated circuits, software programs, and architectures that outperform their conventional \u201caccurate\u201d counterparts in terms of resources (power, area, and/or performance). Approximate Computing has achieved tremendous success in many application domains and target among other image processing [2, 4], computer vision [1, 15], computer graphics [14, 15], machine learning [9, 14], signal processing [8, 14], financial analysis [1, 14], database search [3, 14], and scientific computing [11, 18]. The error resilience of such application domains and the relaxed constraints regarding the quality of the produced results, constituting Approximate Computing as an applicable design paradigm, originate from: (1) The user\u2019s intention to accept inaccuracies and results of lower quality. (2) The limited human perception, e.g., in multimedia applications. (3) The lack of perfect/golden results for validation, e.g., in data mining applications. (4) The lack of a unique answer/solution, e.g., in machine learning applications. (5) The application\u2019s self-healing property, i.e., its capability to absorb/compensate errors by default. (6) The application\u2019s inherent approximate nature, e.g., in probabilistic calculations, iterative algorithms, and learning systems. (7) The application\u2019s analog/noisy real-world input data, e.g., in multimedia/signal processing. The prominent outcome of approximate systems, in parallel with the ever-increasing demand for efficient and sustainable computing, has attracted a vast research interest. In this context, approximation techniques are applied at different design layers, i.e., from software/programs to hardware/circuits. Motivated by the benefits of Approximate Computing, as well as the great momentum it has gained over the last years, we conduct a survey that is presented in two parts.\nAxC Survey\nYear Coverage\nPages #\nReferences #\nSW Tech.\nHW Tech.\nArch. Approx.\nAI/ML\nMemories\nFrameworks\n& Tools\nMetrics\nBenchmarks\nCPU/FPGA/\nGPU/ASIC\nTerminology\nChallenges\n[59]\n2013\n6\n65\n\u2713\n\u2713\n\u2248\n\u2248\n\u2717\n\u2717\n\u2713\n\u2717\n\u2248\n\u2717\n\u2717\n[127]\n2015\n331\n84\n\u2713\n\u2248\n\u2713\n\u2248\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n\u2713\n[206]\n2015\n15\n59\n\u2713\n\u2713\n\u2713\n\u2717\n\u2248\n\u2717\n\u2717\n\u2717\n\u2248\n\u2717\n\u2717\n[197]\n2015\n6\n54\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2248\n\u2717\n\u2717\n[174]\n2016\n6\n47\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n\u2248\n\u2717\n\u2717\n\u2248\n\u2717\n\u2717\n[131]\n2017\n4\n40\n\u2713\n\u2713\n\u2717\n\u2248\n\u2248\n\u2717\n\u2717\n\u2717\n\u2248\n\u2717\n\u2717\n[12]\n2017\n6\n72\n\u2713\n\u2713\n\u2713\n\u2248\n\u2248\n\u2248\n\u2717\n\u2713\n\u2248\n\u2717\n\u2713\n[181]\n2020\n391\n235\n\u2248\n\u2713\n\u2713\n\u2717\n\u2713\n\u2248\n\u2717\n\u2717\n\u2248\n\u2717\n\u2713\nOur work Pt. 1\n2023\n341\n221\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nPt. 2\n2023\n351\n296\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n1 Single-column pages\nOur goal is to cover the entire spectrum of Approximate Computing. The contribution and the content of our two-part survey is analyzed in Section 2.\n# 2 APPROXIMATE COMPUTING SURVEY\nScope and Contribution: The literature includes surveys on Approximate Computing that target specific areas, e.g., arithmetic circuits [71] and logic synthesis [168], or focus on a single approximation technique, e.g., precision scaling [35]. In [200] and [13], a thorough analysis of software and hardware approximation techniques, respectively, is provided, but they are both laser-focused on DNN applications. Similarly, the survey of [43] mainly reviews the impact of AxC on edge computing and none of the [13, 35, 43, 71, 168, 200] surveys cover approximations from system level down to circuit level. In contrast, our survey covers the entire computing stack, reviewing and classifying all the state-of-the-art approximation techniques from the software, hardware, and architecture layers for a wide range of application domains. Table 1 reports a list of surveys that can be classified along with our survey, showing a qualitative comparison and key aspects that characterize each work. In our work, we form the AxC stack (pyramid of design layers) as shown in Figure 1, and analyze techniques and approaches from all the layers. In fact, this is the traditional computing stack with the addition of various kind of approximation techniques across the design layers (from the application and software down to the hardware and device). More explicitly, our survey constitutes a comprehensive and detailed guide that provides stepby-step explanation of key concepts, techniques, and applications of the AxC paradigm. The reader will have a complete view on AxC principles and works that implement and evaluate software, hardware, application-specific and architectural approximations. This survey also acts as a tutorial on the state-of-the-art approximation techniques. The main objectives and contributions of our survey are: 1) to attribute definitions in key AxC aspects and explain the main terminology, 2) to analyze the state-of-the-art works, identify approximation categories and cluster the reviewed works with respect to the approximation type/approach, 3) to survey application domains of AxC including the impact of approximations on them, and 4) to identify and discuss open challenges and future directions as a step towards the realization of approximate applications.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b575/b57520cd-0075-409e-9747-b494b3d509f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. The Approximate Computing stack: approximation techniques in the design abstraction layers.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/09e3/09e37787-f1d5-4ab6-96e5-0150c89d5018.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Organization of our two-part survey on Approximate Computing</div>\nOrganization: As shown in Figure 2, our survey is divided into two parts, which constitute standalone manuscripts focusing on different aspects/areas of Approximate Computing: Part I: It is presented in the current paper, and it introduces the AxC paradigm (terminology and principles) and reviews software & hardware approximation techniques. Part II: It is presented in [94], and it reviews application-specific & architectural approximation techniques and introduces the AxC applications (domains, quality metrics, benchmarks). The remainder of the article (Part I of survey) is organized as follows. Section 3 provides fundamental concepts of AxC, while the next two sections (Sections 4-5) present the reviewed and classified software- and hardware-level works, respectively. Finally, Section 6 concludes the article.\n# 3 THE APPROXIMATE COMPUTING PARADIGM 3.1 Terminology of Approximate Computing\nEven though approximate computations have been examined since the 1960s (e.g., Mitchell\u2019s logarithmic-based multiplication/division [125]), the first systematic efforts to define the Approximate Computing paradigm started in the late 2000s. Various terms have been used in the literature to describe strategies for delivering approximate architectures, programs, and circuits. Approximate Computing is synonymous or overlaps with these terms. Chakradhar et al. [27] define Best-Effort Computing as \u201cthe approach of designing software/hardware computing systems with reduced workload, improved parallelization and/or approximate components towards enhanced efficiency and scalability\u201d. The term Relaxed Programming is introduced by Carbin et al. [23] to express \u201cthe transformation of programs with approximation methods and relaxed semantics to enable greater flexibility in their execution\u201d. Chippa et al. [38] use the term Scalable Effort Design for \u201cthe systematic approach that embodies the notion of scalable effort into the design process at different levels of abstraction, involving mechanisms to vary the computational effort and control knobs to achieve the best possible trade-off between energy efficiency and quality of results\u201d. According to Mittal [127], \u201cApproximate Computing exploits the gap between the accuracy required by the applications/users and that provided by the computing system to achieve diverse optimizations\u201d. Han and Orshansky [59] distinguish Approximate Computing from Probabilistic/Stochastic Computing, stating that \u201cit does not involve assumptions on the stochastic nature of the underlying processes implementing the system and employs deterministic designs for producing inaccurate results\u201d. Another interesting point of view is expressed by Sampson [164], who claims that Approximate Computing is based on \u201cthe idea that we are hindering the efficiency of the computer systems by demanding too much accuracy from them\u201d. In this article, we attribute the following definition:\nTable 2 describes the most frequently-used terms in Approximate Computing. The term error is used to indicate that the output result is different from the accurate result (produced with conventional computing). Error is distinguished from fault, which refers to an unexpected condition (e.g., stuck-at-logic in circuits, bit-flips in memories, faults in operating systems) that causes the system to unintentionally output erroneous results. Another significant term is accuracy, which is defined as the distance between the approximate and the accurate result and is measured using application-specific and general-computing error metrics. Accuracy is distinguished from precision, which expresses the differentiation between nearby discrete values and does not refer to errors of Approximate Computing but to quantization noise (inserted by the real-to-digital value mapping). Moreover, in Approximate Computing, the term Quality-of-Service (QoS) is used to describe the overall quality of the results regarding accuracy and errors.\n# 3.2 Principles of Approximate Computing\nTo enable and realize significant efficiency gains through approximations, the design of approximate systems should be guided using the following steps/principles: Application Analysis: The quality requirements and metrics vary across applications. Therefore, it is essential to analyze the application in detail to identify the acceptable QoS and specify the error metrics that can truly quantify the output quality for evaluation and comparison.\nACM Computing Surveys, Under Review. July 2023.\n<div style=\"text-align: center;\">Table 2. Terminology of Approximate Computing.</div>\nTerm\nDescription\nError-Resilient Application\nThe application that allows computation errors and accepts results of lower quality.\nQuality of Service\nThe quality of the results in terms of errors and accuracy.\nAccuracy Constraint\nThe quality requirements that the results need to satisfy.\nError Bound/Threshold\nThe maximum error allowed in the results.\nGolden Result\nThe result that is obtained from the original accurate computations.\nAcceptable Result\nThe result that satisfies the accuracy constraints and error bounds of the application.\nVariable Accuracy\nThe capability of providing different levels of accuracy.\nNon-Critical Task/Computation\nThe task/computation that can be safely approximated due to its small impact on\nthe quality of the output results.\nError Analysis\nThe study involving metrics, mathematics and simulations to examine the range,\nfrequency, scaling, and/or propagation of the errors.\nApproximation Technique/Method\nThe systematic and disciplined approach to insert computation errors in exchange\nfor gains in power, energy, area, latency, and/or throughput.\nApproximation Degree/Strength\nThe aggressiveness of the approximation technique in terms of errors inserted and\ntasks/computations approximated.\nApproximation Configuration\nAn instance of the parameters and settings of the approximation technique.\nFrozen Approximation\nThe approximation that is fixed and cannot be re-configured at a different degree.\nDynamic Approximation Tuning\nThe capability of adjusting the approximation degree at runtime to satisfy the desired\nerror constraints.\nCross-Layer Approximation\nThe approximation that is applied at multiple design abstraction layers (software,\nhardware, architecture).\nHeterogeneous Approximation\nThe approximation that applies concurrently multiple configurations of different\ndegree within the same system.\nApplication-Driven Approximation\nThe approximation that is applied with respect to the error resilience and sensitivity\nof the targeted application.\nDevice-Driven Approximation\nThe approximation that is applied with respect to the targeted device/technology\n(e.g., CPU, GPU, ASIC, FPGA).\nApproximate Space Exploration\nThe study involving error analysis and resource gain quantification to examine\ntrade-offs and select the most suitable approximation techniques/configurations.\nApproximation Localization\nThe systematic approach to locate the tasks/computations and design regions that\nare offered for approximation.\nError Modeling\nThe process of emulating the errors inserted by the approximations.\nError Prediction\nThe process of predicting errors before computing the final result.\nError Detection\nThe process of identifying an error occurrence.\nError Compensation\nThe process of modifying the erroneous result to reduce the error.\nError Correction\nThe process of replacing the erroneous result with the accurate one.\nWorkload Analysis: Not all tasks/computations in an application can be approximated. Therefore, it is important to identify the non-critical tasks/computations (to be approximated) and isolate them from the critical ones. This is essential to enable disproportionate benefits, i.e., significant improvements in efficiency for a negligible loss in quality. Development of Approximation Methodology: To achieve ultra-high efficiency gains while ensuring that acceptable quality is maintained, approximations are required to be introduced systematically in the system. Moreover, the sources of disproportionate benefits are distributed across different layers of the computing stack. Therefore, to achieve a superior quality\u2013efficiency trade-off, the\ndevelopment of sophisticated methodologies that can exploit the cross-layer knowledge of the system and deploy approximations systematically across various layers and sub-systems of the given system becomes an important step. Development of Error Models: Error estimation is vital for comparing different approximations. However, empirical evaluation of approximate implementations is time-consuming and costly (especially when inducing approximations at multiple design layers or at low-level hardware implementations). Therefore, it is essential to build models that can emulate the errors and examine the output quality of the system when approximated. Design Space Exploration: Typically, various types of approximations can be deployed in a system, where each sub-system/system module may have a completely different set of approximations that lead to disproportionate benefits. Therefore, design space exploration is performed to examine different approximation configurations, evaluate the approximation space and make decisions regarding the final approximate implementation that yields significant efficiency gains while meeting user-defined quality and performance constraints. These exploration methodologies are usually supported by error and performance models to efficiently search the approximation space. Error Analysis: Input distribution can have a profound impact on the resilience of the approximated system. Therefore, it is important to study the errors for different input distributions using appropriate error/QoS metrics. Quantification of Results: This is performed to prove the resilience of the application and ensure that constraints about QoS and/or resources are met.\n# 4 SOFTWARE APPROXIMATION TECHNIQUES\nIn this section, we classify and present approximation techniques that are applied at software level, i.e., the higher level of the design abstraction hierarchy. The goal of software Approximate Computing is to improve the execution time of the program and/or the energy consumption of the system. The techniques of the literature, illustrated in Figure 3, can be categorized into five classes: (i) Selective Task Skipping, (ii) Approximate Memoization, (iii) Relaxed Synchronization, (iv) Precision Scaling, (v) Data Sampling, and (vi) Approximate Programming Languages. Typical software approximation techniques integrate some of the following features: approximation libraries/frameworks, compiler extensions, accuracy tuning tools, runtime systems, and language annotations. Moreover, numerous of these techniques allow the programmer to specify QoS constraints, provide approximate code variants, and mark the program regions/tasks for approximation. The remainder of this section reports representative state-of-the-art works for software approximation techniques. The references of these works are summarized in Table 3. The literature also includes software approximation frameworks, such as ACCEPT [165] and OPPROX [126], which apply multiple state-of-the-art approximation techniques.\n# 4.1 Selective Task Skipping\n4.1.1 Loop Perforation. The loop perforation technique aims at skipping some of the loop iterations in a software program to provide performance/energy gains in exchange for QoS loss. Subsequently, we present several relevant works [15, 63, 80, 99, 122, 135, 177, 178, 184] involving design space exploration on loop perforation with programming frameworks and profiling tools. Starting with one of the first state-of-the-art works, the SpeedPress compiler [63] supports a wide range of loop perforation types, i.e., modulo, truncation, and randomized. It takes as input the original source code, a set of representative inputs, as well as a programmer-defined QoS acceptability model, and outputs a loop perforated binary. In the same context, Misailovic et al. [122] propose a QoS profiler to identify computations that can be approximated via loop perforation. The proposed profiling tool searches the space of loop perforation and generates results for multiple\nACM Computing Surveys, Under Review. July 2023.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f244/f244726f-f960-4ce2-9766-27ad392b3740.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig. 3. Classification of software approximation techniques in six main classes: Selective Task Skipping, Approx mate Memoization, Relaxed Synchronization, Precision Scaling, Data Sampling, and Approximate Programming anguages.</div>\n<div style=\"text-align: center;\">ig. 3. Classification of software approximation techniques in six main classes: Selective Task Skipping, Approxmate Memoization, Relaxed Synchronization, Precision Scaling, Data Sampling, and Approximate Programming anguages.</div>\n<div style=\"text-align: center;\">Table 3. Classification of software approximation techniques.</div>\nSW Approximation Class\nReferences\nLoop Perforation\n[15, 63, 80, 99, 122, 135, 177, 178, 184]\nComputation Skipping\n[6, 21, 101, 148, 153, 154, 194, 195]\nMemory Access Skipping\n[82, 85, 119, 163, 210, 219]\nApproximate Memoization\n[8, 19, 28, 83, 110, 124, 149, 162, 187, 217]\nRelaxed Synchronization\n[22, 116, 121, 123, 152, 155, 180, 182]\nPrecision Scaling\n[20, 36, 37, 44, 45, 57, 81, 88\u201390, 102, 117, 157, 158, 185, 212]\nData Sampling\n[3, 9, 55, 64, 79, 86, 91, 139, 144, 145, 205, 220]\nApprox. Programming Languages\n[1, 11, 14, 17, 18, 23, 24, 50, 56, 76, 105, 114, 120, 137, 138, 166, 167, 179, 186]\nperforation configurations. In [178], the same authors propose a methodology to exclude critical loops, i.e., whose skipping results in unacceptable QoS, and perform exhaustive and greedy design space explorations to find the Pareto-optimal perforation configurations for a given QoS constraint. In [177], the authors propose an architecture that employs a profiler to identify non-critical loops towards their perforation. To protect code segments that can be affected by the perforated loops, the architecture is equipped with HaRE, i.e., a hardware resilience mechanism. Another interesting work is GraphTune [135], which is an input-aware loop perforation scheme for graph algorithms. This approach analyzes the input dependence of graphs to build a predictive model that finds near-optimal perforation configurations for a given accuracy constraint. Li et al. [99] propose a compiling & profiling system, called Sculptor, to improve the conventional loop perforation, which skips a static subset of iterations. More specifically, Sculptor dynamically skips a subset of the loop instructions (and not entire iterations) that do not affect the output accuracy. More recently, the authors of [15] develop LEXACT, which is a tool for identifying non-critical code segments and monitoring the QoS of the program. LEXACT searches the loop perforation space, trying to find perforation configurations that satisfy pre-defined metrics. The loop perforation technique has been also used in approximation frameworks for heterogeneous multi-core systems combining various approximation mechanisms. Tan et al. [184] propose\na task scheduling algorithm that employs multiple approximate versions of the tasks with loops perforated. Kanduri et al. [80] target applications whose main computations are continuously repeated and tune the loop perforation at runtime.\n4.1.2 Computation Skipping. This technique omits the execution of blocks of codes according to the acceptable QoS loss, programmer-defined constraints, and/or runtime predictions regarding the output accuracy [6, 21, 101, 115, 148, 153, 154, 194, 195]. Compared to loop perforation, these techniques do not focus only on skipping loop iterations, but also skip higher-level computations/tasks e.g., an entire convolution operation. Most of the state-of-the-art works perform application-specific computation skipping. Meng et al. [115] introduce a parallel template to develop approximate programs for iterativeconvergence recognition & mining algorithms. The proposed programming template provides several strategies (implemented as libraries) for task dropping, such as convergence-based computation pruning, computation grouping in stages, and early termination of iterations. Another interesting work involving application-specific computation skipping is presented in [21]. The authors of this work study the error tolerance of the supervised semantic indexing algorithm to make approximation decisions. Regarding their task dropping approach, they choose to omit the processing of common words (e.g., \u201cthe\u201d, \u201cand\u201d) after the initial iterations, as these computations have negligible impact on the output accuracy. The authors of [148] propose two techniques to find computations with low impact on the QoS of the Reduce-and-Rank computation pattern, targeting to approximate or skip them completely. To identify these computations, the first technique uses intermediate reduction results and ranks, while the second one is based on the spatial or temporal correlation of the input data (e.g., adjacent image pixels or successive video frames). Similarly to the other state-of-the-art works, Vassiliadis et al. [194, 195] propose a programming environment that skips (or approximates) computations according to programmer-defined QoS constraints. More specifically, the programmer expresses the significance of the tasks using pragmas directives, optionally provides approximate variants of tasks, and specifies the desired task percentage to be executed accurately. Based on these constraints, the proposed system makes decisions at runtime regarding the approximation/skipping of the less significant tasks. Rinard [153] builds probabilistic distortion models based on linear regression to study the impact of computation skipping on the output accuracy. In particular, the programmer partitions the computations into tasks, which are then marked as \u201ccritical\u201d or \u201cskippable\u201d through random skip executions. The probabilistic models estimate the output distortion as function of the skip rates of the skippable tasks. This approach is also applied in parallel programs [154], where probabilistic distortion models are employed to tune the early phase termination at barrier synchronization points, targeting to keep all the parallel cores busy. Significant research has been also conducted on skipping the computations of Convolutional Neural Networks (CNNs). Lin et al. [101] introduce PredictiveNet to predict the sparse outputs of the nonlinear layers and skip a large subset of convolutions at runtime. The proposed technique, which does not require any modification in the original CNN structure, examines the most-significant part of the convolution to predict if the nonlinear layer output is zero, and then decides whether to skip the remaining least-significant part computations or not. In the same context, Akhlaghi et al. [6] propose SnaPEA, exploiting the convolution\u2013activation algorithmic chain in CNNs (activation inputs the convolution result and outputs zero if it is negative). This technique early predicts negative convolution results, based on static re-ordering of the weights and monitoring of the partial sums\u2019 sign bit, in order to skip the rest computations.\nACM Computing Surveys, Under Review. July 2023.\n4.1.3 Memory Access Skipping. Another approach to improve the execution time and energy consumption at the software level is the memory access skipping. Such techniques [82, 85, 119, 163, 210, 219] aim at avoiding high-latency memory operations, while they inherently reduce the number of computations. Miguel et al. exploit the approximate data locality [119] to skip the required memory accesses due to L1 cache miss. In particular, they employ a load value approximator, which learns value patterns using a global history buffer and an approximator table, to estimate the memory data values. RFVP [210] uses value prediction instead of memory accessing. When selected load operations miss in the cache memory, RFVP predicts the requested vales without checking for misprediction or recovering the values. As a result, timing overheads from pipeline flushes and re-executions are avoided. Furthermore, a tunable rate of cache misses is dropped after the value prediction to eliminate long memory stalls. Similarly, the authors of [85] propose a framework that skips costly last-level cache misses according to a programmer-defined error constraint and an heuristic predicting skipped data. To improve the performance of CUDA kernels on GPUs, Samadi et al. [163] propose a runtime approximation framework, called SAGE, which focuses on optimizing the memory operations among other functionalities. The approximations lie in skipping selective atomic operations (used by kernels to write shared variables) to avoid conflicts leading to performance decrease. Furthermore, SAGE reduces the number of memory accesses by packing the read-only input arrays, and thus, allowing to access more data with fewer requests. Karakoy et al. [82] propose a slicing-based approach to identify data (memory) accesses that can be skipped to deliver energy/performance gains within an acceptable error bound. The proposed method applies backward and forward code slicing to estimate the gains from skipping each output data. Moreover, the \u20180\u2019 value is used for each data access that is not performed. The ApproxANN framework [219], besides performing approximate computations, skips memory accesses on neural networks according to the neuron criticality. More specifically, a theoretical analysis is adopted to study the impact of neurons on the output accuracy and characterize their criticality. The neuron approximation under a given QoS constraint is tuned by an iterative algorithm, which applies the approximations and updates the criticality of each neuron (it may change due to approximations in other neurons).\n# 4.2 Approximate Memoization\nThe memoization technique stores results of previous calculations or pre-computed values in memory to use them instead of performing calculations. Namely, this memory functions as a look-up table that maps a set of data identifiers to a set of stored data. Subsequently, we focus on approximate memoization techniques [19, 28, 83, 124, 162, 187] relying on software frameworks, compilers and programmer\u2019s decisions. Nevertheless, we note that there are also approaches [8, 110, 149, 217] requiring hardware modification to support memoization. Chadhuri et al. [28] propose an approximate memoization for computations in loops. Prior to executing an expensive function within a loop, this technique checks a look-up table to find if this computation was previously performed for similar input data. In this case, the cached result is used, otherwise, the function is executed and the new computation is stored in the look-up table. Paraprox [162] is a software framework for identifying common patterns in data-parallel programs and applying tailored approximations to them. For the Map & Scatter/Gather patterns, Paraprox uses memoization rather than performing computations. In particular, it fills a look-up table with pre-computed data, which are obtained from the execution of the Map & Scatter/Gather function for some representative inputs, and performs runtime look-up table queries instead of the conventional computations.\niACT [124] is another approximation framework that applies runtime memoization among other functionalities. The programmer uses pragmas to declare the functions for memoization and specify the error tolerance percentage. For each function call-site, the framework creates a global table to store pairs of function arguments and output results. In case the function arguments are already stored in the table (within an error bound), the corresponding output results are returned. Otherwise, the function is accurately executed and the new input\u2013output pairs are stored in the table. The ATM approach [19] performs runtime task memoization, relying on hashing functions to store the task inputs and an adaptive algorithm to automatically decide whether to use memoization or execute the task. The programmer needs to use pragmas to specify the tasks that are suitable for memoization. The authors of [83] introduce an approximate memoization mechanism for GPU fragment shading operations, which reduces the precision of the input parameters and performs partial matches. To identify approximate memoization opportunities, they characterize various fragment shader instructions in terms of memoization hits and output accuracy. Moreover, runtime policies are proposed to tune the precision according to the errors introduced. Contrary to the aforementioned techniques, TAF-Memo [187] is an output-based function memoization technique, i.e., it memoizes function calls based on their output history. TAF-Memo checks for temporal locality by calculating the relative arithmetic difference of two consecutive output values from the same function call-site. In case this difference is below the acceptable error constraint, memoization is applied by returning the last computed output for the following function calls.\n# 4.3 Relaxed Synchronization\nThe execution of parallel applications on multi/many-core systems requires time-consuming synchronization to either access shared data or satisfy data dependencies. As a result, various techniques [22, 116, 121, 123, 152, 155, 180, 182] have been proposed to relax the conventional synchronization requirements that guarantee error-free execution, delivering performance gains in exchange for QoS loss. The authors of [152] propose the RaC methodology to systematically relax synchronization, while always satisfying a programmer-defined QoS constraint. Initially, the programmer specifies the parallel code segments, and then applies the four-step RaC methodology. This methodology identifies criteria for quantifying the acceptable QoS, selects the relaxation points, modifies the code to enable the execution of both the original and relaxed versions, and selects the suitable relaxation degree (i.e., which instances to relax for each synchronization point). Misailovic et al. [123] propose the Dubstep system, which relaxes the synchronization of parallelized programs based on a \u201cfindtransform-navigate\u201d approach. More specifically, Dubstep performs a profiling-based analysis of the original program to find possible optimizations, inserts opportunistic synchronization and barriers, and finally, performs an exploration including accuracy, performance and safety analysis for the transformed program. QuickStep [121] is a system for approximately parallelizing sequential programs, i.e., without preserving the semantics of the original program, within statistical accuracy bounds. Among other transformations, QuickStep replicates shared objects to eliminate the bottlenecks of synchronized operations on them. HELIX-UP [22] is another parallelizing compiler that selectively relaxes strict adherence to program semantics to tackle runtime performance bottlenecks, involving profiling and user interaction to tune QoS. The compiler also offers a synchronization-relaxing knob to decrease the inter-core communication overhead by synchronizing sequential segments with prior iterations. More recently, the authors of [182] introduce PANDORA, which is an approximate parallelizing framework based on symbolic regression machine learning and sampled outputs of the original function. To avoid timing bottlenecks, such as data movement and synchronization, and improve parallelism, PANDORA eliminates loop-carried dependencies using fitness functions and constraints\nACM Computing Surveys, Under Review. July 2023.\nregarding error and performance. In [180], the authors exploit the concept of approximate shared value locality to reduce synchronization conflicts in programs using optimistic synchronization. The reduction of conflicts on approximately local variables, detected for a given similarity constraint, is achieved through an arbitration mechanism that imprecisely shares the values between threads. The authors of [116] apply aggressive coarse-grained parallelism on recognition & mining algorithms by relaxing or even ignoring data dependencies between different iterations. As a result, the timing overheads are reduced in comparison with the conventional parallel implementation, which also applies parallelization only within the iteration (iterations are executed serially). Rinard [155] introduces synchronization-free updates to shared data structures by eliminating the conventional use of mutual exclusion and dropping array elements at the worst scenario. Moreover, the same work applies relaxed barrier synchronization, allowing the threads to pass the barrier without stalling to wait for the other threads.\n# 4.4 Precision Scaling\nPrecision scaling (tuning) refers to the discipline reduction of the accurate numerical precision, resulting in improved calculation speed and/or memory bandwidth [35]. The state-of-the-art software-level works [20, 36, 37, 44, 45, 57, 81, 88\u201390, 102, 117, 157, 158, 185, 212] address several challenges, such as the scaling degree, scaling automation, mixed precision, and dynamic scaling. Starting with works based on formal methods to reduce the precision and examine the errors, the Gappa tool [45] automates the study of rounding errors in elementary functions and floatingpoint calculations using interval arithmetic. An extended version of this tool is Gappa++ [102], which provides automated analysis of numerical errors in a wide range of computations, i.e., fixedpoint, floating-point, linear and non-linear. This tool integrates several features, such as operation rewriting to facilitate the isolation of rounding errors, and affine arithmetic to accurately bound linear calculations with correlated errors. FPTuner [36] is a tool that performs formal error analysis based on symbolic Taylor expansions and quadratically constrained quadratic programming. It searches for precision allocations that satisfy constraints such as the number of operators at a given precision and the number of type casts. Rosa [44] is a source-to-source compiler that combines satisfiability modulo theories with interval arithmetic to bound the round-off errors of the fixedand floating-point formats. Several works of the literature employ heuristics and automated search to scale the precision of floating-point programs. Precimonious [158] searches all the program variables in their order of declaration using the delta-debugging algorithm, and lowers their precision according to an error constraint specified by the programmer. In the same context, HiFPTuner [57] firstly groups dependent variables that may require the same precision, and then performs a customized hierarchical search. Lam et al. [90] introduce a framework that employs the breadth-first search algorithm to identify code regions that can tolerate lower precision. Similarly to this technique, CRAFT [89] performs binary searches to initially determine the required program precision, and then truncate the results of some of the floating-point instructions. Towards the detection of large floating-point errors, the authors of [37] propose S3FP. This tool is based on an heuristic-guided search to find the inputs causing the largest errors. The Blame Analysis [157] combines concrete and shadow execution to generate a blame set for the program instructions, which contains the minimum precision requirements under a giver error constraint. This approach can be also used in cooperation with the previous search-based works, and specifically, as pre-processing to reduce the search space. Schkufza et al. [170] treat the scaling of floating-point precision as a stochastic search problem. In particular, they repeatedly apply random program transformations and use a robust search to guarantee the maximum errors.\nACM Computing Surveys, Under Review. July 2023.\nThe concept of dynamic precision scaling, i.e., the tuning the precision at runtime with respect to the input data and error sensitivity, has been studied in [212]. The dynamic scaling framework of this work integrates an offline application profiler, a runtime monitor to track workload changes, and an accuracy controller to adjust the precision accordingly. ApproxMA [185] dynamically scales the precision of data memory accesses in algorithms such as mixture model-based clustering. This framework integrates a runtime precision controller, which generates custom bit-widths according to the QoS constraints, and a memory access controller, which loads the scaled data from memory. The custom bit-widths are generated by analyzing a subset of data and intermediate results and calculating metrics regarding the error appearance and the number of tolerable errors. Mixed floating-point precision has been also studied in high-performance computing workloads. ADAPT [117] uses algorithmic differentiation, i.e., a technique for numerically evaluating the derivative of a function corresponding to the program, to estimate the output error of high-performance computing workloads. It provides a precision sensitivity profile to guide the development of mixedprecision programs. In the same context, the authors of [20] propose an instruction-based search that explores information about the dynamic program behaviour and the temporal locality. To enable mixed floating-point precision in GPUs, the authors of [88] propose the GPUMixer tool, which relies on static analysis to find code regions where precision scaling improves the performance. Next, GPUmixer performs a dynamic analysis involving shadow computations to examine if the scaled program configurations satisfy the accuracy constraints. In the same context, PreScaler [81] is an automatic framework that generates precision-scaled OpenCL programs, considering both kernel execution and data transfer. Initially, it employs a system inspector to collect information about precision scaling on the target platform, and an application profiler to identify memory objects with floating-point elements for potential scaling. This information is exploited by a decision maker that finds the best scaling configuration using decision tree search on a minimized space.\n# 4.5 Data Sampling\nApproximate Computing is also exploited in big data analysis, in an effort to reduce the increased number of computations and storage requirements due to the large amount of input data. The key idea is to perform computations on a representative data sample rather than on the entire input dataset. Therefore, various data sampling methods [3, 9, 55, 64, 79, 86, 91, 139, 144, 145, 205, 220] are examined to provide real-time processing with error bounds in applications involving stream analytics, database search, and model training. EARL [91] is an extension of Hadoop (i.e., a software framework that provides distributed storage and big data processing on clusters), which delivers early results with reliable error bounds. It applies statistics-based uniform sampling from distributed files. Goiri et al. [55] propose the ApproxHadoop framework to generate approximate MapReduce programs based on task dropping and multi-stage input sampling. They also bound the errors using statistical theories. The programmer tunes the approximation by specifying either the desired error bound or the task dropping and input sampling ratios. Similarly, ApproxSpark [64] performs sampling at multiple arbitrary points of long chains of transformations to facilitate the aggregation of huge amounts of data. This framework models the clustering information of transformations as a data provenance tree, and then computes the approximate aggregate values as well as error thresholds. Moreover, the sampling rates are dynamically selected according to programmer-specified error thresholds. Sampling methods have been also examined in stream analytics. StreamApprox [145] is an approximate stream analytics system that supports both batched and pipelined stream processing. It employs two sampling techniques, i.e., stratified and reservoir sampling, to approximate the outputs with rigorous error bounds. IncApprox [86] combines approximate and incremental\ncomputations to provide stream analytics with bounded error. This system executes a stratified sampling algorithm that selects data for which the results have been memoized from previous runs, and adjusts the computations to produce an incrementally updated output. On the other hand, PrivApprox [144] combines sampling and randomized response to provide both approximate computations and privacy guarantee. This system integrates a query execution interface that enables the systematic exploration of the trade-off between accuracy and query budget. ApproxIoT [205] facilitates approximate stream analytics at the edge by combining stratified reservoir sampling and hierarchical processing. A variety of sampling methods have been employed in approximate query processing systems for databases. BlinkDB [3] performs approximate distributed query processing, supporting SQLbased aggregation queries with time and error constraints. It creates stratified samples based on past queries, and uses an heuristic-based profiler to dynamically select the sample that meets the query\u2019s constraints. Another system applying approximate big-data queries is Quickr [79], which integrates operators sampling multiple join inputs into a query optimizer, and then searches for an appropriate sampled query plan. Sapprox [220] is a distribution-aware system that employs the occurrences of sub-datasets to drive the online sampling. In particular, the exponential number of sub-datasets is reduced to a linear one using a probabilistic map, and then, cluster sampling with unequal probability theory is applied for sub-dataset sampling. Sapprox also determines the optimal sampling unit size in relation with approximation costs and accuracy. Numerous works of the literature use data sampling to decrease the increased computational cost of model training in machine learning applications. Zombie [9] is a two-stage system that trains approximate models based on clustering and active learning. The first stage applies offline indexing to organize the dataset into index groups of similar elements. Subsequently, the stage of online querying uses the index groups that are likely to output useful features to creates the training subset of data. BlinkML [139] approximately trains a model on a small sample, while providing accuracy guarantees. The sample is obtained through uniform random sampling, however, in case of very large datasets, a memory-efficient algorithm is employed.\n# 4.6 Approximate Programming Languages\nThe high-level approximation of software programs has been examined through approximate programming languages, i.e., language extensions that allow the programmer to systematically declare approximate code regions, variables, loops, and functions, insert randomness in the program, and/or specify error constraints. The literature involves numerous works [1, 11, 14, 17, 18, 23, 24, 50, 56, 76, 105, 114, 120, 137, 138, 166, 167, 179, 186] that enable approximate procedural, object-oriented, and probabilistic programming. Ansel et al. [11] introduce a set of PetaBricks language extensions that allow the programmer to write code of variable accuracy. These extensions expose the performance\u2013accuracy trade-off to the compiler, which automatically searches the algorithmic space to tune the program according to the programmer\u2019s accuracy constraints. Eon [179] is a programming language that allows the programmer to annotate program flows (paths) with different energy states. The Eon runtime system predicts the workload and energy of the system, and then adjusts the execution of flows according to the programmer\u2019s declarations and the energy constraints. In the same context, Baek and Chilimbi [14] propose Green, which is a two-phase programming framework providing language extensions to approximate expensive functions and loops. The programmer uses pragma-like annotations to specify approximate variants of functions. In the calibration phase, Green builds a model to quantify the QoS loss and the performance/energy gains. This model is then used in the operational phase to generate an approximate program satisfying the programmer\u2019s QoS constraint. DECAF [18] is a type-based approximate programming language that allows the programmer to specify the\ncorrectness probability for some of the program variables. The DECAF type system also integrates solver-aided type inference to automatically tune the type of the rest variables, code specialization, and dynamic typing. Flikker [105] provides language annotations to mark the program variables and partition the data into critical and non-critical regions (the latter are stored in unreliable memories). Topaz [1] is a task-based language that maps tasks onto approximate hardware and uses an outlier detector to find and re-execute the computations producing unacceptable results. In [23], the authors introduce language constructs for generating approximate programs and proof rules for verifying the acceptability properties. Rely [24] is an imperative language that allows the programmer to introduce quantitative reliability specifications for generating programs with data stored in approximate memory and inexact arithmetic/logical operations. Chisel [120] automates the selection of Rely\u2019s approximations while satisfying the programmer-defined reliability and accuracy constraints. To solve this optimization problem, Chisel employs an integer programming solver. All these works include safety analysis and program verification for sequential programs. In contrast, Parallely [50] is a programming language for approximating parallel programs through canonical sequentialization, i.e., a verification method that generates sequential programs capturing the semantics of parallel programs. Targeting approximations in Java programs, the authors of [166] propose EnerJ, i.e., a language extension providing type qualifiers to specify data that can be approximately stored or computed. EnerJ guarantees isolation of the approximate computations. FlexJava [137] offers another set of language extensions to annotate approximate programs. Using an approximation safety analysis, FlexJava automates the approximation of data and operations while ensuring safety guarantees. ExpAX [138] allows the programmer to explicitly specify error expectations for a subset of Java. Based on an approximation safety analysis, it identifies operations that are candidate for approximation, and then, an heuristic-based framework approximates those that statistically satisfy the error expectations. Significant research has also been conducted on probabilistic programming languages. Church [56] is a probabilistic language that inserts randomness on a deterministic function subset using stochastic functions. The Church semantics are defined in terms of evaluation histories and conditional distributions on the latter. Similarly, Venture [114] is another language that enables the specification of probabilistic models and inference problems. The Anglican [186] language and runtime system provides probabilistic evaluation model and functional representations, e.g., distributions and sequences of random variables. Uncertain<T> [17] is a language abstraction that manipulates data as probability distributions. More specifically, random variables are declared as \u201cuncertain\u201d and a Bayesian network for representing computations is build, where nodes correspond to the variables and edges correspond to conditional variable dependencies. The Uncertain<T> runtime system performs hypothesis tests and sampling to evaluate the network. In the same context, Sampson et al. [167] use probabilistic assertions on random variables. Their tool, called MayHap, performs probabilistic evaluation by statically building a Bayesian representation network based on the input distribution and dynamically interpreting it via sampling. In the same context, AxProf [76] is a profiling-based framework for analyzing randomized approximate programs. The programmer specifies probabilistic predicates for the output, i.e., regarding the expectation of the output value and/or the probability that the output satisfies a condition, and AxProf generates approximate programs based on statistical tests.\n# 5 HARDWARE APPROXIMATION TECHNIQUES\nIn this section, we classify and introduce the hardware approximation techniques, which are applied at the lower level of the design abstraction hierarchy. These techniques aim to improve the area power consumption, and performance of the circuits i.e., the basic building blocks of accelerators\nACM Computing Surveys, Under Review. July 2023.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b190/b1908296-a612-4297-800e-91e1daa5a5f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4. Classification of hardware approximation techniques in three main classes: Circuit Functional Approx mation, Voltage Over-Scaling and Over-Clocking.</div>\nprocessors, and computing platforms. The hardware approximation techniques can be categorized into three classes: (i) Circuit Functional Approximation (CFA), (ii) Voltage Over-Scaling (VOS), and (iii) Over-Clocking (OC). In approximate hardware, we distinguish two types of errors, i.e., the functional errors (produced by CFA) and the timing errors (produced by VOS and OC). Figure 4 illustrates the hardware approximation techniques, including a further taxonomy to sub-classes. In the remainder of this section, we present state-of-the-art works, organized according to the proposed classification of Table 4. We note that, even though some works may belong to more than one sub-class, we opt to assign them to their prevalent one and highlight their relevant features.\n# 5.1 Circuit Functional Approximation\nCircuit functional approximation modifies the original accurate design by reducing its circuit complexity at logic level. Typical CFA approaches include: (i) the modification of the circuit\u2019s truth table, (ii) the use of an approximate version of the initial hardware algorithm, (iii) the use of small inexact components as building blocks, and (iv) approximate circuit synthesis. The main target of CFA is the arithmetic circuits [71], as they constitute the key processing units of processors and accelerators, and thus, they inherently affect their power efficiency and performance. Interestingly, the literature provides several open-source libraries of approximate arithmetic circuits, such as ApproxAdderLib [173], EvoApprox8b [132] and SMApproxLib [188]. In this survey, we focus on approximate adders, multipliers, and dividers. However, we note that numerous works design and evaluate other approximate arithmetic operations, such as circuits for Multiplicationand-Accumulation (MAC) [30, 54], square root [70], squaring [113], square-accumulate [53], and Coordinate Rotation Digital Computer (CORDIC) [33]. The literature also includes automated methods for generating approximate circuits, which are presented in the context of approximate logic synthesis. Moreover, there are works applying functional approximations based on the inputs such as [111], where the authors configure and assign different approximation operators for a given data flow program based on the input workload. It is also important to mention that methodologies for digital hardware design based on approximate arithmetic circuits and formats have been proposed [96].\nACM Computing Surveys, Under Review. July 2023.\nHW Approximation Class\nTechnique/Approach\nAdder Approximation\nUse of Approximate Full Adder Cells [42, 58, 140, 208]\nSegmentation and Carry Prediction [5, 47, 65, 77, 84, 173, 207, 211]\nMultiplier Approximation\nTruncation and Rounding [51, 60, 93, 95, 98, 189, 213]\nApproximate Radix Encodings [69, 97, 107, 196, 203, 204, 221]\nUse of Approximate Compressors [4, 49, 129, 161, 183]\nLogarithmic Approximation [10, 108, 142, 159]\nDivider Approximation\nBit-width Scaling [61, 70]\nUse of Approximate Adder/Subtractor Cells [2, 31, 32, 34]\nSimplification of Computations [66, 106, 160, 190, 214]\nApproximate Synthesis\nStructural Netlist Transformation [25, 103, 169, 198]\nBoolean Rewriting [62, 118, 151, 199]\nHigh-Level Approximate Description [26, 92, 133, 134, 209]\nEvolutionary Synthesis [132, 172, 191\u2013193]\nVoltage Over-Scaling\nSlack Re-distribution [78]\nCircuit Re-design and Architecture Modification [29, 128, 218]\nFine-Grained Scaling [136, 201, 216]\nError Modeling [68, 74, 109, 146, 215]\nOver-Clocking\nTight Synthesis [7]\nCircuit Re-design and Architecture Modification [150, 176, 202]\nError Detection & Correction [39, 100, 147]\nError Prediction [40, 72, 73, 75, 112, 156]\n5.1.1 Approximate Adders. Significant research has been conducted on the design of approximate area- and power-efficient adders. The approximation techniques for inexact adders involve: (i) use of approximate full adder cells [42, 58, 140, 208] and (ii) segmentation and carry prediction [5, 47, 65, 77, 84, 173, 207, 211]. In the following, we present representative state-of-the-art works with approximate adders. The IMPACT adders are based on inexact full adders cells, which are approximated at the transistor level to deliver up to 45% area reduction [58]. Another transistor-level cell approximation is proposed in [208], where the AXA 4-transistor XOR/XNOR-based adders are implemented, delivering up to 31% gain in dynamic power consumption. Moreover, in [140], approximate reverse carry propagate full adders are used to build the hybrid RCPA adders. Targeting higher level approximations, the OLOCA adder splits the addition into accurate and approximate segments [42], and for the latter, it employs OR gates for the most-significant bit additions and outputs constant \u20181\u2019 for the least-significant ones. To reduce the worst-case carry propagation delay, Kim et al. [84] propose a carry prediction scheme leveraging the less-significant input bits, which is 2.4\u00d7 faster than the conventional ripplecarry adder. Similarly, Hu et al. [65] introduce a carry speculating method to segment the carry chain in their design, which also performs error and sign correction. Compared to the accurate adder, the proposed design is 4.3\u00d7 faster and saves 47% power. The quality constraint of applications may vary during runtime, thus, research efforts have also focused on designing dynamically configurable adders that can tune their accuracy. In [77], the authors propose an accuracy-configurable adder, called ACA, which consists of several subadders and an error detection & correction module. The accuracy is controlled at runtime, while operation in accurate mode is also supported. Another dynamically configurable adder, called\nGDA, is proposed in [211], where multiplexers select the carry input either from the previous sub-adder or the carry prediction unit, providing a more graceful degradation of the accuracy. In the same direction, the GeAr adder [173] employs multiple sub-adders of equal length to variable approximation modes. This architecture also supports accurate mode via a configurable error correction unit. Akbari et al. [5] introduce the RAP-CLA adder, which splits the conventional carry look-ahead scheme into two segments, i.e., the approximate part and the augmenting part, supporting approximate and accurate mode. When operating at the approximate mode, the augmenting part is power-gated to reduce power consumption. Another carry-prediction-based approach supporting both modes is the SARA design [207]. This adder uses carry ripple sub-adders, and the carry prediction does not require a dedicated circuitry. Finally, the BSCA adder, which is based on a block-based carry speculative approach [47], integrates an error recovery unit and non-overlapped blocks consisting of a sub-adder, a carry prediction unit, and a selection unit. 5.1.2 Approximate Multipliers. The multiplication circuits have attracted significant interest from the research community. The literature includes a plethora of inexact multipliers that can be categorized according to the prevailing approximation techniques: (i) truncation and rounding [51, 60, 93, 95, 98, 189, 213], (ii) approximate radix encodings [69, 97, 107, 196, 203, 204, 221], (iii) use of approximate compressors [4, 49, 129, 161, 183], and (iv) logarithmic approximation [10, 108, 142, 159]. Subsequently, we introduce the state-of-the-art works from each category. Starting with the rounding and truncation techniques, the DRUM multiplier [60] dynamically reduces the input bit-width, based on the leading \u20181\u2019 bits, to achieve 60% power gain in exchange for mean relative error of 1.47%. Zendegani et al. propose the RoBa multiplier [213], which rounds the operands to the nearest exponent-of-two and performs a shift-based multiplication in segments. In [98], the PR approximate multiplier perforates partial products and applies rounding to the remaining ones, delivering up to 69% energy gains. The same approximation technique is integrated in the mantissa multiplication of floating-point numbers to create the AFMU multiplier [95]. Vahdat et al. propose the TOSAM multiplier [189] that truncates the input operands according to their leading \u20181\u2019 bit. To decrease the error, the truncated values are rounded to the nearest odd number. In [93], different rounding, perforation and encoding schemes are combined to extract the most energy-efficient multiplication circuits. Finally, Frustaci et al. [51] implement an alternative dynamic truncation with correction, along with an efficient mapping for the remaining partial product bits. Next, we present multipliers generating their partial products based on approximate radix encodings. Liu et al. [107] modify the Karnaugh map of the radix-4 encoding to create approximate encoders for generating the least-significant partial product bits. A similar approach is followed in [196], where approximate radix-4 partial product generators are designed. Jiang et al. [69] use an approximate adder to generate the \u00b13\u00d7 multiplicand term in the radix-8 multiplier. In [203], the authors propose a hybrid low-radix encoding that encodes the most-significant bits with the accurate radix-4 encoding and the least-significant bits with the an approximate radix-8 encoding. A similar approach is used in [204], where the authors propose approximate radix-8 multipliers for FPGA-based design. Targeting to high-order radix, the authors of [97] propose the hybrid high-radix encoding, which applies both the accurate radix-4 and approximate radix-2\ud835\udc58encodings. Correspondingly, in [221], a radix-256 encoder is proposed for approximate multiplication circuits. Several works employ approximate compressors for the partial product accumulation. Momeni et al. [129] modify the truth table of the accurate 4:2 compressor to create two simplified designs and use them in the Dadda multiplier. The authors in [4] design 4:2 compressors, again for Dadda multipliers, which can switch between accurate and approximate mode at runtime, providing 68% lower power consumption. In [161], an approximate 4:2 compressor is implemented in FinFET\nbased on a three-input majority gate, and then it is used in the Dadda architecture along truncation. Esposito et al. [49] introduce a new family of approximate compressors and assign them to each column of the partial product matrix according to their allocation algorithm. Another interesting work is the design of approximate compressors for multipliers using the stacking circuit concept [183]. Regarding the approximate logarithmic multipliers, Liu et al. [108] employ a truncated binarylogarithm converter and inexact adders for the mantissa addition to design the ALM family of multipliers. The logarithmic-based REALM multiplier [159] partitions the power-of-two intervals of the input operands into segments, and determines an error compensation factor for each one. The ILM multiplier [10] differentiates from the conventional design, as it rounds the input operands to their nearest power-of-two using a nearest \u20181\u2019 bit detector. Pilipovic et al. [142] propose a two-stage trimming logarithmic multiplier, which firstly, reduces the bit-width of the input operands, and then, the bit-width of the mantissas. 5.1.3 Approximate Dividers. The division circuits have received less attention than adders and multipliers. Nevertheless, the literature provides numerous works aiming to reduce the large critical paths of the conventional dividers. The approximation techniques for division circuits can be categorized as follows: (i) bit-width scaling [61, 70], (ii) use of approximate adder/subtractor cells [2, 31, 32, 34], and (iii) simplification of computations [66, 106, 160, 190, 214]. The first class of approximation techniques uses exact dividers with reduced bit-width. The approximate divider of [61] dynamically selects the most relevant bits from the input operands and performs accurate division at lower bit-width, providing up to 70% power gains in exchange for 3% average error. The design makes use of leading \u20181\u2019 bit detectors, priority encoders, multiplexers, subtractor and barrel shifter. Similarly, the AAXD divider of [70] detects the leading \u20181\u2019 bits and uses a pruning scheme to extract the bits that will be given as input to the divider. Additionally, the design integrates an error correction unit to form the final output. Regarding the second class of approximation techniques, Chen et al. [31] perform the subtraction of the non-restoring array divider with inexact subtractor circuits employing pass transistor logic. For their divider, called AXDnr, the authors examine different schemes regarding which subtractions of the division array to approximate. Similarly, in the AXDr divider of [32], some of the subtractions of the restoring array divider are performed with inexact subtractor circuits. The use of inexact cells has also been examined in the high-radix SRT divider [34]. In this divider, called HR-AXD, the inexact cell is a signed-digit adder that is employed according to different replacement schemes, along with cell truncation and error compensation. Adams et al. [2] introduce two approximate division architectures, called AXRD-M1 and AXRD-M2, which deliver up to 46% area and 57% power gains, respectively, compared to the exact restoring divider. The first design replaces some of the restoring divider cells with inexact ones of simplified logic, while the second one involves the elimination of some rows of the divider. Targeting to perform the division with alternative simplified computations, the SEERAD divider [214] rounds the divisor to a specific form based on the leading \u20181\u2019 bit position, and thus, the division is transformed to shift-&-add multiplication. In the same context, Vahdat et al. [190] propose the TruncApp divider that multiplies the truncated dividend with the approximate inverse divisor. Targeting to model the division operation, the CADE divider of [66], performs the floating-point division by subtracting the input mantissas. To compensate a large error (estimated by analyzing the most-significant input bits), a pre-computed value is retrieved from memory. In [106], the proposed AXHD divider approximates the least-significant computations of the division using an non-iterative logarithmic approach that is based on leading \u20181\u2019 bit detection and subtraction of the logarithmic mantissas. Finally, Saadat et al. [160] propose approximate integer and floating-point\nbased on a three-input majority gate, and then it is used in the Dadda architecture along truncation. Esposito et al. [49] introduce a new family of approximate compressors and assign them to each column of the partial product matrix according to their allocation algorithm. Another interesting work is the design of approximate compressors for multipliers using the stacking circuit concept [183]. Regarding the approximate logarithmic multipliers, Liu et al. [108] employ a truncated binarylogarithm converter and inexact adders for the mantissa addition to design the ALM family of multipliers. The logarithmic-based REALM multiplier [159] partitions the power-of-two intervals of the input operands into segments, and determines an error compensation factor for each one. The ILM multiplier [10] differentiates from the conventional design, as it rounds the input operands to their nearest power-of-two using a nearest \u20181\u2019 bit detector. Pilipovic et al. [142] propose a two-stage trimming logarithmic multiplier, which firstly, reduces the bit-width of the input operands, and then, the bit-width of the mantissas.\ndividers with near-zero error bias, called INZeD and FaNZeD, respectively, by combining an erro correction method with the classical approximate logarithmic divider.\n5.1.4 Approximate Synthesis. An automated approach to generate inexact circuits is the approximate logic synthesis. This method provides increased approximation diversity, i.e., it generates multiple approximate circuit variants, without relying on the manual approximation inserted by the designer, such as in the case of the aforementioned arithmetic approximations. Another benefit of approximate synthesis is that several techniques generate the approximate variant that leads to the maximum hardware gains for a given approximation/error constraint. The state-of-the-art techniques can be categorized as follows [168]: (i) structural netlist transformation [25, 103, 169, 171, 198], (ii) Boolean rewriting [62, 118, 151, 199], (iii) high-level approximate description [26, 92, 133, 134, 209], and (iv) evolutionary synthesis [132, 172, 191\u2013193]. Several works of the literature employ a direct acyclic graph to represent the circuit netlist, where each node corresponds to a gate. In this context, the GLP technique [171] prunes nodes with an iterative greedy approach according to their impact on the final output and their toggle activity. In contrast, the CC framework [169] performs an exhaustive exploration of all possible node subsets that can be pruned without surpassing the error constraint. Venkataramani et al. [198] propose SASIMI, which is based on a greedy heuristic to find signal pairs assuming the same value and substitute one with the other. This automatic synthesis framework guarantees that the user-defined quality constraint is satisfied, and generates accuracy-configurable circuits. To apply stochastic netlist transformation, the SCALS framework [103] maps an initial gate-level network to the targeted technology (standard-cell or FPGA), and then iteratively extracts sets of sub-netlists and inserts random approximations in them. These sub-netlists are evaluated using statistical hypothesis testing. Castro-Codinez et al. [25] propose the AxLS framework, which converts the Verilog netlist to XML format and then applies typical transformation techniques, e.g., gate pruning, with respect to an error threshold. The second category includes techniques that apply approximations in a formal Boolean representation of the circuit before it is synthesized. The SALSA approach [199] encodes the error constraints into a quality logic function, which compares the outputs of the accurate and approximate circuits. Towards logic simplification, SALSA computes the \u201cobservability don\u2019t cares\u201d for each output of the approximate circuit, i.e., the set of input values for which the output is insensitive. In the same direction, but for sequential circuits, Ranjan et al. introduce ASLAN [151]. This framework generates several approximate variants of the combinational blocks, and then identifies the best approximations for the entire sequential circuit based on a gradient-descent approach. Miao et al. [118] use a two-phase Boolean minimization algorithm to address the problem of approximate synthesis. The first phase solves the problem under a given constraint for error magnitude, and the second phase iteratively finds a solution that also satisfies the error frequency constraint. In an iterative manner, the BLASYS methodology [62] partitions the circuit into smaller circuits, and for each one, it generates an approximate truth table based on Boolean matrix factorization. The approximate sub-circuits are synthesized and the trade-off between error and power/area efficiency for the entire circuit is evaluated. Regarding approximations introduced at the hardware description level, Yazdanbakhsh et al. [209] propose the Axilog language annotations, which provide syntax and semantics for approximate design and reuse in Verilog. Axilog allows the designer to partition the design into accurate and approximate segments. ABACUS [134] is another interesting work that parses the behavioral Verilog description of the design to create its abstract syntax tree. Next, a set of diverse transformations is applied to the tree to create approximate variants, which are then written in Verilog. An expanded version of ABACUS is introduced in [133], where sorting-based evolutionary\nalgorithms are employed for design space exploration. Moreover, the new ABACUS version focuses on approximations in critical paths to facilitate the reduction of the supply voltage. Lee et al. [92] generate approximate designs in Verilog from C accurate descriptions. The proposed framework computes data statistics and mobility information for the given design, and employs an heuristic solver for optimizing the energy\u2013quality trade-off. Targeting to high-level synthesis, the AxHLS approach [26] performs a design space exploration based on analytical models to identify the best arithmetic approximations for a given error constraint. Starting from a C description, AxHLS adopts scheduling and binding operations to apply the approximations provided by the exploration and generate the Verilog code. The fourth class of techniques for automated synthesis of approximate circuits is based on evolutionary algorithms, i.e., heuristic-based search algorithms that treat circuit approximation as multi-objective optimization problem and generate a set of solutions. In this context, Sekanina et al. [172] use Cartesian genetic programming to minimize the error in adders considering the number of logic gates as constraint. This approach is extended in [192], where approximate multipliers and median filters are evolved through randomly seeded Cartesian genetic programming. Based on the same utilities, the authors of [132] propose the EvoApprox8b library of approximate adders and multipliers. This library is generated by examining various trade-offs between accuracy and hardware efficiency, and offers different approximation variants and circuit architectures. In [193], a search-based technique for evolutionary circuit synthesis for FPGAs is proposed. In particular, this approach represents the circuit as a directed acyclic graph, and re-synthesizes approximate configurations based on Cartesian genetic programming. Vasicek et al. [191] adjust the approximation degree with respect to the significance of the inputs. To do so, they adopt a weighted error metric to determine the significance of each input vector and use Cartesian genetic programming to minimize the circuit\u2019s area while satisfying a threshold.\n# 5.2 Voltage Over-Scaling\nVoltage over-scaling aims to reduce the circuit\u2019s supply voltage below its nominal value, while keeping the clock frequency constant. The circuit operation at a lower voltage value produces timing errors due to the failure of the critical paths to meet the delay constraints. Nevertheless, considering that power consumption depends on the voltage value, VOS techniques are continously examined in the literature. An exploration and quantification of the benefits and overheads of VOS is presented in [87]. Research involving VOS can be classified in the following categories: (i) slack re-distribution [78], (ii) circuit re-design and architecture modification [29, 128, 218], (iii) fine-grained scaling [136, 201, 216], and (iv) error modeling [68, 74, 109, 146, 215]. Kahng et al. [78] shift the timing slack of the frequently executed near-critical paths through slack redistribution, and thus, reduce the minimum voltage at which the error rate remains acceptable. The proposed technique is based on post-layout cell resizing to deliver the switching activity-aware slack redistribution. More specifically, a heuristic finds the voltage satisfying the desired error rate, and then increases the transistor width of the cells to optimize the frequently executed paths. In [128], the authors optimize building blocks for more graceful degradation under VOS, using two techniques, i.e., dynamic segmentation & error compensation and delay budgeting of chained datapath. The first technique bit-slices the datapath of the adder and employs a multi-cycle error correction circuitry that tracks the carries. The second technique adds transparent latches between chained arithmetic units to distribute the clock period. To facilitate VOS, Chen et al. [29] build their designs on the residue number system, which provides shorter critical paths than conventional arithmetic. They also employ the reduced precision redundancy scheme to eliminate the timing errors. Another interesting work is Thundervolt [218], which provides error recovery in the MAC units of systolic arrays. To detect timing errors, Thundervolt employs Razor shadow flip-flops. In\ncase an error occurs in a MAC, a multiplexer forwards the previous MAC\u2019s accurate partial sum (stored in the Razor flip-flop) to the next MAC. Targeting fine-grained VOS solutions, i.e., the use of different voltages across the same circuit architecture, Pandey et al. propose GreenTPU [136]. This technique stores input sequences producing timing errors in MACs. As a result, when such an input sequence pattern is identified, the voltage of the MAC is scaled accordingly to prevent timing errors. In the same context, the authors of [201] propose NN-APP. This framework analyzes the error propagation in neural networks to model the impact of VOS on accuracy. Based on this analysis, as well as an error resilience study for the neurons, NN-APP uses a voltage clustering method to assign the same voltage to neurons with similar error resilience. Another fine-grained VOS approach is proposed in [216]. This framework provides voltage heterogeneity by using a greedy algorithm to solve the optimization problem of grouping and assigning the voltage of arithmetic units to different islands. The analysis of errors in circuits under VOS is considered a key factor, as it guides the aggressiveness of voltage scaling towards the acceptable error margins. In [109], an analytical method to study the errors in voltage over-scaled arithmetic circuits is proposed. Similarly, the authors of [68] introduce a probabilistic approach to model the errors of the critical paths. In the same category, we include works relying on simulations to analyze the errors of VOS. Ragavan et al. [146] characterize arithmetic circuits in terms of energy efficiency and errors using transistor-level SPICE simulation for various voltages. Based on this characterization, they propose a statistical model to simulate the behavior of arithmetic operations in VOS systems. By exploiting machine learning methods, Jiao et al. [74] propose LEVAX to model voltage over-scaled functional units. This input-aware model is trained on data from gate-level simulations to predict the timing error rate for each output bit. To provide accurate VOS-aware gate-level simulation, Zervakis et al. propose VOSsim [215]. This framework performs an offline characterization of the flip-flop for timing violations, and calculates the cell delays for the targeted voltage, enabling gate-level simulation under VOS.\n# 5.3 Over-Clocking\nOver-clocking (or frequency over-scaling) aims to operate the circuit/system at higher clock frequencies than those that respect the critical paths. As a result, timing errors are induced in exchange for increased performance. A trade-off analysis between accuracy and performance when over-clocking FPGA-based designs is presented in [175]. In the same work, the authors show that OC outperforms the traditional bit truncation for the same error constraint. For our analysis, we consider that the state-of-the-art works of the domain focus on the following directions: (i) tight synthesis [7], (ii) circuit re-design and architecture modification [150, 176, 202], (iii) error detection & correction [39, 100, 147], and (iv) error prediction [40, 72, 73, 75, 112, 156]. The first approach towards the reduction of timing errors caused by OC optimizes the critical paths of the design. In this context, the SlackHammer framework [7] synthesizes circuits with tight delay constraints to reduce the number of near-critical paths, and thus, decrease the probability of timing errors when frequency is over-scaled. At first, SlackHammer isolates the paths and identifies potential delay optimizations. Based on the isolated path analysis, the framework performs an iterative synthesis with tighter constraints for the primary outputs with negative slack. The second class of techniques aims at modifying the conventional circuit architecture to facilitate frequency OC and increase the resilience to timing errors. The retiming technique [150] re-defines the boundaries of combinational logic by moving the flip-flops backward or forward between the stages. Based on this circuit optimization, the synthesis is relaxed by ignoring the paths that are bottleneck to minimum period retiming. Targeting different circuit architectures, Shi et al. [176] adopt an alternative arithmetic, called online, and show that online-based circuits are more resilient to the timing errors of OC than circuits with traditional arithmetic. The modification of the initial\nneural network model to provide resilience in timing errors has also attracted research interest. In this direction, Wang et al. [202] propose an iterative reclocking-and-retraining framework for operating neural network circuits at higher frequencies under a given accuracy constraint. The clock frequency is gradually increased and the network\u2019s weights are updated through back-propagation training until to find the maximum frequency for which the timing errors are mitigated and the accuracy constraint is satisfied. Several works propose circuits for timing error detection & correction, enabling the use of over-clocking. These techniques either improve the frequency value of the first failure, i.e., the first timing error, or reduce the probability of timing errors. TIMBER [39] masks timing errors by borrowing time from successive pipeline stages. According to this approach, the use of discrete time-borrowing flip-flops and continuous time-borrowing latches slows down the appearance of timing errors with respect to the frequency scaling. Ragavan et al. [147] detect and correct timing errors by employing a dynamic speculation window on the double-sampling scheme. This technique adds an extra register, called shadow and clocked by a second \u201cdelayed\u201d clock, at the end of the pipelined path to sample the output data at two different time instances. The proposed approach also uses an online slack measurement to adaptively over-clock the design. The TEAI approach [100] is based on the locality of the timing errors in software-level instructions, i.e., the tendency of specific instructions to produce timing errors. TEAI identifies these instructions at runtime, and sends error alarms to hardware, which is equipped with error detection & correction circuits. Significant research has also been conducted on predicting the timing errors in advance, allowing to over-scale the frequency according to the acceptable error margins. In [156], the authors introduce an instruction-level error prediction system for pipelined micro-processors, which stalls the pipeline when critical instructions are detected. Their method is based on gate-level simulations to find the critical paths that are sensitized during the program execution. Similarly, Constantin et al. [40] obtain the maximum delays for each arithmetic instruction through gate-level simulations, and dynamically exploit timing margins to apply frequency over-scaling. In addition to instruction-level prediction models, there are numerous works that build models based on machine learning and simulations of functional units. A representative work of this approach is WILD [72], which builds a workload-dependent prediction model using logistic regression. In the same direction, SLoT [73] is a supervised learning model that predicts timing errors based on the inputs and the clock frequency. At first, SLoT performs gate-level simulation to extract timing class labels, i.e., \u201ctiming error\u201d or \u201cno timing error\u201d, for different inputs and frequencies. These classes are then used, along with features extracted from random data pre-processing, to train the error prediction model. Towards the same approach, TEVoT [75] uses machine learning to build a timing error prediction model that can predict the timing errors under different clock speeds and operating conditions, which are used to estimate the output quality of error-tolerant applications (e.g., image processing). DEVoT [112] is an extension of TEVoT that formulates the timing error prediction as a circuit dynamic delay prediction problem, saving significant prediction resources.\n# 6 CONCLUSION\nIn this article, we presented Part I of our survey on Approximate Computing, which focuses on key aspects of this novel design paradigm (motivation, terminology, and principles) and reviews the stateof-the-art software and hardware approximation techniques. We performed both coarse-grained and fine-grained classification: for each software/hardware-level technique, besides assigning it to a higher-level approximation class (e.g., precision scaling, voltage over-scaling), we also included it in a lower-level class with respect to its technical/implementation details (e.g., radix encoding, error prediction). In Part II of our survey, we review the state-of-the-art software & hardware\napplication-specific approximation techniques and architecture-level approximations in processors and memories. We also present the application spectrum of Approximate Computing, including an analysis of use cases reporting remarkable results per technique and application domain, as well as we report well-established benchmark suites and error metrics for Approximate Computing.\n# ACKNOWLEDGEMENT\nThis research is supported in parts by ASPIRE, the technology program management pillar of Abu Dhabi\u2019s Advanced Technology Research Council (ATRC), via the ASPIRE Awards for Research Excellence.\n# REFERENCES\nACM Computing Surveys, Under Review. July 2023.\nACM Computing Surveys, Under Review. July 2023.\nACM Computing Surveys, Under Review. July 2023.\nACM Computing Surveys, Under Review. July 2023.\nACM Computing Surveys, Under Review. July 2023.\nReceived 18 July 2023\n",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to address the increasing demand for efficient computing solutions in light of the limitations of traditional computing paradigms, particularly in the context of approximate computing, which offers a way to trade off accuracy for performance and power efficiency.",
            "scope": "The survey covers software and hardware approximation techniques across the entire computing stack, from high-level programming to low-level hardware implementations. It excludes surveys that focus narrowly on specific areas or techniques, providing a comprehensive overview of the state-of-the-art in approximate computing."
        },
        "problem": {
            "definition": "The survey focuses on the challenges posed by the growing computational demands of applications in AI, machine learning, and multimedia processing, which traditional computing methods struggle to meet due to power and performance constraints.",
            "key obstacle": "Researchers face the challenge of balancing computational accuracy with the need for power efficiency and performance, particularly as the limits of Moore's Law and Dennard's Law become more pronounced."
        },
        "architecture": {
            "perspective": "The survey introduces the Approximate Computing (AxC) paradigm as a framework that allows for intentional inaccuracies in computing to achieve efficiency gains.",
            "fields/stages": "The survey organizes approximation techniques into software, hardware, and architectural levels, providing a structured classification that reflects the different layers of the computing stack."
        },
        "conclusion": {
            "comparisions": "The survey compares various approximation techniques in terms of their effectiveness and applicability across different domains, highlighting the trade-offs involved in adopting approximate methods.",
            "results": "Key findings suggest that approximate computing can significantly enhance performance and power efficiency in applications with inherent error tolerance, paving the way for new design strategies in computing systems."
        },
        "discussion": {
            "advantage": "Existing research has demonstrated that approximate computing can lead to substantial improvements in performance and energy efficiency, particularly in applications where exact results are not critical.",
            "limitation": "Current studies often lack comprehensive evaluations across different application domains and fail to address the trade-offs between accuracy and efficiency systematically.",
            "gaps": "There remains a need for further exploration of the implications of approximation in diverse computing environments and the development of standardized metrics for evaluating approximate methods.",
            "future work": "Future research should focus on refining approximation techniques, exploring their integration into existing computing frameworks, and establishing benchmarks for assessing their performance across various applications."
        },
        "other info": {
            "additional details": {
                "authors": [
                    "Vasileios Leon",
                    "Muhammad Abdullah Hanif",
                    "Giorgos Armeniakos",
                    "Xun Jiao",
                    "Muhammad Shafique",
                    "Kiamal Pekmestzi",
                    "Dimitrios Soudris"
                ],
                "publication": "Approximate Computing Survey, Part I: Terminology and Software & Hardware Approximation Techniques",
                "journal": "ACM Computing Surveys",
                "year": 2023
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey introduces the Approximate Computing (AxC) paradigm as a framework that allows for intentional inaccuracies in computing to achieve efficiency gains."
        },
        {
            "section number": "1.2",
            "key information": "This survey aims to address the increasing demand for efficient computing solutions in light of the limitations of traditional computing paradigms."
        },
        {
            "section number": "1.3",
            "key information": "The survey covers software and hardware approximation techniques across the entire computing stack, providing a comprehensive overview of the state-of-the-art in approximate computing."
        },
        {
            "section number": "1.4",
            "key information": "Key findings suggest that approximate computing can significantly enhance performance and power efficiency in applications with inherent error tolerance."
        },
        {
            "section number": "2.1",
            "key information": "The survey organizes approximation techniques into software, hardware, and architectural levels, providing a structured classification that reflects the different layers of the computing stack."
        },
        {
            "section number": "2.2",
            "key information": "The survey focuses on the challenges posed by the growing computational demands of applications in AI, machine learning, and multimedia processing."
        },
        {
            "section number": "6.1",
            "key information": "Researchers face the challenge of balancing computational accuracy with the need for power efficiency and performance, particularly as the limits of Moore's Law and Dennard's Law become more pronounced."
        },
        {
            "section number": "6.3",
            "key information": "Future research should focus on refining approximation techniques, exploring their integration into existing computing frameworks, and establishing benchmarks for assessing their performance across various applications."
        }
    ],
    "similarity_score": 0.5605085752354794,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1832_natur/papers/Approximate Computing Survey, Part I_ Terminology and Software & Hardware Approximation Techniques.json"
}