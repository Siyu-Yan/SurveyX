{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1210.6412",
    "title": "A Comparison of Sequential and GPU Implementations of Iterative Methods to Compute Reachability Probabilities",
    "abstract": "We consider the problem of computing reachability probabilities: given a Markov chain, an initial state of the Markov chain, and a set of goal states of the Markov chain, what is the probability of reaching any of the goal states from the initial state?  This problem can be reduced to solving a linear equation Ax = b for x, where A is a matrix and b is a vector.  We consider two iterative methods to solve the linear equation: the Jacobi method and the biconjugate gradient stabilized (BiCGStab) method.  For both methods, a sequential and a parallel version have been implemented.  The parallel versions have been implemented on the compute unified device architecture (CUDA) so that they can be run on a NVIDIA graphics processing unit (GPU).  From our experiments we conclude that as the size of the matrix increases, the CUDA implementations outperform the sequential implementations. Furthermore, the BiCGStab method performs better than the Jacobi method for dense matrices, whereas the Jacobi method does better  for sparse ones.  Since the reachability probabilities problem plays a key role in probabilistic model checking, we also compared the implementations for matrices obtained from a probabilistic model checker.  Our experiments support the conjecture by Bosnacki et al. that the Jacobi method is superior to Krylov subspace methods, a class to which the BiCGStab method belongs, for probabilistic model checking.",
    "bib_name": "cormiebowins2012comparisonsequentialgpuimplementations",
    "md_text": "# A Comparison of Sequential and GPU Implementations of Iterative Methods to Compute Reachability Probabilities\u2217\nElise Cormie-Bowins\nDisCoVeri Group, Department of Computer Science and Engineering, York University 4700 Keele Street, Toronto, ON, M3J 1P3, Canada\nWe consider the problem of computing reachability probabilities: given a Markov chain, an initial state of the Markov chain, and a set of goal states of the Markov chain, what is the probability of reaching any of the goal states from the initial state? This problem can be reduced to solving a linear equation A \u00b7 x = b for x, where A is a matrix and b is a vector. We consider two iterative methods to solve the linear equation: the Jacobi method and the biconjugate gradient stabilized (BiCGStab) method. For both methods, a sequential and a parallel version have been implemented. The parallel versions have been implemented on the compute unified device architecture (CUDA) so that they can be run on a NVIDIA graphics processing unit (GPU). From our experiments we conclude that as the size of the matrix increases, the CUDA implementations outperform the sequential implementations. Furthermore, the BiCGStab method performs better than the Jacobi method for dense matrices, whereas the Jacobi method does better for sparse ones. Since the reachability probabilities problem plays a key role in probabilistic model checking, we also compared the implementations for matrices obtained from a probabilistic model checker. Our experiments support the conjecture by Bosnacki et al. that the Jacobi method is superior to Krylov subspace methods, a class to which the BiCGStab method belongs, for probabilistic model checking.\n# 1 Introduction\nGiven a Markov chain, an initial state of the Markov chain, and a set of goal states of the Markov chain, we are interested in the probability of reaching any of the goal states from the initial state. This probability is known as the reachability probability. These reachability probabilities play a key role in several fields, including probabilistic model checking (see, for example, [1, Chapter 10]) and performance evaluation (see, for example, [9]). As we will sketch in Section 2, computing reachability probabilities can be reduced to solving a linear equation of the form A\u00b7x = b for x, where A is an n\u00d7n-matrix and b is an n-vector. Although the equation can be solved by inverting the matrix A, such an approach becomes infeasible for large matrices due to the computational complexity of matrix inversion. For instance, Gauss-Jordan elimination has time complexity O(n3) (see, for example, [12, Chapter 2]). For large matrices, iterative methods are used instead. The iterative methods compute successive approximations to obtain a more accurate solution to the linear system at each iteration. For an overview of linear methods, we refer the reader to, for example, [3, Chapter 2]. In this paper, we consider two linear methods, namely the Jacobi method and the biconjugate gradient stabilized (BiCGStab) method. We have implemented a sequential version and a parallel version of the Jacobi and BiCGStab method. The sequential versions have been implemented in C. The parallel versions have been implemented\nusing NVIDIA\u2019s compute unified device architecture (CUDA). It allows us to run C code on a graphics processing unit (GPU) with hundreds of cores. Currently, CUDA is only supported by NVIDIA GPUs. Our CUDA implementation of the Jacobi method is based on the one described by Bosnacki et al. in [4, 5]. When we started this research, we were aware of the paper by Gaikwad and Toke [7] that mentions a CUDA implementation of the BiCGStab method. Since we did not have access to their code, we implemented the BiCGStab method in CUDA ourselves. To compare the performance of our four implementations, we constructed three sets of tests. First of all, we randomly generated matrices with varying sizes and densities. Our experiments show that the BiCGStab method is superior to the Jacobi method for denser matrices. They also demonstrate a consistent performance benefit from using CUDA to implement the Jacobi method. However, we observe that the CUDA version of the BiCGStab method is only beneficial for larger, denser matrices. For the smaller and sparser matrices, the sequential version of the BiCGStab method outperforms the CUDA version. Secondly, we used an extension of the model checker Java PathFinder (JPF) to generate matrices. JPF can check properties of Java code, such as the absence of uncaught exceptions. Zhang [14] created a probabilistic extension of JPF that can check properties of randomized sequential algorithms. We used this extension to generate transition probability matrices corresponding to the Java code of two randomized sequential algorithms. The Jacobi method performed better than the BiCGStab method for these matrices. This supports the conjecture by Bosnacki et al. [4, 5] that the Jacobi method is superior to Krylov subspace methods, a class to which the BiCGStab method belongs, for probabilistic model checking. Finally, we randomly generated matrices with the same sizes and densities as the matrices produced by JPF. We obtained very similar results. This suggests that size and density are the main determinants of which implementation performs best on probabilistic model checking data, and whether CUDA will be beneficial, rather than other properties unique to matrices found in probabilistic model checking.\n# 2 Reachability Probabilities of a Markov Chain\nIn this paper, the term \u201cMarkov chain\u201d refers to a discrete-time Markov chain. Below, we review the well-known problem of computing the reachability probabilities of a Markov chain. Our presentation is based on [1, Section 10.1]. A Markov chain is a triple M = (S,P,s0) consisting of\n\u2022 a nonempty and finite set S of states, and\n# \u2022 a transition probability function P : S\u00d7S \u2192[0,1] satisfying for all s \u2208S,\n\u2211 s\u2032\u2208S P(s,s\u2032) = 1,\n\u2022 and an initial state s0 \u2208S.\nThe transition probability function can be represented as a matrix. This matrix has rows and columns corresponding to the states of the Markov chain, and entries representing the probabilities of the transitions between these states. For example, the probability transition function of the Markov chain depicted\n22\ncan be represented by the matrix\n\uf8ef\uf8f0 \uf8fa\uf8fb A path of a Markov chain M is an infinite sequence s1s2 ... of states such that P(si,si+1) > 0 for all i \u22651. We denote the set of paths of M that start in state s by Paths(s). For the above Markov chain, the set of paths starting in state 0 is {(02)n1\u03c9 | n \u2208N}\u222a{(02)n 0 3\u03c9 | n \u2208N}\u222a{(02)\u03c9}. To define the probability of events such as eventually reaching a set of states from the initial state, we associate a probability space with a Markov chain M and a state s. Recall that a probability space consists of a set, a \u03c3-algebra, and a probability measure. In this case, the set is Paths(s). The \u03c3-algebra is generated by the so-called cylinder sets. Given a finite sequence of states s1 ...sn, its cylinder set Cyl(s1 ...sn) is defined by\nFor the above Markov chain, we have that Cyl(03) = {0 3\u03c9}. We define Pr(Cyl(s1 ...sn)) = \u220f P(si,si+1).\nRecall that Pr can be uniquely extended to a probability measure on the \u03c3-algebra generated by th cylinder sets. For the above Markov chain, we have that Pr(Cyl(03)) = 0.5.\n# 2.1 Property of Interest\nIn the following, we are interested in two particular events. First of all, given a Markov chain M , a state s of the Markov chain and a set of states GS, known as the goal states, of the Markov chain, we are interested in the probability of reaching a state in GS in one transition when starting in s. We denote the set of paths starting in s that reach a state in GS in one transition by {\u03c0 \u2208Paths(s) | \u03c0 |= \u20ddGS}. This set can be shown to be measurable, that is, it belongs to the \u03c3-algebra. Its probability we denote by Pr(s |= \u20ddGS). Secondly, given a Markov chain M , a state s of the Markov chain and a set of goal states GS, we are interested in the probability of reaching a state in GS in zero or more transitions when starting in s. We denote the set of paths starting in s that reach a state in GS in zero or more transitions by {\u03c0 \u2208Paths(s) | \u03c0 |= \u2666GS}. Also this set can be shown to be measurable. Its probability we denote by Pr(s |= \u2666GS). The problem we are examining in this paper is the following: Given a Markov chain M , and set of goal states GS, what is the probability to reach a state in GS in zero or more transitions from the initial state? In other words, what is the probability of states in GS eventually being reached? That is, we want to compute Pr(s0 |= \u2666GS), where s0 is the initial state of M . This is what is referred to as computing reachability probabilities in [1, Chapter 10.1.1].\n# 2.2 Partitioning the Set of States\nTo compute the reachability probabilities, one usually first partitions the set of states of the Markov chain into three parts, based on their probability to reach the set GS of goal states:\nTo compute the reachability probabilities, one usually first partitions the set of states of the Markov chain into three parts, based on their probability to reach the set GS of goal states: \u2022 S=1 = {s \u2208S | Pr(s |= \u2666GS) = 1} \u2022 S=0 = {s \u2208S | Pr(s |= \u2666GS) = 0} \u2022 S? = S\\(S=1 \u222aS=0) The partitioning of the set of states can be done easily by considering the underlying digraph of the Markov chain. The vertices of this digraph are the states of the Markov chain. There is an edge from state s to state s\u2032 if and only if P(s,s\u2032) > 0. Using graph algorithms, such as depth-first-search or breadthfirst-search, the set of states can be partitioned as follows: \u2022 To find S=0, determine the set of all states that can reach GS (including the states in GS). The complement of this set is S=0. \u2022 To find S=1, determine the set of all states that can reach S=0. The complement of this set is S=1. \u2022 To find S?, simply take the complement of S=1 \u222aS=0. Consider the above Markov chain and let 3 be the only goal state. Then S=0 = {1}, S=1 = {3} and S? = {0,2}.\n# 2.3 Computing Reachability Probabilities by State\nTo compute the reachability probabilities, one must determine the probability of the initial state leading to a state in GS, that is, one has to compute Pr(s0 |= \u2666GS). To do so, one must also determine the probabilities of reaching a state in GS from other states. For each state s we compute xs, which is the probability of reaching GS from s, that is, xs = Pr(s |= \u2666GS). The values of xs, for s \u2208S, can be expressed as a vector, x. For any state s \u2208S=1, by definition xs = 1. Similarly, for each s \u2208S=0, xs = 0. So once the states have been partitioned into the three sets described above, the only values of x that need to be calculated are {xs | s \u2208S? }. These values satisfy the following equation:\nSo, we will create a matrix A, which includes only the transition probabilities between states in S?. For each s, s\u2032 \u2208S?, As,s\u2032 = P(s,s\u2032). To aid in calculations, we will also create a vector b. For each s \u2208S?, bs = Pr(s |= \u20ddGS), that is, the probability of a state in GS being reached from s in one transition. Consider the above Markov chain and let 3 be the only goal state. Then states 1 and 3 are excluded because they do not belong to S?, and\nThe equation for x above can be written as x = A\u00b7x+b. Rearranged, this becomes (I\u2212A)\u00b7x = b, where I is an identity matrix. A and b are already known from the Markov chain. So, x can be found by solving the linear equation (I\u2212A)\u00b7x = b for x. There are several ways to solve the equation (I \u2212A) \u00b7 x = b for x. Most obviously, one can find (I \u2212A)\u22121. However, methods to find matrix inverses tend to have high computational complexity and,\nhence, for large matrices this becomes infeasible. For instance, Gauss-Jordan elimination has time complexity O(n3) (see, for example, [12, Chapter 2]). Iterative approximation methods find solutions that are within a specified margin of error of the exact solutions, and can work much more quickly. The methods used in this paper are in this category, and will be discussed in more detail next.\n# 3 Iterative Methods\nSolving the linear equation (I \u2212A) \u00b7 x = b for x can be very time-consuming, especially when A is large. To solve this equation, there are numerous iterative methods. These methods compute successive approximations to obtain a more accurate solution to the linear system at each iteration. For an overview of linear methods, we refer the reader to, for example, [3, Chapter 2]. These iterative methods can be classified into two groups: the stationary methods and the nonstationary ones. In stationary methods, the same information is used in each iteration. As a consequence, these methods are usually easier to understand and implement. However, convergence of these methods may be slow. In this paper, we consider one stationary linear method, namely the Jacobi method. In nonstationary methods, the information used may change per iteration. These methods are usually more intricate and harder to implement, but often give rise to faster convergence. In this paper we focus on one particular nonstationary linear method, namely the biconjugate gradient stabilized (BiCGStab) method. The BiCGStab method was developed by Van der Vorst [13]. It is a type of Krylov subspace method. Unlike most of these methods, it can solve non-symmetric linear systems, and was designed to minimize the effect of rounding errors. If exact arithmetic is used, it will terminate in at most n iterations for an n \u00d7 n matrix [13, page 636]. In practice, it often requires much fewer iterations to find an approximate solution. The un-preconditioned version of this method was used, as the sequential and GPU performance of the preconditioning method would be a separate issue. For some matrices a preconditioner is necessary to use the BiCGStab method, but that is not the case for the data encountered here. Buchholz [6] did a limited comparison between a preconditioned and un-preconditioned version of the BiCGStab method specifically on matrices that represent stochastic processes, and it does not show a large performance difference.\n# 3.1 The Jacobi Method\nGiven the matrix A and the vector b, the Jacobi method returns an approximate solution for x in the linear equation A\u00b7x = b. Below, we only present its pseudocode. For a detailed discussion of the Jacobi method, we refer the reader to, for example, [3, Section 2.2.1].\nlinear equation A\u00b7x = b. Below, we only method, we refer the reader to, for exam 1 Ja cobi (A,b ) : 2 x := random v e c t o r 3 repeat 4 x\u2032 := x 5 for a l l i = 1...n do 6 xi := 1 Ai,i \u00b7(bi \u2212\u2211j\u0338=i Ai,j \u00b7x\u2032 j) 7 u n t i l x i s a c c u r a t e enough 8 return x\n# 3.2 The BiCGStab Method\nLike the Jacobi method, the BiCGStab method returns an approximate solution to A \u00b7 x = b for a given matrix A and vector b. Again, we only present the pseudocode. For more details, we refer the reader to the highly cited paper [13] in which Van der Vorst introduced the method. 1 BiCGStab (A,b ) : 2 x := random v e c t o r 3 r := b\u2212A\u00b7x 4 q := random v e c t o r such t h a t q\u00b7r \u0338= 0 5 y := 1; a := 1; w := 1; v := 0 ; p := 0 6 repeat 7 y\u2032 := y 8 y := q\u00b7r 9 p := r+ y\u00b7a y\u2032\u00b7w \u00b7(p\u2212w\u00b7v) 10 v := A\u00b7p 11 a := y q\u00b7v 12 s := r\u2212a\u00b7v 13 t := A\u00b7s 14 w := t\u00b7s t\u00b7t 15 x := x+a\u00b7p+w\u00b7s 16 r := s\u2212w\u00b7t 17 u n t i l x i s a c c u r a t e enough 18 return x\n# 4 General Purpose Graphics Processing Units (GPGPU)\nGraphics processing units (GPUs) were primarily developed to improve the performance of graphicsintensive programs. The market driving their production has traditionally been video game players. They are a throughput-oriented technology, optimized for data-intensive calculations in which many identical operations can be done in parallel on different data. Unlike most commercially available multi-core central processing units (CPUs), which normally run up to eight threads in parallel, GPUs are designed to run hundreds of threads in parallel. They have many more cores than CPUs, but these cores are generally less powerful. GPUs sacrifice latency and single-thread performance to achieve higher data throughput [8]. Originally, GPU APIs were designed exclusively for graphics use, and their underlying parallel capabilities were difficult to access for other types of computation. This changed in 2006 when NVIDIA released CUDA, the first architecture and API designed to allow GPUs to be used for a wider variety of applications [10, Chapter 1]. All parallel algorithms discussed in this paper were implemented using CUDA. Different generations of CUDA-enabled graphics cards have different compute capability levels, which indicate the sets of features that they support. The graphics card used here is the NVIDIA GTX260, which has compute capability 1.3. Cards with higher compute capability have been released by NVIDIA since the GTX260 was developed. The GTX260 has 896MB global on-card memory. This GPU supports double-precision floatingpoint operations, which is required for BiCGStab. Lower compute capabilities (1.2 and below) do not\nsupport this. The GTX260 does not support atomic floating-point operations. This limits the way operations that require dot-products and other row sums can be separated into threads. Since these sums require all elements of a row to be summed to a global variable, and atomic addition can not be used to protect the integrity of the sum, the structure of the threads must do so. Here, we have simply created one thread per row and done these additions sequentially. The next generation of NVIDIA GPUs (compute capability 2.0 and higher) support atomic floating-point addition, though only on single-precision numbers. Current GPUs have limited memory to store matrix data. For all but the smallest matrices, some sort of compression is necessary so that they can be transferred to the GPU in their entirety. For this project, data is stored using a compressed row storage, as described in [3, page 57]. In this format, only the non-zero elements of the matrix are recorded. The size of an n \u00d7 n matrix with m non-zero elements compressed in this manner is O(n + m), whereas uncompressed it is O(n2). This representation saves significant space when used to store sparse matrices. A compressed row matrix consists of three vectors. The vector rstart contains integers and has length n+1, where rstarti is the total number of non-zero elements in the first i rows. The vector col also contains integers and has length m. It stores the column position of each non-zero element. The vector nonzero contains floating points and has length m. It stores the non-zero elements of the matrix. For example, the matrix \uf8ee \uf8f9\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/44cf/44cf7586-ab62-4f55-86df-1ba22b10bbdd.png\" style=\"width: 50%;\"></div>\nis represented by the vectors\n# 5 Parallel Implementations in CUDA\nNext, we present parallel versions of the Jacobi method and the BiCGStab method. These parallel versions are implemented in CUDA.\n# 5.1 The Jacobi Method\nThe Jacobi method was parallelized as in [4]. Given an n\u00d7n-matrix A and an n-vector b, in the parallel implementation of the Jacobi method n threads are created. For each thread i, where 0 \u2264i < n, the following algorithm is used. Essentially, one thread traverses each row of the matrix A to compute the corresponding element of x. Thus, each element of x is computed in paralllel.\nd := d \u2212nonzero j \u00b7xcolj xi := d Ai,i i f xi i s not a c c u r a t e enough then terminate = f a l s e\n9 10 11 12\n f xi i s not a c c u r a t e enough then terminate = f a l s e\nThe variable terminate is shared by all the threads to determine when the iteration can stop. Note that line 5\u201310 of the code above corresponds to line 5\u20136 of the Jacobi method in Section 3.1. Thus, this algorithm represents one iteration of the Jacobi method. The main program repeatedly launches the corresponding code on the GPU, waiting between executions for all threads to complete, until x reaches the desired accuracy.\n# 5.2 The BiCGStab Method\nEach iteration of the BiCGStab method consists of several matrix and vector multiplications, that each result in a vector. As most of these must be done in order, they were parallelized individually by creating one thread for each element of the resulting vectors. For instance, the operation in line 15 is split between n threads, so for each 0 \u2264i < n a thread does the following:\nAnd thus each element of the vector is calculated in parallel. The matrix operation in line 10 is done a follows for each of the n threads:\nwhere Ai denotes the ith row of the matrix A. Dot products are done sequentially. It would be possible to increase parallelism by splitting these into multiple sums done in parallel. This may require too much overhead to result in a significant performance gain. Currently, CUDA requires all threads running on a GPU to execute the same code. There are some separate steps of the BiCGStab method which could be executed in parallel, but this is not possible with a single GPU of the type used here. This may be possible using the next generation of GPUs, or multiple GPUs. However, most steps of the algorithm must be done in sequence. Below is an abbreviated version of the CUDA code used to implement BiCGStab. To save space, non-essential code has been removed, and some steps are summarized in square brackets. Kernel numbering corresponds to the line numbers of the algorithm in Section 3.2. Kernels for subsequent steps are combined when they require the same number of threads. Sequential steps are done on the GPU with a single thread, since this avoids time-consuming data transfers between the GPU and host computer. The first portion of the code, below, executes on the CPU. Pointers prefixed by d indicate data stored on the GPU, and n is the dimension of the matrix. The matrix A is represented in row-compressed form on the GPU as d col, d rstart and d nonzero.\n}\n__global__ static void step11Kernel(double *d_a, double *d_y, double *d_q, double *d_v, int n) double dot_q_v = 0; //dot product of q,v for(int i = 0; i < n; i++){ dot_q_v += d_q[i] * d_v[i]; } *d_a = *d_y / dot_q_v;\n}\n# 6 Performance on Randomly Generated Matrices\nFor these tests, random matrices were generated. The entries were random positive integers, placed in random locations. The matrices were then modified to have non-zero diagonal entries and be diagonallydominant, which ensured that both the Jacobi and BiCGStab methods were able to solve equations containing them. For each matrix A, a vector b of random integers was also created. This formed the equation A\u00b7x = b, to be solved for x. Each trial used a newly-generated matrix and vector. Figure 1 shows the performances of the four implementations on random matrices of several sizes, and varying densities. The matrix densities are similar to those encountered in probabilistic model checking. These graphs indicate that the implementations\u2019 relative performances change their order as the density increases. Furthermore, as the size of the matrix increases, the density at which the performance order changes becomes lower. Thus, which implementation performs best depends on the size and density of the matrix it is being used on.\nGenerally, the graphs in Figure 1 show that the BiCGStab method is superior to the Jacobi method for denser matrices, but the Jacobi method performs best for very sparse matrices. They also demonstrate a consistent performance benefit from using CUDA to implement the Jacobi method. However, the third graph shows that the CUDA version of the BiCGStab method is only beneficial for larger, denser matrices. For other matrices, the sequential version of the BiCGStab method outperforms the CUDA version.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/20ef/20ef5a8e-b5c6-4b97-b4f9-bda0699403fc.png\" style=\"width: 50%;\"></div>\nFigure 1: The effect of varying density on performance, for three matrix sizes. A logarithmic scale is used on the y-axis. Average times are based on 20 trials. The standard deviations of the times measured are too small to be shown. To confirm this, Figure 2 and Figure 3 show the implementations\u2019 performances on random matrices with 10% density. As the sizes of these relatively dense matrices increase, the CUDA versions of both implementations increasingly outperform their sequential counterparts, and the BiCGStab method significantly outperforms the Jacobi method.\nFigure 1: The effect of varying density on performance, for three matrix sizes. A logarithmic scale i used on the y-axis. Average times are based on 20 trials. The standard deviations of the times measured are too small to be shown.\nTo confirm this, Figure 2 and Figure 3 show the implementations\u2019 performances on random matrices with 10% density. As the sizes of these relatively dense matrices increase, the CUDA versions of both implementations increasingly outperform their sequential counterparts, and the BiCGStab method significantly outperforms the Jacobi method.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4328/4328e316-5564-4696-bd1c-de65c9537644.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b2b5/b2b509e6-a1d4-45e1-b27d-f25af4506e1f.png\" style=\"width: 50%;\"></div>\nFor these tests, the implementations were tested on matrices representing the transition probability functions of Markov chains based on actual randomized algorithms. These matrices were then reduced to only S? states and subtracted from the identity matrix, as discussed in Section 2.3. JPF was used on two randomized algorithms to create this data. The biased die algorithm simulates a biased dice roll by means of biased coin flips, and random quick sort is a randomized version of the quick sort algorithm. Both algorithms were coded in Java by Zhang, who also created the JPF extension that outputs transition probability matrices of Markov chains that correspond to the code being checked [14]. The JPF search strategies used were chosen to create the largest S? matrices relative to the size of the searched space. JPF\u2019s built-in depth first search was used for random quick sort, and a strategy called probability first search that prioritizes high-probability transitions, also written by Zhang [14], was used for the biased die example. The results of the random quick sort tests are shown in Figure 4, and the results of the biased die tests in Figure 5. Error bars representing standard deviations of each data point are too small to be visible in the graphs. The matrix sizes in the random quick sort tests are much smaller than those in the biased die tests because, while the matrices for random quick sort are initially the same size as or larger than those produced for biased die, much fewer states belong to the S? set. Furthermore, note that the densities of these matrices decrease as their sizes increase. The sizes and densities of the matrices used in these tests are shown in Table 1. The relative performances of the sequential and CUDA implementations of the Jacobi method, and the sequential implementation of the BiCGStab method, are different for each algorithm. The best performance on the random quick sort data was from the sequential implementation of the Jacobi method, while the best performance for biased die was from the CUDA implementation of the Jacobi method. The CUDA implementation of the BiCGStab method performs worst in both cases.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e12a/e12acb6c-6715-4e20-8880-c1f7f6ac2645.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af70/af702c01-4956-47c0-af79-ec0a1e24e50a.png\" style=\"width: 50%;\"></div>\nFor these tests, random matrices were generated with the same densities and sizes as those produced by JPF (as in Table 1). The JPF matrices are reduced transition probability matrices, subtracted from the identity matrix. So, they have entries in the interval [-1, 1] with locations based on the structure of a Markov chain. In contrast, entries in the randomized matrices are randomly-located positive integers, as described in Section 6. Unlike in the JPF matrix tests, each trial uses a different matrix and vector. However, the standard deviations of the times measured are still too small to be shown on the graphs. Performance results for these matrices are shown in Figure 6 and Figure 7. It is apparent that the ordering of the different implementations\u2019 performances is the same as it was for matrices of the same sizes and densities generated by JPF. This suggests that size and density are the main determinants of which implementation performs best on probabilistic model checking data, and whether CUDA will be beneficial, rather than other properties unique to the matrices found in probabilistic model checking. Generally, it appears that for the particular types of matrix encountered during probabilistic model\nSizes and Densities of S? Matrices\nRandom Quick Sort\nBiased Die\nn\nm\ndensity\nn\nm\ndensity\n92\n211\n0.025\n667\n1333\n0.003\n118\n263\n0.019\n1333\n2665\n0.001\n142\n312\n0.015\n2000\n3999\n0.001\n173\n379\n0.013\n2668\n5335\n0.001\n198\n430\n0.011\n3647\n7293\n0.001\n228\n491\n0.009\n4647\n9293\n0.000\n250\n536\n0.009\n5647\n11293\n0.000\n284\n606\n0.008\n6647\n13293\n0.000\n313\n669\n0.007\n7647\n15293\n0.000\nTable 1: Sizes and densities of the matrices produced by JPF for the two algorithms. n is the matri dimension, and m is the number of non-zero entries.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6de5/6de59def-c164-4d64-8da0-199d963eca3b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Performance on randomized matrices, with the same sizes and densities as the matrices produced by JPF when checking the random quick sort algorithm. Averages times are based on 40 matrices</div>\nchecking, implementing the BiCGStab method in CUDA does not improve performance. CUDA does, however, improve the performance of the Jacobi method.\nIn [4], the authors conjecture that the Jacobi method would be more suitable for probabilistic model checking using CUDA than Krylov subspace methods such as the BiCGStab method. This seems to be true. However, the results in Section 6 suggest that this is due to the superior performance of the Jacobi method on sparse matrices in general, rather than BiCGStab\u2019s higher memory requirements as proposed in [4]. For the probabilistic model checking data, the matrix density decreases as the size increases, so the conditions in which the CUDA BiCGStab implementation performs best (larger, denser matrices, as in Figure 2) are not encountered.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f9d/0f9d4e61-96f9-4e11-bee9-69f66bb7cb41.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Performance - Randomized Matrices [density same as Biased Die for each size]</div>\n<div style=\"text-align: center;\">  Figure 7: Performance on randomized matrices, with the same sizes and densities as the matrices produced by JPF when checking the biased die algorithm. Average times are based on 40 matrices.</div>\n# 8 Related and Future Work\nRelated work by Bosnacki et al. [4] tests a CUDA implementation of the Jacobi method on data obtained by another probabilistic model checker, PRISM. They used different probabilistic algorithms in their probabilistic model checker, and for each algorithm their results show a benefit from GPU usage. In a later expansion of their research [5], they test a CUDA implementation of the Jacobi method that uses a backward segmented scan, and one using two GPUs, which further improve performance. In their second paper they also compare the performance on 32- and 64-bit systems, and find that for one of the three algorithms they model check, the CPU algorithm on the 64-bit system outperforms the CUDA implementation. In another related paper, Barnat et al. [2] improve the performance of the maximal accepting predecessors (MAP) algorithm for LTL model checking by implementing it using CUDA. Future work could include experiments on model checking data generated from the probabilistic algorithms used with the model checker in [4], to allow closer comparison with that work. Another possibility is to implement the CUDA BiCGStab algorithm using multiple GPUs, so that different steps can be run in parallel. In [11], Kwiatkowska et al. suggest to consider the strongly connected components of the underlying digraph of the Markov chain in reverse topological order. Since the sizes and densities of the matrices corresponding to these strongly connected components may be quite different from the size and density of the matrix corresponding to the Markov chain, we are interested to see whether this approach will favour different implementations. Acknowledgements We would like to thank Stefan Edelkamp for providing us their CUDA implementation of the Jacobi method so that we could ensure that our implementation was similar. We also thank\n# References\n1] Christel Baier & Joost-Pieter Katoen (2008): Principles of Model Checking. The MIT Press.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of computing reachability probabilities in Markov chains, which is essential for probabilistic model checking and performance evaluation. Previous methods, such as direct matrix inversion, are computationally infeasible for large matrices, necessitating the exploration of iterative methods like the Jacobi and BiCGStab methods, both in sequential and parallel implementations using CUDA.",
        "problem": {
            "definition": "The problem involves calculating the probability of reaching a set of goal states from an initial state in a Markov chain, which can be formulated as solving a linear equation A \u00b7 x = b.",
            "key obstacle": "The main difficulty lies in the high computational complexity associated with traditional matrix inversion methods, particularly for large matrices, making iterative methods more suitable."
        },
        "idea": {
            "intuition": "The idea arises from the need to efficiently compute reachability probabilities in large Markov chains, leveraging iterative methods that can handle large systems without direct inversion.",
            "opinion": "The proposed idea involves implementing both the Jacobi method and the BiCGStab method in parallel using CUDA to improve performance on large matrices.",
            "innovation": "The primary innovation is the parallelization of these iterative methods on GPUs, which allows for significant performance improvements compared to sequential implementations, especially for larger, denser matrices."
        },
        "method": {
            "method name": "Jacobi and BiCGStab methods",
            "method abbreviation": "J and BiCGStab",
            "method definition": "These methods solve the linear equation A \u00b7 x = b iteratively, providing approximations to the solution without requiring matrix inversion.",
            "method description": "The methods compute successive approximations to the solution of the linear system, with the Jacobi method being a stationary method and the BiCGStab method being a nonstationary Krylov subspace method.",
            "method steps": [
                "Initialize an approximation vector.",
                "Iterate to update the approximation based on the matrix A and vector b.",
                "Check for convergence and repeat until the desired accuracy is achieved."
            ],
            "principle": "The effectiveness of these methods stems from their iterative nature, allowing them to converge to a solution without the prohibitive costs associated with direct matrix inversion."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using randomly generated matrices of varying sizes and densities, as well as matrices derived from the Java PathFinder model checker for probabilistic algorithms.",
            "evaluation method": "Performance was assessed by comparing execution times of the sequential and CUDA implementations across different matrix configurations, measuring accuracy and convergence rates."
        },
        "conclusion": "The experiments demonstrate that CUDA implementations significantly outperform sequential ones for larger matrices, with the BiCGStab method excelling in dense scenarios while the Jacobi method performs better for sparse matrices.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include improved performance on large-scale problems due to parallel processing and the ability to handle various matrix densities effectively.",
            "limitation": "A limitation of the method is that the CUDA implementation of the BiCGStab method is less beneficial for smaller, sparser matrices, where sequential methods may still outperform it.",
            "future work": "Future research could explore implementing the BiCGStab method on multiple GPUs and investigating the performance of different iterative methods on strongly connected components of Markov chains."
        },
        "other info": {
            "acknowledgements": "The authors thank Stefan Edelkamp for providing their CUDA implementation of the Jacobi method for comparison."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of computing reachability probabilities in Markov chains, which is essential for probabilistic model checking and performance evaluation."
        },
        {
            "section number": "1.2",
            "key information": "The main difficulty lies in the high computational complexity associated with traditional matrix inversion methods, particularly for large matrices, making iterative methods more suitable."
        },
        {
            "section number": "1.3",
            "key information": "The proposed idea involves implementing both the Jacobi method and the BiCGStab method in parallel using CUDA to improve performance on large matrices."
        },
        {
            "section number": "2.1",
            "key information": "The problem involves calculating the probability of reaching a set of goal states from an initial state in a Markov chain, which can be formulated as solving a linear equation A \u00b7 x = b."
        },
        {
            "section number": "2.2",
            "key information": "The primary innovation is the parallelization of these iterative methods on GPUs, which allows for significant performance improvements compared to sequential implementations, especially for larger, denser matrices."
        },
        {
            "section number": "5.3",
            "key information": "A limitation of the method is that the CUDA implementation of the BiCGStab method is less beneficial for smaller, sparser matrices, where sequential methods may still outperform it."
        },
        {
            "section number": "6.1",
            "key information": "Future research could explore implementing the BiCGStab method on multiple GPUs and investigating the performance of different iterative methods on strongly connected components of Markov chains."
        }
    ],
    "similarity_score": 0.5553945079011783,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0214_large/papers/A Comparison of Sequential and GPU Implementations of Iterative Methods to Compute Reachability Probabilities.json"
}