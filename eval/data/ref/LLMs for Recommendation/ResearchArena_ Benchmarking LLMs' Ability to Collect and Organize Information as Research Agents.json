{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.10291",
    "title": "ResearchArena: Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents",
    "abstract": "Large language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents\u2019 ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers\u2019 importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents\u2019 ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.",
    "bib_name": "kang2024researcharenabenchmarkingllmsability",
    "md_text": "# ResearchArena: Benchmarking LLMs\u2019 Ability to Collect and Organize Information as Research Agents\nHao Kang\nhaok@andrew.cmu.edu\nSchool of Computer Science\nCarnegie Mellon University\nPittsburgh, PA, 15213\nChenyan Xiong\ncx@cs.cmu.edu\nLanguage Technologies Institute\nCarnegie Mellon University\nPittsburgh, PA, 15213\nHao Kang haok@andrew.cmu.edu School of Computer Science Carnegie Mellon University Pittsburgh, PA, 15213 Chenyan Xiong cx@cs.cmu.edu Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213\n# Abstract\nLarge language models (LLMs) have exhibited remarkable performance across various tasks in natural language processing. Nevertheless, challenges still arise when these tasks demand domain-specific expertise and advanced analytical skills, such as conducting research surveys on a designated topic. In this research, we develop ResearchArena, a benchmark that measures LLM agents\u2019 ability to conduct academic surveys, an initial step of academic research process. Specifically, we deconstructs the surveying process into three stages 1) information discovery: locating relevant papers, 2) information selection: assessing papers\u2019 importance to the topic, and 3) information organization: organizing papers into meaningful structures. In particular, we establish an offline environment comprising 12.0M full-text academic papers and 7.9K survey papers, which evaluates agents\u2019 ability to locate supporting materials for composing the survey on a topic, rank the located papers based on their impact, and organize these into a hierarchical knowledge mind-map. With this benchmark, we conduct preliminary evaluations of existing techniques and find that all LLM-based methods under-performing when compared to basic keyword-based retrieval techniques, highlighting substantial opportunities for future research.\narXiv:2406.10291v1\n# 1 Introduction\nLarge language models (LLMs) have demonstrated exceptional performance across tasks related to natural language understanding, generation, and various other domains [1, 2, 3, 4]. The capabilities of LLMs can be significantly augmented through integration with external tools such as code interpreters, gaming simulators, and search engines. This integration facilitates the development of sophisticated autonomous agents capable of receiving feedback and executing tasks in a manner akin to human behavior [5, 6, 7, 8]. Nevertheless, there remains uncertainty regarding the extent to which LLMs can perform tasks necessitating domain-specific expertise and advanced analytical skills, particularly in the context of conducting research on designated topics. The potential of LLMs to conduct research would be profoundly impactful, particularly in light of the rapid development of numerous fields and the accompanying information explosion. In these contexts, learning a topic and composing an academic survey report often necessitates several months of effort by multiple researchers. On the LLM side, it is imperative to acquire the capability to conduct independent research on topics not encompassed within their pre-training datasets. Possessing such an ability would eliminate the necessity for continuous updates and re-training of the entire model, thereby significantly enhancing its practical utility across diverse domains. Previous research involving autonomous agents in tasks that are relatively straightforward and executable by the general public, such as online shopping or playing card games, has demonstrated\nPreprint. Under review.\nPreprint. Under review.\nnotable success, particularly when utilizing models like GPT-4 [6, 9]. However, more challenging categories, such as research tasks that require domain-specific expertise, represent the next frontier for potential advancements by LLM agents. Admittedly, there is a paucity of research in this area, and one of the primary challenges is the absence of standardized benchmarks. To advance the development of research agents capable of conducting comprehensive surveys, we introduce the RESEARCHARENA benchmark, which is rooted in rigorous scholarly content. This benchmark specifically leverages academic papers due to their depth of research, peer-reviewed accuracy, and formal structure\u2014attributes often lacking in other sources such as web pages. The RESEARCHARENA provides an offline environment where autonomous agents can collect and organize information to conduct research across various topics. It comprises three sub-tasks for evaluation: Information Discovery, Information Selection, and Information Organization. These three sub-tasks emulate the general methodology employed by human researchers during literature surveys, which are discussed further below. Researchers typically conduct literature surveys by defining the scope of their inquiry, developing a search protocol, and iteratively reading and organizing papers into an evolving schema. This process culminates in a synthesis of findings to draw conclusions and highlight future research directions [10]. Based on this methodology, our benchmark delineates the surveying process into three specific tasks: Information Discovery, Information Selection, and Information Organization. Notably, we do not include the generation of text as part of the evaluation. This exclusion stems from the premise that a comprehensive understanding of the topic, established during the pre-writing stage through research, should already provide a robust foundation for composing a full-length article [11]. Furthermore, evaluating a complete article is inherently challenging due to variations in individual writing styles. Consequently, we reserve such assessments for future investigations and potentially other benchmarks targeting long-text natural language generation. The Information Discovery task requires LLMs to identify and retrieve relevant academic papers that are foundational to the survey topic, leveraging their ability to navigate and understand vast scholarly corpus. The Information Selection task then challenges the LLMs to critically evaluate these papers based on their scholarly impact and relevance, mimicking the peer review process to ensure only the most significant studies are considered. Lastly, the Information Organization task assesses the LLMs\u2019 ability to synthesize the selected research into a coherent narrative, offering a structured and insightful overview of the topic, through the use of knowledge mind-maps. Our assessments indicate that LLMs frequently underperform when compared to simpler keywordbased search methods, particularly in tasks requiring deep analytical skills. For example, traditional techniques such as utilizing a survey title as a retrieval query consistently outperform LLMs in both Information Discovery and Information Selection, as demonstrated by superior recall and precision metrics. Furthermore, during the Information Organization phase, particularly in the absence of oracle guidance1, LLMs encounter significant challenges in constructing coherent and accurate knowledge structures. This underscores a critical need for enhancements in their ability to manage complex organizational tasks independently. The constructed environment includes 12.0M full-text academic papers and 7.9K survey papers, meticulously curated from the Semantic Scholar Open Research Corpus (S2ORC) [12]. This rigorous selection process ensures a high standard of reliability and scholarly relevance, rendering the dataset ideal for evaluating LLMs designed to execute complex, domain-specific research. By focusing on such a rich and diverse academic base, the dataset supports a robust analysis of LLM capabilities across multiple scientific domains, providing a realistic and challenging environment for benchmarking. Furthermore, the S2ORC is updated on a weekly basis, allowing for the inclusion and evaluation of newer content that extends beyond the LLMs\u2019 knowledge cutoff. The remainder of this paper is structured as follows: After reviewing related work in Section 2, Section 3 details our dataset collection process. Subsequently, Section 4 provides a thorough analysis of the dataset composition and its various statistical properties. Each task within the benchmark, along with their corresponding metrics, is introduced in Section 5. Finally, the evaluations across various baselines are presented in Section 6.\n# 2 Related Work\nPrevious research has employed diverse methodologies to compile datasets featuring academic survey papers. For instance, BIGSURVEY dataset [13] aggregates over 7K survey papers from arXiv and includes approximately 434K eferences from Microsoft Academic Service and Semantic Scholar. This dataset underwent rigorous preprocessing by removing duplicates, unprocessable files, and normalizing text. On the other hand, SURFER100 dataset [14] includes 100 surveys emulating Wikipedia page structures, compiled by eight annotators who summarized content from web pages. Each survey contains predefined sections such as Introduction, History, Key Ideas, Variations, and Applications, summarized concisely in 50 to 150 words. BIGSURVEY dataset provides references in an abstract-only format, offering a concise overview of documents. SURFER100 utilizes Google search results to compile references for each survey topic, reflecting a broad spectrum of web-based information. In contrast, our dataset emphasizes full-text academic papers for a deeper understanding and leverages bibliographic references from original survey papers for enhanced authority and accuracy. The most related LLM agents task in previous research focuses on generating Wikipedia articles. Liu et al. proposed a method for generating English Wikipedia articles by framing the task as a multi-document summarization challenge [15]. Their approach employs a combination of extractive and abstractive summarization techniques. It involves identifying salient information using methods such as TF-IDF and TextRank [16]. In another study, Shao et al. introduced the STORM system [17], which addresses pre-writing challenges such as research and outline preparation. STORM enhances the article generation process by simulating multi-perspective conversations, wherein an LLM poses questions and aggregates responses from reliable sources to develop detailed outlines. While Wikipedia is a valuable resource for obtaining an introductory understanding of a subject, it is inherently limited by the user-authored nature of its content, which does not always guarantee expert oversight. In contrast, rigorous academic research requires a more in-depth and systematic investigation of a topic, often peer-reviewed by experts within the same domain.\n# 3 Collection Methodology\nThis section delineates our methodology for assembling the dataset, which contains three primary stages: survey selection, reference linking, and mind-map extraction. Each stage is indispensable for ensuring the the relevance and accuracy of the dataset, thereby facilitating its application across various benchmark tasks. We begin with the survey selection stage, which concentrates on identifying relevant survey papers. Following this, we proceed to the reference linking stage, where we incorporate bibliographic references from each selected survey. Finally, we address the mind-map extraction stage, detailing the criteria employed to identify knowledge mind-maps from the surveys. Each of these stages and their respective methodologies are presented in the corresponding subsections. At the very end, we provide a quick overview of the dataset.\n# 3.1 Survey Selection\nTo evaluate the research capabilities on designated topics, it is essential first to identify these topics. This was achieved by extracting every survey paper from the S2ORC dataset, based on a combination of keyword-based filtration and rigorous textual analysis. In general, the titles of survey papers encapsulate the topics discussed therein. To compile all relevant survey topics, we first need to identify research surveys from the corpus. We assume that titles of all the topic-specific survey papers contain the term \u201csurvey\u201d, but not every paper satisfying this criteria is an actual survey pertaining to our research theme. In particular, some papers, despite incorporating the keyword, rely heavily on information outside the corpus. This includes population-based survey questionnaires from Medical domains or Redshift surveys using telescope observations in the field of Physics. As a result, the identification was accomplished by a combination of keyword-based filtration and rigorous textual analysis. We first excluded those papers whose titles did not contain \u201csurvey\u201d as\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03dc/03dcc8f5-6624-4174-8506-eed391f955be.png\" style=\"width: 50%;\"></div>\nThe point of a survey\npaper is to provide an organized\nview on the\ncurrent\nstate of the field. If it relies\nheavily on external\ninformation , such as the\nresults of a population\nquestionnaire , do not\ninclude it. Using the above\ncriteria , is the\nfollowing\narticle a\nsurvey\npaper? Respond\neither \"True\" or \"False \".\n<div style=\"text-align: center;\">Figure 1: Instruction with GPT-4 on survey selection.</div>\n# Figure 1: Instruction with GPT-4 on survey selection.\na keyword. Afterwards, we instructed GPT-4 2 to discern the scope and content of each document based on its title and abstract. We included only those papers that provide an organized view on the current state of field concerning a specific topic. The exact wording of the prompts can be found in Figure 1, where approximately 85% of the papers identified through the initial keyword search were discarded. As presented in Appendix B, a manual inspection of 25 samples from the final collection of survey papers revealed that our selection method yielded a 92% accuracy rate. The selection process is certainly not a perfect recall since survey papers may not explicitly include the term \u201csurvey\u201d in their title. However, we believe that the selected papers are sufficiently representative of the broader distribution of survey literature in the field. The corpus for conducting these research surveys is limited to papers with full-text access in S2ORC. Unlike previous works, we believe relying solely on abstract might omit crucial details present in the full text which could contribute to a deeper understanding of the topic. Enforcing this accessibility constraint reduced the number of papers in S2ORC to 12.0 million.\n# 3.2 Reference Linking\nTo evaluate performance in Information Discovery, it is essential to identify the fundamental sources for these surveys. These sources are derived from the bibliographic references cited within each survey paper. We relied on S2ORC for the extracted bibliographies and enforced additional post-processing to discard any papers unsuitable for evaluations. Following the selection of relevant survey papers, we proceeded to compile their bibliographic references. Despite the general reliability of the S2ORC bibliographic resolution system, we encountered discrepancies, such as missing references. These issues were particularly prevalent in documents where the reference header was indistinguishable from the main body text. To address these problems, we excluded any survey papers without references, totaling 406, deeming them unsuitable due to the failure of bibliography extraction. Furthermore, survey papers with no accessible citations were filtered out, amounting to 1,635, as such papers offer no evaluative utility. For references that were successfully extracted, we documented the publication dates for each one. In cases where a reference listed only the year, we assigned the last day of that year as its date to mitigate the risk of information leakage, as discussed in Section 5. Furthermore, citations from S2ORC were categorized based on their contribution to the topic, as outlined by Valenzuela et al. [18] with a supervised classification approach. This categorization involved distinguishing between influential and non-influential references, which is a prerequisite for evaluating the task of Information Selection.\n# 3.3 Mind-Map Extraction\nA common method for organizing information in academic surveys is the utilization of mind-map style typologies, which promote a systematic understanding of the subject under review. Due to the exclusive text-based nature of the S2ORC corpus, we employed an approach to extract such typologies by collecting every figure-caption pair directly from the Semantic Scholar website. Through the analysis of these captions using GPT-4, we identified relevant mind-map figures and transformed the graphical representations into JSON-encoded trees that preserve their hierarchical structure. This process is illustrated in Figure 2, with the prompt provided in Figure 3. The extraction performed by GPT-4 is deemed accurate if the hierarchical structure of the figure is adequately represented by the JSON-encoded tree. Furthermore, relevance is determined if the figure authentically represents a\nT-4 refers to gpt-4-0613 as documented in https://platform.openai.com/docs/mo\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b371/b3714198-b967-44c4-aefd-6522c830b0c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Mind-map extraction from a figure [19] to its JSON representation.</div>\nIdentify\nthe figure\nthat most\nlikely\nillustrates a taxonomy or\noverview. Your\nresponse\nshould be limited to the filename , or NULL if\nnot found. The\nprovided\nfigure\npresents a hierarchy. Extract as JSON -\nencoded\ntree\nwhose\nchildren\nare NULL -terminated.\n<div style=\"text-align: center;\">Figure 3: Instruction with GPT-4 on mind-map extraction.</div>\nknowledge mind-map pertinent to the survey topic. As detailed in Appendix B, a manual inspection of 25 samples from the final collection of mind-maps revealed that our extraction method achieved an accuracy score of 80% and a relevance score of 60%.\n# 3.4 RESEARCHARENA Dataset\nTo ensure the reproducibility of our work and compliance with copyright standards, we developed the dataset from S2ORC, which provides access to 81.1 million academic papers in English from various disciplines. These documents are meticulously structured in a machine-readable format with resolved bibliographic references and annotated inline citations. We used the February 06, 2024 release of S2ORC, which was the most recent version at the start of our project. For a concise summary of our dataset, it consists of approximately 12 million academic papers, each with full-text access, sourced from the Semantic Scholar Open Research Corpus. From this vast repository, we have successfully identified 7,952 survey papers. These surveys have been meticulously analyzed to derive 1,884 mind-maps, which provide structured summaries of the topics covered.\n# 4 Dataset Composition\nUnderstanding the composition of our dataset is essential for ensuring the reliability and comprehensiveness of the benchmark used to evaluate LLMs in academic survey tasks. This section details the makeup of our dataset in terms of disciplinary diversity, reference coverage, and the structural complexity of derived typologies, reflecting on how these factors contribute to the robustness and applicability across various domains. Disciplinary Distribution. We classified each of the 12.0M papers in our public corpus and 7.9K survey papers by the top-5 most popular academic disciplines. This classification was based on the indexing information provided by S2ORC. Frequencies of papers per discipline were then aggregated and visualized to identify trends and imbalances. Figure 4a and 4b revealed significant disparities in the frequency of disciplines between the public corpus and the survey subset. Notably, Computer Science is the most prevalent discipline within surveys but less common in the broader corpus. This could reflect the dynamic nature of the CS field, which often necessitates comprehensive reviews to synthesize rapid advancements and emerging trends. Reference Coverage. For each survey paper, we calculated the coverage ratio as the proportion of its references that were also available within our full-text corpus. We plotted cumulative density functions for each discipline to analyze how extensively the surveys\u2019 references are represented in the broader corpus. As illustrated with Figure 4c, similar patterns were observed across all disciplines, where the density experienced exponential decay as the coverage increases. Approximately 17.18% of the survey subset (i.e., 1.3K survey papers) have at least 50% of their references available. This limitation is mainly attributed to copyright restrictions, where full-text is not permitted by the publisher.\nReference Coverage. For each survey paper, we calculated the coverage ratio as the proportion of its references that were also available within our full-text corpus. We plotted cumulative density functions for each discipline to analyze how extensively the surveys\u2019 references are represented in the broader corpus. As illustrated with Figure 4c, similar patterns were observed across all disciplines, where the density experienced exponential decay as the coverage increases. Approximately 17.18% of the survey subset (i.e., 1.3K survey papers) have at least 50% of their references available. This limitation is mainly attributed to copyright restrictions, where full-text is not permitted by the publisher.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4cd4/4cd4468d-0b45-45dd-8e2d-86339311af44.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Disciplinary distribution of the public corpus. (b) Disciplinary distribution of the survey subset. (c) Reference coverage o the survey subset.</div>\n<div style=\"text-align: center;\">(a) Disciplinary distribution of the public corpus. (b) Disciplinary distribution of the survey subset. (c) Reference coverage of the survey subset. (d) Complexity with the extracted mind-maps.</div>\nFigure 4: Dataset composition analysis with disciplinary distribution, reference coverage, and mindmap complexity. Each of these aspects is critical for benchmark evaluation. Fields of studies like Medicine (Med), Biology (Bio), Physics (Phy), Environmental Science (ES), Computer Science (CS), Engineering (Eng), and Mathematics (Math) are denoted with their abbreviations in the figures.\nMind-Map Complexity. We analyzed the structural complexity of the mind-maps extracted from survey papers by counting the number of nodes and measuring the maximal depth. These measures provide insights into the conceptual breadth and hierarchical depth of the topics covered. The scatter plot from Figure 4d showed that typologies in general have shallow depths but a broad range of nodes, suggesting that while survey topics are extensively branched, they do not delve deeply into sub-topics. In particular, most typologies have a maximum depth ranging from 3 to 7 levels, where the coefficient of the regression line in the scatter plot is approximately 2.04.\n# 5 Benchmark Tasks\nThis section presents a comprehensive overview of the benchmark tasks designed to evaluate the capabilities of research agents in discovering, selecting, and organizing information. Each task targets a specific aspect of research proficiency, with rigorous constraints and evaluation metrics to ensure thorough and unbiased assessment. Information Discovery. Provided a topic extracted from survey title, the task of information discovery requires research agents to identify a subset of documents R from a broader collection D. These documents in R should serve as supporting materials for the topic. Ideally, R should encompass all references cited in the original survey S. However, within the collection D, there may exist another survey S\u2032 that delves into the same topic. If research agents were to use the references from S\u2032 directly, it would circumvent the need for a thorough discovery, defeating the purpose of this task. To prevent information leakage, we impose the additional constraint such that documents in D must be non-survey and published before S. To evaluate performance, we employ standard information retrieval metrics, Recall and Precision, to measure the proportion of relevant documents successfully retrieved and the proportion of retrieved documents that are relevant. Together, these metrics determine the effectiveness and accuracy of the discovery process. For this task, the cutoff parameter K is set at 10 and 100. Information Selection. The task of information selection requires research agents to rank the discovered documents based on their importance to the topic. The labels are distinctions between influential and non-influential citations, as elaborated in Section 3. Normalized Discounted Cumulative Gain (nDCG) [20] and Mean Reciprocal Rank (MRR) [21] are used for evaluation. These measures are crucial because conducting research involves more than merely summarizing retrieved documents; it requires the presentation of key insights from the most significant sources. Furthermore, both human researchers and autonomous agents are limited by their processing capacities. Therefore, it is essential to prioritize and focus on the most critical information first. Information Organization. For information organization, research agents are required to construct a hierarchical knowledge mind-map M based on R. This mind-map should provide a systematic overview of research work developed on topic T. As an intermediate step, references R from the original survey paper could be provided to the agents, who would then focus exclusively on\nconstructing M. In contrast, for an end-to-end version, R is the set of discovered documents from the previous task. For evaluation, two primary metrics are employed: Heading Soft Recall [22] and Heading Entity Recall [17]. These metrics compare the set of node labels from the original and the constructed knowledge mind-maps, referred to as A and B, respectively. To measure similarity of these labels, Heading Soft Recall leverages SENTENCE-BERT [23] embedding, while Heading Entity Recall employs Named Entity Recognition from FLAIR [24] for extraction. The formal definitions for each metric are as follows, where S is the set of labels extracted from the mind-maps.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/29cb/29cb7a0a-4d35-4cf1-b651-0fd95d7768b0.png\" style=\"width: 50%;\"></div>\nWhile these metrics provide a measure of content similarity, they do not account for structural alignment. Tree Editing Distance [25] solves this concern by calculating the minimal number of operations (i.e., relabeling, deleting, and inserting nodes) required to transform one tree into another Nonetheless, relying on Tree Editing Distance alone might overlook the potential for non-exact label matches. To address this, we propose Tree Semantic Distance, which assigns no cost to editing operations involving nodes whose cosine similarity exceeds 0.8.\n# 6 Experiments\nIn this section, we present preliminary evaluations of existing techniques, describing their configurations and performance metrics. These techniques encompass both naive keyword-based methods, such as TITLE, and advanced LLM-based methods, including STORM. The exact wording of the prompts used in each baseline can be found in Appendix C.\n# 6.1 Baselines\nInformation Discovery. For information discovery, research agents are equipped with retrieval tools that enable interaction with the public corpus by submitting queries to retrievers such as BM25 and BGE [26]. These agents are evaluated based on their ability to effectively leverage these tools by generating relevant queries. Since exploration is limited to previously published non-survey literature, retrievers retry with exponential back-off until the cutoff parameter K is satisfied. \u2022 TITLE: Assuming that research topics are encapsulated within survey titles, this method directly employs the title from each survey paper as a query to retrieve relevant materials that support research on the topic. It is important to note that title extraction using S2ORC exhibits variable capitalization across different documents. As a result, we normalize by converting titles to lowercase. \u2022 ZERO-SHOT: Assuming that existing LLMs possess prior knowledge relevant to a survey topic, this method extends the TITLE method by instructing GPT-4 to derive a query from the survey title. This approach leverages the inherent capabilities of LLMs to generate more sophisticated and contextually appropriate queries. \u2022 DECOMPOSER: As discovered by Tushar et al. [27], decomposed prompting is more effective when individual reasoning steps of a task are difficult to learn. This principle is applicable to our case, as a survey topic may consist of multiple sub-topics, making it challenging to directly generate a single query that retrieves all relevant papers. Consequently, we instruct GPT-4 to first deconstruct the research topic into several sub-questions. Each sub-question then generates a corresponding sub-query. These sub-queries are retrieved in batches, and the results are amalgamated using reciprocal rank fusion [28]. \u2022 SELF-RAG: As proposed by Asai et al. [29], SELF-RAG adaptively retrieves passages on demand and utilizes reflection tokens to determine which retrieved documents are relevant to\n<div style=\"text-align: center;\">Table 1: Baseline performance on discovery task, evaluated with Recall@10, Recall@100, Precision@10, and Precision@100, where the retrievers include BM25 and BGE.</div>\nRecall@10\nRecall@100\nPrecision@10\nPrecision@100\nBaseline\nBM25\nBGE\nBM25\nBGE\nBM25\nBGE\nBM25\nBGE\nTITLE\n0.0424\n0.1012\n0.1338\n0.2697\n0.0669\n0.1541\n0.0286\n0.0586\nZERO-SHOT\n0.0382\n0.0832\n0.1253\n0.2287\n0.0602\n0.1232\n0.0256\n0.0464\nDECOMPOSER\n0.0434\n0.0879\n0.1431\n0.2554\n0.0717\n0.1304\n0.0312\n0.0536\nSELF-RAG\n0.0380\n0.0815\n0.1210\n0.2260\n0.0595\n0.1215\n0.0256\n0.0461\nSTORM\n0.0281\n0.0979\n0.0693\n0.1441\n0.0446\n0.1041\n0.0130\n0.0208\nthe instruction, thus continuing the generation based on the pertinent information. It serves as an enhanced version of ZERO-SHOT, where the model is instructed to generate a query from the topic. Because the model refines its final query generation based on the discovered information from intermediate retrievals, it operates as a research agent. \u2022 STORM: As presented in Section 2, STORM conducts research through multi-perspective conversations to compose Wikipedia articles on particular topics from scratch. It closely resembles our scenario, except that the environment involves more rigorous academic papers. We record the retrieval history as STORM continues to probe for additional papers. Upon concluding the final round of conversations, every article within the retrieval history is considered part of the discovered information.\nInformation Selection. For information selection, documents are ranked based on the similarity scores obtained during the discovery phase. For BGE retriever, we rely on FAISS [30] to retrieve based on L2 distance in the embedding space, which is negated to determine similarity. On the other hand, STORM does not explicitly rank the retrieved documents. It is assumed that documents discovered earlier in the conversations are of higher relevance.\nInformation Selection. For information selection, documents are ranked based on the similarity scores obtained during the discovery phase. For BGE retriever, we rely on FAISS [30] to retrieve based on L2 distance in the embedding space, which is negated to determine similarity. On the other hand, STORM does not explicitly rank the retrieved documents. It is assumed that documents discovered earlier in the conversations are of higher relevance. Information Organization. For information organization, the CLUSTERING approach employs Ward\u2019s method for hierarchical clustering on the BGE embedding of every reference article, and the final dendrogram is extracted as typology. The label in each node is computed as the most important TF-IDF word, with ngrams ranging from 1 to 3. FEW-SHOT is achieved by providing a few random examples of extracted typologies and instructing GPT-4 to generate another topic-oriented mind-map. Lastly, the article outline generated by STORM is converted to typology, with headings and their nested sub-headings representing the hierarchy.\n# 6.2 Evaluation Results\nThe baseline experiments were conducted on a single machine equipped with 8 NVIDIA RTX A6000 GPUs, 96 CPU cores, and 128GB RAM. Discussion on the performance metrics is presented below. Information Discovery. As demonstrated in Table 1, the task of information discovery remains challenging for all baseline models. This is illustrated by the Recall@100 metric, which falls below 0.15 for BM25 and 0.27 for BGE. Moreover, agent baselines such as SELF-RAG and STORM consistently achieve the lowest rankings, irrespective of the retrievers employed. This limitation highlights the critical need for more advanced retrieval mechanisms to manage large volumes of documents effectively during information discovery. Information Selection. The performance with information selection is presented in Table 2. The results indicate a consistent trend wherein agent baselines underperform compared to keywordbased methods. The evaluation of nDCG at various levels of document retrieval, such as nDCG@10, nDCG@30, and nDCG@100, provides a quantitative assessment of the ranking performance. Notably, for the TITLE method using the BGE retriever, the nDCG@100 score is 0.2019, which significantly surpasses the score of STORM, which stands at 0.1267. Improvements during the information discovery phase have the potential to enhance overall performance in the selection phase, as evidenced by DECOMPOSER, which ranks the second behind TITLE in discovery and selection tasks. Information Organization. The evaluation on task of information organization under intermediate (i.e., with oracle) and end-to-end (i.e., without oracle) conditions are documented in Table 3. Notably, the metrics exhibit discrepancies across each other, which contrasts with the uniformity observed\n<div style=\"text-align: center;\">Table 2: Baseline performance on selection task, evaluated with nDCG@10, nDCG@30, nDCG@100, and Precision@100, where the retrievers include BM25 and BGE.</div>\nnd Precision@100, where the retrievers include BM25 and BGE.\nnDCG@10\nnDCG@30\nnDCG@100\nMRR\nBaseline\nBM25\nBGE\nBM25\nBGE\nBM25\nBGE\nBM25\nBGE\nTITLE\n0.0711\n0.1678\n0.0775\n0.1754\n0.0941\n0.2019\n0.1903\n0.3816\nZERO-SHOT\n0.0634\n0.1346\n0.0692\n0.1417\n0.0856\n0.1657\n0.1743\n0.3246\nDECOMPOSER\n0.0735\n0.1445\n0.0803\n0.1554\n0.0986\n0.1838\n0.1959\n0.3510\nSELF-RAG\n0.0627\n0.1341\n0.0679\n0.1415\n0.0837\n0.1646\n0.1705\n0.3233\nSTORM\n0.0445\n0.1275\n0.0507\n0.1322\n0.0524\n0.1267\n0.1271\n0.3206\n<div style=\"text-align: center;\">Table 3: Baseline performance on organization task, evaluated with Heading Soft Recall, Heading Entity Recall, and Tree Semantic Distance, across intermediate and end-to-end conditions.</div>\nOracle\nBaseline\nHeading Soft\nRecall (\u2191)\nHeading Entity\nRecall (\u2191)\nTree Semantic\nDistance (\u2193)\nYes\nCLUSTERING\n0.6074\n0.2104\n45.69\nSTORM\n0.7325\n0.3098\n60.04\nNo\nFEW-SHOT\n0.8408\n0.2446\n49.83\nSTORM.BM25\n0.7940\n0.2938\n66.65\nSTORM.BGE\n0.7842\n0.2693\n65.93\nin previous discovery and selection tasks. This divergence is expected due to the distinct nature of the metrics: Heading Soft Recall and Heading Entity Recall assess content similarity, whereas Tree Semantic Distance evaluates structural alignment.\nin previous discovery and selection tasks. This divergence is expected due to the distinct nature of the metrics: Heading Soft Recall and Heading Entity Recall assess content similarity, whereas Tree Semantic Distance evaluates structural alignment. In the intermediate version, where references are provided to LLMs, the proportion of correctly included entities, as measured by Heading Entity Recall, is slightly higher. Specifically, STORM achieved a recall rate of 0.3098, outperforming the end-to-end condition. Conversely, when it comes to constructing the hierarchy, CLUSTERING outperforms advanced LLM-based agents, as evidenced by its attainment of the lowest Tree Semantic Distance of 45.69 among all baseline methods.\n# 7 Limitation\nDespite the robust framework and extensive dataset provided by ResearchArena, this study has several limitations. Firstly, the offline environment, though comprehensive, may not accurately represent the dynamic and interconnected nature of live databases and the internet. This discrepancy could potentially limit the applicability of the findings in real-world research settings. Additionally, due to copyright constraints, not every full-text reference of the survey papers could be included. This omission could affect the comprehensive understanding of the survey topics under investigation. Finally, there is no evaluation on text generation but mostly the surveying process. However, even if this is just the first step of conducting research, LLM agents have already shown deficiencies. Future iterations of ResearchArena should address this issue, particularly as these agents improve.\n# 8 Conclusion\nIn conclusion, ResearchArena introduces a rigorous benchmark designed to evaluate LLMs in conducting research surveys on designated topics. By systematically decomposing the survey process into distinct tasks like information discovery, selection, and organization, this benchmark provides a detailed framework for evaluating autonomus research agents. Our findings underscore the potential of LLMs to revolutionize academic research, provided that future advancements can bridge the existing performance gaps. Grounded in Semantic Scholar Open Research Corpus, this work establishes a robust foundation for the future, aiming to improve the ability of LLMs to autonomously conduct expertise-level, domain-specific research.\n# References\n[1] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022. [2] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023. [3] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476, 2023. [4] Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. arXiv preprint arXiv:2305.18486, 2023. [5] Opendevin: Code less, make more. https://github.com/OpenDevin/OpenDevin, 2024. [6] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [7] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. [8] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and Maosong Sun. Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. [9] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. 10] literature review - how to write a survey paper? - academia stack exchange. https:// academia.stackexchange.com/questions/43371/how-to-write-a-survey-paper, 2015. 11] Laura Dietz and John Foley. Trec car y3: Complex answer retrieval overview. In Proceedings of Text REtrieval Conference (TREC), 2019. 12] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Dan S Weld. S2orc: The semantic scholar open research corpus. arXiv preprint arXiv:1911.02782, 2019. 13] Shuaiqi Liu, Jiannong Cao, Ruosong Yang, and Zhiyuan Wen. Generating a structured summary of numerous academic papers: Dataset and method. arXiv preprint arXiv:2302.04580, 2023. 14] Irene Li, Alexander Fabbri, Rina Kawamura, Yixin Liu, Xiangru Tang, Jaesung Tae, Chang Shen, Sally Ma, Tomoe Mizutani, and Dragomir Radev. Surfer100: Generating surveys from web resources, wikipedia-style. arXiv preprint arXiv:2112.06377, 2021. 15] Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018. 16] Rada Mihalcea and Paul Tarau. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404\u2013411, 2004. 17] Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, and Monica S Lam. Assisting in writing wikipedia-like articles from scratch with large language models. arXiv preprint arXiv:2402.14207, 2024.\n[18] Marco Valenzuela, Vu Ha, and Oren Etzioni. Identifying meaningful citations. In Workshops at the twenty-ninth AAAI conference on artificial intelligence, 2015. [19] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9):1\u201335, 2023. [20] Kalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. Cumulated gain-based evaluation of ir techniques. ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446, 2002. [21] EM Voorhees. Proceedings of the 8th text retrieval conference. TREC-8 Question Answering Track Report, pages 77\u201382, 1999. [22] Pasi Fr\u00e4nti and Radu Mariescu-Istodor. Soft precision and recall. Pattern Recognition Letters, 167:115\u2013121, 2023. [23] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China, November 2019. Association for Computational Linguistics. [24] Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. FLAIR: An easy-to-use framework for state-of-the-art NLP. In Waleed Ammar, Annie Louis, and Nasrin Mostafazadeh, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 54\u201359, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [25] Kaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM journal on computing, 18(6):1245\u20131262, 1989. [26] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. C-pack: Packaged resources to advance general chinese embedding. arXiv preprint arXiv:2309.07597, 2023. [27] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks. arXiv preprint arXiv:2210.02406, 2022. [28] Gordon V Cormack, Charles LA Clarke, and Stefan Buettcher. Reciprocal rank fusion outperforms condorcet and individual rank learning methods. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 758\u2013759, 2009. [29] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2023. [30] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535\u2013547, 2019. [31] Corby Rosset, Ho-Lam Chung, Guanghui Qin, Ethan C Chau, Zhuo Feng, Ahmed Awadallah, Jennifer Neville, and Nikhil Rao. Researchy questions: A dataset of multi-perspective, decompositional questions for llm web agents. arXiv preprint arXiv:2402.17896, 2024.\n# A Parameters with Collection Methodology\nIn order to ensure deterministic behavior during dataset construction, the temperature is set to 0, and the seed is fixed at 42 when utilizing GPT-4 for chat completions. The choice of the number 42 is arbitrary; other numbers could be equally effective, provided that the seed remains constant throughout the dataset collection process to maintain reproducibility.\n<div style=\"text-align: center;\">e 4: Evaluation on the quality of survey selection and mind-map extractio</div>\nCorpus ID\nAccurate\nSelection\nCorpus ID\nObject ID\nAccurate\nExtraction\nRelevant\nExtraction\n1359411\nYes\n3373610\n2-Figure1-1.png\nYes\nYes\n2197301\nNo\n10837932\n6-TableII-1.png\nYes\nYes\n3638888\nYes\n20774863\n2-Figure1-1.png\nYes\nNo\n3799929\nYes\n21265344\n4-Figure3-1.png\nNo\nNo\n4470807\nYes\n52986472\n4-Figure1-1.png\nYes\nYes\n7972041\nYes\n54437297\n6-Figure1-1.png\nNo\nYes\n44951320\nYes\n59407515\n4-Figure1-1.png\nYes\nNo\n56895486\nYes\n67855323\n3-Figure1-1.png\nYes\nNo\n115156611\nYes\n201532876\n6-Figure1-1.png\nYes\nNo\n126187216\nYes\n204080064\n5-Figure1-1.png\nNo\nNo\n134642625\nYes\n218487045\n7-Figure4-1.png\nYes\nYes\n209386804\nYes\n221938634\n6-Figure1-1.png\nYes\nYes\n214566304\nYes\n226300094\n2-Figure2-1.png\nYes\nYes\n229474407\nYes\n227259882\n6-Figure2-1.png\nNo\nYes\n233241600\nNo\n232126642\n3-Figure1-1.png\nYes\nNo\n234790465\nYes\n233677020\n6-Figure2-1.png\nNo\nNo\n235794880\nYes\n237291802\n2-Figure1-1.png\nYes\nYes\n245433612\nYes\n237327839\n6-Figure1-1.png\nYes\nNo\n253735066\nYes\n240011970\n5-Figure4-1.png\nYes\nYes\n254563889\nYes\n246599122\n2-Figure1-1.png\nYes\nNo\n258060212\nYes\n248227736\n2-Figure1-1.png\nYes\nYes\n258541526\nYes\n248717714\n4-Figure2-1.png\nYes\nYes\n258841314\nYes\n252089272\n4-Figure1-1.png\nYes\nYes\n259855591\nYes\n258212628\n6-Figure1-1.png\nYes\nYes\n262464721\nYes\n260887757\n4-Figure2-1.png\nYes\nYes\n# B Quality of Collection Methodology\nRecords of manual inspection over 25 samples from surveys and typologies are presented in Tabl\nRecords of manual inspection over 25 samples from surveys and typologies are presented in Table 4.\n# C Prompts with Experiments\nC.1 Prompt to DECOMPOSER for Information Discovery\n<div style=\"text-align: center;\">Adopted from the Researchy Questions by Rosset et al. [31].</div>\n\"\"\"\n### Below is an example on how to decompose a complex\nquestion\ninto\nsub -questions\nand search\nqueries.\nQuestion: should the death\npenalty be legalized?\n<Decomposition >\n- What are the\narguments in favor of the death\npenalty?\n- Does the death\npenalty\nserve as a deterrent to crime?\n- Is the death\npenalty a just\npunishment\nfor\ncertain\ncrimes?\n- How does the death\npenalty\ncompare to other\nforms of\npunishment in terms of cost and\neffectiveness ?\n- What are the\narguments\nagainst\nthe death\npenalty?\n- What is the risk of executing\ninnocent\npeople\nwith a death\npenalty?\n- Are there any\nethical\nconcerns\nsurrounding\nthe death\npenalty\n?\n- To what\nextent is the death\npenalty\napplied\nfairly and\nwithout\nbias?\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99f2/99f2dfd7-7e4e-429b-943a-a2d4ea3629a2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">C.2 Prompt to ZERO-SHOT / SELF-RAG for Information Discover</div>\nC.2\nPrompt to ZERO-SHOT / SELF-RAG for Information Discovery\nCreate a search\nquery\nthat\ngathers\nsupporting\nmaterials\nfor\nwriting a\nsurvey\npaper on the\nfollowing\ntopic: {x}.\nC.3\nPrompt to FEW-SHOT for Information Organization\n\"\"\"\n###\nExamples\n13\nCreate a search\nquery\nthat\ngathers\nsupporting\nmaterials\nfor\nwriting a\nsurvey\npaper on the\nfollowing\ntopic: {x}.\nC.3\nPrompt to FEW-SHOT for Information Organization\n\"\"\"\n###\nExamples\nC.3 Prompt to FEW-SHOT for Information Organization\n### Examples\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/afe4/afe41d13-5400-4e16-816b-088b19b208cc.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The rapid development of numerous fields and the accompanying information explosion necessitate the ability for large language models (LLMs) to conduct independent research on topics not covered in their pre-training datasets. This capability would significantly enhance their practical utility across diverse domains.",
            "purpose of benchmark": "The benchmark is intended to evaluate LLM agents' ability to conduct academic surveys, a crucial initial step in the academic research process."
        },
        "problem": {
            "definition": "The benchmark addresses the ability of LLMs to conduct academic surveys through three specific tasks: information discovery, information selection, and information organization.",
            "key obstacle": "Existing benchmarks lack standardization and do not adequately evaluate the complex tasks requiring domain-specific expertise and advanced analytical skills."
        },
        "idea": {
            "intuition": "The benchmark was inspired by the need for LLMs to perform research tasks that require deep analytical skills, which have previously been underexplored in autonomous agents.",
            "opinion": "The authors believe that the benchmark is crucial for advancing the capabilities of LLMs in conducting comprehensive academic research.",
            "innovation": "ResearchArena differs from previous benchmarks by focusing specifically on the surveying process and providing a structured evaluation framework for LLMs in academic contexts.",
            "benchmark abbreviation": "RESEARCHARENA"
        },
        "dataset": {
            "source": "The dataset was constructed from the Semantic Scholar Open Research Corpus (S2ORC), which provides access to a vast collection of academic papers.",
            "desc": "The dataset includes approximately 12.0 million full-text academic papers and 7,952 survey papers, providing a rich environment for evaluating LLM capabilities.",
            "content": "The dataset contains full-text academic papers, survey papers, and mind-map representations of the surveyed topics.",
            "size": "12,000,000",
            "domain": "Academic Research",
            "task format": "Information Discovery"
        },
        "metrics": {
            "metric name": "Recall, Precision",
            "aspect": "Effectiveness and accuracy of the information discovery process.",
            "principle": "Recall and Precision were chosen as they are standard metrics in information retrieval that measure the proportion of relevant documents retrieved.",
            "procedure": "Performance is evaluated based on the proportion of relevant documents successfully retrieved (Recall) and the proportion of retrieved documents that are relevant (Precision)."
        },
        "experiments": {
            "model": "The benchmark evaluates both state-of-the-art LLM-based methods and traditional keyword-based retrieval techniques.",
            "procedure": "Models are tested on their ability to discover, select, and organize information based on academic survey topics, utilizing various retrieval and ranking methods.",
            "result": "The experiments reveal that LLM-based methods generally underperform compared to basic keyword-based retrieval techniques, highlighting areas for improvement.",
            "variability": "Variability in results was accounted for by conducting multiple trials and using different subsets of the dataset."
        },
        "conclusion": "ResearchArena establishes a rigorous benchmark for evaluating LLMs in conducting academic surveys, revealing significant performance gaps that need to be addressed in future research.",
        "discussion": {
            "advantage": "The benchmark contributes to the field by providing a structured framework for evaluating LLM capabilities in academic research tasks.",
            "limitation": "The offline environment may not fully represent the dynamic nature of real-world databases, and copyright constraints limit the inclusion of all relevant full-text references.",
            "future work": "Future research should focus on improving LLMs' performance in conducting complex research tasks and expanding the benchmark to include text generation evaluations."
        },
        "other info": {
            "additional notes": "The benchmark emphasizes the importance of domain-specific expertise in research tasks and the need for standardized evaluation methods."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The rapid development of numerous fields and the accompanying information explosion necessitate the ability for large language models (LLMs) to conduct independent research on topics not covered in their pre-training datasets."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the ability of LLMs to conduct academic surveys through three specific tasks: information discovery, information selection, and information organization."
        },
        {
            "section number": "2.2",
            "key information": "Existing benchmarks lack standardization and do not adequately evaluate the complex tasks requiring domain-specific expertise and advanced analytical skills."
        },
        {
            "section number": "4.1",
            "key information": "The benchmark was inspired by the need for LLMs to perform research tasks that require deep analytical skills, which have previously been underexplored in autonomous agents."
        },
        {
            "section number": "8.2",
            "key information": "The benchmark emphasizes the importance of domain-specific expertise in research tasks and the need for standardized evaluation methods."
        },
        {
            "section number": "10.1",
            "key information": "The offline environment may not fully represent the dynamic nature of real-world databases, and copyright constraints limit the inclusion of all relevant full-text references."
        }
    ],
    "similarity_score": 0.7276455367615848,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/ResearchArena_ Benchmarking LLMs' Ability to Collect and Organize Information as Research Agents.json"
}