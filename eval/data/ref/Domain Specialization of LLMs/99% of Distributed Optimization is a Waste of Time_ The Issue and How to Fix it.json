{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1901.09437",
    "title": "99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it",
    "abstract": "Many popular distributed optimization methods for training machine learning models fit the following template: a local gradient estimate is computed independently by each worker, then communicated to a master, which subsequently performs averaging. The average is broadcast back to the workers, which use it to perform a gradient-type step to update the local version of the model. It is also well known that many such methods, including SGD, SAGA, and accelerated SGD for over-parameterized models, do not scale well with the number of parallel workers. In this paper we observe that the above template is fundamentally inefficient in that too much data is unnecessarily communicated by the workers, which slows down the overall system. We propose a fix based on a new update-sparsification method we develop in this work, which we suggest be used on top of existing methods. Namely, we develop a new variant of parallel block coordinate descent based on independent sparsification of the local gradient estimates before communication. We demonstrate that with only $m/n$ blocks sent by each of $n$ workers, where $m$ is the total number of parameter blocks, the theoretical iteration complexity of the underlying distributed methods is essentially unaffected. As an illustration, this means that when $n=100$ parallel workers are used, the communication of $99\\%$ blocks is redundant, and hence a waste of time. Our theoretical claims are supported through extensive numerical experiments which demonstrate an almost perfect match with our theory on a number of synthetic and real datasets.",
    "bib_name": "mishchenko201999distributedoptimizationwaste",
    "md_text": "# 99% of Distributed Optimization is a Waste of Time: The Issue and How to Fix it\n# Konstantin Mishchenko\nntin Mis KAUST\nThuwal, Saudi Arabia\n 4 Jun 2019\n# Peter Richt\u00b4arik\u2217\ner Richt KAUST\nThuwal, Saudi Arabia peter.richtarik@kaust.edu.sa\nMany popular distributed optimization methods for training machine learning models fit the following template: a local gradient estimate is computed independently by each worker, then communicated to a master, which subsequently performs averaging. The average is broadcast back to the workers, which use it to perform a gradient-type step to update the local version of the model. It is also well known that many such methods, including SGD, SAGA, and accelerated SGD for over-parameterized models, do not scale well with the number of parallel workers. In this paper we observe that the above template is fundamentally inefficient in that too much data is unnecessarily communicated by the workers, which slows down the overall system. We propose a fix based on a new update-sparsification method we develop in this work, which we suggest be used on top of existing methods. Namely, we develop a new variant of parallel block coordinate descent based on independent sparsification of the local gradient estimates before communication. We demonstrate that with only m/n blocks sent by each of n workers, where m is the total number of parameter blocks, the theoretical iteration complexity of the underlying distributed methods is essentially unaffected. As an illustration, this means that when n = 100 parallel workers are used, the communication of 99% blocks is redundant, and hence a waste of time. Our theoretical claims are supported through extensive numerical experiments which demonstrate an almost perfect match with our theory on a number of synthetic and real datasets.\n# 1 Introduction\nIn this work we are concerned with parallel/distributed algorithms for solving finite sum minimization problems of the form\n\ufffd where each fi is convex and smooth. In particular, we are interested in methods which employ n parallel units/workers/nodes/processors, each of which has access to a single function fi and its gradients (or unbiased estimators thereof). Let x\u2217be an optimal solution of (1). In practical parallel \u2217Also affiliated with the Moscow Institute of Physics and Technology, Dolgoprudny, Russia.\nPreprint. Under review.\n(1)\nwhere the expectation is with respect to a distribution of training examples stored locally at machine i. More typically, however, each machine contains a very large but finite number of examples (for simplicity, say there are l examples on each machine), and fi is of the form\nIn the rest of this section we provide some basic motivation and intuitions in support of our approach. To this purpose, assume, for simplicity of exposition, that fi is of the finite-sum form (3). In typical modern machine learning workloads, the number of machines n is much smaller than the number of data points on each machine l. In a large scale regime (i.e., when the model size d, the number of data points nl, or both are large), problem (1) needs to be solved by a combination of efficient methods and modern hardware. In recent years there has been a lot of progress in designing new algorithms for solving this problem using techniques such as stochastic approximation [34], variance reduction [35, 17, 9], coordinate descent [25, 32, 41] and acceleration [26], resulting in excellent theoretical and practical performance. The computational power of the hardware is increasing as well. In recent years, a very significant amount of such increase is due to parallelism. Since many methods, such as minibatch Stochastic Gradient Descent (SGD), are embarrassingly parallel, it is very simple to use them in big data applications. However, it has been observed in practice that adding more resources beyond a certain limit does not improve iteration complexity significantly. Moreover, having more parallel units makes their synchronization harder due to so-called communication bottleneck. Minibatch versions of most variance reduced methods2 such as SAGA [9] or SVRG [17] scale even worse in parallel setting \u2013 they do not guarantee, in the worst case, any speedup from using more than one function at a time. Unfortunately, numerical experiments show that this is not a proof flaw, but rather a real property of these methods [12]. A similar observation was made for SVRG in [42], where it was shown that only a small number of partial derivatives are needed at each iteration. Since there are too many possible situations, we choose to focus on black-box optimization, although we admit that much can be achieved by assuming the sparsity structure. In fact, for any method there exists a toy situation where the method would scale perfectly \u2013 one simply needs to assume that each function fi depends on its own subset of coordinates and minimize each fi independently. This can be generalized assuming sparsity patterns [18, 19] to get almost linear scaling if any coordinate appears in a small number of functions. Our interest, however, is in explaining situations as in [12] where the models almost do not scale. In this paper, we demonstrate that a simple trick of independent block sampling can remedy the problem of scaling, to a substantial but limited extent. To illustrate one of the key insights of our paper on a simple example, in what follows consider a thought experiment in which GD is a baseline method we would want to improve on.\n# 1.1 From gradient descent to block coordinate descent and back\nA simple benchmark in the distributed setting is a parallel implementation of gradient descent (GD). GD arises as a special case of the more general class of block coordinate descent methods (BCD) [33]. The conventional way to run BCD for problem (1) is to update a single or several blocks3 of x, chosen at random, on all n machines [33, 10], followed by an update aggregation step. Such updates on each worker typically involve a gradient step on a subspace corresponding to the selected blocks. Importantly, and this is a key structural property of BCD methods, the same set of blocks is updated on each machine. If communication is expensive, it often makes sense to do more work on each machine, which in the context of BCD means updating more blocks. A particular special case is to update all blocks, which leads to parallel implementation of GD for problem (1), as mentioned above.\n2We shall mention that there are already a few variance reduced methods that scale, up to some level, linearly in a parallel setup: Quartz for sparse data [30], Katyusha [2], or SAGA/SVRG/SARAH with importance sampling for non-convex problems [16]. 3Assume the entries of x are partitioned into several non-overlapping blocks.\n(2)\n(3)\nMoreover, it is known that the theoretical iteration complexity of BCD improves as the number of blocks updated increases [33, 28, 29]. For these and similar reasons, GD (or one of its variants, such as GD with momentum), is often the preferable method to BCD. Having said that, we did not choose to describe BCD only to discard it at this point; we shall soon return to it, albeit with a twist.\n# 1.2 From gradient descent to independent block coordinate desce\nBecause of what we just said, iteration complexity of GD will not improve by any variant running BCD; it can only get worse. Despite this, we propose to run BCD, but a new variant which allows each worker to sample an independent subset of blocks instead. This variant of BCD for (1) was not considered before. As we shall show, our independent sampling approach leads to a better-behaved aggregated gradient estimator when compared to that of BCD, which in turn leads to better overall iteration complexity. We call our method independent block coordinate descent (IBCD). We provide a unified analysis of our method, allowing for a random subset of \u03c4m out of a total of m blocks to be sampled on each machine, independently from other machines. GD arises as a special case of this method by setting \u03c4 = 1. However, as we show (see Corollary 1), the same iteration complexity guarantee can be obtained by choosing \u03c4 as low as \u03c4 = 1/n. The immediate consequence of this result is that it is suboptimal to run GD in terms of communication complexity. Indeed, GD needs to communicate all m blocks per machine, while IBCD achieves the same rate with m/n blocks per machine only. Coming back to the abstract, consider an example with n = 100 machines. In this case, when compared to GD, IBCD only communicates 1% of the data. Because the iteration complexities of the two methods are the same, and if communication cost is dominant, this means that the problem can be solved in just 1% of the time. In contrast, and when compared to the potential of IBCD, parallel implementation of GD inevitably wastes 99% of the time. The intuition behind why our approach works lies in the law of large numbers. By averaging independent noise we reduce the total variance of the resulting estimator by the factor of n. If, however, the noise is already tiny, as, in non-accelerated variance reduced methods, there is no improvement. On the other hand, (uniform) block coordinate descent (CD) has variance proportional to 1/\u03c4 [39], where \u03c4 < 1 is the ratio of used blocks. Therefore, after the averaging step the variance is 1/\u03c4n, which illustrates why setting any \u03c4 > 1/n should not yield a significant speedup when compared to the choice \u03c4 = 1/n. It also indicates that it should be possible to throw away a (1 \u22121/n) fraction of blocks while keeping the same convergence rate.\n# 1.3 Beyond gradient descent and further contributions\nThe goal of the above discussion was to introduce one of the ideas of this paper in a gentle way. However, our independent sampling idea has immense consequences beyond the realm of GD, as we show in the rest of the paper. Let us summarize the contributions here:\nHowever, our independent sampling idea has immense consequences beyond the realm of GD, as we show in the rest of the paper. Let us summarize the contributions here: \u2022 We show that the independent sampling idea can be coupled with variance reduction/SAGA (see Sec 4), SGD for problem (1)+(2) (see Sec F), acceleration (under mild assumption on stochastic gradients; see Sec G) and regularization/SEGA (see Sec 5). We call the new methods ISAGA, ISGD, IASGD and ISEGA, respectively. We also develop ISGD variant for asynchronous distributed optimization \u2013 IASGD (Sec H). \u2022 We present two versions of the SAGA algorithm coupled with IBCD. The first one is for a distributed setting, where each machine owns a subset of data and runs a SAGA iteration with block sampling locally, followed by aggregation. The second version is in a shared data setting, where each machine has access to all functions. This allows for linear convergence even if \u2207fi(x\u2217) \u0338= 0. \u2022 We show that when combined with IBCD, the SEGA trick [14] leads to a method that enjoys a linear rate for problems where \u2207fi(x\u2217) \u0338= 0 and allows for more general objectives which may include a non-separable non-smooth regularizer.\n# 2 Practical Implications and Limitations\nsection, we outline some further limitations and practical implications of \ne outline some further limitations and practical implications of our frame\n#\nName\nOrigin\n\u2207fi(x\u2217) \u0338= 0\nLin. rate\nIn-machine\nrandomization\nNote\n1\nIBCD\nI+ CD [25]\n\u0017\n\u0013\n\u0017\nSimplest\n2\nISEGA\nI + SEGA [14]\n\u0013\n\u0013\n\u0017\nAllows prox\n3\nIBGD\nI+ GD\n\u0017\n\u0013\n\u0017\nBernoulli, no CD\n4\nISAGA\nI+ SAGA [9]\n\u0013\n\u0013\n\u0013\nShared memory\n5\nISAGA\nI+ SAGA [9]\n\u0017\n\u0013\n\u0013\n6\nISGD\nI + SGD [34]\n\u0013\n\u0017\n\u0013\n+ Non-convex\n7\nIASGD\nI + ASGD [37]\n\u0013\n\u0017\n\u0013\n8\nIASGD\nI + ASGD [31]\n\u0013\n\u0017\n\u0013\nAsynchronous\nTable 1: Summary of all algorithms proposed in the paper.\n# 2.1 Main limitation\nThe main limitation of this work is that independent sampling does not generally result in a sparse aggregated update. Indeed, since each machine might sample a different subset of blocks, all these updates add up to a dense one, and this problem gets worse as n increases, other things equal. For instance, if every parallel unit updates a single unique block4, the total number of updated blocks is equal n. In contrast, standard BCD, one that samples the same block on each worker, would update a single block only. For simple linear problems, such as logistic regression, sparse updates allow for a fast implementation of BCD via memorization of the residuals. However, this limitation is not crucial in common settings where broadcast is much faster than reduce.\n# 2.2 Practical implications\nThe main body of this work focuses on theoretical analysis and on verifying our claims via experiments. However, there are several straightforward and important applications of our technique. Distributed synchronous learning. A common way to run a distributed optimization method is to perform a local update, communicate the result to a parameter server using a \u2019reduce\u2019 operation, and inform all workers using \u2019broadcast\u2019. Typically, if the number of workers is significantly large, the bottleneck of such a system is communication. In particular, the \u2019reduce\u2019 operation takes much more time than \u2019broadcast\u2019 as it requires to add up different vectors computed locally, while \u2019broadcast\u2019 informs the workers about the same data (see [22] for a numerical validation that \u2019broadcast\u2019 is 10-20 times faster across a wide range of dimensions). Nevertheless, if every worker can instead send to the parameter server only \u03c4 = 1/n fraction of the d-dimensional update, essentially the server node will receive just one full d-dimensional vector, and thus our approach can compete against methods like QSGD [1], signSGD [3], TernGrad [40], DGC [20] or ATOMO [38]. In fact, our approach may completely remove the communication bottleneck. Distributed asynchronous learning. The main difference with the synchronous case is that only one-to-one communications will be used instead of highly efficient \u2019reduce\u2019 and \u2019broadcast\u2019. Clearly, the communication to the server will be much faster with \u03c4 = 1/n, so the main question is how to make the communication back fast as well. Hopefully, the parameter server can copy the current vector and send it using non-blocking communication, such as isend() in MPI4PY [7]. Then, the communication back will not prevent the server from receiving the new updates. We combine the IBCD approach with asynchronous updates, which leads to a new method: IASGD (Algorithm 8). Distributed sparse learning. Large datasets, such as binary classification data from LibSVM, often have sparse gradients. In this case, the \u2019reduce\u2019 operation is not efficient and one needs to communicate data by sending positions of nonzeros and their values. Moreover, as we prove later, one can use independent sampling with \u21131-penalty, which makes the problem solution sparse. In that case, only communication from a worker to the parameter server is slow, so both synchronous and asynchronous methods gain in performance. Methods with local subproblems. One can also try to extend our analysis to methods with exact block-coordinate minimization or primal-dual and proximal methods such as Point-SAGA [8],\n4Assume x is partitioned into several \u201cblocks\u201d of variables.\nPDHG [4], DANE [36], etc. There, by restricting ourselves to a subset of coordinates, we may obtain a subproblem that is easier to solve by orders of magnitude. Block-separable problems within machines. Given that the local problem on each machine is block coordinate-wise separable, partial derivative blocks can be evaluated 1/\u03c4 times cheaper than the gradients. Thus, independent sampling improves scalability at no cost. Such problems can be obtained considering the dual problem, as is done in [21], for example. For a comprehensive list of frequently used notation, see Table 2 in the supplementary material.\n# 3 Independent Block Coordinate Descent\n# 3.1 Technical assumptions\nWe present the most common technical assumptions required in order to derive convergence rates. Definition 1. Function F is L smooth if for all x, y \u2208Rd we have:\n  Similarly, F is \u00b5 strongly convex if for all x, y \u2208Rd: F(x) \u2265F(y) + \u27e8\u2207F(y), x \u2212y\u27e9+ \u00b5 2 \u2225x \u2212y\u22252 2.\nIn most results we present, functions fi are required to be smooth and convex, and f strongly convex. Assumption 1. For every i, function fi is convex, L smooth and function f is \u00b5 strongly convex. As mentioned, since independent sampling does not preserve the variance reduction property, in some of our results we shall consider \u2207fi(x\u2217) = 0 for all i. Assumption 2. For all 1 \u2264i \u2264n we have \u2207fi(x\u2217) = 0.\nIn Sec 4 we show that Assumption 2 can be dropped once the memory is shared among the machines. Further, in Sec 5 we show that Assumption 2 can be dropped even in the fully distributed setup using the SEGA trick. Lastly, Assumption 2 is naturally satisfied in many applications. For example, in least squares setting min \u2225Ax \u2212b\u22252 2, it is equivalent to existence of x\u2217such that Ax\u2217= b. On the other hand, current state-of-the-art deep learning models are often overparameterized so that they allow zero training loss, which is again equivalent to \u2207fi(x\u2217) = 0 for all i (however, such problems are typically non-convex).\n# 3.2 Block structure of Rd\nLet Rd be partitioned into m blocks u1, . . . , um of arbitrary sizes, so that the parameter space is R|u1| \u00d7 \u00b7 \u00b7 \u00b7 R|um|. For any vector x \u2208Rd and a set of blocks U we denote by xU the vector that has the same coordinate as x in the set of blocks U and zeros elsewhere.\nIn order to provide a quick taste of our results, we first present the IBCD method described in th introduction and formalized as Algorithm 1.\nA key parameter of the method is 1/m \u2264\u03c4 \u22641 (chosen so that \u03c4m is an integer), representing a fraction of blocks to be sampled by each worker. At iteration t, each machine independently samples a subset of \u03c4m blocks U t i \u2286{u1, . . . , um}, uniformly at random. The ith worker then performs a subspace gradient step of the form xt+1 i = xt \u2212\u03b3(\u2207fi(xt))U t i , where \u03b3 > 0 is a stepsize. Note that only coordinates of xt belonging to U t i get updated. This is then followed by aggregating all n gradient updates: xt+1 = 1 n \ufffd i xt+1 i .\n# \ufffd 3.4 Convergence of IBCD\nTheorem 1 provides a convergence rate for Algorithm 1. Admittedly, the assumptions of Theorem 1 are somewhat restrictive; in particular, we require \u2207fi(x\u2217) = 0 for all i. However, this is necessary.\n(4)\n(5)\nAlgorithm 1\n1: Input: x0 \u2208Rd, partition of Rd into m blocks u1, . . . , um, ratio of blocks to be sampled \u03c4,\nstepsize \u03b3, # of parallel units n\n2: for t = 0, 1, . . . do\n3:\nfor i = 1, . . . , n in parallel do\n4:\nSample independently and uniformly a subset of \u03c4m blocks U t\ni \u2286{u1, . . . , um}\n5:\nxt+1\ni\n= xt \u2212\u03b3(\u2207fi(xt))U t\ni\n6:\nend for\n7:\nxt+1 = 1\nn\n\ufffdn\ni=1 xt+1\ni\n8: end for\nIndeed, in general one can not expect to have \ufffdn i=1(\u2207fi(x\u2217))Ui = 0 (which would be required for the method to converge to x\u2217) for independently sampled sets of blocks Ui unless \u2207fi(x\u2217) = 0 for all i. As mentioned, the issue is resolved in Sec 5 using the SEGA trick [14]. Theorem 1. Suppose that Assumptions 1, 2 hold. For Algorithm 1 with \u03b3 = n \u03c4n+2(1\u2212\u03c4) 1 2L we have\nE \ufffd \u2225xt \u2212x\u2217\u22252 2 \ufffd \u2264 \ufffd 1 \u2212 \u00b5 2L \u03c4n \u03c4n+2(1\u2212\u03c4) \ufffdt \u2225x0 \u2212x\u2217\u22252 2.\n\ufffd \ufffd \ufffd \ufffd As a consequence of Theorem 1, we can choose \u03c4 as small as 1/n and get, up to a constant factor, the same convergence rate as gradient descent, as described next. Corollary 1. If \u03c4 = 1/n, the iteration complexity5 of Algorithm 1 is O(L/\u00b5 log 1/\u03f5).\n# 3.5 Optimal block sizes\nIf we naively use coordinates as blocks, i.e. all blocks have size equal 1, the update will be very sparse and the efficient way to send it is by providing positions of nonzeros and the corresponding values. If, however, we partition Rd into blocks of size approximately equal d/n, then on average only one block will be updated by each worker. This means that it will be just enough for each worker to communicate the block number and its entries, which is twice less data sent than when using coordinates as blocks.\n# 4 Variance Reduction\nAs the first extension of IBCD, we inject independent coordinate sampling into SAGA6 [9], resulting in a new method we call ISAGA. We consider two different settings for ISAGA. The first one is standard distributed setup (1), where each fi is of the fine-sum form (3). The idea is to run SAGA with independent coordinate sampling locally on each worker, followed by aggregating the updates. However, as for IBCD, we require \u2207fi(x\u2217) = 0 for all i. The second setting is a shared data/memory setup; i.e., we assume that all workers have access to all functions from the finite sum. This allows us to drop Assumption 2. Due to space limitations, we present distributed ISAGA in Sec E of the supplementary.\n# 4.1 Shared data ISAGA\nWe now present a different setup for ISAGA in which the requirement \u2207fi(x\u2217) = 0 is not needed. Instead of (1), we rather solve the problem\n\ufffd with n workers all of which have access to all data describing f. Therefore, all workers can evaluate \u2207\u03c8j(x) for any 1 \u2264j \u2264N. Similarly to plain SAGA, we remember the freshest gradient information in vectors \u03b1j, and update them as\n5Number of iterations to reach \u03f5 accurate solution. 6Independent coordinate sampling is not limited to SAGA and can be similarly applied to other variance reduction techniques.\n(6)\n(7)\nwhere jt i is the index sampled at iteration t by machine i, and j\u2032 refers to all indices that were not sampled at iteration t by any machine. The iterate updates within each machine are taken only on a sampled set of coordinates, i.e., xt+1 i = xt \u2212\u03b3(\u2207\u03c8jt i (xt) \u2212\u03b1t jt i + \u03b1t)U t i . where \u03b1t stands for the average of all \u03b1, and thus it is a delayed estimate of \u2207f(xt). Lastly, we set the next iterate as the average of proposed iterates by each machine xt+1 = 1 n \ufffdn i=1 xt+1 i . The formal statement of the algorithm is given in the supplementary as Algorithm 4. Theorem 2. Suppose that function f is \u00b5 strongly convex and each \u03c8i is L smooth and convex. If \u03b3 \u2264 1 L( 3  +\u03c4), then for iterates of Algorithm 4 we have\n\ufffd \ufffd \ufffd As in Sec E, the choice \u03c4 = 1/n yields a convergence rate which is, up to a constant factor, the same as the convergence rate of SAGA. Therefore, Algorithm 4 enjoys the desired parallel linear scaling, without the additional requirement of Assumption 2. Corollary 2 formalizes the claim. Corollary 2. Consider the setting from Theorem 2. Set \u03c4 = 1/n and \u03b3 = n/5L. Then c = 3/n2, \u03c1 = min {\u00b5/5L, 1/3N} and the complexity of Algorithm 4 is O (max{L/\u00b5, N} log 1/\u03b5).\n# 5 Beyond Assumption 2 and Regularization\nFor this section only, let us consider a regularized objective of the form minx\u2208Rd f(x) \u225c1 n \ufffdn i=1 fi(x) + R(x),\nFor this section only, let us consider a regularized objective of the form \ufffd\n\ufffd where R is a closed convex regularizer such that its proximal operator is computable: prox\u03b3R(x) \u225c argminy \ufffd R(y) + 1 2\u03b3 \u2225y \u2212x\u22252 2 \ufffd . In this section we propose ISEGA: an independent sampling variant of SEGA [14]. We do this in order to both i) avoid Assumption 2 (while keeping linear convergence) and ii) allow for R. Original SEGA learns gradients \u2207f(xt) from sketched gradient information via the so called sketch-and-project process [11], constructing a vector sequence ht. In ISEGA on each machine i we iteratively construct a sequence of vectors ht i which play the role of estimates of \u2207fi(xt). This is done via the following rule:\n\u2207 \u2212 The key idea is again that these vectors are created from random blocks independently sampled on each machine. Next, using ht, SEGA builds an unbiased gradient estimator gt i of \u2207fi(xt) as follows: gt i = ht i + 1 \u03c4 (\u2207fi(xt) \u2212ht i)U t i . (10)\n\u2207 \u2212 Then, we average the vectors gt i and take a proximal step.\nUnlike coordinate descent, SEGA (or ISEGA) is not limited to separable proximal operators since, as follows from our analysis , ht i \u2192\u2207fi(x\u2217). Therefore, ISEGA can be seen as a variance reduced version of IBCD for problems with non-separable regularizers. The price to be paid for dropping Assumption 2 and having more general objective (8) is that updates from each worker are dense, in contrast to those in Algorithm 1. In order to be consistent with the rest of the paper, we only develop a simple variant of ISEGA (Algorithm 2) in which we consider block coordinate sketches with uniform probabilities and non-weighted Euclidean metric (i.e. B = I in notation of [14]). It is possible to develop the theory in full generality as in [14]. However, we do not do this for the sake of simplicity. We next present the convergence rate of ISEGA (Algorithm 2). Theorem 3. Suppose Assumption 1 holds. Algorithm 2 with \u03b3 = min{ 1 4L(1+ 1 n\u03c4 ), 1 \u00b5 \u03c4 + 4L n\u03c4 } satisfies E[\u2225xt \u2212x\u2217\u22252 2] \u2264(1 \u2212\u03b3\u00b5)t\u03a60, where \u03a60 = \u2225x0 \u2212x\u2217\u22252 2 + \u03b3 2L\u03c4n \ufffdn i=1 \u2225h0 \u2212\u2207f(x\u2217)\u22252 2. Note that if the condition number of the problem is not too small so that n = O (L/\u00b5) (which is usually the case in practice), ISEGA scales linearly in the parallel setting. In particular, when doubling the number of workers, each worker can afford to evaluate only half of the block partial derivatives while keeping the same convergence speed. Moreover, setting \u03c4 = 1/n, the rate corresponds, up to a constant factor, to the rate of gradient descent. Corollary 3 states the result.\n\ufffd   Note that if the condition number of the problem is not too small so that n = O (L/\u00b5) (which is usually the case in practice), ISEGA scales linearly in the parallel setting. In particular, when doubling the number of workers, each worker can afford to evaluate only half of the block partial derivatives while keeping the same convergence speed. Moreover, setting \u03c4 = 1/n, the rate corresponds, up to a constant factor, to the rate of gradient descent. Corollary 3 states the result.\n(8)\n(9)\n(10)\nAlgorithm 2 ISEGA\n1: Input: x0 \u2208Rd, initial gradient estimates h0\n1, . . . , h0\nn \u2208Rd, partition of Rd into m blocks\nu1, . . . , im, ratio of blocks to be sampled \u03c4, stepsize \u03b3, # parallel units n\n2: for t = 0, 1, . . . do\n3:\nfor i = 1, . . . , n in parallel do\n4:\nSample independently and uniformly a subset of \u03c4m blocks U t\ni\n5:\ngt\ni = ht\ni + 1\n\u03c4 (\u2207fi(xt) \u2212ht\ni)U t\ni\n6:\nht+1\ni\n= ht\ni + \u03c4(gt\ni \u2212ht)\n7:\nend for\n8:\nxt+1 = prox\u03b3R\n\ufffd\nxt \u2212\u03b3 1\nn\n\ufffdn\ni=1 gt\ni\n\ufffd\n9: end for\n<div style=\"text-align: center;\">Figure 1: Comparison of SAGA and Algorithm 4 for various values of n (number of workers) and \u03c4 = n\u22121 on LibSVM datasets. Stepsize \u03b3 = 1 L(3n\u22121+\u03c4) is chosen in each case.</div>\nCorollary 3. Consider the setting from Theorem 3. Suppose that L/\u00b5 \u2265n and choose \u03c4 = 1/n. Then, complexity of Algorithm 2 is O(L/\u00b5 log 1/\u03f5). Remark 1. Parallel implementation Algorithm 2 would be to always send (\u2207fi(xk))U t i to the server; which keeps updating vector ht and takes the prox step.\n# 6 Experiments\nIn this section, we numerically verify our theoretical claims. Recall that there are various settings where it is possible to make practical experiments (see Sec 2), however, we do not restrain ourselves to any of them in order to deliver as clear a message as possible. Due to space limitations, we only present a small fraction of the experiments here. A full and exhaustive comparison, together with the complete experiment setup description, is presented in Sec I of the supplementary material. In the first experiment presented here, we compare SAGA against ISAGA in a shared data setup (Algorithm 4) for various values of n with \u03c4 = 1/n in order to demonstrate linear scaling. We consider logistic regression problem on LibSVM data [5]. The results (Figure 1) corroborate our theory: indeed, setting n\u03c4 = 1 does not lead to a decrease in the convergence rate when compared to the original SAGA. The next experiment (Figure 2) supports an analogous claim for ISEGA (Algorithm 2). We run the method for several (n, \u03c4) pairs for which n\u03c4 = 1; on logistic regression problems and LibSVM data. We also plot convergence of gradient descent with the analogous stepsize. As our theory predicts, all the methods exhibit almost the same convergence rate.7 Note that for n = 100, Algorithm 2 throws away 99% of partial derivatives while keeping the same convergence speed as GD, which justifies the title of the paper.\nhave chosen the stepsize \u03b3 = 1/2L for GD, as this is the baseline to Algorithm  in fact set \u03b3 = 1/L for GD and get 2\u00d7 faster convergence. However, this is only\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d0a/9d0ad891-6681-43fd-8329-4997cc473715.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Comparison of Algorithm 2 for various (n, \u03c4) such that n\u03c4 = 1 and GD on LibSVM datasets. Stepsize 1/(L \ufffd 1 + 1 n\u03c4 \ufffd ) was chosen for Algorithm 2 and 1 2L for GD.</div>\n# References\n[1] Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD: Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural Information Processing Systems, pages 1709\u20131720, 2017. [2] Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pages 1200\u20131205. ACM, 2017. [3] Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. SignSGD: Compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018. [4] Antonin Chambolle and Thomas Pock. A first-order primal-dual algorithm for convex problems with applications to imaging. Journal of Mathematical Imaging and Vision, 40(1):120\u2013145, 2011. [5] Chih-Chung Chang and Chih-Jen Lin. LibSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011. [6] Dominik Csiba and Peter Richt\u00b4arik. Importance sampling for minibatches. The Journal of Machine Learning Research, 19(1):962\u2013982, 2018. [7] Lisandro D Dalcin, Rodrigo R Paz, Pablo A Kler, and Alejandro Cosimo. Parallel distributed computing using Python. Advances in Water Resources, 34(9):1124\u20131139, 2011. [8] Aaron Defazio. A simple practical accelerated method for finite sums. In Advances in Neural Information Processing Systems, pages 676\u2013684, 2016. [9] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646\u20131654, 2014. 10] Olivier Fercoq and Peter Richt\u00b4arik. Accelerated, parallel, and proximal coordinate descent. SIAM Journal on Optimization, 25(4):1997\u20132023, 2015. 11] Robert M Gower and Peter Richt\u00b4arik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660\u20131690, 2015. 12] Robert M Gower, Peter Richt\u00b4arik, and Francis Bach. Stochastic quasi-gradient methods: Variance reduction via Jacobian sketching. arXiv preprint arXiv:1805.02632, 2018. 13] Dmitry Grishchenko, Franck Iutzeler, J\u00b4er\u02c6ome Malick, and Massih-Reza Amini. Asynchronous distributed learning with sparse communications and identification. arXiv preprint arXiv:1812.03871, 2018. 14] Filip Hanzely, Konstantin Mishchenko, and Peter Richt\u00b4arik. SEGA: Variance reduction via gradient sketching. In Advances in Neural Information Processing Systems, pages 2083\u20132094, 2018. 15] Filip Hanzely and Peter Richt\u00b4arik. Accelerated coordinate descent with arbitrary sampling and best rates for minibatches. arXiv preprint arXiv:1809.09354, 2018. 16] Samuel Horv\u00b4ath and Peter Richt\u00b4arik. Nonconvex variance reduced optimization with arbitrary sampling. arXiv preprint arXiv:1809.04146, 2018. 17] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315\u2013323, 2013. 18] R\u00b4emi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Asaga: Asynchronous parallel saga. In Artificial Intelligence and Statistics, pages 46\u201354, 2017. 19] R\u00b4emi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien. Improved asynchronous parallel optimization analysis for stochastic incremental methods. The Journal of Machine Learning Research, 19(1):3140\u20133207, 2018. 20] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017.\n[21] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I. Jordan, Peter Richt\u00b4arik, and Martin Tak\u00b4a\u02c7c. Adding vs. averaging in distributed primal-dual optimization. In The 32nd International Conference on Machine Learning, pages 1973\u20131982, 2015. [22] Konstantin Mishchenko, Eduard Gorbunov, Martin Tak\u00b4a\u02c7c, and Peter Richt\u00b4arik. Distributed learning with compressed gradient differences. arXiv preprint arXiv:1901.09269, 2019. [23] Konstantin Mishchenko, Franck Iutzeler, and J\u00b4er\u02c6ome Malick. A distributed flexible delay-tolerant proximal gradient algorithm. arXiv preprint arXiv:1806.09429, 2018. [24] Konstantin Mishchenko, Franck Iutzeler, J\u00b4er\u02c6ome Malick, and Massih-Reza Amini. A delay-tolerant proximal-gradient algorithm for distributed learning. In International Conference on Machine Learning, pages 3584\u20133592, 2018. [25] Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341\u2013362, 2012. [26] Yurii Nesterov. A method for solving the convex programming problem with convergence rate O(1/k\u02c62). In Dokl. Akad. Nauk SSSR, volume 269, pages 543\u2013547, 1983. [27] Yurii Nesterov. Introductory lectures on convex optimization: A basic course. Kluwer Academic Publishers, 2004. [28] Zheng Qu and Peter Richt\u00b4arik. Coordinate descent with arbitrary sampling I: Algorithms and complexity. Optimization Methods and Software, 31(5):829\u2013857, 2016. [29] Zheng Qu and Peter Richt\u00b4arik. Coordinate descent with arbitrary sampling II: Expected separable overapproximation. Optimization Methods and Software, 31(5):858\u2013884, 2016. [30] Zheng Qu, Peter Richt\u00b4arik, and Tong Zhang. Quartz: Randomized dual coordinate ascent with arbitrary sampling. In Advances in Neural Information Processing Systems 28, pages 865\u2013873, 2015. [31] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 693\u2013701. Curran Associates, Inc., 2011. [32] Peter Richt\u00b4arik and Martin Tak\u00b4a\u02c7c. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function. Mathematical Programming, 144(1-2):1\u201338, 2014. [33] Peter Richt\u00b4arik and Martin Tak\u00b4a\u02c7c. Parallel coordinate descent methods for big data optimization. Mathematical Programming, 156(1-2):433\u2013484, 2016. [34] Herbert Robbins and Sutton Monro. A stochastic approximation method. In Herbert Robbins Selected Papers, pages 102\u2013109. Springer, 1985. [35] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. Mathematical Programming, 162(1-2):83\u2013112, 2017. [36] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using an approximate Newton-type method. In International Conference on Machine Learning, pages 1000\u20131008, 2014. [37] Sharan Vaswani, Francis Bach, and Mark Schmidt. Fast and faster convergence of SGD for over-parameterized models and an accelerated perceptron. arXiv preprint arXiv:1810.07288, 2018. [38] Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen Wright. ATOMO: Communication-efficient learning via atomic sparsification. In Advances in Neural Information Processing Systems, pages 9872\u20139883, 2018. [39] Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-efficient distributed optimization. In Advances in Neural Information Processing Systems, pages 1306\u20131316, 2018. [40] Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in Neural Information Processing Systems, pages 1509\u20131519, 2017. [41] Stephen J Wright. Coordinate descent algorithms. Mathematical Programming, 151(1):3\u201334, 2015.\n[42] Tuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329\u20133337, 2014.\n42] Tuo Zhao, Mo Yu, Yiming Wang, Raman Arora, and Han Liu. Accelerated mini-batch randomized block coordinate descent method. In Advances in neural information processing systems, pages 3329\u20133337, 2014.\n# A Table of Frequently Used Notation\nGeneral\nx\u2217\nOptimal solution of the optimization problem\nn\nNumber of parallel workers/machines\nSec. 3.2\n\u03c4\nRatio of coordinate blocks to be sampled by each machine\nSec. 3.2\nd\nDimensionality of space x \u2208Rd\nSec. 3.2\nm\nNumber of coordinate blocks\nSec. 3.2\nfi\nPart of the objective owned by machine i\n(1)\nL\nEach fi is L smooth\nAs. 1 and (4)\n\u00b5\nf is \u00b5 strongly convex\nAs. 1 and (5)\nU t\ni\nSubset of blocks sampled at iteration t and worker i\n\u03b3\nStepsize\ng\nUnbiased gradient estimator\nSAGA\n\u03b1j\nDelayed estimate of j-th objective\n(11), (7)\nN\nFinite sum size for shared data problem\n(6)\nl\nNumber of datapoints per machine in distributed setup\n(3)\nLt\nLyapunov function\n(24)\nSGD\ngt\ni\nUnbiased stochastic gradient; Egt\ni = \u2207fi(xt)\n\u03c32\nAn upper bound on the variance of stochastic gradients\nAs. 3\nSEGA\nR\nRegularizer\n(8)\nht\ni\nSequence of biased estimators for \u2207fi(xt)\n(9)\ngt\ni\nSequence of unbiased estimators for \u2207fi(xt)\n(10)\n\u03a6t\nLyapunov function\nThm. 3\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ef6/2ef61c95-cb44-480f-a8f6-64b513649a71.png\" style=\"width: 50%;\"></div>\nTable 2: Summary of frequently used notation.\n<div style=\"text-align: center;\">Table 2: Summary of frequently used notation.</div>\n# B Future Work\nWe sketch several possible extensions of this work.\n\u2022 Combining the tricks from the paper. Distributed ISAGA requires \u2207fi(x) = 0. We believe it would be possible to develop SEGA approach on top of it (such as it is developed on top of coordinate descent) and drop the mentioned requirement. We also believe it should be possible to accelerate the combination of SEGA and ISAGA. \u2022 Convergence in the asynchronous setup. We have provided theoretical results for parallel algorithms in the synchronous setting and the asynchronous theory is a very natural future step. Moreover, as we mentioned before, it has very direct practical implications. We believe that it possible to extend the works [24, 23] to design a method with proximable regularizer (for \u21131 penalty) that would communicate little both sides. \u2022 Importance sampling. Standard coordinate descent exploits a smoothness structure of objective (either via coordinate-wise smoothness constants or more generally using a smoothness matrix) in order to sample coordinates non-uniformly [29, 6, 15]. It would be interesting to derive an importance sampling in our setting in order to converge even faster.\n# C IBGD: Bernoulli alternative to IBCD\nAs an alternative to computing a random block of partial derivatives of size \u03c4m, it is possible to compute the whole gradient with probability \u03c4, and attain the same complexity result. While this can be inserted in all algorithms we propose, we only present an alternative to IBCD, which we call IBGD.\nAlgorithm 3 Independent Bernoulli Gradient Descent (IBGD)\n1: Input: x0 \u2208Rd, probability of computing the whole gradient \u03c4, stepsize \u03b3, # of parallel units n\n2: for t = 0, 1, . . . do\n3:\nfor i = 1, . . . , n in parallel do\n4:\nSet gt\ni =\n\ufffd\u2207fi(xt)\nwith probability\n\u03c4\n0\nwith probability\n1 \u2212\u03c4\nindependently\n5:\nxt+1\ni\n= xt \u2212\u03b3gt\ni\n6:\nend for\n7:\nxt+1 = 1\nn\n\ufffdn\ni=1 xt+1\ni\n8: end for\n# Theorem 4. Suppose that Assumptions 1, 2 hold. For Algorithm 3 with \u03b3 = n \u03c4n+2(1\u2212\u03c4) 1 2L we have\nE \ufffd \u2225xt \u2212x\u2217\u22252 2 \ufffd \u2264 \ufffd 1 \u2212\u00b5 2L \u03c4n \u03c4n + 2(1 \u2212\u03c4) \ufffdt \u2225x0 \u2212x\u2217\u22252 2.\n\ufffd \ufffd Note that IBGD does not perform sparse updates to the server; it is either full (dense), or none. This resembles the most naive asynchronous setup \u2013 where each iteration, a random subset of machines communicates with the server8. Our findings thus show that we can expect perfect linear scaling for such unreal asynchronous setup. In the honest asynchronous setup, we shall still expect good parallel scaling once the sequence of machines that communicate with server somewhat resembles a fixed uniform distribution.\n# D Shared data ISAGA \u2013 algorithm\nAlgorithm 4 ISAGA with shared data\n1: Input: x0 \u2208Rd, \u03b10\n1, . . . , \u03b10\nN partition of Rd into m blocks u1, . . . , um, ratio of blocks to be\nsampled \u03c4, stepsize \u03b3, # parallel units n\n2: Set \u03b10 \u225c1\nN\n\ufffdn\ni=1 \u03b10\ni\n3: for t = 0, 1, . . . do\n4:\nSample uniformly set of indices {jt\n1, . . . , jt\nn} \u2286{1, . . . , N} without replacement\n5:\nfor i = 1, . . . , n in parallel do\n6:\nSample independently and uniformly a subset of \u03c4m blocks U i\nt\n7:\nxt+1\ni\n= xt \u2212\u03b3(\u2207\u03c8jt\ni (xt) \u2212\u03b1t\njt\ni + \u03b1t)U t\ni\n8:\n(\u03b1t+1\njt\ni )U t\ni = \u03b1t\njt\ni + (\u2207\u03c8jt\ni (xt) \u2212\u03b1t\njt\ni )U t\ni\n9:\nend for\n10:\nFor j \u0338\u2208{jt\n1, . . . , jt\nn} set (\u03b1t+1\nj\n) = \u03b1t\ni\n11:\nxt+1 = 1\nn\n\ufffdn\ni=1 xt+1\ni\n12:\n\u03b1t+1 = 1\nn\n\ufffdN\nj=1 \u03b1t+1\nj\n13: end for\n# E Distributed ISAGA\nIn this section we consider problem (1) with fi of the finite-sum structure (3). Just like SAGA, every machine remembers the freshest gradient information of all local functions (stored in arrays \u03b1ij), and\nIn this section we consider problem (1) with fi of the finite-sum structure (3). Just like SAGA, every machine remembers the freshest gradient information of all local functions (stored in arrays \u03b1ij), and\n8In reality, there subset is not drawn from a fixed distribution\nupdates them once a new gradient information is observed. Given that index jt i is sampled on i-th machine at iteration t, the iterate update step within each machine is taken only on a sampled set of coordinates:\nAbove, \u03b1t i stands for the average of \u03b1 variables on i-th machine, i.e. it is a delayed estimate  \u2207fi(xt). Since the new gradient information is a set of partial derivatives of \u2207fijt i (xt), we shall update\nLastly, the local results are aggregated. See Algorithm 5 for details.\nAlgorithm 5 Distributed ISAGA\n1: Input: x0 \u2208Rd, # parallel units n, i-th unit owns l functions fI1, . . . , fil, partition of Rd into\nm blocks u1, . . . , um, ratio of blocks to be sampled \u03c4, stepsize \u03b3, initial vectors \u03b10\nij \u2208Rd for\n1 \u2264i \u2264n, 1 \u2264j \u2264l\n2: Set \u03b10 \u225c1\nN\n\ufffdn\ni=1 \u03b10\ni\n3: for t = 0, 1, . . . do\n4:\nfor i = 1, . . . , n in parallel do\n5:\nSample independently & uniformly jt\ni \u2208[l]\n6:\nSample independently & uniformly a subset of \u03c4m blocks U i\nt\n7:\nxt+1\ni\n= xt \u2212\u03b3(\u2207fijt\ni (xt) \u2212\u03b1t\nijt\ni + \u03b1t\ni)U t\ni\n8:\n\u03b1t+1\nijt = \u03b1t\nijt\ni + (\u2207fijt\ni (xt) \u2212\u03b1t\nijt\ni )U t\ni\n9:\nFor any j \u0338= jt\ni set \u03b1t+1\nij\n= \u03b1t\nij\n10:\n\u03b1t+1 = 1\nl\n\ufffdl\nj=1 \u03b1t+1\nij\n11:\nend for\n12:\nxt+1 = 1\nn\n\ufffdn\ni=1 xt+1\ni\n13: end for\nThe next result provides a convergence rate of distributed ISAGA. Theorem 5. Suppose that Assumptions 1, 2 hold. If \u03b3 \u2264 1 L( 3 n +\u03c4), for iterates of distributed ISAGA we have\n  \ufffd \ufffd where \u03a80 \u225c\ufffdn i=1 \ufffdl j=1 \u2225\u03b1t ij \u2212\u2207fij(x\u2217)\u22252 2, \u03d1 \u225c\u03c4 min \ufffd \u03b3\u00b5, 1 l \u2212 2 n2lc \ufffd \u22650 and c \u225c1 n( 1 \u03b3L \u2212 1 n \u2212\u03c4) > 0.\nThe choice \u03c4 = n\u22121 yields a convergence rate which is, up to a constant factor, the same as convergence rate of original SAGA. Thus, distributed ISAGA enjoys the desired parallel linear scaling. Corollary 4 formalizes this claim. Corollary 4. Consider the setting from Theorem 5. Set \u03c4 = 1 n and \u03b3 = n 5L. Then c = 3 n2 , \u03c1 = min \ufffd\u00b5 5L, 1 3nl \ufffd and the complexity of distributed ISAGA is\n# F SGD\nIn this section, we apply independent sampling in a setup with a stochastic objective. In particular, we consider problem (1) where fi is given as an expectation; see (2). We assume we have access to a stochastic gradient oracle which, when queried at xt, outputs a random vector gt i whose mean is \u2207fi(xt): Egt i = \u2207fi(xt).\n(11)\nOur proposed algorithm\u2014ISGD\u2014evaluates a subset of stochastic partial derivatives for the local objective and takes a step in the given direction for each machine. Next, the results are averaged and followed by the next iteration. We stress that the coordinate blocks have to be sampled independently within each machine.\nAlgorithm 6 ISGD\n1: Input: x0 \u2208Rd, partition of Rd into m blocks u1, . . . , um, ratio of blocks to be sampled \u03c4,\nstepsize sequence {\u03b3t}\u221e\nt=1, # parallel units n\n2: for t = 0, 1, . . . do\n3:\nfor i = 1, . . . , n in parallel do\n4:\nSample independently and uniformly a subset of \u03c4m blocks U t\ni \u2286{u1, . . . , um}\n5:\nSample blocks of stochastic gradient (gt\ni)U t\ni such that E[gt\ni | xt] = \u2207fi(xt)\n6:\nxt+1\ni\n= xt \u2212\u03b3t(gt\ni)U t\ni\n7:\nend for\n8:\nxt+1 = 1\nn\n\ufffdn\ni=1 xt+1\ni\n9: end for\n<div style=\"text-align: center;\">Algorithm 6 ISGD</div>\nIn order to establish a convergence rate of ISGD, we shall assume boundedness of stochastic gradients for each worker. Assumption 3. Consider a sequence of iterates {xt}\u221e t=0 of Algorithm 6. Assume that gt i is an unbiased estimator of \u2207fi(xt) satisfying E\u2225gt i \u2212\u2207fi(xt)\u22252 2 \u2264\u03c32. Assumption 4. Stochastic gradients of function fi have bounded variance at the optimum of f: E\u2225gi \u2212\u2207fi(x\u2217)\u22252 2 \u2264\u03c32, where gi is a random vector such that Egi = \u2207fi(x\u2217). Next, we present the convergence rate of Algorithm 6. Since SGD is not a variance reduced algorithm, it does not enjoy a linear convergence rate and one shall use decreasing step sizes. As a consequence, Assumption 2 is not required anymore since there is no variance reduction property to be broken. Theorem 6. Let Assumptions 1 and 3 hold. If \u03b3t = 1 a+ct, where a = 2 \ufffd \u03c4 + 2(1\u2212\u03c4) n \ufffd L, c = 1 4\u00b5\u03c4, then for Algorithm 6 we can upper bound E[f(\u02c6xt) \u2212f(x\u2217)] by a2 \ufffd 1 \u2212\u03c4\u00b5 a \ufffd \u2225x0 \u2212x\u2217\u22252 2 \u03c4(t + 1)a + c\u03c4 2 t(t + 1) + \u03c32 + (1 \u2212\u03c4) 2 n \ufffdn i=1 \u2225\u2207fi(x\u2217)\u22252 2 n \ufffd 1 + 1 t \ufffd a + nc 2 (t + 1) , \ufffd\n\ufffd Note that the residuals decrease as O(t\u22121), which is a behavior one expects from standard SGD Moreover, the leading complexity term scales linearly: if the number of workers n is doubled, one can afford to halve \u03c4 to keep the same complexity. Corollary 5. Consider the setting from Theorem 6. Then, iteration complexity of Algorithm 6 is \ufffd \ufffd \ufffd\nAlthough problem (1) explicitly assumes convex fi, we also consider a non-convex extension, where smoothness of each individual fi is not required either. Theorem 7 provides the result. Theorem 7 (Non-convex rate). Assume f is L smooth, Assumption 3 holds and for all x \u2208Rd the difference between gradients of f and fi\u2019s is bounded: 1 n \ufffdn i=1 \u2225\u2207f(x) \u2212\u2207fi(x)\u22252 2 \u2264\u03bd2 for some constant \u03bd \u22650. If \u02c6xt is sampled uniformly from {x0, . . . , xt}, then for Algorithm 6 we have\nAgain, the convergence rate from Theorem 7 scales almost linearly with \u03c4: with doubling the number of workers one can afford to halve \u03c4 to keep essentially the same guarantees. Note that if n is sufficiently large, increasing \u03c4 beyond a certain threshold does not improve convergence. This is a slightly weaker conclusion to the rest of our results where increasing \u03c4 beyond n\u22121 might still offer speedup. The main reason behind this is the fact that SGD may be noisy enough on its own to still benefit from the averaging step.\n# Corollary 6. Consider the setting from Theorem 7. i) Choose \u03c4 \u2265 1 n and \u03b3 = \u221an L \u221a \u03c4t \u2264 1 2L(\u03c4/2+(1\u2212\u03c4)/n). Then\nE\u2225\u2207f(\u02c6xt)\u22252 2 \u2264 2 \u221a t\u03c4n \ufffdf(x0) \u2212f \u2217 L + (1 \u2212\u03c4)\u03bd2 \ufffd = O \ufffd1 \u221a t \ufffd .\nii) For any \u03c4 there is sufficiently large n such that choosing \u03b3 = O \ufffd \u03f5 \u03c4L2 \ufffd yields complexity O \ufffd L2 \u03f52 \ufffd The complexity does not improve significantly when \u03c4 is increased.\n# G Acceleration\nHere we describe an accelerated variant of IBCD in the sense of [26]. In fact, we will do something more general and accelerate ISGD, obtaining the IASGD algorithm. We again assume that machine i owns fi, which is itself a stochastic objective as in (2) with an access to an unbiased stochastic gradient gt every iteration: Egt i = \u2207fi(xt). A key assumption for the accelerated SGD used to derive the best known rates [37] is so the called strong growth of the unbiased gradient estimator. Definition 2. Function \u03c6(x) = E\u03b6\u03c6(x, \u03b6) satisfies the strong growth condition with parameters \u03c1, \u03c32, if for all x we have E\u2225\u2207\u03c6(x, \u03b6)\u22252  \u2264\u03c1\u2225\u2207\u03c6(x)\u22252  + \u03c32.\nIn order to derive a strong growth property of the gradient estimator coming from the independent block coordinate sampling, we require a strong growth condition on f with respect to f1, . . . , fn and also a variance bound on stochastic gradients of each individual fi.\nAssumption 5. Function f satisfies the strong growth condition with respect to f1, . . . , fn :\nSimilarly, given that gi = gi(x) provides an unbiased estimator of \u2207fi(x), i.e. Egi = \u2207fi(x), variance of gi is bounded as follows for all i:\nVar [gi] \u2264\u00af\u03c1\u2225\u2207fi(x)\u22252 2 + \u00af\u03c32.\nNote that the variance bound (13) is weaker than the strong growth property as we always have Var [gi] \u2264E \ufffd \u2225gi\u22252 2 \ufffd .\nNote that the variance bound (13) is weaker than the strong growth property as we always have Var [gi] \u2264E \ufffd \u2225gi\u22252 2 \ufffd . Given that Assumption 5 is satisfied, we derive a strong growth property for the unbiased gradient\nNote that the variance bound (13) is weaker than the strong growth property as we always have Var [gi] \u2264E \ufffd \u2225gi\u22252 2 \ufffd . Given that Assumption 5 is satisfied, we derive a strong growth property for the unbiased gradient estimator q \u225c 1 n\u03c4 \ufffdn i=1(\u2207gi)Ui in Lemma 1. Next, IASGD is nothing but the scheme from [37] applied to stochastic gradients q. For completeness, we state IASGD as Algorithm 7.\n1: Input:\nStarting point y0 = v0 \u2208Rd, partition of Rd into m blocks u1, . . . , um, ratio of\nblocks to be sampled \u03c4, stepsize \u03b3, number of parallel units n, acceleration parameter sequences\n{a, b, \u03b7}\u221e\nt=0\n2: for t = 0, 1, . . . do\n3:\nxt = atvt + (1 \u2212at)yt\n4:\nfor i = 1, . . . , n in parallel do\n5:\nSample independently and uniformly a subset of \u03c4m blocks U t\ni \u2282{u1, . . . , um}\n6:\nSample blocks of stochastic gradient (gt\ni)U t\ni such that E[gt\ni | xt] = \u2207fi(xt)\n7:\nend for\n8:\nqt =\n1\nn\u03c4\n\ufffdn\ni=1(gt\ni)U t\ni\n9:\nyt+1 = xt \u2212\u03b3qt\n10:\nvt+1 = btvt + (1 \u2212bt)xt \u2212\u03b7t\u03b3qt.\n11: end for\n(12)\n(13)\n\ufffd   \ufffd It remains to use the stochastic gradient q (with the strong growth bound from Lemma 1) as a gradient estimate in [37][Theorem 6], which we restate as Theorem 8 for completeness. Theorem 8. Suppose that f is L smooth, \u00b5 strongly convex and Assumption 5 holds. Then, for a specific choice of parameter sequences {a, b, \u03b7}\u221e t=0 (See [37][Theorem 6] for details), iterates of IASGD admit an upper bound on E \ufffd f(xt+1) \ufffd \u2212f(x\u2217) of the form \ufffd \ufffd\nThe next corollary provides a complexity of Algorithm 7 in a simplified setting where \u00af\u03c32 = \u02dc\u03c32 = 0. Note that \u02dc\u03c32 = 0 implies \u2207fi(x\u2217) = 0 for all i. It again shows a desired linear scaling: given that we double the number of workers, we can halve the number of blocks to be evaluated on each machine and still keep the same convergence guarantees. It also shows that increasing \u03c4 beyond \u02dc\u03c1\u00af\u03c1 n does not improve the convergence significantly. Corollary 7. Suppose that \u00af\u03c32 = \u02dc\u03c32 = 0. Then, complexity of IASGD is \ufffd \ufffd\nTheorem 8 shows an accelerated rate for strongly convex functions applying [37][Thm 6] to the bound. A non-strongly convex rate can be obtained analogously from [37][Thm 7].\n# H Asynchronous ISGD\nIn this section we extend ISGD algorithm to the asynchronous setup. In particular, we revisit the method that was considered in [13], extend its convergence to stochastic oracle and show better dependency on quantization noise.\nAlgorithm 8 Asynchronous ISGD\n1: Input: x0 \u2208Rd, partition of Rd into m blocks u1, . . . , um, ratio of blocks to be sampled \u03c4,\nstepsize \u03b3, # parallel units n\n2: for t = 0, 1, . . . do\n3:\nWorker i = it is making update\n4:\nwt\u2212dt\ni = 1\nn\n\ufffdn\nj=1 xt\u2212dt\ni\nj\n5:\nxt\u2212dt\ni = prox\u03b3R(wt\u2212dt\ni)\n6:\nSample independently and uniformly a subset of \u03c4m blocks U t\ni \u2286{u1, . . . , um}\n7:\nSample blocks of stochastic gradient (gt\ni)U t\ni such that E[gt\ni | xt] = \u2207fi(xt\u2212dt\ni)\n8:\nxt\ni = xt\u2212dt\ni \u2212\u03b3(gt\ni)U t\ni\n9:\nSend (gt\ni)U t\ni and receive wt+1 = 1\nn\n\ufffdn\nj=1 xt+1\nj\n10: end for\n11: Output: xt = prox\u03b3R(wt)\nLet us denote the delay of worker i at moment t by dt i. Theorem 9. Assume f1, . . . , fn are L-smooth and \u00b5-strongly convex and let Assumption 4 be satisfied. Let us run Algorithm 8 for t iterations and assume that delays are bounded: dt i \u2264M for any i and t. If \u03b3 \u2264 1 2L(\u03c4+ 2 n ), then\n  where C \u225cmaxi=1,...,n \u2225x0 \u2212x\u2217 i \u22252 2, x\u2217 i \u225cx\u2217\u2212\u03c4\u03b3\u2207fi(x\u2217) and \u230a\u00b7\u230bis the floor operator.\n(14) (15)\nPlugging \u03b3 = 1 2L(\u03c4+ 2 n ) gives complexity that will be significantly improving from increasing \u03c4 until \u03c4 = 1 n, and then only if \u03c4 jumps from 1 n to 1. In contrast, doubling \u03c4 from 2 n to 4 n would make little difference. We note that if \u21131 penalty is used, in practice zt i should be rather computed on the parameter server side because it will sparsify the vector for communication back.\n# I Extra Experiments\nWe present exhaustive numerical experiments to verify the theoretical claims of the paper. The experiments are performed in a simulated environment instead of the honestly distributed setup, as we only aim to verify the iteration complexity of proposed methods. First, in Sec I.1 provides the simplest setting in order to gain the best possible insight \u2013 Algorithm 1 is tested on the artificial quadratic minimization problem. We compare Algorithm 1 against both gradient descent (GD) and standard CD (in our setting: when each machine samples the same subset of coordinates). We also study the effect of changing \u03c4 on the convergence speed. In the remaining parts, we consider a logistic regression problem on LibSVM data [5]. Recall that logistic regression problem is given as\nwhere A is data matrix and b is vector of data labels: bj \u2208{\u22121, 1}9. In the distributed scenario (everything except of Algorithm 4), we imitate that the data is evenly distributed to n workers (i.e. each worker owns a subset of rows of A and corresponding labels, all subsets have almost the same size). As our experiments are not aimed to be practical at this point (we aim to properly prove the conceptual idea), we consider multiple of rather smaller datasets: a1a (d = 123, n = 1605), mushrooms (d = 112, n = 8124), phishing (d = 68, n = 11055), w1a (d = 300, n = 2477). The experiments are essentially of 2 types: one shows that setting n\u03c4 = 1 does not significantly violate the convergence of the original method. In the second type of experiments we study the behavior for varying \u03c4, and show that beyond certain threshold, increasing \u03c4 does not significantly improve the convergence. The threshold is smaller as n increases, as predicted by theory.\n# I.1 Simple, well understood experiment\nIn this section we study the simplest possible setting \u2013 we test the behavior of Algorithm 1 on the artificial quadratic minimization problem. The considered quadratic objective is set as\n# In this section we study the simplest possible setting \u2013 we test the behavior of Algorithm 1 on the artificial quadratic minimization problem. The considered quadratic objective is set as\nfi(x) \u225c1 2x\u22a4Mix, Mi \u225cvv\u22a4+ \ufffd I \u2212vv\u22a4\ufffd AiA\u22a4 i \u03bbmax \ufffd AiA\u22a4 i \ufffd\ufffd I \u2212vv\u22a4\ufffd ,\nfi(x) \u225c1 2x\u22a4Mix, Mi \u225cvv\u22a4+ \ufffd I \u2212vv\u22a4\ufffd AiA\u22a4 i \u03bbmax \ufffd AiA\u22a4 i \ufffd\ufffd I \u2212vv\u22a4\ufffd , v = v\u2032 \u2225v\u2032\u2225, (17\nwhere entries of v\u2032 \u2208Rd and Ai \u2208Rd\u00d7o are sampled independently from standard normal distribution.\nwhere entries of v\u2032 \u2208Rd and Ai \u2208Rd\u00d7o are sampled independently from standard normal distribution.\nIn the first experiment (Figure 3), we compare Algorithm 1 with n\u03c4 = 1 against gradient descent (GD) and two versions of coordinate descent - a default version with stepsize 1 L, and a coordinate descent with importance sampling (sample proportionally to coordinate-wise smoothness constants) and optimal step sizes (inverse of coordinate-wise smoothness constants). In all experiments, gradient descent enjoys twice better iteration complexity than Algorithm 1 which is caused by twice larger stepsize. However, in each case, Algorithm 1 requires fewer iterations to CD with importance sampling, which is itself significantly faster to plain CD.\n(16)\n(17)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f63/6f63d141-a117-4663-8065-22c03d546d95.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison of gradient descent, (standard) coordinate descent, (standard) coordinate descent with importance sampling and Algorithm 1 on artificial quadratic problem (17).</div>\nNext, we study the effect of changing \u03c4 on the iteration complexity of Algorithm 1. Figure 4 provides the result. The behavior predicted from theory is observed \u2013 increasing \u03c4 over n\u22121 does not significantly improve the convergence speed, while decreasing it below n\u22121 slows the algorithm notably.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ebdf/ebdf0ab0-3fde-462c-99b7-b5c4965a9985.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Behavior of Algorithm 1 for different \u03c4 on a simple artificial quadratic problem (17).</div>\n# I.2 ISGD\nIn this section we numerically test Algorithm 6 for logistic regression problem. As mentioned, fi consists of set of (uniformly distributed) rows of A from (16). We consider the most natural unbiased stochastic oracle for the \u2207fi \u2013 gradient computed on a subset data points from fi. In all experiments of this section, we consider constant step sizes in order to keep the setting as simple as possible and gain as much insight from the experiments as possible. Therefore, one can not expect convergence to the exact optimum. In the first experiment, we compare standard SGD (stochastic gradient is computed on single, randomly chosen datapoint every iteration) against Algorithm 6 varying n and choosing \u03c4 = 1 n\nfor each n. The results are presented by Figure 5. We see that, as our theory suggests, SGD and Algorithm 6 have always very similar performance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c8c/5c8c214f-d778-433f-81e0-81409b3ddb05.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Comparison of SGD (gradient evaluated on a single datapoint) and Algorithm 6 with n\u03c4 = 1. Constant \u03b3 = 1 5L was used for each algorithm. Label \u201cbatch size\u201d indicates how big minibatch was chosen for stochastic gradient of each worker\u2019s objective.</div>\nNext, we study the dependence of the convergence speed on \u03c4 for various values of n. Figure 6 presents the results. In each case, \u03c4 influences the convergence rate (or the region where the iterates oscillate) significantly, however, the effect is much weaker for larger n. This is in correspondence with Corollary 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/422c/422c2601-7bd7-4a64-a194-a0c0536eb32c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Behavior of Algorithm 6 while varying \u03c4. Label \u201cSGD\u201d corresponds to the choice n = 1, \u03c4 = 1. Stepsize \u03b3 = 1 3L was used in every case.</div>\n# I.3 IASGD\nIn this section we numerically test Algorithm 7 for logistic regression problem. As in the last section, fi consists of set of (uniformly distributed) rows of A from (16). The stochastic gradient is taken as a gradient on a subset data points from each fi. Note that Algorithm 7 depends on a priori unknown strong growth parameter \u02c6\u03c1 of unbiased stochastic gradient q10. Therefore, we first find empirically optimal \u02c6\u03c1 for each algorithm run by grid search and report only the best performance for each algorithm. The first experiment (Figure 7) verifies the linearity claim \u2013 we vary (n, \u03c4) such that n\u03c4 = 1. As predicted by theory, the behavior of presented algorithms is almost indistinguishable.\n10Formulas to obtain parameters of Algorithm 7 are given in [37].\n10Formulas to obtain parameters of Algorithm 7 are given in [37].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d74d/d74d4ee6-a1d0-46cf-ba75-da274f215d53.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Comparison of Algorithm 7 for various (n, \u03c4) such that n\u03c4 = 1. Label \u201cASGD\u201d corresponds to the choice n = 1, \u03c4 = 1. Label \u201cbatch size\u201d indicates how big minibatch was chosen for stochastic gradient of each worker\u2019s objective. Parameter \u03c1 was chosen by grid search.</div>\nNow, we once again check how different values of \u03c4 affect the convergence speed for several values of n. Figure 8 presents the results. In every case, \u03c4 slightly influences the convergence rate (or the region where the iterates oscillate), although the effect is weaker for larger n. Note that theory predicts diminishing effect of \u03c4 only above \u00af\u03c1\u02dc\u03c1 n , in contrast to other sections, where the limit is 1 n.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8720/87209eeb-5342-4787-a41b-8f4897095be0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Behavior of Algorithm 7 while varying \u03c4. Label \u201cASGD\u201d corresponds to the choice n = 1, \u03c4 = 1. Parameter \u03c1 was chosen by grid search.</div>\n# I.4 ISAGA\nWe also study the convergence of shared data ISAGA \u2013 Algorithm 4. As previously, first experiment compares default SAGA against Algorithm 4 for various values of n with \u03c4 = n\u22121. Again, the results (Figure 911) shows what theory claims \u2013 setting n\u03c4 = 1 does not violate a convergence rate of the original SAGA.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f878/f8785564-28bd-4110-919b-0a91cf5fae9f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Comparison of SAGA and Algorithm 4 for various values n and \u03c4 = n\u22121. Stepsiz \u03b3 = 1 L(3n\u22121+\u03c4) is chosen in each case.</div>\nThe second experiment of this section shows the convergence behavior for varying \u03c4 of Algorithm 4. The results (Figure 10) show that, for small n, the ratio of coordinates \u03c4 affects the speed heavily. However, as n increases, the effect of \u03c4 is diminishing.\n11Figure 9 is identical to Figure 1. We present it again for completeness.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b149/b149a0f0-0206-4b1a-a71c-10d7d1104d14.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Comparison of Algorithm 4 for different values of \u03c4. Stepsize \u03b3 = 1 L(3n\u22121+\u03c4) is chosen in each case. For this experiment, we choose smaller regularization; \u21132 = 0.000025.</div>\n# I.5 ISEGA\nLastly, we numerically test Algorithm 2, and its linear convergence without Assumption 2. For simplicity, we consider R(x) = 0 in (8). In the first experiment (Figure 11), we compare Algorithm 2 for various (n, \u03c4) such that n\u03c4 = 1. For illustration, we also plot convergence of gradient descent with the analogous stepsize. As theory predicts, the method has almost same convergence speed.12\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/096f/096f4d9f-4097-431c-988a-623985509714.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Comparison of Algorithm 2 for various (n, \u03c4) such that n\u03c4 = 1 and GD. Stepsize 1 L(1+ 1  ) was chosen for Algorithm 2 and 1 2L for GD.</div>\n12We have chosen stepsize \u03b3 = 1 2L for GD, as this is the baseline to Algorithm 2 with zero variance. One can in fact set \u03b3 = 1 L for GD and get 2 times faster convergence. However, this is still only a constant factor.\nThe second experiment of this section shows the convergence behavior for varying \u03c4 of Algorithm 2. Again, the results (Figure 12) indicate that \u03c4 has a heavy impact on the convergence speed for small n. However, as n increases, the effect of \u03c4 is diminishing. In particular, for increasing \u03c4 beyond n\u22121 does not yield a significant speedup.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d22/8d22dda2-c299-4374-8918-56a5987a4129.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: Comparison of Algorithm 2 for different values of \u03c4. Stepsize \u03b3 = 1 L(1+ 1 n\u03c4 ) is chosen  each case.</div>\n# J Proofs for Section 3\n# J.1 Key techniques\nThe most important equality used many times to prove the results of this paper is a simp decomposition of expected distances into the distance of expectation and variance:\nwhere X is any random vector with finite variance and a is an arbitrary vector from Rd. As almost every algorithm we propose average all updates coming from workers, it will be useful to bound the expected distance of mean of n random variables from the optimum. Lemma 2 provides the result. Lemma 2. Suppose that xt+1 = 1 n \ufffdn i=1 xt i. Then, we have \ufffd \ufffd\nwhere X is any random vector with finite variance and a is an arbitrary vector from Rd. As almost every algorithm we propose average all updates coming from workers, it will be useful to bound the expected distance of mean of n random variables from the optimum. Lemma 2 provides the result. Lemma 2. Suppose that xt+1 = 1 n \ufffdn i=1 xt i. Then, we have E\u2225xt+1 \u2212x\u2217\u22252 2 \u2264 \ufffd\ufffd\ufffd\ufffd\ufffd 1 n n \ufffd i=1 Ext+1 i \u2212x\u2217 \ufffd\ufffd\ufffd\ufffd\ufffd 2 2 + 1 n2 n \ufffd i=1 E\u2225xt+1 i \u2212Ext+1 i \u22252 2.\n(18)\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd Now let us proceed to expectations. Note that for any random vector X we have E\u2225X\u22252 2 = \u2225EX\u22252 2 + E\u2225X \u2212EX\u22252 2.\nApplying this to random vector X \u225c1 n \ufffdn i=1 xt i \u2212x\u2217, we get\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd In addition, in all minibatching schemes xt+1 i are conditionally independent given xt. Therefore, for the variance term we get\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd\ufffd\ufffd In addition, in all minibatching schemes xt+1 i are conditionally independent given xt. Therefore, for the variance term we get\n\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd Plugging it into our previous bounds concludes the proof.\n# J.2 Proof of Theorem 1\nf. From Lemma 8, using \u03c3 = 0 and \u2207fi(x\u2217) = 0, we immediately obtai \ufffd\nProof. From Lemma 8, using \u03c3 = 0 and \u2207fi(x\u2217) = 0, we immediately obtain E \ufffd \u2225xt+1 \u2212x\u2217\u22252 2 \u2225xt\ufffd \u2264(1 \u2212\u00b5\u03b3\u03c4)\u2225xt \u2212x\u2217\u22252 2 = \ufffd 1 \u2212\u00b5 2L \u03c4n \u03c4n + 2(1 \u2212\u03c4) \ufffd \u2225xt \u2212x\u2217\u22252 2. It remains to apply the above inequality recursively.\nE \ufffd \u2225xt+1 \u2212x\u2217\u22252 2 \u2225xt\ufffd \u2264(1 \u2212\u00b5\u03b3\u03c4)\u2225xt \u2212x\u2217\u22252 2 = \ufffd 1 \u2212\u00b5 2L \u03c4n \u03c4n + 2(1 \u2212\u03c4) \ufffd \u2225xt \u2212x\u2217\u22252 2.\nE \ufffd \u2225xt+1 \u2212x\u2217\u22252 2 \u2225xt\ufffd \u2264(1 \u2212\u00b5\u03b3\u03c4)\u2225xt \u2212x\u2217\u22252 2 = \ufffd 1 \u2212\u00b5 2L \u03c4n \u03c4n + 2(1 \u2212\u03c4) \ufffd \u2225xt \u2212x\u2217\u22252 2. It remains to apply the above inequality recursively.\n# J.3 Proof of Theorem 4\nProof. Clearly,\nExt+1 i = xt \u2212\u03b3\u03c4\u2207fi(xt).\nLet us now elaborate on the second moments. Thus, \ufffd\nhus, E\u2225xt+1 i \u2212Ext+1 i \u22252 2 = \u03b32E \ufffd \u2225gt i \u2212\u03c4\u2207fi(xt)\u22252 2 \ufffd = \u03c4(1 \u2212\u03c4)\u2225\u2207fi(x)\u22252.\n\ufffd \ufffd Note that the above equality is exactly (27) with \u03c3 = 0. Thus, one can use Lemma 8 (with usin \u03c3 = 0 and \u2207fi(x\u2217) = 0) obtaining\nE \ufffd \u2225xt+1 \u2212x\u2217\u22252 2 \u2225xt\ufffd \u2264(1 \u2212\u00b5\u03b3\u03c4)\u2225xt \u2212x\u2217\u22252 2 = \ufffd 1 \u2212\u00b5 2L \u03c4n \u03c4n + 2(1 \u2212\u03c4) \ufffd \u2225xt \u2212x\u2217\u22252 2. t remains to apply the above inequality recursively.\n\ufffd \ufffd It remains to apply the above inequality recursively.\n# K Missing Parts from Sections 4 and E\n# K.1 Useful Lemmata\nLet us start with a variance bound, which will be useful for both Algorithm 5 and Algorithm 4. Define \u03a6(x) = 1 k \ufffdk i=1 \u03c6i(x), and define x+ = x\u2212\u03b3(\u2207fj(x)\u2212\u03b1j + \u00af\u03b1)U for (uniformly) randomly chosen index 1 \u2264j \u2264k and subset of blocks U of size \u03c4m. Define also \u00af\u03b1 = 1 k \ufffdk i=1 \u03b1i.\n19)\n\n\nLemma 3 (Variance bound). Assume \u03c6 is \u00b5-strongly convex and \u03c6j is L-smooth and convex for all j. Suppose that x\u22c6= argmin \u03a6(x). Then, for any x we have E\u2225x+ \u2212Ex+\u22252 2 \u22642\u03b32\u03c4 \uf8eb \uf8ed2L(\u03c6(x) \u2212\u03c6(x\u22c6) + 1 k k \ufffd j=1 \u2225\u03b1j \u2212\u2207\u03c6j(x\u22c6)\u22252 2 \uf8f6 \uf8f8. (20) Proof. Since x+ = x \u2212\u03b3(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)U and Ex+ = x \u2212\u03b3\u03c4\u2207\u03c6(x), we get E\u2225x+ \u2212Ex+\u22252 2 = \u03b32E \u2225\u03c4\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)U\u22252 2 = \u03b32E\u2225(\u03c4\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1))U\u22252 2 + \u03b32E\u2225\u03c4\u2207\u03c6(x) \u2212(\u03c4\u2207\u03c6(x))U\u22252 2 = \u03b32\u03c4E\u2225\u03c4\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)\u22252 2 + \u03b32(1 \u2212\u03c4)\u03c4 2\u2225\u2207\u03c6(x)\u22252 2. We will leave the second term as is for now and obtain a bound for the first one. Note that the expression inside the norm is now biased: E[\u03c4\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)] = (\u03c4 \u22121)\u2207\u03c6(x). Therefore, E\u2225\u03c4\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)\u22252 2 = (1 \u2212\u03c4)2\u2225\u2207\u03c6(x)\u22252 2 + E\u2225\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)\u22252 2. Now, since \u2207\u03c6j(x) and \u03b1j are not independent, we shall decouple them using inequality \u2225a + b\u22252 2 \u2264 2\u2225a\u22252 2 + 2\u2225b\u22252 2. In particular, E\u2225\u2207\u03c6(x) \u2212(\u2207\u03c6j(x) \u2212\u03b1j + \u03b1)\u22252 2 = E\u2225\u2207\u03c6(x) \u2212\u2207\u03c6j(x) + \u2207\u03c6j(x\u22c6) \u2212\u2207\u03c6j(x\u22c6) + \u03b1j \u2212\u03b1\u22252 2 \u22642E\u2225\u2207\u03c6(x) \u2212\u2207\u03c6j(x) + \u2207\u03c6j(x\u22c6)\u22252 2 + 2E\u2225\u03b1j \u2212\u2207\u03c6j(x\u22c6) \u2212\u03b1\u22252 2. Both terms can be simplified by expanding the squares. For the first one we have: E\u2225\u2207\u03c6(x) \u2212\u2207\u03c6j(x) + \u2207\u03c6j(x\u22c6)\u22252 2 = \u2225\u2207\u03c6(x)\u22252 2 \u22122 \u27e8\u2207\u03c6(x), E [\u2207\u03c6j(x) \u2212\u2207\u03c6j(x\u22c6)]\u27e9 + E\u2225\u2207\u03c6j(x) \u2212\u2207\u03c6j(x\u22c6)\u22252 2 = \u2212\u2225\u2207\u03c6(x)\u22252 2 + 1 k k \ufffd j=1 \u2225\u2207\u03c6j(x) \u2212\u2207\u03c6j(x\u22c6)\u22252 2.\nLemma 3 (Variance bound). Assume \u03c6 is \u00b5-strongly convex and \u03c6j is L-smooth and convex for all j. Suppose that x\u22c6= argmin \u03a6(x). Then, for any x we have \uf8eb \uf8f6\nSimilarly,\n\ufffd Coming back to the first bound that we obtained for this lemma, we deduce E\u2225x+ \u2212Ex+\u22252 2 \u2264\u03b32\u03c4 \uf8eb \uf8ed(1 \u2212\u03c4)2\u2225\u2207\u03c6(x)\u22252 2 \u22122\u2225\u2207\u03c6(x)\u22252 2 + 2 k k \ufffd j=1 \u2225\u2207\u03c6j(x) \u2212\u2207\n\ufffd The coefficient before \u2225\u2207\u03c6(x)\u22252 2 is equal to \u03b32\u03c4((1 \u2212\u03c4)2 \u22122 + (1 \u2212\u03c4)\u03c4) = \u03b32\u03c4(1 \u2212\u03c4 \u22122) so we can drop this term. By smoothness of each \u03c6j,\n \u2212 where in the last step we used 1 k \ufffdk j=1 \u2207\u03c6j(x\u22c6) = 0.\n(20)\n21)\n(21)\n(21)\n) \ufffd n N \ufffd \u03c4\u2225\u2207fj(xt) \u2212\u2207fj(x\u2217)\u22252 2 + (1 \u2212\u03c4) \u2225\u03b1t j \u2212\u2207fj(x\u2217)\u22252 2\nSimilarly, for distributed setup we get \uf8ee \uf8f9\nSimilarly, for distributed setup we get E \uf8ee \uf8f0 l \ufffd j=1 \u2225\u03b1t+1 ij \u2212\u2207fij(x\u2217)\u22252 2 \uf8f9 \uf8fb\u2264\u03c4 1 l l \ufffd j=1 \u2225\u2207fij(xt) \u2212\u2207fij(x\u2217)\u22252 2 + \ufffd 1 \u2212\u03c4 l \ufffd l \ufffd j=1 \u2225\u03b1t ij \u2212\u2207fij(x\u2217)\u2225 Using (21), the first sum of right hand side can be bounded by 2LN(f(xt) \u2212f(x\u2217)) or 2Ll(f(xt) \u2212 f(x\u2217)).\n\uf8f0 \uf8fb Using (21), the first sum of right hand side can be bounded by 2LN(f(xt) \u2212f(x\u2217)) or 2Ll(f(xt) \u2212 f(x\u2217)).\n# K.2 Proof of Theorem 5\nc = 1 n \ufffd 1 \u03b3L \u22121 n \u2212\u03c4 \ufffd \u22651 n \ufffd3 n + \u03c4 \u22121 n \u2212\u03c4 \ufffd > 0. Furthermore, \u03b3\u00b5 \u22650, so to show \u03c1 \u22650 it is enough to mention 1 l \u2212 2 n2lc = 1 l \u2212 2 n2l( 1 \u03b3L \u22121 n \u2212\u03c4) \u22651 l \u2212 2 n2l( 3 n +\u03c4\u22121 n \u2212\u03c4) = 0.\n  Now we proceed to the proof of convergence. We are going to decompose the expected distance from xt+1 to x\u2217into its variance and the distance of expected iterates, so let us analyze them separately. The variance can be bounded as follows: \uf8eb\n(22)\nAs is usually done for SAGA, we are going to prove convergence using a Lyapunov function. Name let us define \uf8ee \uf8f9\n\uf8f0 \uf8fb where c = 1 n \ufffd 1 \u03b3L \u22121 n \u2212\u03c4 \ufffd . Using Lemma 4 together with the bounds above, we get",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of inefficiencies in popular distributed optimization methods for training machine learning models, particularly those that compute local gradient estimates independently by each worker and then communicate them to a master for averaging. It highlights that existing methods, including SGD and SAGA, do not scale well with the number of parallel workers, leading to excessive communication of redundant data, which slows down the overall system. A new update-sparsification method is proposed to address these inefficiencies.",
        "problem": {
            "definition": "The problem addressed in this paper is the inefficient communication and computational overhead in distributed optimization methods, particularly when scaling with an increasing number of workers.",
            "key obstacle": "The core obstacle is the communication bottleneck caused by the redundancy in the data communicated by workers, which results in wasted resources and time during the optimization process."
        },
        "idea": {
            "intuition": "The intuition behind the proposed method is that independent block sampling can significantly reduce the amount of data communicated while maintaining the same convergence rates as existing methods.",
            "opinion": "The proposed idea involves using independent block coordinate descent (IBCD), where each worker samples a subset of blocks independently, thereby reducing communication without sacrificing convergence rates.",
            "innovation": "The key innovation of this method is that it allows for a significant reduction in the amount of data communicated (up to 99% less) while achieving the same theoretical iteration complexity as traditional methods like gradient descent."
        },
        "method": {
            "method name": "Independent Block Coordinate Descent",
            "method abbreviation": "IBCD",
            "method definition": "IBCD is a distributed optimization method that allows workers to independently sample a subset of blocks from the parameter space, reducing the communication load while maintaining convergence rates.",
            "method description": "The method involves each worker sampling a fraction of blocks to compute local updates, which are then aggregated to form a global update.",
            "method steps": [
                "Initialize parameters and partition the parameter space into blocks.",
                "In each iteration, each worker samples a subset of blocks independently.",
                "Each worker computes updates based on its sampled blocks.",
                "Aggregate the updates from all workers to form a global update."
            ],
            "principle": "The effectiveness of IBCD lies in the law of large numbers, which allows for the reduction of variance in the aggregated updates, thus improving convergence rates while minimizing communication."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using synthetic and real datasets to compare the performance of IBCD against traditional methods like SGD and SAGA, particularly focusing on communication efficiency and convergence rates.",
            "evaluation method": "The performance was assessed by measuring the convergence rates and communication overhead in various distributed settings, demonstrating that IBCD achieves similar convergence rates with significantly reduced communication."
        },
        "conclusion": "The experiments confirm that the proposed IBCD method can effectively reduce communication overhead in distributed optimization settings while maintaining convergence rates comparable to traditional methods, thereby addressing the inefficiencies highlighted in the problem statement.",
        "discussion": {
            "advantage": "The main advantage of IBCD is its ability to drastically reduce communication costs (up to 99%) while achieving similar or better convergence rates compared to existing methods.",
            "limitation": "A limitation of IBCD is that it does not necessarily result in a sparse aggregated update, which can lead to dense updates as the number of workers increases.",
            "future work": "Future research could explore methods to further optimize communication efficiency, investigate the performance of IBCD in asynchronous settings, and extend the analysis to more complex optimization problems."
        },
        "other info": {
            "additional details": {
                "authors": [
                    "Konstantin Mishchenko",
                    "Peter Richtarik"
                ],
                "affiliations": [
                    "KAUST, Thuwal, Saudi Arabia"
                ],
                "date": "4 Jun 2019"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The paper discusses the architecture of distributed optimization methods, specifically highlighting the inefficiencies in popular methods like SGD and SAGA when scaling with the number of parallel workers."
        },
        {
            "section number": "2.2",
            "key information": "The paper introduces the Independent Block Coordinate Descent (IBCD) method, which allows workers to independently sample a subset of blocks from the parameter space, thereby enhancing the performance of distributed optimization."
        },
        {
            "section number": "6.2",
            "key information": "The paper identifies computational resource constraints as a challenge in distributed optimization methods, particularly the communication bottleneck caused by redundant data communicated by workers."
        },
        {
            "section number": "6.3",
            "key information": "The paper explores model generalization and robustness by discussing how the proposed IBCD method maintains convergence rates while reducing communication overhead, addressing inefficiencies in the optimization process."
        },
        {
            "section number": "6.4",
            "key information": "The paper notes that while IBCD reduces communication costs significantly, it does not necessarily lead to sparse aggregated updates, raising concerns about bias and efficiency in larger worker settings."
        }
    ],
    "similarity_score": 0.5565081661618985,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0239_large/papers/99% of Distributed Optimization is a Waste of Time_ The Issue and How to Fix it.json"
}