{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.04600",
    "title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
    "abstract": "Objective: This study aims to summarize the usage of Large Language Models (LLMs) in the process of creating a scientific review. We look at the range of stages in a review that can be automated and assess the current state-of-the-art research projects in the field. Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model. ChatGPT was used to clean extracted data and generate code for figures in this manuscript, ChatGPT and Scite.ai were used in drafting all components of the manuscript, except the methods and discussion sections. Results: 3,788 articles were retrieved, and 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n=126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n=26, 15.1%) were actual reviews that used LLM during their creation. Most citations focused on automation of a particular stage of review, such as Searching for publications (n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled performance of GPT-based and BERT-based models, the former were better in data extraction with mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8), while being slightly less accurate in title and abstract screening stage (Maccuracy=77.3%, SD=13.0). Discussion/Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. The results looked promising, and we anticipate that LLMs will change in the near future the way the scientific reviews are conducted.",
    "bib_name": "scherbakov2024emergencelargelanguagemodels",
    "md_text": "# The emergence of Large Language Models (LLM) as a tool in literature reviews: an  LLM automated systematic review \nDmitry Scherbakov, PhD1,*, Nina Hubig, PhD 1,2,*, Vinita Jansari, PhD2, Alexander \nBakumenko, MSc2 , Leslie A. Lenert, MD1\nBakumenko, MSc2 , Leslie A. Lenert, MD1  \n Biomedical Informatics Center, Department of Public Health Sciences, Medical University of  South Carolina (MUSC), Charleston, South Carolina, USA.  2 Clemson University, School of Computing , Charleston, South Carolina, USA.  * Authors with equal contribution. \nCorresponding author: Leslie Lenert (lenert@musc.edu)  Address: 22 WestEdge Street, Suite 200, Room WG213, Charleston, South Carolina, 29403, USA \nManuscript word count: 2987 (excluding title page, abstract, references, tables,  acknowledgements, funding, data availability, competing interests, and author contributions  statements)  Abstract word count: 301 (excluding keywords)  \nHighest academic degrees of authors (in order):  DS : PhD, Postdoctoral scholar  NH : PhD, Assistant professor  VJ : PhD, Postdoctoral scholar  AB : MSc, Doctoral student  LL : MD, MS, FACP, FACMI, Professor \nThis study aims to summarize the usage of Large Language Models (LLMs) in the process of  creating a scientific review. The idea behind this publication is to turn the tool \u201con itself\u201d,  conducting a systematic review of research projects using LLMs for systematic and other types of reviews by using a set of LLM tools. \nObjective: This study aims to summarize the usage of Large Language Models (LLMs)  in the process of creating a scientific review. We look at the range of stages in a review that can  be automated and assess the current state-of-the-art research projects in the field.  Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus,  Dimensions, and Google Scholar databases by human reviewers. Screening and extraction  process took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model. ChatGPT was used to clean extracted data and generate code for figures in this manuscript,  ChatGPT and Scite.ai were used in drafting all components of the manuscript, except the  methods and discussion sections.  Results:  3,788 articles were retrieved, and 172 studies were deemed eligible for the final  review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review  automation (n=126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n=26, 15.1%) were actual reviews that used LLM during their  creation. Most citations focused on automation of a particular stage of review, such as Searching  for publications (n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled  performance of GPT-based and BERT-based models, the former were better in data extraction  with mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8), while being slightly less  accurate in title and abstract screening stage (Maccuracy=77.3%, SD=13.0 vs Maccuracy=80.9%  SD=11.8).   Discussion/Conclusion: Our LLM-assisted systematic review revealed a significant  number of research projects related to review automation using LLMs.  The results looked  promising, and we anticipate that LLMs will change in the near future the way the scientific  reviews are conducted, significantly reducing the time required to generate systematic reviews of the literature and expanding how systematic reviews are used to guide science.  Keywords: Large Language Models, Review Automation, Systematic Review, Scoping  Review, Covidence. \nThe abundance of scientific information available can be overwhelming, posing a  challenge for researchers to navigate relevant data. Consequently, scoping and systematic  reviews that are helping scientists synthesize the evidence have seen a significant increase over  the years. Toh & Lee noted an exponential rise in the number of scoping reviews, with 2,665  scoping reviews being published in 2020 alone, compared to less than 10 reviews annually  before 2009 1. The same trend is observed in systematic reviews and meta-analyses, for example in cardiology over 2,400 meta-analyses were published in 2019, quadruple the number from  2012 2.    A completion of a review requires substantial resources; further, there is often  unpredictable uncertainty in the amount of resources required 3. The time to complete a single  systematic review varies, but authors typically give estimates in months and even years 4.  Screening automation platforms, such as Covidence 5, facilitate systematic and scoping reviews by streamlining established guidelines, such as the Preferred Reporting Items for Systematic  Reviews and Meta-Analyses (PRISMA) and PICO (Population, Intervention, Comparison, and  Outcome) to ensure transparency and rigor in the review process 6. The  use of such platforms  may reduce the time to complete reviews by providing tools that automate key tasks, such as  removing duplicate references, generating flow-charts of the screening process, visual extraction designers, and workflows for several independent reviewers.    Although, for example, Covidence, includes features to reduce the time to complete  screening, such as key term highlighting and embedded natural processing (NLP) algorithm 7  it primarily organizes the significant manual work that is still needed from human reviewers like  screening and extraction.. Each of these steps normally requires two independent analysts, with  third optional human expert supervising the process and resolving the disagreements.    Even with two reviewers double-checking each other, as much as 3% of relevant citation are missed, and if only a single reviewer is used (for example, in rapid reviews), as many as 13% of relevant publications can be missed 8. The relatively weak performance of humans in  screening relevant articles has led some investigators to develop natural language processing  tools 9-12 to automate screening. A recent statement by the National Institute for Health and Care Excellence (NICE) highlights a big potential of AI in the systematic review process automation \nLarge Language Models (LLM) recently emerged as one of the most powerful NLP tool across different ranges of tasks. By conducting this review, we wanted to evaluate the natural  extension of the use of LLMs to guide and direct the review process. Thus, this systematic  review aims to (1) summarize the current state-of-the-art research projects using LLMs to  automate the review process, (2) look at the range of review types and review stages that are  being automated, (3) assess the quality of each research project, (4) assess the performance of  LLMs used for automation.  As LLMs are used as a possible substitute for a human reviewer, the idea behind this  publication is to turn the tool \u201con itself\u201d, conducting a systematic review of research projects  using LLMs for systematic reviews.  \n# Methods \nThe study's research plan was formulated by the author team and adjusted based on the guidance provided by the preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015: elaboration and explanation 14 and the latest JBI checklist 15 for conducting systematic reviews. The review protocol was registered in the Open Science Framework (OSF) database 16.  We decided that to be included in the review, citations had to be centered around the usage of LLMs in automation of different phases of systematic review. Only English-language journal publications were considered, including, conference abstracts, and review publications that used LLMs in their creation.   Publications were excluded if they:  \u25cf  Did not use some kind of LLM (e.g. ChatGPT, Mistral, GPT-3.5, BERT)  \u25cf  Did not describe automation of any stage of the review process  \u25cf  The paper was a review article itself that did not use LLM to conduct the review  \u25cf  Full text of the article could not be retrieved or was not in English \nThe initial search was conducted by a human reviewer (DS) in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases. Table 1 presents the search strategy for the databases.   Table 1. Search strategy.  \n((\"large language models\" OR \"large language model\" OR \"LLM\" OR \"LLMs\" OR \"ChatGPT\" OR \"GPT-3\" OR  \"GPT-4\" OR \"LLaMA\" OR \"Mistral\" OR \"Mixtral\" OR \"BARD\" OR \"BERT\" OR \"Claude\" OR \"PaLM\" OR  \"Gemini\" OR \"Copilot\") AND (\"systematic review*\" OR \"scoping review*\" OR \"literature review*\" OR \"narrative  review*\" OR  \"umbrella review*\" OR \"rapid review*\" OR \"integrative review*\" OR \"evidence synthesis\" OR  \"meta-analysis\"))   Source: Authors\u2019 own work \nAll citations were then uploaded to Covidence. The screening and extraction process took place in Covidence with the help of LLM plugin for Covidence that our team developed. This plugin is used during screening and extraction phases. The process of using LLM for screening and extraction is shown on Figure 1. \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fdbc/fdbc0b70-3076-4f44-bd14-8535c4680270.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1. LLM workflow added into Covidence for screening and extraction. Source: Authors\u2019 own work. </div>\nThe developed add-on works by interacting with the Covidence platform programmatically via an intermediary software solution that was created in Python and R. The solution passes contents between Covidence and the LLM OpenAI gpt-4o model . Once the LLM generates the\nresponse, a script automates actions in Covidence, such as clicking Include/Exclude buttons or  leaving notes.   The review process involved three stages that were automated by Covidence add-on:  abstract screening, full-text screening, and extraction. In each stage, two human reviewers  calibrated by screening a sample to refine inclusion criteria and extraction categories. They then  created and tested prompts for the LLM. LLM inference was programmed to run inference 3 times  to determine the final decision (e.g., \"include\" or \"exclude\") based on the majority vote. Three  prompts per phase are detailed in Supplementary Appendix S1.  For abstract screening, LLM and human reviewers voted for consensus, and a human expert  consensus was established. In full-text screening and extraction, a single human reviewer verified  LLM results. Extraction precision was measured, and for categories with low precision (<80%), a  manual reviewer validated LLM outputs. Benchmarks are provided in Supplementary Appendix  S2.  The data charting form for extraction were designed by human experts (DS, VJ, AB, LL,  and NH) and adopted into the LLM prompt to collect the following primary information:   \u25cf  Author, year, title;   \u25cf  Country and/or US state;  \u25cf  What types of reviews were automated;  \u25cf  Stage of review automated in the research project;   \u25cf  LLM type used;   \u25cf  Performance metrics reported by authors during each stage of the review. In  particular, Accuracy, Precision, Recall, Specificity, and F1 were extracted, if other  metrics were used instead, they were grouped under \u201cOther metrics\u201d category;  \u25cf  Brief information on how were these performance metrics calculated;   \u25cf  Brief information on reported timesaving;  \u25cf  What was general opinion of the study team on the usage of LLMs in review  automation (positive, negative, or mixed) with a citation to support this viewpoint.     Human reviewers (DS, VJ, AB) performed quality assessment of given studies using a set \nHuman reviewers (DS, VJ, AB) performed quality assessment of given studies using a set of selected categories from the reviewed studies and a points-based scale:  \n\u25cf  Ratings of the universities where authors are affiliated (the data was link ranking 2024 17), maximum value across all co-author affiliations was use \u25a0  Ranked 1 to 100: 2 points   \u25a0  100-1000: 1 point  \u25a0  >1000: 0 point  \u25cf  Number of samples (full-texts or abstracts) that authors used to comp performance metrics:  \u25a0  More than 200: 2 points   \u25a0  50-200: 1 point  \u25a0  <50: 0 points    \u25cf  Sources of the funding of the research project (public, private or mixed)   \u25a0  Public funding: 2 points   \u25a0  No funding: 1 point  \u25a0  Private funding: 0 points   \u25cf  Impact factor of the journal  18  \u25a0  More than 5: 2 points   \u25a0  1 to 5: 1 point   \u25a0  Less than 1: 0 points  \u25cf  Is the paper an actual review which used LLM?  \u25a0  A review: 2 points  \u25a0  Not a review (methods paper): 1 point  \u25cf  Were performance metrics (benchmarks) reported?   \u25a0  2 points for reporting performance metrics   \u25a0  0 points for no metrics \nIf value in any above category could not be determined (e.g. no match for university or  impact factor, or unknown value in category), then the NA value was assigned. Based on the mean  of points across all the quality categories, studies were classified as low (<1 points), medium (1 to  1.5 points) or high quality (>=1.5 points).  An LLM tool by Google (NotebookLM, version from August 2024) along with a manual  review (DS, VJ, AB) was used to cross check the extraction results for the fields where precision \nof extraction was low (<0.8) during the benchmark. Again, ChatGPT (4o model) was used to clean the extraction data: format the case, remove duplicates, rename similar entries to a common name. The data was then manually fed into the chat window by a human reviewer (DS). Scite.ai (version from August 2024) was used to draft parts of the introduction and discussion sections, while ChatGPT was used to draft the abstract and results section of this review by generating R code snippets to produce all figures (except Figure 1 which was generated by Covidence). ChatGPT was also used to draft the text of the results section, which was then corrected by our team where needed. Human experts edited and verified the final LLM-generated draft of the manuscript.   Additionally, we report the time saving and the computational costs in Supplementary Appendix S3. We used our own time measurements and reference data from experienced reviewers to calculate time-saving 19. \n# Results\nFigure 2 outlines the PRISMA article selection process for this study. Initially, 3,788 studies were identified across several databases: PubMed (n = 2,174), Scopus (n = 1,207), Dimensions (n = 356), and Google Scholar (n = 48), along with 3 additional studies from citation searching. Following the removal of 447 duplicates (1 manually and 446 by Covidence), 3,341 studies remained for the screening phase.  During the title and abstract screening process, 3,041 studies were excluded, leaving 300 studies for retrieval and full-text eligibility assessment. Out of these 300 studies, 128 were excluded for various reasons, with the most common being \u201cThe paper does not describe the automation of any stage of the review process\u201d (n = 88). A total of 172 studies were included in the final review.   \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c7bd/c7bda891-a270-4b5b-aa17-eecdb6b1c123.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2. Flow diagram of the systematic review process. Source: Authors\u2019 own work / Covidence.</div>\nFigure 2. Flow diagram of the systematic review process. Source: Authors\u2019 own work / Covidence. \n   Figure 3 shows the geographic distribution of studies across 43 countries. Most citations are from  the US (n=60, 34.9%), followed by Australia (n=14, 8.14%), the UK and China (n=13, 7.6%), and Germany  (n=11, 6.4%). Other notable contributors include Canada (n=7, 4.1%) and India (n=6, 3.5%). Austria,  Ireland, Italy, the Netherlands, and South Korea each contributed 4 studies (2.3%), while countries like  New Zealand, France, Japan, and others provided 3 (1.7%). The rest contributed 1\u20132 studies.  In the US, 47 studies had state-level data. Tennessee, New York, and Massachusetts led  with 5 citations each (10.6%), followed by California (n=4, 8.5%). North Carolina and Ohio  contributed 3 studies (6.4%), while several other states provided 2 (4.3%) or 1 (2.1%) citation \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7057/70576d66-03f3-4054-a7eb-220f15959044.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8337/833738b9-c131-4c94-8ae7-5fb9631d7331.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3. A: Publications by country of origin; B: Publications by state in the US. Source ChatGPT-generated code using extracted data from this manuscript (Supplemental Table S7) </div>\nFigure 4A shows the types of reviews discussed in automation papers. The most frequently  mentioned type is \u2018Systematic Review\u2019 (n=118, 68.6%), followed by \u2018Literature/Narrative  Review\u2019 (n=37, 21.5%) and \u2018Meta-Analysis\u2019 (n=19, 11.0%). The remaining categories include  \u2018Scoping Review\u2019 (n=8, 4.7%), \u2018Other/Non-specific\u2019 (n=14, 8.1%), and \u2018Rapid Review\u2019 (n=6,  3.5%). \u2018Umbrella Review\u2019 has a smaller representation with 2 mentions (1.2%).  Figure 4B illustrates the stages of review discussed in automation papers. The most  frequently mentioned stage is \u2018Searching for publications\u2019 (n=60, 34.9%), followed by \u2018Data  extraction\u2019 (n=54, 31.4%) and \u2018Evidence synthesis/summarization\u2019 (n=32, 18.6%). Other  categories with notable mentions include \u2018Title and abstract screening\u2019 (n=43, 25.0%), \u2018Drafting  a publication\u2019 (n=22, 12.8%), \u2018Full-text screening\u2019 (n=14, 8.1%), \u2018Quality and bias assessment\u2019  (n=12, 7.0%), \u2018Publication classification\u2019 (n=10, 5.8%), \u2018Other stages\u2019 (n=6, 3.5%), and \u2018Code  and plots generation\u2019 (n=4, 2.3%). \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8375/8375f51a-3e56-492c-97a2-b1566b003848.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5094/50942df1-fba3-43bf-a897-fbfe0c621b39.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4. A. Types of review automated B. Which stages of review are automated in the pape Source: ChatGPT-generated code using extracted data from this manuscript (Supplemental Table S7) </div>\n The most frequently mentioned AI model is GPT/ChatGPT, with 126 occurrences (73.3%),  showing its widespread use (Supplemental Figure S5). BERT-based models are also notable with 32  mentions (18.6%). LLaMA/Alpaca models have 8 mentions (4.7%), followed by Google Bard/Gemini with  5 (2.9%), and Claude models with 7 (4.1%). Other models like BART (n=3, 1.7%) and Mistral (n=4, 2.3%)  are less frequent. Several models, including Bing and XLNet, have 2 mentions each (1.2%), while many  others are mentioned just once (0.6%).  Of the 172 citations, 79 (45.9%) reported common metrics like Accuracy, Precision/Recall,  and F1, while 36 (20.9%) used less common metrics like G-score and Jaccard similarity. The  remaining 57 publications (33.1%) relied on qualitative assessments.  Figure 5 shows performance metrics for GPT- and BERT-based models. GPT models had  lower accuracy in title/abstract screening (M=77.34, SD=13.06) compared to BERT models  (M=80.87, SD=11.81). However, GPT models performed better in data extraction, with precision  (M=83.07, SD=10.43) and recall (M=85.99, SD=9.82), while BERT models had lower precision  (M=61.06, SD=31.26) and similar recall (M=80.03, SD=10.09). In title/abstract screening, BERT \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f6ef/f6ef19c9-e97b-47ce-803f-08b390180299.png\" style=\"width: 50%;\"></div>\nmodels had higher precision (M=65.6, SD=17.65) but lower recall (M=72.93, SD=23.95) than GPT models (precision M=63.2, SD=24.34; recall M=80.42, SD=23.31). \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/612e/612eeaea-b41b-4a25-8dd7-24400df17efa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5. Performance metrics reported for the three most common automated stages A: for  GPT-based models. B: for BERT-based models. Source: ChatGPT-generated code using extracted data from this manuscript (Supplemental Table S7). </div>\nFigure 5. Performance metrics reported for the three most common automated stages A: for  GPT-based models. B: for BERT-based models. Source: ChatGPT-generated code using extracted data from this manuscript (Supplemental Table S7). \nMajority of reviewed publications were papers describing how LLM could be used to  automate a certain phase of the review (n=146, 84.9%) (Supplemental Figure S6A). Only 26  (15.1%) papers were actual reviews conducted with some help from LLM tools. Majority of  authors were positive about the usage of LLMs in reviews (n=120, 69.8%), with 43 citations  (25.0%) containing mixed or cautious views on LLM usage (Supplemental Figure S6B). Only 9  (5.2%) study teams had negative experiences with LLM usage. Most studies had public funding  reported (n=97, 56.4%) (Supplemental Figure S6C). When considering all the factors together,  such as funding, journal impact factor, sample size, reported metrics, and others (see Methods), 72  citations (41.9%) appear to be of high quality, with 73 citations being medium quality (42.4%)  (Supplemental Figure S6D).   \nSupplemental Table S7 presents the extraction table with all extracted categories across  172 citations. \nOur LLM-assisted systematic review revealed a significant number of research projects related to review automation with LLM. Indeed, other researchers have noted promising results\nor LLMs in different areas, such as understanding human language and generating contextuall ppropriate responses 20-22.  \n# for LLMs in different areas, such as understanding human language and generating contextual appropriate responses 20-22.  \nDespite finding a significant number of projects using LLMs to automate some stages of  the review process only few papers focused on the full cycle of review automation 23,24. There  might be perceived publication barriers, for example, journals recently started to ask about LLMgenerated content, although we don\u2019t have information on whether this leads to changes in  reviewing process. Growing number of LLM-generated papers will probably eventually change  how review is conducted (reviewers might be assisted by LLMs or review paper format could be  eventually replaced by online real-time information retrieval).  The strength of present review is in large-scale (over 3000 abstracts screened, and 172 fulltext publications eligible for extraction) automation of different stages of review, including  drafting the manuscript sections, and plot generation. Only few citations focused on automation of  full cycle of review, while most focused only on specific areas like extraction or screening,  including our own previous systematic review where GPT-3.5 was used with LDA-based topic  modelling for validation of human findings 25. In contrast, the LLM-based method that we applied  in this work demonstrated its direct applicability, by facilitating the automation of the abstract and  full-text screening, data extraction, as well as the knowledge synthesis stages, with the discussed  constraints. Furthermore, our method is domain-agnostic, thus it can be integrated into large-scale  review projects across different domains. The implications of such automation include reducing  human workload and improving overall efficiency of systematic reviews. Furthermore, such tool  in its more mature form will require less expertise from human reviewers, which could contribute  to the democratization of systematic and scoping review process, with the potential to add features  related to meta-analysis into the process.  GPT-based LLM were the most dominant type of LLM and the one that seems to show  remarkable results on the data extraction, arguably the most complex and time-consuming stage  of any review. It\u2019s usage for literature reviews is obvious, at this moment there are little restrictions  on the type of information users can load into ChatGPT, and published papers are unlikely to  contain any sensitive information, making ChatGPT with its high-performing model and  developed API an obvious choice. At the same time smaller models like BERT, Llama or Mistral \ncan be run and fine-tuned locally with much less cost, so we expect to see more automation projects with this LLM in the future.26 \n# Limitations \nWe used calibrated LLMs as reviewers in this project. Some extraction categories, such as performance metrics, had relatively lower accuracy, so the results of this extraction category should be taken with caution. Nevertheless, in this review LLMs achieved remarkable results in accuracy, making it possible to delegate time-consuming phases of review to LLMs. Studies generally recommend a single reviewer approach in some cases like rapid reviews27. However we believe that the LLM approach could substitute human reviewers, and human effort should be redirected to supervision of the review process.   A further limitation of this work is the simplified scoring system we introduced for research evaluation, which, using arbitrary weightings, may overlook key aspects like the novelty, robustness, and relevance of the studies. Future research should focus on improving LLM performance metrics, particularly precision and recall in lower-accuracy extraction categories. Additionally, integrating and evaluating different LLMs, possibly in combination with other AI models, should be explored to enhance performance. The short- and long-term impact of these integrations on review quality, along with ethical considerations, must also be assessed to maintain research credibility and trust. \n# Conclusion\nThe use of LLMs in review automation is rapidly growing, with expected radical changes in scientific evidence synthesis. LLMs are likely to significantly reduce the time needed for reviews while producing similar or higher-quality data in greater quantities than manual reviews. Research shows it is becoming increasingly difficult to distinguish between LLM-generated and human-written text.28 and the presence of LLM generated texts in scientific publications in\ngrowing exponentially 29. To promote transparency and proper acknowledgment, researchers are  encouraged to openly disclose their use of LLMs in academic papers, providing information on  the prompts employed and the sections of text affected 30.  Despite early successes, few systematic reviews using LLMs were identified in our review.  Although still in its early stages, AI-assisted reviews are already yielding impressive results, with  growing interest as researchers develop semi-automated pipelines. However, generating  trustworthy and useful AI-driven reviews still presents both technological and ethical challenges,  particular for quantitative meta-analyses comparing treatment effects. However, the conduct of  more simple systematic reviews, such as scoping reviews, appears to be well within the capabilities  of current or near future AI methods.    \ngrowing exponentially 29. To promote transparency and proper acknowledgment, researchers are encouraged to openly disclose their use of LLMs in academic papers, providing information on the prompts employed and the sections of text affected 30. \nDespite early successes, few systematic reviews using LLMs were identified in our review.  Although still in its early stages, AI-assisted reviews are already yielding impressive results, with  growing interest as researchers develop semi-automated pipelines. However, generating  trustworthy and useful AI-driven reviews still presents both technological and ethical challenges,  particular for quantitative meta-analyses comparing treatment effects. However, the conduct of  more simple systematic reviews, such as scoping reviews, appears to be well within the capabilities  of current or near future AI methods.    \n# Author contributions\nLL, NH, and DS conceived and designed the review. DS developed the LLM screening  automation add-on for Covidence. DS and VJ contributed to search strategy development. DS,  AB and VJ performed the benchmarks for LLM and designed LLM prompts. DS, AB and VJ  verified data extraction results. VJ and AB researched third-party components that were used to  create the review. AB developed the script for journal impact factor assessment. DS analyzed th data and drafted the manuscript with the help of scite.ai and ChatGPT. NH and LL found the  resources to conduct the review. All authors critically reviewed and revised the manuscript and  approved the final version for submission.  \n# Funding \nThis publication was supported, in part, by the National Center for Advancing  Translational Sciences of the National Institutes of Health under Grant Number UL1 TR001450. Dr. Scherbakov was supported by grant T15 LM013977, Biomedical Informatics and Data  Science for Health Equity Research (SC BIDS4Health). This publication was supported in part  by a Smart-state Chair endowment. The content is solely the responsibility of the authors and  does not necessarily represent the official views of the National Institutes of Health.  \n# Conflicts of interest statement\nThe authors have no competing interests to declare.\n1.  Toh TS, Lee JH. Statistical note: Using scoping and systematic reviews. Pediatric Critical Care  Medicine 2021;22(6):572-575.  2.  Abushouk AI, Yunusa I, Elmehrath AO, et al. Quality assessment of published systematic  reviews in high impact cardiology journals: revisiting the evidence pyramid. Frontiers in  Cardiovascular Medicine 2021;8:671569.  3.  Borah R, Brown AW, Capers PL, Kaiser KA. Analysis of the time and workers needed to condu systematic reviews of medical interventions using data from the PROSPERO registry. BMJ ope 2017;7(2):e012545.  4.  Munn Z, Peters MD, Stern C, Tufanaru C, McArthur A, Aromataris E. Systematic review or  scoping review? Guidance for authors when choosing between a systematic or scoping review  approach. BMC medical research methodology 2018;18:1-7.  5.  Kellermeyer L, Harnke B, Knight S. Covidence and rayyan. Journal of the Medical Library  Association: JMLA 2018;106(4):580.  6.  Chan JL, Murphy KA, Sarna JR. Myoclonus and cerebellar ataxia associated with COVID-19: a case report and systematic review. Journal of Neurology 2021:1-32.  7.  (https://www.covidence.org/blog/machine-learning-the-game-changer-for-trustworthy-evidence 8.  Gartlehner G, Affengruber L, Titscher V, et al. Single-reviewer abstract screening missed 13  percent of relevant studies: a crowd-based, randomized controlled trial. Journal of clinical  epidemiology 2020;121:20-28.  9.  Marshall IJ, Wallace BC. Toward systematic review automation: a practical guide to using  machine learning tools in research synthesis. Systematic Reviews 2019;8(1):163. DOI:  10.1186/s13643-019-1074-9.  10.  Rasheed Z, Waseem M, Syst\u00e4 K, Abrahamsson P. Large language model evaluation via multi a agents: Preliminary results. arXiv preprint arXiv:240401023 2024. \n11.  Wang S, Scells H, Zhuang S, Potthast M, Koopman B, Zuccon G. Zero-shot Generative Large  Language Models for Systematic Review Screening Automation.  European Conference on  Information Retrieval: Springer; 2024:403-420.  12.  Zaki M, Namireddy SR, Pittie T, et al. Natural language processing-guided meta-analysis and  structure factor database extraction from glass literature. Journal of Non-Crystalline Solids: X  2022;15:100103. DOI: https://doi.org/10.1016/j.nocx.2022.100103.  13.  National Institute for Health and Care Excellence. Use of AI in evidence generation: NICE  position statement.  (https://www.nice.org.uk/about/what-we-do/our-research-work/use-of-ai-inevidence-generation--nice-positionstatement?utm_medium=social&utm_source=linkedin&utm_campaign=aiposition).  14.  Moher D, Shamseer L, Clarke M, et al. Preferred reporting items for systematic review and meta analysis protocols (PRISMA-P) 2015 statement. Systematic reviews 2015;4:1-9.  15.  Aromataris E, Fernandez R, Godfrey CM, Holly C, Khalil H, Tungpunkom P. Summarizing  systematic reviews: methodological development, conduct and reporting of an umbrella review  approach. JBI Evidence Implementation 2015;13(3):132-140.  16.  Scherbakov D. Large language models in scoping and systematic reviews automation: an  automated systematic review [protocol registration]. DOI:  https://doi.org/10.17605/OSF.IO/EJKSY.  17.  QS World University Rankings 2024.  (https://www.kaggle.com/datasets/joebeachcapital/qsworld-university-rankings-2024?resource=download).  18.  Dinga J. Updated List of Journal Impact Factor 2022 _ Journal Citation Report 2022 and Journal Quartiles 20222022.  19.  Haddaway NR, Westgate MJ. Predicting the time needed for environmental systematic reviews  and systematic maps. Conservation Biology 2019;33(2):434-443.  20.  Liu Z. ChatGPT - A New Milestone in the Field of Education. Applied and Computational  Engineering 2024;35(1):129-133. DOI: 10.54254/2755-2721/35/20230380. \n11.  Wang S, Scells H, Zhuang S, Potthast M, Koopman B, Zuccon G. Zero-shot Generative Large  Language Models for Systematic Review Screening Automation.  European Conference on  Information Retrieval: Springer; 2024:403-420.  12.  Zaki M, Namireddy SR, Pittie T, et al. Natural language processing-guided meta-analysis and  structure factor database extraction from glass literature. Journal of Non-Crystalline Solids: X  2022;15:100103. DOI: https://doi.org/10.1016/j.nocx.2022.100103.  13.  National Institute for Health and Care Excellence. Use of AI in evidence generation: NICE  position statement.  (https://www.nice.org.uk/about/what-we-do/our-research-work/use-of-ai-inevidence-generation--nice-positionstatement?utm_medium=social&utm_source=linkedin&utm_campaign=aiposition).  14.  Moher D, Shamseer L, Clarke M, et al. Preferred reporting items for systematic review and meta analysis protocols (PRISMA-P) 2015 statement. Systematic reviews 2015;4:1-9.  15.  Aromataris E, Fernandez R, Godfrey CM, Holly C, Khalil H, Tungpunkom P. Summarizing  systematic reviews: methodological development, conduct and reporting of an umbrella review  approach. JBI Evidence Implementation 2015;13(3):132-140.  16.  Scherbakov D. Large language models in scoping and systematic reviews automation: an  automated systematic review [protocol registration]. DOI:  https://doi.org/10.17605/OSF.IO/EJKSY.  17.  QS World University Rankings 2024.  (https://www.kaggle.com/datasets/joebeachcapital/qsworld-university-rankings-2024?resource=download).  18.  Dinga J. Updated List of Journal Impact Factor 2022 _ Journal Citation Report 2022 and Journal Quartiles 20222022.  19.  Haddaway NR, Westgate MJ. Predicting the time needed for environmental systematic reviews  and systematic maps. Conservation Biology 2019;33(2):434-443.  20.  Liu Z. ChatGPT - A New Milestone in the Field of Education. Applied and Computational  Engineering 2024;35(1):129-133. DOI: 10.54254/2755-2721/35/20230380. \n21.  Mu Y. The Potential Applications and Challenges of ChatGPT in the Medical Field. International Journal of General Medicine 2024;Volume 17:817-826. DOI: 10.2147/ijgm.s456659.  22.  Tlili A, Shehata B, Adarkwah MA, et al. What if the Devil Is My Guardian Angel: ChatGPT as a Case Study of Using Chatbots in Education. Smart Learning Environments 2023;10(1). DOI:  10.1186/s40561-023-00237-x.  23.  Schopow N, Osterhoff G, Baur D. Applications of the Natural Language Processing Tool  ChatGPT in Clinical Practice: Comparative Study and Augmented Systematic Review. JMIR  Med Inform 2023;11:e48933. DOI: 10.2196/48933.  24.  Teperikidis E, Boulmpou A, Potoupni V, Kundu S, Singh B, Papadopoulos C. Does the long-term administration of proton pump inhibitors increase the risk of adverse cardiovascular outcomes? A ChatGPT powered umbrella review. Acta Cardiol 2023;78(9):980-988. DOI:  10.1080/00015385.2023.2231299.  25.  Noe-Steinmuller N, Scherbakov D, Zhuravlyova A, Wager TD, Goldstein P, Tesarz J. Defining  suffering in pain: a systematic review on pain-related suffering using natural language processing Pain 2024;165(7):1434-1449. DOI: 10.1097/j.pain.0000000000003195.  26.  Agapiou A, Lysandrou V. Interacting with the Artificial Intelligence (AI) Language Model  ChatGPT: A Synopsis of Earth Observation and Remote Sensing in Archaeology. Heritage  2023;6(5):4072-4085. DOI: 10.3390/heritage6050214.  27.  Waffenschmidt S, Knelangen M, Sieben W, B\u00fchn S, Pieper D. Single screening versus  conventional double screening for study selection in systematic reviews: a methodological  systematic review. BMC Med Res Methodol 2019;19(1):132. (In eng). DOI: 10.1186/s12874019-0782-0.  28.  Orenstrakh MS, Karnalim O, Suarez CA, Liut M. Detecting llm-generated text in computing  education: A comparative study for chatgpt cases. arXiv preprint arXiv:230707411 2023.  29.  Liang W, Zhang Y, Wu Z, et al. Mapping the increasing use of llms in scientific papers. arXiv  preprint arXiv:240401268 2024. \nHosseini M, Resnik DB, Holmes KL. The Ethics of Disclosing the Use of Artificial Intellige Tools in Writing Scholarly Manuscripts. Research Ethics 2023;19(4):449-465. DOI:  10.1177/17470161231180449. \n# Table of Contents\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55b2/55b2a9c9-60fc-432c-b926-4bd167d47934.png\" style=\"width: 50%;\"></div>\n# Table S1. LLM Prompts used for screening and extraction. ....................................................................................................................... 25  Table S2. Benchmark of abstract screening phase (N=100 abstracts). ....................................................................................................... 26  Table S3. Benchmark of full-text screening phase (N=30 full-text PDFs). ................................................................................................. 26  Table S4. Benchmark of full-text extraction phase (N=15 full-text PDFs). ................................................................................................ 27 \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bda4/bda44eef-18d1-4591-a489-7dffcac79b65.png\" style=\"width: 50%;\"></div>\nPhase of \nthe review  \nLLM prompt  \nAbstract \nscreening  \nSummarise the text abstract of a full research paper (article), and given the below criteria list, say if the full paper is likely to be included, excluded, or unclear. \nDefinition of review. Review is a type of publication that synthesises knowledge from other publications.  \nReviews include systematic, scoping reviews, meta-analysis, evidence synthesic, umbrella and rapid reviews, literature, narrative reviews, and other type of \nreviews.  \nA review typically has the following stages:  \nresearch question generation, creating a search strategy for a review, screening of literature, extraction of information, quality and bias assessment, evidence \nsynthesis, writing a paper, generating code/plots for the review and generating tables.  \n  \nCriteria list for exclusion/inclusion.  \nInclude: Paper should be using some kind of large language models (LLM), like ChatGPT, GPT-3.5, GPT-4, Claude, BERT, BARD, Mistral, PaLM, Gemini, \nCopilot, Llama, Mixtral, and similar.  \nInclude: Paper should be focused on automation of any stage of the review process listed above.  \nExclude: If any of the Include criteria doesn't match.  \nExclude: The paper is a review itself (types of review are listed above). However, if this review reports that it uses LLM for any review stage (stages of review \nare listed above), then include it.   \nExclude: Paper is not related to automation of any parts of the review.  \nExclude: Paper is a book chapter or compilation of conference papers (but single conference papers should be included).  \nExclude: Paper mentions related technology like code generation with LLM but it is not related to creating a review (see definition of review above).  \nExclude: Abstract and title are too brief and don't contain enough information to make the decision.  \nFollow this format:  \n1) First provide some explanations why each study should be included or excluded.  \n2) Then format your output as follows, strictly follow this format, use equal(=) sign, if study is excluded, write 'answer=excluded', if study is included output \n'answer=included', or if it is unclear write 'answer=unclear'.  \nFull-text \nscreening  \nLook at the research paper (article), and given the below criteria list, say if the full paper is to be included, excluded, or unclear.   \n   \nDefinition of review. Review is a type of publication that synthesises knowledge from other publications.  \nReviews include systematic, scoping reviews, meta-analysis, evidence synthesic, umbrella and rapid reviews, literature, narrative reviews, and other type of \nreviews.  \nA review typically has the following stages:  \nresearch question generation, creating a search strategy for a review, review protocol creation, screening of literature, extraction of information, quality and \nbias assessment, evidence synthesis, writing a paper, generating code/plots for the review and generating tables.  \n  \nCriteria list.  \nPaper should be focused on automation on any stage of the review with large language models (LLM).  \nIf any of the following exclusion reason match, then exclude the article.  \nExclude reason 1: Paper doesn't use some kind of large language models (LLM), like ChatGPT, GPT-3.5, GPT-4, Claude, BERT, BARD, Mistral, PaLM, \nGemini, Copilot, Llama, Mixtral, and similar.  \nExclude reason 2: Paper doesn't describe automation of any stage of the review process.  \nExclude reason 3: Rather than covering automation of stages of the review process, paper is the review itself. However, if the paper is a review and uses some \nelement of review automation to perform the review, then include it.   \nExclude reason 4: Paper matches the focus (review automation with LLM), but it doesn't evaluate or report performance of any phase of the review process. \nEvaluation means verification by human experts. Often after evaluation performance metrics are reported which include, but not limited to: accuracy, F1, \nprecision, recall, sensitivity, error rate, time saved, and others.  \nExclude reason 5: Full text couldn't be retrieved.  \n Follow this format:  \n1) First provide some explanations why each study should be included or excluded.   \n2) Provide citation from text showing what NLP method was used and mental health problem explored.  \n3) Output the following:   \ninclude=yes/no/unclear  \nexclude_reason=reason_number (choose only one)  \n  \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cb0e/cb0e128d-5b6f-4216-9a37-28e602b37e82.png\" style=\"width: 50%;\"></div>\nSource: Authors\u2019 own analysis\n<div style=\"text-align: center;\">Table S2. Benchmark of abstract screening phase (N=100 abstracts).</div>\n  \nSensitivity \nSpecificity \nPos Pred \nValue \nNeg Pred \nValue \nPrecision \nRecall \nF1 \nPrevalence \nDetection \nRate \nDetection \nPrevalence \nBalanced \nAccuracy \nReviewer 1 \nvs Consensus \n0.77 \n0.98 \n0.97 \n0.82 \n0.97 \n0.77 \n0.86 \n0.48 \n0.37 \n0.38 \n0.88 \nReviewer 2 \nvs Consensus \n0.69 \n1 \n1 \n0.78 \n1 \n0.69 \n0.81 \n0.48 \n0.33 \n0.33 \n0.84 \nHuman \nconsensus vs \nConsensus \n0.79 \n1 \n1 \n0.84 \n1 \n0.79 \n0.88 \n0.48 \n0.38 \n0.38 \n0.9 \nLLM vs \nConsensus \n0.94 \n0.83 \n0.83 \n0.93 \n0.83 \n0.94 \n0.88 \n0.48 \n0.45 \n0.54 \n0.88 \n Source: Authors\u2019 own analysis \nSource: Authors\u2019 own analysis \n<div style=\"text-align: center;\">Table S3. Benchmark of full-text screening phase (N=30 full-text PDFs)</div>\n  \nSensitivity \nSpecificity \nPos Pred \nValue \nNeg Pred \nValue \nPrecision \nRecall \nF1 \nPrevalence \nDetection \nRate \nDetection \nPrevalence \nBalanced \nAccuracy \nReviewer 1 \nvs Consensus \n1 \n1 \n1 \n1 \n1 \n1 \n1 \n0.76 \n0.76 \n0.76 \n1 \nLLM vs \nConsensus \n1 \n0.5 \n0.86 \n1 \n0.86 \n1 \n0.93 \n0.76 \n0.76 \n0.88 \n0.75 \n Source: Authors\u2019 own analysis \nSource: Authors\u2019 own analysis \n<div style=\"text-align: center;\">Table S4. Benchmark of full-text extraction phase (N=15 full-text PDFs).</div>\nCategory \nCountry \nReview stage \nautomated \nLLM type used \nPerformance \nmetrics of LLM \nSample size \nReview type \nautomated in the \nstudy \nAuthors opinion \non LLM \nCitation to \nsupport authors \nopinion \nType of funding \nused \nPrecision \n1 \n0.93 \n0.93 \n0.8 \n0.8 \n0.53 \n1 \n0.93 \n0.73 \nRecall \n0.86 \n0.8 \n0.86 \n0.33 \n0.53 \n0.8 \n0.93 \n0.93 \n0.8 \n Source: Authors\u2019 own analysis \n Source: Authors\u2019 own analysis \nTime-saving and computational costs.  Our review utilized approximately 500$ in OpenAI Azure costs for GPT-4o model.   We estimate that we saved time in screening 3241 abstracts (100 were manually screened for benchmark) by two reviewers with an average rate by a single reviewer of 40 abstracts per hour: 3241*2/40 = 162 hours. In addition, we saved time in screening 270 full-text publications (30 were manually screened for benchmark) by two reviewers with an average rate by a single reviewer of 10 full-texts per hour: 270*2/10 = 54 hours. We saved time in full-text extraction of 157 full-text publications (15 were manually extracted for benchmark) for 2 reviewers, but we had to do manual extraction of all papers for some categories where LLM precision/recall was low spending about 15 minutes per each publication, thus, assuming average rate of a single reviewer at 2 full-texts per hour we saved 157*2/2 \u2013 157/4 = 118 hours. In addition, we saved time on drafting and code generation, with an estimated time saving of 50 hours.   Thus, we estimate total time saving of 334 person-hours. \nFigure S5. LLM model types used in the studies\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d9b0/d9b031f8-6329-4963-b79b-715ab9f7dd78.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure S5. LLM types proposed for automation (models mentioned in 2 or more studies shown). Source: ChatGPT-generated code using extracted data </div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce0f/ce0f970d-dd95-4825-afdd-b41d5c667407.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure S6. A: Type of citation (a review with LLM usage or a methods paper), B: Overall opinion of citation authors on LLM usage in review, C: Funding sources reported in the study, D: Overall quality of evidence. Source: ChatGPT-generated code using extracted data </div>\n<div style=\"text-align: center;\">Table S7. Complete table of extracted categories. \u2020 denotes categories that were verified by a human reviewer to ensure precision of extraction. </div>\nStudy \nTitle \nCountry/\nUS State \nReview stage\u2020 \n \nReview type\u2020 \nLLM type\u2020 \nPerformance metrics\u2020 \nOther metrics \nreported\u2020 \nDetails on performance \nmetrics \nSample size\u2020 \nTime savings \nreported \nReview \nor \nmethods \nstudy\u2020 \nFun-\nding \nQuality of \nevidence  \nOverall \nopinion\u2020 \nCitation from study \nGuo, 2024 \n[1] \nAutomated Paper Screening for \nClinical Reviews Using Large \nLanguage Models: Data \nAnalysis Study \nCanada \nTitle and abstract \nscreening \nSystematic \nreview, Scoping \nreview \nGPT / \nChatGPT \nGPT-4.Title and abstract \nscreening.Accuracy=91.0; \nGPT-4.Title and abstract \nscreening.F1=60.0 \nYes \nAccuracy: computed by \ndividing papers selected \nby both GPT and human \nreviewers by the total \nnumber of papers. Macro \nF1-score: not specified in \ndetail. Sensitivity: \ncalculated for both \nincluded and excluded \npapers. Interrater \nreliability (kappa and \nPABAK): computed \nagainst the human-\nreviewed papers. \n24307 \nReduction in \nScreening Time \nwith Gpt for the \nNoa Dataset Was \nApproximately \n643 Minutes and \nCost \nApproximately 25 \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nPositive \n\"Large language models have \nthe potential to streamline the \nclinical review process, save \nvaluable time and effort for \nresearchers, and contribute to \nthe overall quality of clinical \nreviews.\" \nHaltaufde\nrheide, \n2024 [2] \nThe Ethics of ChatGPT in \nMedicine and Healthcare: A \nSystematic Review on Large \nLanguage Models (LLMs) \nGermany \nSearching for \npublications \nRapid Review, \nSystematic review \nGPT / \nChatGPT \nNot mentioned / Qualitative \nNo \nNot extracted/Not \napplicable \n796 \nNot extracted/Not \napplicable \nReview \npaper \nPublic \nHigh \nMixed \n\"Ethical examination of LLMs \nin healthcare is still nascent \nand struggles to keep pace \nwith rapid technical \nadvancements.\" \nSun, 2024 \n[3] \nHow good are large language \nmodels for automated data \nextraction from randomized \ntrials? \nChina \nData extraction \nSystematic review \nChatPDF, \nClaude \nChatPDF.Data \nextraction.Kappa =93.0; \nClaude.Data \nextraction.kappa=80.0 \nYes \nNot extracted/Not \napplicable \n49 \nNot extracted/Not \napplicable \nReview \npaper \nPublic \nHigh \nMixed \n\"Whilst promising, the \npercentage of correct \nresponses is still unsatisfactory \nand therefore substantial \nimprovements are needed for \ncurrent AI tools to be adopted \nin research practice.\" \nSusnjak, \n2023 [4] \nPrisma-dfllm: An extension of \nprisma for systematic literature \nreviews using domain-specific \nfinetuned large language \nmodels \nNew \nZealand \nSearching for \npublications, \nTitle and abstract \nscreening, Full-\ntext screening, \nData extraction, \nEvidence \nsynthesis/summar\nization \nSystematic \nreview, \nSystematic review \nGPT / \nChatGPT \nNot mentioned / Qualitative \nNo \nNot extracted/Not \napplicable \nNot specified \nNot extracted/Not \napplicable \nMethods \npaper \nPublic \nMedium \nPositive \n\"The proposed extended \nPRISMA FLLM checklist of \nreporting guidelines provides a \nroadmap for researchers \nseeking to implement this \napproach.\" \nSusnjak, \n2024 [5] \nAutomating research synthesis \nwith domain-specific large \nlanguage model fine-tuning \nNew \nZealand \nEvidence \nsynthesis/summar\nization, Data \nextraction \nSystematic review \nGPT / \nChatGPT, \nMistral \nNot mentioned / Qualitative \nYes \nnot extracted \nSYNTHESIS \nOF \nKNOWLED\nGE=4962 \nNot Extracted \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nPositive \n\"AI technologies can \neffectively streamline SLRs, \nensuring both efficiency and \naccuracy in information \nretrieval.\" \nTang, \n2023 [6] \nEvaluating large language \nmodels on medical evidence \nsummarization \nUSA \nEvidence \nsynthesis/summar\nization \nMeta-analysis, \nSystematic review \nGPT / \nChatGPT \nNot mentioned / Qualitative \nYes \nPerformance metrics were \ncalculated by comparing \nthe generated summaries \nagainst reference \nsummaries using ROUGE-\nL, METEOR, and BLEU \nscores, which measure \noverlap and precision of n-\ngrams. \nGPT-\n3.5.synthesis \nof \nknowledge=5\n3 \n, \nChatGPT.syn\nthesis of \nknowledge=5\n3 \nNot Reported \nMethods \npaper \nPublic \nHigh \nNegative \n\"Our study demonstrates that \nautomatic metrics often do not \nstrongly correlate with the \nquality of summaries ... LLMs \ncould be susceptible to \ngenerating factually \ninconsistent summaries and \nmaking overly convincing or \nuncertain statements, leading \nto potential harm due to \nmisinformation.\" \nTran, \n2024 [7] \nSensitivity and Specificity of \nUsing GPT-3.5 Turbo Models \nfor Title and Abstract Screening \nin Systematic Reviews and \nMeta-analyses \nFrance \nTitle and abstract \nscreening \nRapid Review, \nSystematic review \nGPT / \nChatGPT \nGPT-35 Turbo.Title and \nabstract \nscreening.Recall=87.2; \nGPT-35 Turbo.Title and \nabstract \nscreening.Specificity=52.2 \nNo \nComparing output of GPT-\n3.5 models under balanced \nand sensitive rules with \noriginal decisions from \nauthors at title and abstract \nlevel, with sensitivities and \nspecificities calculated \nusing continuity corrected \ncell counts. \n22665 \nReducing the \nNumber of \nCitations Before \nManual Screening \nfrom 2 to 45 4 \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nMixed \n\"The GPT-3.5 Turbo model \nmay be used as a second \nreviewer for title and abstract \nscreening, at the cost of \nadditional work to reconcile \nadded false positives.\" \nBlasingam\ne, 2024 [8] \nEvaluating a Large Language \nModel\u2019s Ability to Answer \nUSA/Tenn\nessee \nEvidence \nsynthesis/summar\nization \nOther/Non-\nspecific \nGPT / \nChatGPT \nNot mentioned / Qualitative \nYes \nNot extracted/Not \napplicable \n216 \nNot extracted/Not \napplicable \nMethods \npaper \nPublic \nHigh \nPositive \n\"we envision this being the \nfirst of a series of \ninvestigations designed to \nClinicians\u2019 Requests for \nEvidence Summaries \nfurther our understanding of \nhow current and future \nversions of generative AI can \nbe used and integrated into \nmedical librarians workflow\" \nYan, 2023 \n[9] \nLeveraging Generative AI to \nPrioritize Drug Repurposing \nCandidates: Validating \nIdentified Candidates for \nAlzheimer\u2019s Disease in Real-\nWorld Clinical Datasets \nUSA/Tenn\nessee \nEvidence \nsynthesis/summar\nization \nMeta-analysis \nGPT / \nChatGPT \nNot mentioned / Qualitative \nYes \nCalculated using Cox \nproportional hazards \nregression models \ncomparing the risk of \nAlzheimers disease in \nindividuals exposed to a \ndrug repurposing \ncandidate and propensity \nscore-matched individuals \nnever exposed to the drug \nGPT-\n4.synthesis of \nknowledge=2\n0 \nNot Reported \nMethods \npaper \nPublic \nMedium \nPositive \n\"Our findings suggest that \nChatGPT can generate quality \nhypotheses for drug \nrepurposing... With minimal \ncosts, ChatGPT has the \ncapacity and scalability to \nsubstantially accelerate the \nreview process.\" \nLi, 2024 \n[10] \nEvaluating the Effectiveness of \nLarge Language Models in \nAbstract Screening: A \nComparative Analysis \nUSA/Nort\nh Carolina \nTitle and abstract \nscreening \nMeta-analysis, \nSystematic review \nGPT / \nChatGPT, \nGoogle PaLM, \nLlama or \nAlpaca, Hybrid \nChatGPT4.Title and \nabstract \nscreening.Accuracy=90.2; \nChatGPT4.Title and \nabstract \nscreening.Recall=89.1; \nChatGPT4.Title and \nabstract \nscreening.Specificity=90.7; \nChatGPT35.Title and \nabstract \nscreening.Accuracy=73.6; \nChatGPT35.Title and \nabstract \nscreening.Recall=74.1; \nChatGPT4.Title and \nabstract \nscreening.Specificity=78.0; \nGoogle PaLM.Title and \nabstract \nscreening.Accuracy=78.6; \nGoogle PaLM.Title and \nabstract \nscreening.Recall=49.9; \nGoogle PaLM.Title and \nabstract \nscreening.Specificity=96.8; \nMeta Llama 2.Title and \nabstract \nscreening.Accuracy=74.8; \nMeta Llama 2.Title and \nabstract \nscreening.Recall=91.9; \nMeta Llama 2.Title and \nabstract \nscreening.Specificity=65.7; \nHybrid.Title and abstract \nscreening.Accuracy=95.5; \nHybrid.Title and abstract \nscreening.Recall=53.9; \nHybrid.Title and abstract \nscreening.Specificity=98.4 \nNo \nSensitivity is defined as \nthe number of true \npositives divided by the \nsum of true positives and \nfalse negatives, specificity \nas the number of true \nnegatives divided by the \nsum of true negatives and \nfalse positives, and \naccuracy as sum of true \npositives and true \nnegatives divided by the \ntotal number of abstracts. \n200 \nProcessing 200 \nAbstracts with \nEach Llm Took \nApproximately 10 \n20 Minutes using a \nSingle Thread \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nMedium \nMixed \n\"While LLM tools are not yet \nready to completely replace \nhuman experts in abstract \nscreening, they show great \npromise in revolutionizing the \nprocess.\" \nWilkins, \n2023 [11] \nAutomated title and abstract \nscreening for scoping reviews \nusing the GPT-4 Large \nLanguage Model \nAustralia \nTitle and abstract \nscreening \nScoping review \nGPT / \nChatGPT \nGPT-4.Title and abstract \nscreening.Accuracy=84.0; \nGPT-4.Title and abstract \nscreening.Recall=71.0; \nGPT-4.Title and abstract \nscreening.Specificity=89.0 \nYes \nAccuracy was calculated \nas the proportion of correct \ndecisions (both inclusions \nand exclusions) made by \nGPT-4 compared to the \nconsensus human reviewer \ndecision. Sensitivity was \ncalculated as the \nproportion of true \npositives (correct \ninclusions) out of all actual \npositives (sources that \nshould be included). \nSpecificity was calculated \nGPT-\n4.abstract \nscreening=11\n47 \nNot Reported \nMethods \npaper \nPublic \nHigh \nPositive \n\"GPTscreenR demonstrates \nthe potential for LLMs to \nsupport scholarly work and \nprovides a user-friendly \nsoftware framework that can \nbe integrated into existing \nreview pipelines.\" \nClinicians\u2019 Requests for \nEvidence Summaries \nYan, 2023 \n[9] \nLeveraging Generative AI to \nPrioritize Drug Repurposing \nCandidates: Validating \nIdentified Candidates for \nAlzheimer\u2019s Disease in Real-\nWorld Clinical Datasets \nUSA/Tenn\nessee \nEvidence \nsynthesis/summar\nization \nMeta-analysis \nGPT / \nChatGPT \nNot mentioned / Qualitative \nYes \nCalculated using Cox \nproportional hazards \nregression models \ncomparing the risk of \nAlzheimers disease in \nindividuals exposed to a \ndrug repurposing \ncandidate and propensity \nscore-matched individuals \nnever exposed to the drug \nGPT-\n4.synthesis of \nknowledge=2\n0 \nNot Repo\nLi, 2024 \n[10] \nEvaluating the Effectiveness of \nLarge Language Models in \nAbstract Screening: A \nComparative Analysis \nUSA/Nort\nh Carolina \nTitle and abstract \nscreening \nMeta-analysis, \nSystematic review \nGPT / \nChatGPT, \nGoogle PaLM, \nLlama or \nAlpaca, Hybrid \nChatGPT4.Title and \nabstract \nscreening.Accuracy=90.2; \nChatGPT4.Title and \nabstract \nscreening.Recall=89.1; \nChatGPT4.Title and \nabstract \nscreening.Specificity=90.7; \nChatGPT35.Title and \nabstract \nscreening.Accuracy=73.6; \nChatGPT35.Title and \nabstract \nscreening.Recall=74.1; \nChatGPT4.Title and \nabstract \nscreening.Specificity=78.0; \nGoogle PaLM.Title and \nabstract \nscreening.Accuracy=78.6; \nGoogle PaLM.Title and \nabstract \nscreening.Recall=49.9; \nGoogle PaLM.Title and \nabstract \nscreening.Specificity=96.8; \nMeta Llama 2.Title and \nabstract \nscreening.Accuracy=74.8; \nMeta Llama 2.Title and \nabstract \nscreening.Recall=91.9; \nMeta Llama 2.Title and \nabstract \nscreening.Specificity=65.7; \nHybrid.Title and abstract \nscreening.Accuracy=95.5; \nHybrid.Title and abstract \nscreening.Recall=53.9; \nHybrid.Title and abstract \nscreening.Specificity=98.4 \nNo \nSensitivity is defined as \nthe number of true \npositives divided by the \nsum of true positives and \nfalse negatives, specificity \nas the number of true \nnegatives divided by the \nsum of true negatives and \nfalse positives, and \naccuracy as sum of true \npositives and true \nnegatives divided by the \ntotal number of abstracts. \n200 \nProcessing\nAbstracts \nEach Llm \nApproximat\n20 Minutes \nSingle Th\nWilkins, \n2023 [11] \nAutomated title and abstract \nscreening for scoping reviews \nusing the GPT-4 Large \nLanguage Model \nAustralia \nTitle and abstract \nscreening \nScoping review \nGPT / \nChatGPT \nGPT-4.Title and abstract \nscreening.Accuracy=84.0; \nGPT-4.Title and abstract \nscreening.Recall=71.0; \nGPT-4.Title and abstract \nscreening.Specificity=89.0 \nYes \nAccuracy was calculated \nas the proportion of correct \ndecisions (both inclusions \nand exclusions) made by \nGPT-4 compared to the \nconsensus human reviewer \ndecision. Sensitivity was \ncalculated as the \nproportion of true \npositives (correct \ninclusions) out of all actual \npositives (sources that \nshould be included). \nSpecificity was calculated \nGPT-\n4.abstract \nscreening=11\n47 \nNot Repo\nas the proportion of true \nnegatives (correct \nexclusions) out of all \nactual negatives (sources \nthat should be excluded). \nOami, \n2024 [12] \nAccuracy and reliability of data \nextraction for systematic \nreviews using large language \nmodels: A protocol for a \nprospective study \nJapan \nData extraction \nSystematic review \nGPT / \nChatGPT, \nClaude, Google \nBard / Gemini \nNot mentioned / Qualitative \nYes \nAccuracy, F1, Precision, \nand Recall were calculated \nby comparing LLM-\nextracted data to a \nreference standard created \nby human reviewers. \nNot \nextracted/Not \napplicable \nSubstantial \nReduction in Time \nCompared to \nConventional \nMethods Exact \nTime Savings not \nReported \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nMixed \n\"T\nand\nr\np\next\nac\nWoelfle, \n2024 [13] \nBenchmarking Human-AI \nCollaboration for Common \nEvidence Appraisal Tools \nSwitzerlan\nd, \nUSA/Calif\nornia \nQuality and bias \nassessment \nMeta-analysis, \nSystematic review \nClaude, GPT / \nChatGPT, \nMistral \nClaude-3-Opus.Quality and \nbias \nassessment.Accuracy=70.0; \nClaude-2.Quality and bias \nassessment.Accuracy=70.0; \nGPT-4.Quality and bias \nassessment.Accuracy=69.0; \nGPT-35.Quality and bias \nassessment.Accuracy=63.0; \nMixtral-8x22B.Quality and \nbias \nassessment.Accuracy=64.0; \nClaude-3-Opus.Quality and \nbias \nassessment.Accuracy=74.0; \nClaude-2.Quality and bias \nassessment.Accuracy=63.0; \nGPT-4.Quality and bias \nassessment.Accuracy=70.0; \nGPT-35.Quality and bias \nassessment.Accuracy=53.0; \nMixtral-8x22B.Quality and \nbias \nassessment.Accuracy=59.0; \nClaude-3-Opus.Quality and \nbias \nassessment.Accuracy=45.0; \nClaude-2.Quality and bias \nassessment.Accuracy=44.0; \nGPT-4.Quality and bias \nassessment.Accuracy=38.0; \nGPT-35.Quality and bias \nassessment.Accuracy=55.0; \nMixtral-8x22B.Quality and \nbias \nassessment.Accuracy=48.0 \nYes \nAgreement with human \nconsensus measured by \naccuracy (agreement \nfraction) and Cohens \nkappa. \nClaude-3-\nOpus.bias or \nquality \nassessment=5\n04, Claude-\n2.bias or \nquality \nassessment=5\n04, GPT-\n4.bias or \nquality \nassessment=5\n04, GPT-\n3.5.bias or \nquality \nassessment=5\n04, Mixtral-\n8x22B.bias \nor quality \nassessment=5\n04 \n \nClaude-3-\nOpus.bias or \nquality \nassessment=1\n12, Claude-\n2.bias or \nquality \nassessment=1\n12, GPT-\n4.bias or \nquality \nassessment=1\n12, GPT-\n3.5.bias or \nquality \nassessment=1\n12, Mixtral-\n8x22B.bias \nor quality \nassessment=1\n12 \n \nClaude-3-\nOpus.bias or \nquality \nassessment=5\n6, Claude-\n2.bias or \nquality \nassessment=5\n6, GPT-\n4.bias or \nquality \nassessment=5\n6, GPT-\n3.5.bias or \nquality \nNot Reported \nMethods \npaper \nPublic \nHigh \nMixed \napp\nc\nhum\nof\nas the proportion of true \nnegatives (correct \nexclusions) out of all \nactual negatives (sources \nthat should be excluded). \nAccuracy, F1, Precision, \nand Recall were calculated \nby comparing LLM-\nextracted data to a \nreference standard created \nby human reviewers. \nNot \nextracted/Not \napplicable \nSubstantial \nReduction in Time \nCompared to \nConventional \nMethods Exact \nTime Savings not \nReported \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nMixed \n\"This study aims to explore \nand evaluate the effectiveness \nof LLMs in systematic \nreviews, focusing on their \npotential to automate data \nextraction while ensuring high \naccuracy and minimal bias.\" \nAgreement with human \nconsensus measured by \naccuracy (agreement \nfraction) and Cohens \nkappa. \nClaude-3-\nOpus.bias or \nquality \nassessment=5\n04, Claude-\n2.bias or \nquality \nassessment=5\n04, GPT-\n4.bias or \nquality \nassessment=5\n04, GPT-\n3.5.bias or \nquality \nassessment=5\n04, Mixtral-\n8x22B.bias \nor quality \nassessment=5\n04 \n \nClaude-3-\nOpus.bias or \nquality \nassessment=1\n12, Claude-\n2.bias or \nquality \nassessment=1\n12, GPT-\n4.bias or \nquality \nassessment=1\n12, GPT-\n3.5.bias or \nquality \nassessment=1\n12, Mixtral-\n8x22B.bias \nor quality \nassessment=1\n12 \n \nClaude-3-\nOpus.bias or \nquality \nassessment=5\n6, Claude-\n2.bias or \nquality \nassessment=5\n6, GPT-\n4.bias or \nquality \nassessment=5\n6, GPT-\n3.5.bias or \nquality \nNot Reported \nMethods \npaper \nPublic \nHigh \nMixed \n\"Current LLMs alone \nappraised evidence worse than \nhumans. Human-AI \ncollaboration may reduce \nworkload for the second \nhuman rater for the assessment \nof reporting (PRISMA) and \nmethodological rigor \n(AMSTAR) but not for \ncomplex tasks such as \nPRECIS-2.\" \nassessment=5\n6, Mixtral-\n8x22B.bias \nor quality \nassessment=5\n6 \nSchmidt, \n2024 [14] \nExploring the use of a Large \nLanguage Model for data \nextraction in systematic \nreviews: a rapid feasibility \nstudy \nUnited \nKingdom \nData extraction \nSystematic review \nGPT / \nChatGPT \nGPT-4.Data \nextraction.Accuracy=80.0 \nNo \nEach of the models \nresponses was rated either \ncomplete, partial, or \nincorrect by two \nreviewers. If the models \nresponse contained all \nessential information or \ncorrectly did not provide a \nresponse when information \nwas absent, it was rated \ncomplete. If some relevant \ninformation was present \nbut missing other essential \ninformation, it was rated \npartial. Entirely incorrect \nor misleading responses \nwere rated incorrect. \n100 \nNot Reported \nMethods \npaper \nPublic \nMedium \nMixed \n\"Our results show that there \nmight be value in using LLMs, \nfor example as second or third \nreviewers. However, caution is \nadvised when integrating \nmodels such as GPT-4 into \ntools.\" \nYun, 2024 \n[15] \nAutomatically Extracting \nNumerical Results from \nRandomized Controlled Trials \nwith Large Language Models \nUSA/Mass\nachusetts \nData extraction \nMeta-analysis \nGPT / \nChatGPT, \nLlama or \nAlpaca, \nMistral, \nGemma, \nOLMo \nGPT-4.Data \nextraction.F1=73.5; \nGPT-35.Data \nextraction.F1=68.0; \nAlpaca.Data \nextraction.F1=0.0; \nMistral.Data \nextraction.F1=57.6; \nGemma.Data \nextraction.F1=59.0; \nOLMo.Data \nextraction.F1=42.4; \nLLaMA.Data \nextraction.F1=12.4; \nBioMistral.Data \nextraction.F1=27.5 \nYes \nAccuracy calculated as the \nproportion of exact \nmatches; F1 calculated for \nbinary and continuous \noutcomes; MSE calculated \nas the mean standardized \nerror of the log odds ratio. \n172 \nNot Reported \nMethods \npaper \nPublic \nMedium \nMixed \n\"The takeaway from this work \nis that modern LLMs offer a \npromising path toward fully \nautomatic meta-analysis, but \nfurther improvements are \nneeded before this will be \nreliable.\" \nTsai, 2024 \n[16] \nComparative Analysis of \nAutomatic Literature Review \nUsing Mistral Large Language \nModel and Human Reviewers \nTaiwan \nSearching for \npublications, \nTitle and abstract \nscreening, Full-\ntext screening, \nData extraction \nSystematic review \nMistral \nNot mentioned / Qualitative \nYes \nNot extracted/Not \napplicable \n50 \nTime Saving Was \nReported as \nMistral Llm \nCompleting the \nReview Process in \n17 Hours \nCompared to 100 \nHours by Human \nReviewers \nMethods \npaper \nPublic \nMedium \nMixed \n\"The findings indicate that \nwhile the Mistral LLM \nsignificantly surpasses human \nefforts in terms of efficiency \nand scalability, it occasionally \nlacks the analytical depth and \nattention to detail that \ncharacterize human reviews. \nDespite these limitations, the \nmodel demonstrates \nconsiderable potential in \nstandardizing preliminary \nliterature reviews.\" \nRobinson, \n2023 [17] \nBio-SIEVE: Exploring \nInstruction Tuning Large \nLanguage Models for \nSystematic Review Automation \nUnited \nKingdom \nTitle and abstract \nscreening \nSystematic review \nGPT / \nChatGPT, \nLlama or \nAlpaca, \nGuanaco \nChatGPT.Title and abstract \nscreening.Accuracy=60.0; \nChatGPT.Title and abstract \nscreening.Precision=59.0; \nChatGPT.Title and abstract \nscreening.Recall=96.0; \nLLaMA.Title and abstract \nscreening.Accuracy=74.0; \nLLaMA.Title and abstract \nscreening.Precision=82.5; \nLLaMA.Title and abstract \nscreening.Recall=71.5; \nGuanaco.Title and abstract \nscreening.Accuracy=67.2; \nGuanaco.Title and abstract \nscreening.Precision=72.5; \nGuanaco.Title and abstract \nscreening.Recall=84.0 \nNo \nAccuracy, Precision, and \nRecall were calculated \nbased on the comparison \nof model predictions to the \nannotated labels in the test \nset. \nChatGPT.abs\ntract \nscreening=10\n01, \nLLaMA.abstr\nact \nscreening=10\n01, \nGuanaco.abst\nract \nscreening=10\n01 \nNot Reported \nMethods \npaper \nPublic \nHigh \nPositive \n\"Bio-SIEVE lays the \nfoundation for LLMs \nspecialised for the SR process, \npaving the way for future \ndevelopments for generative \napproaches to SR automation.\" \nUittenhov\ne, 2024 \n[18] \nLarge Language Models in \nPsychology: Application in the \nContext of a Systematic \nLiterature Review. \nSwitzerlan\nd \nData extraction \nSystematic review \nGPT / \nChatGPT \nGPT-4 turbo.Data \nextraction.Accuracy=95.0; \nGPT-4 turbo.Data \nextraction.Recall=96.2; \nGPT-4 turbo.Data \nextraction.Specificity=94.0; \nGPT-4 turbo.Data \nextraction.Accuracy=92.5; \nGPT-4 turbo.Data \nextraction.Recall=96.3; \nGPT-4 turbo.Data \nextraction.Specificity=84.2 \nYes \nCohens Kappa was \ncalculated for inter-rater \nreliability. Sensitivity was \ncalculated as TP / (TP + \nFN). Specificity was \ncalculated as TN / (TN + \nFP). Accuracy was \ncalculated as (TP + TN) / \n(TP + TN + FP + FN). The \nArea Under the ROC \nCurve (AUC) was also \ncalculated. \nextraction of \ndata=39 \narticles \nThe Llm \nCompleted Our \nCoding Tasks \nSignificantly \nFaster than the \nHuman Coders \nTaking Only a few \nHours Compared \nto Several Days \nMethods \npaper \nPublic \nMedium \nPositive \n\"Our results suggest that \nresearchers and LLMs can \nwork synergistically, \nimproving efficiency, cost-\neffectiveness, and quality of \nthe systematic literature \nreview process.\" \nWang, \n2024 [19] \nMetaMate: Large Language \nModel to the Rescue of \nAutomated Data Extraction for \nEducational Systematic \nReviews and Meta-analyses \nUSA \nData extraction \nSystematic \nreview, Meta-\nanalysis \nGPT / \nChatGPT \nGPT-4 turbo.Data \nextraction.Precision=93.8; \nGPT-4 turbo.Data \nextraction.Recall=90.0; \nGPT-4 turbo.Data \nextraction.F1=91.8 \nNo \nPrecision, recall, and F1 \nscore were calculated \nbased on correctly \nextracted data (CED), \nmissing data (MD), and \nincorrectly extracted data \n(IED). Precision = CED / \n(CED + IED), Recall = \nCED / (CED + MD), F1 \nScore = 2 * (Precision * \nRecall) / (Precision + \nRecall) \nextraction of \ndata=32 \nNot Reported \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nMedium \nPositive \n\"These findings suggest that \nMetaMate could potentially \nreplace or assist human coders \nin data extraction tasks, while \nmaintaining or improving \nperformance.\" \nHuotala, \n2024 [20] \nThe Promise and Challenges of \nUsing LLMs to Accelerate the \nScreening Process of \nSystematic Reviews \nCanada, \nFinland \nTitle and abstract \nscreening \nSystematic review \nGPT / \nChatGPT \nGPT-35.Title and abstract \nscreening.Precision=65.0; \nGPT-4.Title and abstract \nscreening.Precision=50.0; \nGPT-35.Title and abstract \nscreening.Recall=17.6; \nGPT-4.Title and abstract \nscreening.Recall=41.7 \nYes \nF1 and accuracy were \ncalculated using standard \nformulas: F1 = 2 * \n(precision * recall) / \n(precision + recall), and \naccuracy = (true positives \n+ true negatives) / total \nsamples \nabstract \nscreening=20 \nNot Reported \nMethods \npaper \nPublic \nMedium \nMixed \n\"Citation: Using LLMs for text \nsimplification in the screening \nprocess does not significantly \nimprove human performance. \nUsing LLMs to automate title-\nabstract screening seems \npromising, but current LLMs \nare not significantly more \naccurate than human \nscreeners.\" \nYun, 2023 \n[21] \nAppraising the Potential Uses \nand Harms of LLMs for \nMedical Systematic Reviews \nAustralia, \nChina, \nGreece, \nUnited \nKingdom, \nUSA \nDrafting a \npublication \nSystematic review \nGalactica, \nBioMedLM, \nGPT / \nChatGPT \nNot mentioned / Qualitative \nNo \nQualitative analysis was \nconducted based on expert \ninterviews to evaluate the \noutputs generated by the \nLLMs. \nNA \nNa \nMethods \npaper \nPublic \nMedium \nMixed \n\"Participants noted that LLMs \nare inadequate for producing \nmedical systematic reviews \ndirectly given that they do not \nadhere to formal review \nmethods and guidelines.\" \nPrasad, \n2024 [22] \nTowards Development of \nAutomated Knowledge Maps \nand Databases for Materials \nEngineering using Large \nLanguage Models \nIndia \nData extraction \nSystematic review \nGPT / \nChatGPT, \nGoogle Bard / \nGemini \nChatGPT-35 turbo.Data \nextraction.F1=40.0; \nChatGPT-35 turbo.Data \nextraction.F1=47.9; \nGoogle Gemini Pro.Data \nextraction.F1=50.0; \nGoogle Gemini Pro.Data \nextraction.F1=63.0 \nYes \nF1 score was calculated \nusing ROUGE metrics \nwith the formula: 2 * \n(Precision * Recall) / \n(Precision + Recall). Exact \nMatch and Relaxed Match \nwere used to compute \nthese values. \nextraction of \ndata=7 \nNot Reported \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nMedium \nPositive \n\"Our method offers efficiency \nand comprehension, enabling \nresearchers to extract insights \nmore effectively.\" \nSerajeh, \n2024 [23] \nLLMs in HCI Data Work: \nBridging the Gap Between \nInformation Retrieval and \nResponsible Research Practices \nIran, Italy \nData extraction \nOther/Non-\nspecific \nGPT / \nChatGPT, \nLlama or \nAlpaca \nGPT35.Data \nextraction.Accuracy=58.0; \nLLama2.Data \nextraction.Accuracy=56.0; \nGPT35.Data \nextraction.meanabsoluteerro\nr=7.0; \nLlama2.Data \nextraction.meanabsoluteerro\nr=7.6 \nNo \nNot extracted/Not \napplicable \n300 \nNot extracted/Not \napplicable \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nPositive \n\"This strategy not only \nensured accuracy but also \nreduced surveillance risk.\" \nWang, \n2024 [24] \nZero-shot Generative Large \nLanguage Models for \nSystematic Review Screening \nAutomation \nAustralia, \nGermany \nTitle and abstract \nscreening \nSystematic review \nGPT / \nChatGPT, \nLlama or \nAlpaca \nChatGPT.Title and abstract \nscreening.Recall=87.0; \nChatGPT.Title and abstract \nscreening.Recall=93.0; \nLlama.Title and abstract \nscreening.Recall=89.0; \nLlama.Title and abstract \nscreening.Recall=97.0; \nAlpaca.Title and abstract \nscreening.Recall=91.0; \nAlpaca.Title and abstract \nscreening.Recall=99.0 \nYes \nVarious performance \nmetrics (B-AC, success \nrate, WSS) were computed \nacross different datasets by \ncomparing the predicted \ninclusion/exclusion against \nthe ground truth labels. \nabstract \nscreening=60\n0000 \nSignificant \nScreening Time \nSaved Compared \nto State of the Art \nApproaches \nSpecific Time \nSavings not \nQuantified \nMethods \npaper \nPublic \nHigh \nPositive \n\"Our comprehensive \nevaluation using five standard \ntest collections shows that \ninstruction fine-tuning plays \nan important role in screening, \nthat calibration renders LLMs \npractical for achieving a \ntargeted recall, and that \ncombining both with an \nensemble of zero-shot models \nsaves significant screening \ntime compared to state-of-the-\nart approaches.\" \nCai, 2023 \n[25] \nUtilizing ChatGPT to select \nliterature for meta-analysis \nshows workload reduction \nwhile maintaining a similar \nrecall level as manual curation \nThe \nNetherland\ns \nTitle and abstract \nscreening \nMeta-analysis \nGPT / \nChatGPT \nGPT35.screeningtitleandabs\ntract.Precision=91.0; \nGPT4.screeningtitleandabst\nract.Precision=94.0; \ngpt4.screeningtitleandabstra\nct.Recall=98.0; \ngpt35.screeningtitleandabstr\nact.Recall=96.0; \ngpt35.screeningtitleandabstr\nact.F1=94.0; \ngpt4.screeningtitleandabstra\nct.F1=96.0 \nNo \nNot extracted/Not \napplicable \n1000+ \nNot extracted/Not \napplicable \nMethods \npaper \nPublic \nHigh \nPositive \n\"We show here that its \npossible to have automatic \nselection of records for meta-\nanalysis with ChatGPT by \ndeveloping a pipeline named \nLARS\" \nTao, 2024 \n[26] \nGPT-4 Performance on \nQuerying Scientific \nPublications: Reproducibility, \nAccuracy, and Impact of an \nInstruction Sheet \nUSA/Calif\nornia \nData extraction \nSystematic review \nGPT / \nChatGPT \nGPT-4.Data \nextraction.Accuracy=87.0; \nGPT-4.Data \nextraction.Recall=72.0; \nGPT-4.Data \nextraction.Precision=87.0 \nNo \nAccuracy was defined as \nconcordance between the \ncorrect answer and the \nGPT-4 response for \nBoolean and numerical \nquestions. Recall was \ncalculated as the \nproportion of true \npositives out of the sum of \ntrue positives and false \nnegatives. Precision was \ncalculated as the \nproportion of true \npositives out of the sum of \ntrue positives and false \npositives. F1 score was the \nharmonic mean of \nprecision and recall: 2 x \n(recall * precision) / (recall \n+ precision). \n3600 \nThe Overall Cost \nof using the Gpt 4 \nApi Was \nSignificantly \nReduced to \nApproximately \nFive Fold with the \nRelease of Gpt 4 \nTurbo which is \nmore Cost \nEffective but \nExact Time \nSavings Were not \nReported \nMethods \npaper \nPublic \nHigh \nPositive \n\"GPT-4 possesses extensive \nknowledge about HIV drug \nresistance and it reproducibly \nanswers Boolean, numerical, \nand list questions about HIV \ndrug resistance papers. Its \naccuracy, recall, and precision \nof approximately 87%, 73%, \nand 87% without human \nfeedback demonstrate its \npotential at performing this \ntask.\" \nTovar, \n2023 [27] \nAI Literature Review Suite \nUSA/Tenn\nessee \nSearching for \npublications, \nData extraction \nLiterature/Narrati\nve review \nGPT / \nChatGPT, \nLlama or \nAlpaca \nNot mentioned / Qualitative \nNo \nNot extracted/Not \napplicable \nNot \nextracted/Not \napplicable \nNot extracted/Not \napplicable \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nLow \nPositive \n\"AI Literature Review Suite \nstands as a potent ally for \nresearchers, enhancing \nefficiency and quality of \nscholarly endeavors while \npromoting accelerated \ninnovation and progress\" \nTang, \n2024 [28] \nLarge Language Model in \nMedical Information Extraction \nfrom Titles and Abstracts with \nPrompt Engineering Strategies: \nA Comparative Study of GPT-\n3.5 and GPT-4 \nChina, \nHong \nKong SAR \nData extraction \nSystematic review \nGPT / \nChatGPT \nGPT-4.Data \nextraction.Accuracy=68.8; \nGPT-4.Data \nextraction.Accuracy=96.4; \nGPT-35.Data \nextraction.Accuracy=56.8; \nGPT-35.Data \nextraction.Accuracy=99.2 \nNo \nComparison of model \noutputs with ground truth \nusing BERTScore, \nROUGE-1, and a self-\ndeveloped GPT-4 \nevaluator \n100 \n8 to 10 Hours of \nHuman Labor \nReduced to under \n5 Minutes Gpt 3 5 \nor 40 Minutes Gpt \n4 \nMethods \npaper \nUnknow\nn/unrep\norted \nsources \nHigh \nPositive \n\"Our result confirms the \neffectiveness of LLMs in \nextracting medical \ninformation, suggesting their \npotential as efficient tools for \nliterature review",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This study aims to summarize the usage of Large Language Models (LLMs) in the process of creating a scientific review. It seeks to evaluate how LLMs can automate various stages of literature reviews and to assess the current state-of-the-art research projects in this field.",
            "scope": "The survey focuses on the automation of systematic reviews and other types of literature reviews using LLMs. It includes studies that utilize LLMs for stages like searching, data extraction, and drafting, while excluding those that do not involve LLMs or do not describe the automation of any review process."
        },
        "problem": {
            "definition": "The core issue being explored is the efficiency and effectiveness of using LLMs to automate stages of the systematic review process, which traditionally requires significant manual effort.",
            "key obstacle": "The primary challenges include the limited number of studies that have fully automated the review cycle and the potential inaccuracies in LLM outputs, particularly in performance metrics."
        },
        "architecture": {
            "perspective": "The survey introduces a framework that categorizes existing research based on the stages of the review process that can be automated using LLMs, highlighting the effectiveness of different LLM architectures.",
            "fields/stages": "The survey organizes current research into fields such as Searching for publications, Data extraction, Evidence synthesis, and Drafting publications. It assesses the performance of LLMs across these stages."
        },
        "conclusion": {
            "comparisions": "The survey compares the performance of GPT-based models against BERT-based models, finding GPT models to be superior in data extraction while slightly less accurate in title and abstract screening.",
            "results": "The overarching conclusion is that LLMs have the potential to significantly reduce the time and effort required for systematic reviews, although challenges remain in fully automating the process."
        },
        "discussion": {
            "advantage": "Existing research has achieved notable advancements in automating specific stages of reviews, demonstrating the potential of LLMs to enhance efficiency and accuracy in literature synthesis.",
            "limitation": "Current studies often focus on isolated stages of the review process rather than full automation, and there are concerns regarding the reliability of LLM-generated outputs.",
            "gaps": "There remains a gap in comprehensive studies that integrate LLMs across all stages of the systematic review process, as well as in understanding the long-term impacts of LLM usage on review quality.",
            "future work": "Future research should explore multi-stage automation using LLMs, improve performance metrics, and address ethical considerations related to AI in systematic reviews."
        },
        "other info": {
            "Funding": "Supported by the National Center for Advancing Translational Sciences of the National Institutes of Health under Grant Number UL1 TR001450.",
            "Conflicts of interest": "The authors have no competing interests to declare."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The study evaluates how LLMs can automate various stages of literature reviews, highlighting their significance in enhancing the efficiency and effectiveness of the review process."
        },
        {
            "section number": "2.1",
            "key information": "The survey focuses on the automation of systematic reviews and other types of literature reviews using LLMs, defining key concepts such as searching, data extraction, and drafting."
        },
        {
            "section number": "4.1",
            "key information": "The survey introduces a framework categorizing existing research based on the stages of the review process that can be automated using LLMs, assessing the performance of different LLM architectures."
        },
        {
            "section number": "10.1",
            "key information": "Current studies often focus on isolated stages of the review process rather than full automation, highlighting existing challenges in the field of LLM integration."
        },
        {
            "section number": "10.2",
            "key information": "Future research should explore multi-stage automation using LLMs, improve performance metrics, and address ethical considerations related to AI in systematic reviews."
        }
    ],
    "similarity_score": 0.7567279205535932,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/The emergence of Large Language Models (LLM) as a tool in literature reviews_ an LLM automated systematic review.json"
}