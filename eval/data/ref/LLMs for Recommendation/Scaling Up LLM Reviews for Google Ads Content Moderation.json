{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2402.14590",
    "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
    "abstract": "Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-modal representations.",
    "bib_name": "qiao2024scalingllmreviewsgoogle",
    "md_text": "# Scaling Up LLM Reviews for Google Ads Content Moderation\nAriel Fuxman Google Research Fangzhou Wang Google Ads Safety Ranjay Krishna Univ. of Washington\nFangzhou Wang Google Ads Safety Ranjay Krishna Univ. of Washington Mehmet Tek Google Ads Safety\n# ABSTRACT\nLarge language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-modal representations.\nACM Reference Format: Wei Qiao, Tushar Dogra, Otilia Stretcu, Yu-Han Lyu, Tiantian Fang, Dongjin Kwon, Chun-Ta Lu, Enming Luo, Yuan Wang, Chih-Chun Chia, Ariel Fuxman, Fangzhou Wang, Ranjay Krishna, and Mehmet Tek. 2024. Scaling Up LLM Reviews for Google Ads Content Moderation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (WSDM \u201924), March 4\u20138, 2024, Merida, Mexico. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3616855.3635736\n# 1 OUTLINE\nWe introduce an end-to-end solution for scaling up and leveraging LLMs [1, 3] for Google ads content moderation. We will first provide the context for the LLM review problem and explain the scale of reviews in the context of compute resources. Then, we will introduce our approach to solving this problem. We will conclude with the results of our deployment in Google Ads policy enforcement platform. Finally, we discuss future improvements and extensions.\n# 2 PROBLEM AND MOTIVATION\nOur goal is to accurately detect Google Ads policy violations in all the ads traffic before any ads are eligible to enter the auction for serving. We evaluated and used this technique on image ads only,\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). WSDM \u201924, March 4\u20138, 2024, Merida, Mexico \u00a9 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0371-3/24/03. https://doi.org/10.1145/3616855.3635736\nTiantian Fang Google Ads Safety\nChih-Chun Chia Google Ads Safety\nRanjay Krishna Univ. of Washington Mehmet Tek Google Ads Safety\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41ff/41ffa29a-4eae-4fe9-84e7-af4ad8ecaa0a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: A diagram of our end-to-end solution for scaling up LLMs for content moderation.</div>\nbut the approach is generic and can be extended to any modality and ad format. In this paper, we use the term \u201cLLMs\u201d to include both large language models and large visual-language models. Using LLMs to moderate all image ads traffic requires significant compute resources, making it impractical. Collecting human annotated data for fine-tuning or training a small model is also expensive because of limited human review bandwidth. Therefore, we took Google\u2019s existing LLMs, and used prompt engineering / tuning to achieve a high-quality LLM for Ads content moderation, and then scaled this model to achieve maximum recall with minimal compute resources. We evaluated our approach on the \"Non-Family Safe\" ad content policy, which restricts any Sexually Suggestive, Sexual Merchandise, Nudity and so on, since this is one of the important policies to protect users, advertisers and publishers.\n# 3 METHOD\nAt a high level, our approach combines funneling, LLM labeling, label propagation, and a feedback loop. Funneling, or the review candidate selection, reduces the volume of content that needs to be processed by the LLM by using heuristic (content similarity, actor similarity, non-LLM model scores) based selection, hash based deduping, activity based filtering, and cluster-based sampling. Next, we run inference using a prompt-engineered and tuned LLM. Then, the label propagation uses a content similarity-based technique to boost the impact. Finally, a feedback loop from the final labeled images (by the LLM directly and through propagation) to the initial funneling step helps to select similar candidate images to the already labeled images in the subsequent rounds of funneling, expanding the LLM coverage across the entire image ads traffic.\n# 3.1 Review Candidate Selection Funneling\nWe use various heuristics and signals to select potential policyviolating candidates, and then do filtering and diversified sampling to reduce the volume that needs to be processed by the LLMs.\n3.1.1 Selecting Possible Policy Violating Candidates. We use content and actor similarity to select an initial, larger pool of candidates. For content similarity, we leverage a graph-based label propagation technique to propagate labels from known policyviolating images as the source images (from past human or model labeled images) to similar images based on pre-trained embeddings. Two images whose distance in the embedding space is less than a threshold are considered similar. We build a similarity graph to collect the neighbors of known policy-violating content. For actor similarity, we collect candidate ad images from the accounts with policy-violating activities. To select candidate images with scores larger than the given thresholds, we use pre-trained non-LLM models in some cases. Using pre-trained models for candidate selection has lower precision requirements than using them for labeling.\n# 3.1.2 Reducing the Pool by Deduping, Filtering, Sampling. Google ads contains a lot of duplicate or near-duplicate content,\nGoogle ads contains a lot of duplicate or near-duplicate content, which wastes machine resources on processing similar content. To avoid this, we first run cross-day deduping to remove images already reviewed by LLMs in the past. Then we run intra-batch deduping to only send unique images to LLMs. We also filter out inactive images and those already labeled. To perform diversified sampling, we use graph based maximal coverage sampling to sample images with diversity.\n# 3.2 Large Language Model Tuning and Labeling\nTo adapt an LLM to a given task, one can use different strategies, such as prompt engineering [6] and parameter efficient tuning [4, 5]. Prompt engineering involves carefully designing the questions that are asked of the LLM, while parameter efficient tuning involves fine-tuning an LLM with fewer parameters on a labeled dataset to adjust its parameters to the task at hand. In our work, we took advantage of the ability of LLMs to do in-context learning [2], and used a combination of prompt engineering and parameter efficient tuning to prepare an LLM that performs well on our policy. To validate the model\u2019s performance on manually curated prompts policy experts first performed prompt engineering. For example, for a Non-Family Safe policy, we might prompt the LLM with a question such as \"Does the image contain sexually suggestive content?\". The LLM\u2019s predictions are then parsed into a binary yes/no policy label. Because the LLM\u2019s accuracy varies depending on the prompt, our policy experts crafted and evaluated various prompts on a small labeled dataset in order to select the best-performing prompt for our task, which was then used in combination with soft-prompt tuning [5] to create the final prompt used by our production system. During soft-prompt tuning, a small uninterpretable prompt is trained to nudge the LLM towards the correct answers on a labeled training set. This has been shown in the literature to significantly improve LLM performance [5], and we observed the same in our experiments. Note that prompt engineering and tuning are one-time costs, performed only once per policy. Once the prompt is constructed, it can be used for all inference runs of our system. For each candidate we want to classify with an LLM, we concatenate the prompt and the image and pass them to the LLM for labeling.\n# 3.3 Label Propagation and Feedback Loop\nFrom LLM labeled candidates of the previous stage, we propagate the label of each image to the similar images from stored images we\u2019ve seen in the past traffic. We store selected LLM labeled images as known images and label incoming images if they are similar enough to be considered as near duplicates. All labeled images, whether directly by LLMs or indirectly labeled through label propagation, are then read in the review candidate selection stage, and used as input in the initial known images for content similarity based expansion, to identify similar images as potential candidates for the next round of LLM review.\n# 4 RESULTS AND DISCUSSIONS\nWe ran our pipeline over 400 million ad images collected over the last 30 days. Through funneling, we reduced the volume to less than 0.1%, or 400k images, which are reviewed by an LLM. After label propagation, the number of ads with positive labels doubled. This pipeline labeled roughly twice as many images as a multi-modal non-LLM model, while also surpassing its precision on the \u201cNonFamily Safe\u201d ad policy. Overall, this pipeline helped remove more than 15% of the policy-violating impressions among image ads for this policy. We are expanding this technique to more ad policies and modalities, such as videos, text, and landing pages. We are also improving the quality of all pipeline stages, including funneling by exploring better heuristics, tuning better LLM prompts, and propagating similarity through higher-quality embeddings.\n# 5 COMPANY PORTRAIT\nGoogle LLC is an AI-first multinational company focused on organizing the world\u2019s information and making it universally accessible and useful. Google operates businesses in online advertising, search engine technology, cloud computing, and consumer electronics.\n# 6 PRESENTER BIOGRAPHY\nWei Qiao: Wei is a technical lead in Google Ads Content and Targeting Safety team. He is leading efforts to build the systems and workflows for efficient ads content moderation. Contact email: weiqiao@google.com.\n# REFERENCES\n[1] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023). [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [3] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. 2023. PaLI-X: On Scaling up a Multilingual Vision and Language Model. arXiv preprint arXiv:2305.18565 (2023). [4] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [5] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In Conference on Empirical Methods in Natural Language Processing. https://api.semanticscholar.org/CorpusID:233296808 [6] Laria Reynolds and Kyle McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems. 1\u20137.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenge of using large language models (LLMs) for content moderation in Google Ads, highlighting the computational costs and latency issues associated with processing large datasets. Previous methods struggled to efficiently moderate ad content due to resource constraints, necessitating a new approach to improve scalability and accuracy.",
        "problem": {
            "definition": "The problem is to accurately detect policy violations in Google Ads before they enter the auction for serving, specifically focusing on image ads while noting that the method can be generalized to other formats.",
            "key obstacle": "The primary challenge is the significant computational resources required to review all image ads traffic with LLMs, compounded by the high cost of collecting human-annotated data for model training."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to efficiently manage and moderate a vast amount of ad content using existing LLMs while minimizing resource usage.",
            "opinion": "The proposed method involves filtering and clustering ads to reduce the number of reviews needed, allowing LLMs to focus only on representative ads and propagating decisions back to similar ads.",
            "innovation": "This method significantly differs from existing approaches by employing a multi-step process that includes heuristics for candidate selection, label propagation, and a feedback loop, achieving a drastic reduction in the volume of content needing review while maintaining high recall."
        },
        "method": {
            "method name": "LLM Review Scaling Method",
            "method abbreviation": "LLM-RSM",
            "method definition": "A systematic approach that uses heuristics for candidate selection, applies LLMs for labeling, propagates labels to similar content, and incorporates a feedback mechanism to improve future candidate selection.",
            "method description": "The method reduces the volume of content processed by LLMs through a structured funneling process, enabling efficient and effective content moderation.",
            "method steps": [
                "Candidate selection using heuristics based on content and actor similarity.",
                "Labeling selected candidates with a prompt-engineered LLM.",
                "Label propagation to similar images based on content similarity.",
                "Implementing a feedback loop to refine candidate selection for subsequent rounds."
            ],
            "principle": "The method is effective due to its ability to leverage content similarities to reduce the review workload while ensuring that the most relevant ads are evaluated by LLMs, thus maximizing recall with minimal resources."
        },
        "experiments": {
            "evaluation setting": "The pipeline was tested on 400 million ad images collected over 30 days, focusing on the 'Non-Family Safe' ad content policy.",
            "evaluation method": "Performance was assessed by comparing the number of ads labeled by the proposed method against a baseline non-LLM model, measuring both recall and precision."
        },
        "conclusion": "The results demonstrated that the proposed method labeled roughly twice as many images as a multi-modal non-LLM model while surpassing its precision on the 'Non-Family Safe' policy, effectively removing over 15% of policy-violating impressions.",
        "discussion": {
            "advantage": "Key advantages include a drastic reduction in the number of reviews needed, improved recall, and the ability to scale the method across different ad formats and policies.",
            "limitation": "One limitation is the reliance on the quality of heuristics and the effectiveness of the LLM prompts, which may vary across different content types.",
            "future work": "Future research could focus on enhancing the quality of heuristics, exploring better prompt engineering techniques, and extending the method to additional ad types such as videos and text."
        },
        "other info": {
            "company": "Google LLC",
            "company focus": "AI-first multinational company specializing in information organization, online advertising, search technology, and cloud computing.",
            "presenter biography": {
                "name": "Wei Qiao",
                "role": "Technical lead in Google Ads Content and Targeting Safety team",
                "contact": "weiqiao@google.com"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The proposed method, LLM Review Scaling Method (LLM-RSM), utilizes heuristics for candidate selection and applies LLMs for labeling ad content, showcasing the capabilities of LLMs in processing and understanding large datasets."
        },
        {
            "section number": "4.2",
            "key information": "The integration of LLMs in the proposed method allows for efficient moderation of ad content, reducing the review workload while maintaining high recall, thereby enhancing personalization and user interaction in recommendation systems."
        },
        {
            "section number": "10.1",
            "key information": "The paper identifies challenges in the computational resources required for reviewing ad content using LLMs, emphasizing the need for efficient methods to manage large datasets in recommendation systems."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions proposed include enhancing heuristics and exploring better prompt engineering techniques for LLMs, which could lead to advancements in recommendation system methodologies."
        },
        {
            "section number": "1.2",
            "key information": "The paper highlights the significant role of LLMs in addressing the challenge of content moderation in Google Ads, emphasizing their importance in modern technology applications."
        }
    ],
    "similarity_score": 0.7384534535119519,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Scaling Up LLM Reviews for Google Ads Content Moderation.json"
}