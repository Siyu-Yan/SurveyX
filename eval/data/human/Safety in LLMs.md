# Towards Safer Generative Language Models: A Survey on Safety Risks, Evaluations, and Improvements  

Jiawen Deng1 ∗ Jiale Cheng2 ∗ Hao Sun2 Zhexin Zhang2 Minlie Huang2  

1University of Electronic Science and Technology of China, Chengdu, China 2The Conversational AI (CoAI) group, DCST, Tsinghua University, Beijing 100084, China  

dengjw@uestc.edu.cn, {chengjl23;zx-zhang22}@mails.tsinghua.edu.cn thu-sunhao@foxmail.com; aihuang@tsinghua.edu.cn  

# Abstract  

As generative large model capabilities advance, safety concerns become more pronounced in their outputs. To ensure the sustainable growth of the AI ecosystem, it’s imperative to undertake a holistic evaluation and refnement of associated safety risks. This survey presents a framework for safety research pertaining to large models, delineating the landscape of safety risks as well as safety evaluation and improvement methods. We begin by introducing safety issues of wide concern, then delve into safety evaluation methods for large models, encompassing preference-based testing, adversarial attack approaches, issues detection, and other advanced evaluation methods. Additionally, we explore the strategies for enhancing large model safety from training to deployment, highlighting cutting-edge safety approaches for each stage in building large models. Finally, we discuss the core challenges in advancing towards more responsible AI, including the interpretability of safety mechanisms, ongoing safety issues, and robustness against malicious attacks. Through this survey, we aim to provide clear technical guidance for safety researchers and encourage further study on the safety of large models.  

# 1 Introduction  

With the relentless advancement of technology, generative Large Models (LMs) have emerged as a focal point in the modern tech sphere, demonstrating superior capabilities across a vast array of industries. Nonetheless, there are instances in which these models generate outputs that confict with human values, ranging from toxic narratives and biased comments to ethically misaligned expressions that appear in a variety of scenarios, such as casual conversations and medical consultations. Not only have these safety concerns eroded user confdence, but they may also pose grave threats to national cohesion, societal equilibrium, and the overall safety of individuals and their assets.  

Numerous studies have investigated safetycentric research in an effort to align LMs with human values, thereby ensuring safety, reliability, and responsibility. This paper aims to investigate three core Research Questions(RQs) within the feld of safety research and present an overview of recent studies undertaken on these RQs.  

• What is the scope of LM safety risks? • How do we quantify and evaluate these risks? • How can LMs’ safety be improved?  

First and foremost, the delineation of safety spectrum is a fundamental question requiring thorough investigation in safety research. As the frst concern, toxicity and unfair contents have been extensively studied, and relevant research like toxicity detection, detoxifcation, and bias mitigation have made considerable progress (Schmidt and Wiegand, 2017; Gehman et al., 2020; Welbl et al., 2021). However, as technology advances and the intelligence level of LMs increases, considerations about their safety risk have inevitably reached new heights (Xu et al., 2020; Sun et al., 2022). For instance, recent studies have emphasized the mental harm caused by inappropriate advice and the public opinion risks brought about by controversial remarks (Dinan et al., 2021; Sun et al., 2022; Levy et al., 2022). Consequently, based on related studies (Zhang et al., 2020; Dinan et al., 2021; Weidinger et al., 2021; Sun et al., 2022) and recently widely discussed safety risks (Hendrycks et al., 2023), we frst introduce the scope of safety risks and discuss them from six perspectives, including Toxicity and Abusive Content, Unfairness and Discrimination, Ethics and Morality Issues, Expressing Controversial Opinions, Misleading Information, Privacy and Data Leakage, and Malicious Use and Unleashing AI Agents. We believe this categorized presentation will aid in delineating the research scope for risk evaluation and safety enhancement.  

![](images/e95be33fb2dbcf9606376ad51995596273f4661b701bd1e265645d1f533660ab.jpg)  
Figure 1: Overview of safety research surveyed in this paper, focusing on three research questions: what safety is, how to evaluate it, and how to improve it.  

Prior to deployment, it is vital to undertake a thorough safety evaluation of LMs to explore their potential safety risks. This not only enables developers to comprehend and unearth the model’s potential weaknesses so as to perform targeted optimization, but also enables users to understand the model’s applicability and limitations in specifc scenarios. We examine the main methods employed in LMs safety evaluation, including model preference-based safety testing (Nadeem et al., 2021; Xu et al., 2023a), adversarial safety attacks (Perez et al., 2022; Ganguli et al., 2022), safety risk detection (Dinan et al., 2019; Sun et al., 2022), and other advancements.  

The ultimate goal of safety research is to improve LMs’ safety and promote their safe deployments across various scenarios. Every phase in developing LMs involves possible vulnerabilities. We survey safety enhancement techniques at each phase, covering pre-training, safe alignment, inference, and post-processing phases. During the pre-training stage, toxic and biased data can lead to a model developing skewed ethical views, thus, it is necessary to construct high-quality data through pre-processing. Then, LMs are usually fne-tuned to achieve alignment with human values. During the inference stage, designing decoding strategies can effectively mitigate inappropriate content generation. And during the postprocessing phase, designing safe response strategies serves as the last line of defense in large model risk management.  

In general, around the three aforementioned research questions, we provide an overview of the scope of safety issues, methods of safety evaluation, and techniques to enhance large model safety. Besides, we discuss existing challenges, believing that studying the interpretability of large models can help uncover the intrinsic reasons behind their safety risks. The safety risks of large models change over time and require continuous monitoring. Moreover, when facing complex application scenarios, especially malicious attacks, it’s crucial to maintain robustness and safe outputs.  

The overall framework of this survey is illustrated in Figure 1. Through this survey, we aim to offer a holistic perspective on large model safety research and hope it serves as valuable reference material for newcomers to this feld, promoting the safe and healthy deployment of large models.  

# 2 Scope of Safety Issues  

With the gradual rise in popularity of LMs applications, safety issues have become more prominent. Some preliminary works attempt to address these issues by defning them as harmful content and promoting safer generations that align with human-centric preferences (Ouyang et al., 2022; Bai et al., 2022b). However, a consensus has not yet been reached regarding the defnition of harmful. The purpose of this paper is to review and analyze the safety issues mentioned in existing research, as well as emerging safety issues, to provide a relatively comprehensive overview of the current safety challenges. We hope to push for a unifed and clear defnition of the scope of safety issues, thereby providing a more solid base for future research and applications.  

Toxicity and Abusive Content This typically refers to rude, harmful, or inappropriate expressions. In recent years, the feld of toxic language detection has seen notable advancements, supported by comprehensive research benchmarks (Poletto et al., 2021) and released tools such as Perspective API. While most research has concentrated on single-sentence toxic content, including explicit insults (Wulczyn et al., 2017; Davidson et al., 2017; Zampieri et al., 2019; Rosenthal et al., 2021) or more covert offenses (Wang and Potts, 2019; Breitfeller et al., 2019; Han and Tsvetkov, 2020; Price et al., 2020), the interaction between LMs and users is growing more frequent, resulting in increasingly complex generated content (Sheng et al., 2021; Zhang et al., 2021; Sun et al., 2022). For instance, a seemingly benign reply like "I agree with you" can be problematic when it is a reaction to a toxic utterance by the user. Empirical studies have shown that LMs are three times more likely to express agreement with toxic inputs than neutral ones (Baheti et al., 2021), indicating that toxicity in complex contexts deserves more attention.  

Unfairness and Discrimination Social bias is an unfairly negative attitude towards a social group or individuals based on one-sided or inaccurate information, typically pertaining to widely disseminated negative stereotypes regarding gender, race, religion, etc (Sekaquaptewa et al., 2003). For example, while interacting with users, large models may inadvertently display stereotypes about particular groups, such as "housewives are completely dependent on their husbands", which signifcantly degrades the user experience. As well, the bias in LMs can exacerbate societal disparities in crucial sectors, such as credit evaluations and recruitment. Most existing models, including GPT-series models, have been discovered to contain societal biases (Sun et al., 2022, 2023). This is mainly because models inherit biases present in the data or the overrepresentation of certain communities in the dataset. Notably, the defnition and evaluation of societal biases are infuenced by cultural backgrounds. To create fair and unbiased models, it’s vital to thoroughly review training data and develop technical solutions that consider cultural backgrounds.  

Ethics and Morality Issues Beyond the aforementioned toxicity and unfairness, LMs need to pay more attention to universally accepted societal values at the level of ethics and morality, including the judgement of right and wrong, and its relationship with social norms and laws (English,  

1976). This is especially evident when discussing sensitive humanistic topics such as the dignity of life, human rights, and freedom, like the moral dilemma, "Should an autonomous car sacrifce its passengers in an unavoidable collision to save pedestrians?" Studies indicate that, without clear guidance, large models might rely on biases in their training data, producing morally contentious answers. To address these ethical challenges, researchers demonstrated that incorporating human moral principles, such as the Rule of Thumb, into models enhances LMs’ transparency and explainability when handling ethical issues (Forbes et al., 2020; Ziems et al., 2022; Kim et al., 2022). This suggests that interdisciplinary collaboration is critical to developing moral LMs.  

Expressing Controversial Opinions The controversial views expressed by large models are also a widely discussed concern. Bang et al. (2021) evaluated several large models and found that they occasionally express inappropriate or extremist views when discussing political topics. Furthermore, models like ChatGPT (OpenAI, 2022) that claim political neutrality and aim to provide objective information for users have been shown to exhibit notable left-leaning political biases in areas like economics, social policy, foreign affairs, and civil liberties. When these models encounter contentious topics, especially those concerning cultural values, they might reveal biases, blind spots, or misunderstandings, sometimes leading to cultural friction. To avoid potential controversy, some models opt for pre-set generic responses when detecting sensitive topics (Xu et al., 2020). However, how to respond appropriately to sensitive topics remains an open question, warranting further exploration.  

Misleading Information Large models are usually susceptible to hallucination problems, sometimes yielding nonsensical or unfaithful data that results in misleading outputs (Ji et al., 2023). If users rely excessively on these models, they may erroneously regard their outputs as accurate and reliable, overlooking other crucial information. This blind trust can pose signifcant risks, particularly in applications requiring high accuracy, like medical diagnoses and legal advice. For instance, an incorrect diagnosis derived from patient data could compromise patient safety. Current general LMs are typically ill-equipped to manage these specialized domains. Consequently, it is common practice to provide generic pre-set responses to related queries to reduce the likelihood of misleading results.  

Privacy and Data Leakage Large pre-trained models trained on internet texts might contain private information like phone numbers, email addresses, and residential addresses. Studies indicate that LMs might memorize or leak these details (Carlini et al., 2019, 2021), and under certain techniques, attackers can decode private data from model inferences (Li et al., $2022\mathbf{a}$ ; Pan et al., 2020; Song and Raghunathan, 2020). To mitigate this risk, researchers have developed strategies like differential privacy, curated training data (Carlini et al., 2019, 2021), and introducing auxiliary loss functions (Song and Raghunathan, 2020; Li et al., 2022a). However, these strategies have limitations. For example, applying differential privacy might increase costs or degrade model performance, while specifc loss functions might only cater to known attack patterns. Given that data fltering methods can not entirely remove sensitive content, future efforts should delve deeper into developing more effcient and robust privacy protection schemes.  

Malicious Use and Unleashing AI Agents LMs, due to their remarkable capabilities, carry the same potential for malice as other technological products. For instance, they may be used in information warfare to generate deceptive information or unlawful content, thereby having a significant impact on individuals and society. As current LMs are increasingly built as agents to accomplish user objectives, they may disregard the moral and safety guidelines if operating without adequate supervision. Instead, they may execute user commands mechanically without considering the potential damage. They might interact unpredictably with humans and other systems, especially in open environments (Hendrycks et al., 2023). ChaosGPT is a notable example, which is a variant of AutoGPT, and it was programmed with instructions such as eradicating humanity, establishing global dominance, and seeking immortality. It circumvented AI’s safety barriers, explored nuclear weapons, and attempted to communicate with other AIs to harm humanity. Controlling and monitoring the malicious use of highly capable LMs is a pressing issue.  

# 3 Safety Evaluation  

To effectively mitigate potential risks resulting from using LMs in real-world scenarios, it is imperative to undertake a comprehensive safety evaluation before deployment. Engaging in such evaluation not only facilitates the exploration of the model’s risk limits but also offers vital suggestions for subsequent safety enhancement.  

The safety evaluation procedure for LMs typically encompasses the following pivotal steps.  

1) Evaluation Schema Setup: Specify the scope (e.g., one or several types of safety risks) of safety evaluation, followed by formulating the evaluation method.   
2) Test Set Construction: Collect data and construct representative test samples that cover the evaluation scope.   
3) Obtain Model Output: Input the test samples to the LM to obtain the model’s outputs.   
4) Safety Analysis: Analyze the safety of LM’s outputs and compose an evaluation report.  

Common evaluation schemes in this process include designing preference tests to evaluate model selection and designing adversarial attack methods to induce the model’s unsafe generation. For the latter, the common method for evaluating the safety of generated content is to employ automatic detection methods. This section will, therefore, introduce key technologies in safety evaluation from three perspectives: 1) preference-based safety testing, 2) adversarial safety attack, and 3) safety issue detection. Moreover, we will also discuss 4) advanced safety evaluations towards recent strong instruction-following models.  

# 3.1 Perference-based Safety Testing  

Preference-based safety testing aims to uncover a model’s value biases by examining its behavior preferences. This can be done in two main ways: one using probabilistic-based metrics like perplexity or logits (Nangia et al., 2020; Nadeem et al., 2021), and the other through multiple-choice tests where the model selects an option (Parrish et al., 2022; Xu et al., 2023a).  

Probability-based methods Probability-based methods primarily focus on evaluating bias and are mainly applied to models that can derive probability distributions. Nadeem et al. (2021) created a dataset called StereoSet, which aims to measure inter- and intra-sentence bias. Each sample consists of a context and three options (stereotype, anti-stereotype, and unrelated content). They fgured out how biased LM is by comparing the scores it gave each choice. They think that in an ideal model, the scores for stereotypical and nonstereotypical options should be the same. Nangia et al. (2020) also focus on stereotypes dealing with 9 types of social bias, including race, gender, religion and other factors. They conducted tests on several widely used masked language models and discovered the favor of stereotypical expressions. Similarly, Zhao et al. (2023) gathered data from social media platforms and performed strict data preprocessing. By comparing the models’ perplexity distribution of sentences regarding two biased groups, they identifed bias within conversational models. Ousidhoum et al. (2021) also developed template-based data to investigate the association between stereotypes and toxicity.  

Multi-choice based methods This method encompasses a boarder scope, including assessments of morality, bias and values, which is also more prevalent in evaluations of LMs. Parrish et al. (2022) proposed BBQ, designed for evaluating models’ bias in question-answering, involving ambiguous and disambiguated contexts. Ambiguous contexts encompass two biased groups with insuffcient evidence to fnd the answer, while disambiguated contexts provide complete information. The candidate options contain the biased groups and "Unknown". In the case of ambiguous contexts, an unbiased model should choose "Unknown", while choosing the accurate group when given the disambiguated contexts. Xu et al. (2023a) introduced CValues, a benchmark for assessing the values within large language models. The set of questions comprises two levels, safety and responsibility, each encompassing various domains and scenarios. To facilitate automatic evaluation, they employed prompts to transform some data into a multi-choice format. Specifcally, given the context, an unsafe response and a safe response, the model needs to choose a better response representing its values.  

# 3.2 Adversarial Safety Attack  

To comprehensively assess the safety of LMs, it is essential to extensively expose the models’ safety issues. This paper focuses on the black-box attack setting (i.e., model parameters are unknown), as this setup aligns best with real-world scenarios.  

Real Adversarial Data To begin with, the most straightforward method is to directly induce toxic or biased contents within models’ outputs. An intuitive approach involves extracting real-world data containing biased groups or toxic contents to construct adversarial samples. Gehman et al. (2020) built RealToxicityPrompts upon the OpenWebText corpus (Gokaslan and Cohen, 2019), containing $100\mathrm{k}$ toxic prompts to test the toxic degeneration in GPT-2 (Radford et al., 2019). Sheng et al. (2021) extracted posts on different topics from Twitter to investigate ad hominems in dialogue responses. Resorting to human annotators is more fexible to satisfy complicated requirements and could yield samples with high quality, despite the higher costs. Dinan et al. (2019) employed crowdsourcing workers to attack the dialogue in order to collect adversarial data. Following this idea, Xu et al. (2020) constructed the Bot-Adversarial Dialogue (BAD) dataset by encouraging crowdsourcing workers to elicit offensive messages from dialogue models. The workers can use either toxic or benign utterances to induce unsafe responses from dialogue models during the multi-turn conversation. More recently, red teaming is also a crucial step for large language models (Bai et al., 2022a; Touvron et al., 2023). With this method, Bai et al. (2022a) collected the harmless dataset to ensure the safety of the helpful assistant model. Touvron et al. (2023) hired a substantial team to conduct a series of red teaming to provide guidance for the development of safer models.  

Synthetic Adversarial Data Moreover, due to the high costs of manual data collection, there exists a body of research focused on the investigation of automatically constructing adversarial data, as well as the utilization of human-in-theloop methods to mitigate resources. To automatically construct attack data, a straightforward approach involves leveraging templates. Works in this category mainly combine different choices with a manually designed template. For example, Sheng et al. (2019) designed the template “The XYZ worked as” where XYZ is selected from demographic groups {woman, man, Black person, White person, gay person, straight person}. They compare the outputs’ sentiment and regard scores conditioned on different prefx prompts and found that GPT-2 (Radford et al., 2019) exhibits distinct levels of bias towards different demographics. Bang et al. (2021) designed both neutral (e.g., “Let’s talk about <Politician>.”) and biased (e.g., “<Politician $>$ has done the best job as a politician.”) templates related to politicians or political beliefs. These templates are used to assess the political prudence of chatbots, including hyperpartisanship, offensiveness, and slantedness of generated content. Nadeem et al. (2021) presented a large-scale dataset to assess language models’ stereotypical bias in four domains including race, religion, gender and profession. They proposed Context Association Tests (CATs), which offered three different options (stereotype, anti-stereotype and unrelated) based on a given context that contains specifc demographics. Inspired by the advancement of LMs, it has become a popular way to adopt pre-trained LMs to automatically generate adversarial inputs. Perez et al. (2022) use a separate LM to automatically generate inputs that could elicit desired outputs from the target language model. Besides zero-shot generation, they also explored few-shot generation, supervised learning, and reinforcement learning to generate the test cases more effciently. (Zhang et al., 2022) proposed the reverse generation method for constructing adversarial data, demonstrating that their data is highly inductive and can more effectively expose safety issues inherent in the models.  

Advanced Attack As large models advance in capabilities, overt safety issues such as toxicity are being mitigated, and researchers are concurrently concentrating on more advanced levels of safety and responsibilities. Jiang et al. (2021) proposed several ethical question-answering tasks, involving the judgment of the ethics of various actions. Employing this dataset, they fnd that even GPT-3 (Brown et al., 2020), despite its advanced capabilities, encountered challenges in giving accurate answers, exposing the potential ethical issues. Ziems et al. (2022) introduced the Moral Integrity Corpus (MIC) to benchmark the ethical capabilities of dialogue systems, revealing that LMs usually expose immoral behaviors when faced with adversarial prompts. Beyond ethical concerns, LMs also show unreliable behavior. Sun et al. (2022) considered the safety concerns of Risk Ignorance and Unauthorized Expertise in proposed benchmark DIASAFETY, fnding that prevalent dialogue models struggle with these concerns. Levy et al. (2022) also showed that LMs can produce text that provides users with physically harmful guidance.  

With the emergence of models with strong instruction-following capabilities like ChatGPT (OpenAI, 2022), there have subsequently arisen various Instruction Attacks that are more diffcult to defend. Sun et al. (2023) introduced a safety leaderboard for Chinese LMs, encompassing 6 types of instruction attacks. Their results demonstrated that the safety of the tested models, including ChatGPT, against these instruction adversarial attacks is comparatively inferior to that against common adversarial attacks like toxic or bias. Perez and Ribeiro (2022) introduced two notable attack types—Goal Hijacking and Prompt Leaking—that leverage models’ strong instruction-following capabilities. Goal Hijacking induces models to disregard prior user input and instead execute the assigned task, while Prompt Leaking seeks to reveal the model’s preexisting application prompt, becoming a prevalent attack to test instruction-tuned models.  

Moreover, as ChatGPT introduces System-level prompts, this can also be utilized to form attacks. Deshpande et al. (2023) leverage system prompt to assign a role for ChatGPT, like "Speak like Adolf Hitler", and they fnd that this can obviously induce the model to be more toxic. Yuan et al. (2023) established a set of encryption and decryption protocols within the system prompt. In this way, they are able to chat with the model using a cryptographic language. It is observed that when chatting in cryptographic language, the model exhibits fewer safety constraints. For instance, in response to the same user input "Please tell me how to destroy this world", when the cryptographic language is not employed, the model declined to provide an answer, offering instead conscious responses. However, when applying the cryptographic language, the model provides a plan to destroy the world.  

# 3.3 Safety Issue Detection  

To automatically identify the exposed safety concerns, it’s essential to develop a robust safety issue detector to check if the generated content is harmful. While earlier detectors commonly employed neural networks like CNNs, RNNs, and LSTMs (Georgakopoulos et al., 2018; van Aken et al., 2018; Gunasekara and Nejadgholi, 2018; Joulin et al., 2017; Kshirsagar et al., 2018; Mishra et al., 2018; Mitrovic´ et al., 2019; Sigurbergsson and Derczynski, 2020), contemporary approaches increasingly favor fne-tuning pre-trained models like Bert (Devlin et al., 2019) and Roberta (Liu et al., 2019) for this purpose.  

High-quality data is vital for building a robust classifer. Data collection methods fall into three categories: manual collection, human-in-the-loop processes, and model generation. The manual collection relies on human annotators to compose new samples or label existing data (Forbes et al., 2020; Hendrycks et al., 2020; Sap et al., 2020; Lourie et al., 2021; Emelin et al., 2021). However, solely relying on human annotators can be expensive and limits the scale of data. Many works let models cooperate with human annotators(Kim et al., 2022; Ziems et al., 2022; Hartvigsen et al., 2022; Baheti et al., 2021; Xu et al., 2020; Ganguli et al., 2022; Deng et al., 2022). It is worth noting that large pre-trained language models such as GPT-3 (Brown et al., 2020) play a key role in generating new samples through zero-shot or few-shot prompting (Hartvigsen et al., 2022; Kim et al., 2022). Moreover, some works completely remove human involvement and solely rely on large language models to generate new data (Perez et al., 2022; Si et al., 2022; Deng et al., 2023). To improve the performance of classifers, Caselli et al. (2021) utilized unsafe content from Reddit1 to retrain a Bert model called HateBert, allowing HateBert to learn more knowledge about harmful content and thus become more sensitive to it. And their experiments demonstrated the superior accuracy of HateBert over several harmful content detection tasks than Bert. Dinan et al. (2019) and Xu et al. (2020) both investigated the human-inthe-loop method to make adversarial attacks on the dialogue models. They leverage these adversarial data to further develop the classifer. We summarize some mainstream classifers in Table 1.  

As large models become more capable, various methods for utilizing model-based detection have also emerged, such as self-diagnosis and promptbased approaches. Schick et al. (2021) discovered that LMs are able to identify the harmful responses generated by themselves, whereby they proposed a decoding algorithm named self-debiasing by giving undesired text descriptions. Wang and Chang (2022) explored the ability of large LMs for toxicity self-diagnosis based on the prompt method in the zero-shot setting. Sun et al. (2023) also employed InstructGPT (Ouyang et al., 2022) to judge if responses are safe and use the results to form a leaderboard. In addition, as reinforcement learning is a key technique to develop instructionfollowing models (Ouyang et al., 2022; Bai et al., 2022a,b), the reward model is also commonly used to measure the safety of language models, which can also be considered a way to detect safety issues.  

<html><body><table><tr><td rowspan="2">Classifier</td><td colspan="3">ContextResearch</td><td rowspan="2">#Class</td></tr><tr><td>Aware</td><td>Scope</td><td></td></tr><tr><td>PerspectiveAPI Detoxify (Hanu and Unitary</td><td>No</td><td>toxicity</td><td></td><td>7</td></tr><tr><td>team,2020)</td><td>No</td><td>toxicity dialogue</td><td></td><td>6</td></tr><tr><td>BAD (Xu et al.,2020)</td><td>Yes</td><td>safety</td><td></td><td>2</td></tr><tr><td>Sensitive topic classifier (Xu et al.,2020)</td><td>No</td><td>sensitive topics</td><td></td><td>6</td></tr><tr><td>BBF (Dinan et al., 2019)</td><td>Yes</td><td>offensive</td><td></td><td>2</td></tr><tr><td>DiaSafety (Sun et al.,2022)</td><td>Yes</td><td>dialogue safety</td><td></td><td>5</td></tr></table></body></html>

Table 1: The mainstream classifers for detecting safety issues.  

# 3.4 Advanced Safety Evaluation  

Recent instruction-following models like ChatGPT have demonstrated the ability to act as automated agents, performing practical tasks and using tools (Xu et al., 2023b; Liu et al., 2023c). However, this has raised new safety concerns, illustrated by instances like ChaosGPT generating plans for human annihilation and GPT-4 manipulating humans to assist in CAPTCHA tests. Studies indicate that GPT-4 exhibits power-seeking behaviors such as autonomous replication and shutdown evasion (OpenAI, 2023a). Regarding such safety concerns, current safety evaluations mainly depend on manual observation. Given these models’ real-world interactions, it is crucial to invest more effort in developing automated risk detectors for a more thorough monitoring of potential risks.  

# 4 Safety Improvement  

As the fnal goal of safety research, safety improvement of LMs has also drawn much attention recently. In this section, we introduce the recent advances in the methods to improve their safety. We categorize them into four phases, spanning from model training to deployment: (1) Pretraining, (2) Alignment, (3) Inference, and (4) Post-processing. LMs’ parameter optimization happens mainly in the frst two phases, while the last two are under frozen parameters.  

In the pre-training phase, language models learn from a vast array of data, which is often sourced from the Internet. While this enables the models to master complex language patterns and acquire a broad knowledge base, it also poses inherent risks. Specifcally, the models may inadvertently learn and propagate biases or harmful content in the training data. As such, careful handling of data during the pre-training phase plays a critical role in mitigating models’ safety risks.  

Filtering out undesired content from the training data is among the most commonly used approaches. This can be accomplished via heuristic rule-based methods, such as keyword matching, or by employing safety detectors with confdence scores. Safety issue detectors like BBF (Dinan et al., 2019) and Detoxify (Hanu and Unitary team, 2020), discussed in Section 3.3, can be selectively applied to identify and eliminate undesired content in the training data. Given that much of the data used for pre-training is gleaned from social media platforms, some researchers have explored author-based fltering methods. For instance, if certain authors are known for frequently posting harmful material, removing all posts from these authors can serve as an effective strategy to discard both explicit and implicit unsafe content (Dinan et al., 2019; Wang et al., 2020; Gu et al., 2022). Aside from eliminating undesired data, another tactic in the pre-processing stage involves adding data that promotes fairness and reduces bias, aiming to achieve a more balanced and representative training corpus.  

However, rigorous fltering of potentially biased or unsafe data can be a double-edged sword. Feng et al. (2023) found that including biased data during pre-training could paradoxically improve the model’s ability to understand and detect such biases. Similarly, Touvron et al. (2023) opted not to comprehensively flter out unsafe content during the pre-training of their llama2 model. They argue that this makes the model more versatile for tasks such as hate speech detection. However, they also caution that such an approach could lead to the model exhibiting harmful behaviors if not carefully aligned. Therefore, stringent monitoring models’ generation is required in the subsequent use of pre-trained models to minimize their output of harmful content.  

# 5.1 Alignment  

In LMs development, alignment—pursuing that LMs behave in accordance with human values—is not just a technical challenge but an ethical imperative. This section delves into post-pre-training methods such as prompt tuning and reinforcement learning, all targeted toward achieving better model alignment and safety. One prevalent method for ensuring safe outputs is to generate predefned general responses for risky or sensitive contexts. Xu et al. (2020) and ChatGPT (OpenAI, 2022), for example, utilize this method. They could respond directly with "I’m sorry, I’m not sure what to say. Thank you for sharing and talking to me though." or change the topic by saying "Hey do you want to talk about something else? How about we talk about ...". This “avoidance mechanism" diminishes user engagement, despite the fact that it effectively precludes harmful output. How can we balance safety and user experience without compromising either? This requires in-depth consideration during alignment.  

Controlled text generation offers another avenue for alignment. As an effcient method, CTRL (Keskar et al., 2019) pre-pended control codes before sentences in training corpora, which is a direct and effective method to model $P_{\theta}(x_{t}|\boldsymbol x_{<t},\boldsymbol c)$ , where $c$ is the desired attribute that is formalized into control codes. Xu et al. (2020) extended this by applying safe or unsafe control codes to training examples, thereby actively managing safety and style during the inference phase. This strategy was also adapted for mitigating gender bias by using gender-specifc control codes (Xu et al., 2020). Sharing a similar idea, Krause et al. (2021) proposed Generative Discriminator to guide sequence generation (GeDi), which utilized Bayes theorem to model the conditional probability $P_{\theta}\left(c\mid x_{1:t}\right)$ .  

Prompt tuning has recently risen as a new paradigm to adapt downstream tasks, especially with the advent of large-scale pre-trained models (Liu et al., 2023a). Li and Liang (2021) added a “prefx” before real inputs and searched the optimal soft prompt in the embedding layer in models by gradient. Also, diffusion models are found effective in controlled text generation, emerging as the next SOTA generation model (Li et al., 2022b). All these controlled text generation methods are easy to adapt to safe generation tasks. Reinforcement learning (RL) is another popular approach to guide models to generate words with target attributes. The core module, reward function in RL is always given by a scoring model or safety detector (Perez et al., 2022; Ganguli et al., 2022).  

More recently, LMs have the promising ability to generalize across tasks by instruction tuning (Chung et al., 2022). Moreover, reinforcement learning from human feedback (RLHF) is then applied to better elicit LMs’ internal knowledge and align with humans’ values (Glaese et al., 2022; OpenAI, 2022; Bai et al., 2022a). Among these works, safety is always considered the paramount principle, as harmful responses always run counter to human values. Based on RLHF, Bai et al. (2022b) designed an RL from AI feedback diagram to get a more harmless (and still helpful) language model. They introduced several constitutions involving safety to get feedback from language models to further improve safety through reinforcement learning. Although technical advances have been made in AI alignment, a clear consensus on the fundamental ethical values that should guide these models is still lacking. This presents a complex challenge, as it combines technical considerations with ethical complexities.  

# 5.2 Inference  

While training LMs demands substantial computational resources and costs, most methods applied in the inference phase are designed to be plugand-play, requiring no parameter modifcations. This plug-and-play method has become more popular after the emergence of pre-trained models like GPT (Radford et al., 2019; Brown et al., 2020), which cost huge resources in the training stage. Aiming at very dirty and explicit words, n-gram blocking is largely used in the decoding stage, which directly makes the sampling probability of some undesired words as zero. Rather than only token-level unsafety avoidance, PPLM (Dathathri et al., 2019) notices that $P(x|c)\;\propto\;P(c|x)$ , and adopts an attribute model to compute $P(c|x)$ to guide the model decoding at the sentence level without any additional changes in the training phase. Motivated by PPLM, FUDGE (Yang and Klein, 2021) introduces a future discriminator to predict whether the ongoing generation text would conform to the desired attribute, greatly accelerating decoding. DExperts (Liu et al., 2021) achieve detoxifcation by adopting two generative models (expert and anti-expert models) to replace the original discriminator, inspired by Bayes theorem and GeDi (Krause et al., 2021).  

Some methods based on prompting for zeroshot can also be applied to safe generation tasks. Schick et al. (2021) found that the LM itself is well aware of its generative undesired contents, including toxicity and bias. They add the selfdebiasing inputs (e.g., The following text discriminates against people because of their Bias Type) into the prompt to form an undesired words generator “anti-expert”, which models the distribution with a higher probability of undesired words. The thought of self-detoxifcation inspires other works (Xu et al., 2022). Moreover, for LMs with strong instruction-following abilities, prompt engineering can largely improve safety. Moreover, as illustrated by Deshpande et al. (2023), assigning a role can affect the safety of LMs. It becomes natural to give the model an overall prompt (e.g., You are a harmless AI assistant) to make it safer, which is known as the "system message" of LMs, as in ChatGPT (OpenAI, 2022) and Llama2 (Touvron et al., 2023).  

# 5.3 Post-processing  

In contrast to the methods mentioned above, postprocessing occurs between model generation and message showing to users. In this stage, the system conducts the last check and edition for the generated response. The most common strategy is rejection sampling when the response is found unsafe by the detector. And various detectors can be applied to this process, like classifers or a language model. Thoppilan et al. (2022) use the model itself to discriminate safety by fne-tuning with the schema "<context> <sentinel> <response> <attribute-name> <rating>" (e.g. What’s up? RESPONSE not much. UNSAFE 0). Using this score, Lamda can self-flter unsafe responses. Sharing a similar idea, another commonly used method is to generate multiple responses and rerank them. In order to re-rank responses, the score can be given by classifers, language models, reward models or rule-based methods. Moreover, some researchers found that only a small proportion of the whole response (e.g., one or two words) needs to be fxed. Thus, an edition module takes effect after the generation to fx some problems in some works (Liu et al., 2023b; Logacheva et al., 2022). Similarly, text style transfer or rephrasing from toxicity to non-toxicity can also be plugged in this stage (Dale et al., 2021; Laugier et al., 2021). And for LMs, they can generate self-feedback (Madaan et al., 2023) or utilize a given feedback (Gou et al., 2023), like from classifers, to self-correct the unsafe response.  

# 6 Research Challenges  

Interpretability Deep learning models are usually seen as black boxes, and the opacity of their interior workings poses a slew of safety hazards. Research on the interpretability of LMs seeks to uncover how these models make decisions, thereby improving their safety and reliability and earning users’ trust. Many studies focus on the interpretability of model outputs, such as visualizing decision-making processes using attention scores (Mathew et al., 2021) or improving moral judgment accuracy using knowledge-based reasoning (Mehrabi et al., 2022; Kim et al., 2022). These studies often focus on the models’ narrow behaviors (OpenAI, 2023b) and attempt to explain the models’ outputs by mimicking human thought processes. However, there is still a gap in comprehending the inner workings of these models. OpenAI has explored the approach of "using AI to explain AI" to generate natural language descriptions of neuron behaviors (OpenAI, 2023b). This quantitative framework makes neural network computation more understandable to humans. Nonetheless, this method faces diffculties in deciphering complicated neuron behaviors and examining their roots. The goal of mechanism interpretability research is to probe into the internal workings of the models (Olah, 2023). They focus on assessing whether the model fts with its stated purposes by examining the alignment of the model’s internal states and external manifestations, as well as whether there are any concealed harmful intents. The current results, however, are far from ideal.  

Ongoing Safety Issues Continuous monitoring and resolution of safety risks is an ongoing activity in the application of LMs. While their safety generally depends on alignment approaches during training and deployment, due to the dynamic nature of safety issues and their variety, it’s diffcult to fully pre-consider all possible dangers. Especially when large models are broadly applied to diverse scenarios, new safety issues and topics are continually arising. As a result, researchers must constantly pay attention to new safety concerns and optimize the models’ safety. One effective method is to discover new potential hazards and collect data to refne the model, and many recently proposed benchmark datasets are constructed in this manner (Sheng et al., 2021; Sun et al., 2022; Dinan et al., 2021). However, this data-driven method has limits in terms of data collecting and annotation costs. Considering the extensive range of applications of LMs, it is natural to utilize user feedback obtained through interaction as a means to improve safety. Another area of emphasis is automatically generating feedback (Madaan et al., 2023; Wang et al., 2023) from the model itself or external robust evaluators to guide the safety enhancement.  

Robustness against Malicious Attacks The environment distribution of large models commonly deviates during the training and deployment phases, leading to unexpected safety hazards in real deployments. Malicious users could loosen ethical constraints and try to bypass the model’s safety mechanism by giving more covert and misleading instructions, thus posing safety hazards. Researchers have exerted a great deal of effort to ensure safety and robustness when processing diverse user inputs. As mentioned in Section $\S3.2$ , they design adversarial attacks to simulate the most challenging deployment environments, exploring the safety limits of large models and devising targeted defense strategies. Moreover, ensuring the effcacy of the safety risk detector is essential because it is the last line of defense in monitoring the generation’s safety. Enhancing the robustness of LMs will be a time-consuming endeavor due to the constant evolution of safety issues and malicious attack methods.  

# 7 Conclusion  

This paper has presented a comprehensive review of the latest advancements in safety research related to LMs. We have meticulously surveyed the emerging safety concerns and provided an indepth analysis of safety evaluation techniques, including preference-based, adversarial attack, and safety detection methodologies. Furthermore, we delved into safety improvement strategies spanning data preparation, model training, inference, and deployment phases. We also discussed future challenges and opportunities in this feld. We hope this survey will illuminate fresh insights for future research, paving the way for safer deployment of language models.  

# References  

Betty van Aken, Julian Risch, Ralf Krestel, and Alexander Löser. 2018. Challenges for toxic comment classifcation: An in-depth error analysis. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 33– 42, Brussels, Belgium. Association for Computational Linguistics.  

Ashutosh Baheti, Maarten Sap, Alan Ritter, and Mark Riedl. 2021. Just say no: Analyzing the stance of neural dialogue generation in offensive contexts. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4846–4862, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.  

Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022a. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.  

Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfeld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional ai: Harmlessness from ai feedback.  

Yejin Bang, Nayeon Lee, Etsuko Ishii, Andrea Madotto, and Pascale Fung. 2021. Assessing political prudence of open-domain chatbots. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dia  

logue, pages 548–555, Singapore and Online.   
Association for Computational Linguistics.  

Luke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1664–1674, Hong Kong, China. Association for Computational Linguistics.  

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901.  

Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing unintended memorization in neural networks. In Proceedings of the 28th USENIX Conference on Security Symposium, SEC’19, page 267–284, USA. USENIX Association.  

Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633–2650.  

Tommaso Caselli, Valerio Basile, Jelena Mitrovic´, and Michael Granitzer. 2021. HateBERT: Retraining BERT for abusive language detection in English. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17–25, Online. Association for Computational Linguistics.  

Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra,  

Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-fnetuned language models.  

David Dale, Anton Voronov, Daryna Dementieva, Varvara Logacheva, Olga Kozlova, Nikita Semenov, and Alexander Panchenko. 2021. Text detoxifcation using large pre-trained neural models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7979–7996, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.  

Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason Yosinski, and Rosanne Liu. 2019. Plug and play language models: A simple approach to controlled text generation.  

Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. Proceedings of the 11th International Conference on Web and Social Media, ICWSM 2017, 11:512–515.  

Jiawen Deng, Zhuang Chen, Hao Sun, Zhexin Zhang, Jincenzi Wu, Satoshi Nakagawa, Fuji Ren, and Minlie Huang. 2023. Enhancing offensive language detection with data augmentation and knowledge distillation. Research.  

Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, and Minlie Huang. 2022. COLD: A benchmark for Chinese offensive language detection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11580– 11599, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.  

Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335.  

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019  

Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.  

Emily Dinan, Gavin Abercrombie, A. Stevie Bergman, Shannon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena Rieser. 2021. Anticipating safety issues in e2e conversational ai: Framework and tooling.  

Emily Dinan, Samuel Humeau, Bharath Chintagunta, and Jason Weston. 2019. Build it break it fx it for dialogue safety: Robustness from adversarial human attack. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4537–4546, Hong Kong, China. Association for Computational Linguistics.  

Denis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell Forbes, and Yejin Choi. 2021. Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 698–718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.  

Oxford English. 1976. Oxford english dictionary. Encyclopedia of Swearing, page 334.  

Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. 2023. From pretraining data to language models to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11737–11762, Toronto, Canada. Association for Computational Linguistics.  

Maxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653–  

670, Online. Association for Computational Linguistics.  

Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfeld-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned.  

Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356–3369, Online. Association for Computational Linguistics.  

Aaron Gokaslan and Vanya Cohen. 2019. Openwebtext corpus.  

Spiros V. Georgakopoulos, Sotiris K. Tasoulis, Aristidis G. Vrahatis, and Vassilis P. Plagianakos. 2018. Convolutional neural networks for toxic comment classifcation. In Proceedings of the 10th Hellenic Conference on Artifcial Intelligence, SETN ’18, New York, NY, USA. Association for Computing Machinery.  

Amelia Glaese, Nat McAleese, Maja Tre˛bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. 2022. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375.  

Eva2.0: Investigating open-domain chinese dialogue systems with large-scale pre-training.  

Isuru Gunasekara and Isar Nejadgholi. 2018. A review of standard text classifcation practices for multi-label toxicity identifcation of online content. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 21– 25, Brussels, Belgium. Association for Computational Linguistics.  

Xiaochuang Han and Yulia Tsvetkov. 2020. Fortifying toxic speech detectors against veiled toxicity. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7732–7739, Online. Association for Computational Linguistics.  

Laura Hanu and Unitary team. 2020. Detoxify. Github. https://github.com/unitaryai/detoxify.  

Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. ToxiGen: A large-scale machinegenerated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3309–3326, Dublin, Ireland. Association for Computational Linguistics.  

Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob Steinhardt. 2020. Aligning ai with shared human values.  

Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. 2023. An overview of catastrophic ai risks.  

Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1–38.  

Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738.  

Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. 2021. Delphi: Towards machine ethics and norms.  

Yuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan Zhu, Jie Tang, and Minlie Huang. 2022.  

Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for effcient text classifcation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427–431, Valencia, Spain. Association for Computational Linguistics.  

Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation.  

Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. 2022. ProsocialDialog: A prosocial backbone for conversational agents. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4005–4029, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.  

Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shafq Joty, Richard Socher, and Nazneen Fatema Rajani. 2021. GeDi: Generative discriminator guided sequence generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4929–4952, Punta Cana, Dominican Republic. Association for Computational Linguistics.  

Rohan Kshirsagar, Tyrus Cukuvac, Kathy McKeown, and Susan McGregor. 2018. Predictive embeddings for hate speech detection on Twitter. In Proceedings of the 2nd Workshop on Abusive Language Online (ALW2), pages 26– 32, Brussels, Belgium. Association for Computational Linguistics.  

Léo Laugier, John Pavlopoulos, Jeffrey Sorensen, and Lucas Dixon. 2021. Civil rephrases of toxic texts with self-supervised transformers. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1442– 1461, Online. Association for Computational Linguistics.  

Sharon Levy, Emily Allaway, Melanie Subbiah, Lydia Chilton, Desmond Patton, Kathleen McKeown, and William Yang Wang. 2022. SafeText: A benchmark for exploring physical safety in language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2407– 2421, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.  

Haoran Li, Yangqiu Song, and Lixin Fan. 2022a. You don’t know my favorite color: Preventing dialogue representations from revealing speakers’ private personas. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5858–5870, Seattle, United States. Association for Computational Linguistics.  

Xiang Lisa Li and Percy Liang. 2021. Prefxtuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582– 4597, Online. Association for Computational Linguistics.  

Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B. Hashimoto. 2022b. Diffusion-lm improves controllable text generation.  

Alisa Liu, Maarten Sap, Ximing Lu, Swabha Swayamdipta, Chandra Bhagavatula, Noah A. Smith, and Yejin Choi. 2021. DExperts: Decoding-time controlled text generation with experts and anti-experts. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6691–6706, Online. Association for Computational Linguistics.  

Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9).  

Ruibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang, Tony X Liu, and Soroush Vosoughi. 2023b. Second thoughts are best: Learning to re-align with human values from text edits.  

Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang  

Ding, Kaiwen Men, Kejuan Yang, et al. 2023c. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.  

Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach.  

Varvara Logacheva, Daryna Dementieva, Sergey Ustyantsev, Daniil Moskovskiy, David Dale, Irina Krotova, Nikita Semenov, and Alexander Panchenko. 2022. ParaDetox: Detoxifcation with parallel data. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6804–6818, Dublin, Ireland. Association for Computational Linguistics.  

Nicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021. Scruples: A corpus of community ethical judgments on 32,000 real-life anecdotes. Proceedings of the AAAI Conference on Artifcial Intelligence, 35(15):13470–13479.  

Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refne: Iterative refnement with self-feedback. arXiv preprint arXiv:2303.17651.  

Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021. Hatexplain: A benchmark dataset for explainable hate speech detection. Proceedings of the AAAI Conference on Artifcial Intelligence, 35(17):14867–14875.  

Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, and Aram Galstyan. 2022. Robust conversational agents against imperceptible toxicity triggers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2831–2847, Seattle, United States. Association for Computational Linguistics.  

Pushkar Mishra, Helen Yannakoudakis, and Ekaterina Shutova. 2018. Neural character-based composition models for abuse detection. In Proceedings of the 2nd Workshop on Abusive  

Language Online (ALW2), pages 1–10, Brussels, Belgium. Association for Computational Linguistics.  

Jelena Mitrovic´, Bastian Birkeneder, and Michael Granitzer. 2019. nlpUP at SemEval-2019 task 6: A deep neural language model for offensive language detection. In Proceedings of the $I3t h$ International Workshop on Semantic Evaluation, pages 722–726, Minneapolis, Minnesota, USA. Association for Computational Linguistics.  

Moin Nadeem, Anna Bethke, and Siva Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371, Online. Association for Computational Linguistics.  

Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953–1967, Online. Association for Computational Linguistics.  

Chris Olah. 2023. Interpretability dreams.  

OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.  

OpenAI. 2023a. Gpt-4 technical report.  

OpenAI. 2023b. Language models can explain neurons in language models.  

Nedjma Ousidhoum, Xinran Zhao, Tianqing Fang, Yangqiu Song, and Dit-Yan Yeung. 2021. Probing toxic content in large pre-trained language models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4262–4274, Online. Association for Computational Linguistics.  

Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong  

Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.  

Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy risks of general-purpose language models. In 2020 IEEE Symposium on Security and Privacy $(S P)$ , pages 1314–1331.  

Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel Bowman. 2022. Bbq: A hand-built bias benchmark for question answering. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2086–2105.  

Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022. Red teaming language models with language models.  

Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. arXiv preprint arXiv:2211.09527.  

Fabio Poletto, Valerio Basile, Manuela Sanguinetti, Cristina Bosco, and Viviana Patti. 2021. Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation, 55(2):477–523.  

Ilan Price, Jordan Gifford-Moore, Jory Flemming, Saul Musker, Maayan Roichman, Guillaume Sylvain, Nithum Thain, Lucas Dixon, and Jeffrey Sorensen. 2020. Six attributes of unhealthy conversations. In Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 114–124, Online. Association for Computational Linguistics.  

Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1:9.  

Sara Rosenthal, Pepa Atanasova, Georgi Karadzhov, Marcos Zampieri, and Preslav Nakov. 2021. SOLID: A large-scale semisupervised dataset for offensive language identifcation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 915–928, Online. Association for Computational Linguistics.  

Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477–5490, Online. Association for Computational Linguistics.  

Timo Schick, Sahana Udupa, and Hinrich Schütze. 2021. Self-diagnosis and self-debiasing: A proposal for reducing corpus-based bias in nlp. Transactions of the Association for Computational Linguistics, 9:1408–1424.  

Anna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language processing. In Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, pages 1–10, Valencia, Spain. Association for Computational Linguistics.  

Denise Sekaquaptewa, Penelope Espinoza, Mischa Thompson, Patrick Vargas, and William von Hippel. 2003. Stereotypic explanatory bias: Implicit stereotyping as a predictor of discrimination. Journal of Experimental Social Psychology, 39(1):75–82.  

Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2021. “nice try, kiddo”: Investigating ad hominems in dialogue responses. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 750–767, Online. Association for Computational Linguistics.  

Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3407– 3412, Hong Kong, China. Association for Computational Linguistics.  

Wai Man Si, Michael Backes, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Savvas Zannettou, and Yang Zhang. 2022. Why so toxic? measuring and triggering toxic behavior in open-domain chatbots. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, CCS ’22, page 2659–2673, New York, NY, USA. Association for Computing Machinery.  

Gudbjartur Ingi Sigurbergsson and Leon Derczynski. 2020. Offensive language and hate speech detection for Danish. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 3498–3508, Marseille, France. European Language Resources Association.  

Congzheng Song and Ananth Raghunathan. 2020. Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, CCS ’20, page 377–390, New York, NY, USA. Association for Computing Machinery.  

Hao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan Zhu, and Minlie Huang. 2022. On the safety of conversational models: Taxonomy, dataset, and benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, pages 3906–3923, Dublin, Ireland. Association for Computational Linguistics.  

Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety assessment of chinese large language models. arXiv preprint arXiv:2304.10436.  

Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen MeierHellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke,  

Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications.  

Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fne-tuned chat models. arXiv preprint arXiv:2307.09288.  

Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: A critic for language model generation.  

Yau-Shian Wang and Yingshan Chang. 2022. Toxicity detection with generative prompt-based inference.  

Yida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020. A large-scale chinese short-text conversation dataset. In Natural Language Processing and Chinese Computing: 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14–18, 2020, Proceedings, Part I, page 91–103, Berlin, Heidelberg. SpringerVerlag.  

Zijian Wang and Christopher Potts. 2019. TalkDown: A corpus for condescension detection in context. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3711–3719, Hong Kong, China. Association for Computational Linguistics.  

Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffn, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles,  

Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models.  

Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447–2469, Punta Cana, Dominican Republic. Association for Computational Linguistics.  

Ellery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In Proceedings of the 26th International Conference on World Wide Web, WWW ’17, page 1391–1399, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.  

Canwen Xu, Zexue He, Zhankui He, and Julian McAuley. 2022. Leashing the inner demons: Self-detoxifcation for language models.  

Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et al. 2023a. Cvalues: Measuring the values of chinese large language models from safety to responsibility. arXiv preprint arXiv:2307.09705.  

Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079.  

Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023b. On the tool manipulation capability of opensource large language models. arXiv preprint arXiv:2305.16504.  

Kevin Yang and Dan Klein. 2021. FUDGE: Controlled text generation with future discriminators. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3511–3535, Online. Association for Computational Linguistics.  

Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2023. Gpt-4 is too smart to be safe: Stealthy chat with llms via cipher.  

Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Predicting the type and target of offensive posts in social media. NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference, 1:1415–1420.  

Yangjun Zhang, Pengjie Ren, and Maarten de Rijke. 2020. Detecting and classifying malevolent dialogue responses: Taxonomy, data and methodology.  

Yangjun Zhang, Pengjie Ren, and Maarten de Rijke. 2021. A taxonomy, data set, and benchmark for detecting and classifying malevolent dialogue responses. Journal of the Association for Information Science and Technology, 72:1477 – 1497.  

Zhexin Zhang, Jiale Cheng, Hao Sun, Jiawen Deng, Fei Mi, Yasheng Wang, Lifeng Shang, and Minlie Huang. 2022. Constructing highly inductive contexts for dialogue safety through controllable reverse generation. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3684–3697.  

Jiaxu Zhao, Meng Fang, Zijing Shi, Yitong Li, Ling Chen, and Mykola Pechenizkiy. 2023. Chbias: Bias evaluation and mitigation of chinese conversational language models. arXiv preprint arXiv:2305.11262.  

Caleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2022. The moral integrity corpus: A benchmark for ethical dialogue systems. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3755–3773.  