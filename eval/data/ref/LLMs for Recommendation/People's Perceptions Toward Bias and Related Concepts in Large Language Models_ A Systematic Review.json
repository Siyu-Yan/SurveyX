{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2309.14504",
    "title": "People's Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review",
    "abstract": "Large language models (LLMs) have brought breakthroughs in tasks including translation, summarization, information retrieval, and language generation, gaining growing interest in the CHI community. Meanwhile, the literature shows researchers' controversial perceptions about the efficacy, ethics, and intellectual abilities of LLMs. However, we do not know how people perceive LLMs that are pervasive in everyday tools, specifically regarding their experience with LLMs around bias, stereotypes, social norms, or safety. In this study, we conducted a systematic review to understand what empirical insights papers have gathered about people's perceptions toward LLMs. From a total of 231 retrieved papers, we full-text reviewed 15 papers that recruited human evaluators to assess their experiences with LLMs. We report different biases and related concepts investigated by these studies, four broader LLM application areas, the evaluators' perceptions toward LLMs' performances including advantages, biases, and conflicting perceptions, factors influencing these perceptions, and concerns about LLM applications.",
    "bib_name": "wang2024peoplesperceptionsbiasrelated",
    "md_text": "# People\u2019s Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review\nLU WANG, College of Computing & Informatics, Drexel University, USA MAX SONG, College of Computing & Informatics, Drexel University, USA REZVANEH REZAPOUR, College of Computing & Informatics, Drexel University, USA BUM CHUL KWON, IBM Research, USA JINA HUH-YOO, College of Computing & Informatics, Drexel University, USA\nLarge language models (LLMs) have brought breakthroughs in tasks including translation, summarization, information retrieval, and language generation, gaining growing interest in the CHI community. Meanwhile, the literature shows researchers\u2019 controversial perceptions about the efficacy, ethics, and intellectual abilities of LLMs. However, we do not know how people perceive LLMs that are pervasive in everyday tools, specifically regarding their experience with LLMs around bias, stereotypes, social norms, or safety. In this study, we conducted a systematic review to understand what empirical insights papers have gathered about people\u2019s perceptions toward LLMs. From a total of 231 retrieved papers, we full-text reviewed 15 papers that recruited human evaluators to assess their experiences with LLMs. We report different biases and related concepts investigated by these studies, four broader LLM application areas, the evaluators\u2019 perceptions toward LLMs\u2019 performances including advantages, biases, and conflicting perceptions, factors influencing these perceptions, and concerns about LLM applications.\nAdditional Key Words and Phrases: Large Language Models, Generative AI, Bias, Perceptions, User-Centered Design ACM Reference Format: Lu Wang, Max Song, Rezvaneh Rezapour, Bum Chul Kwon, and Jina Huh-Yoo. 2024. People\u2019s Perceptions Toward Bias and Rela Concepts in Large Language Models: A Systematic Review. 1, 1 (March 2024), 24 pages. https://doi.org/10.1145/nnnnnnn.nnnnn\nLu Wang, Max Song, Rezvaneh Rezapour, Bum Chul Kwon, and Jina Huh-Yoo. 2024. People\u2019s Perceptions Toward Bias and Related Concepts in Large Language Models: A Systematic Review. 1, 1 (March 2024), 24 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n# 1 INTRODUCTION\nLarge language models (LLMs), which utilize deep neural networks to learn the relationships between words in natural language and are trained on vast datasets comprising billions of documents sourced from the Internet [79, 85, 88], have brought breakthroughs in various tasks, including translation, summarization, information retrieval, and conversational interactions [59]. Microsoft launched the Bing search engine running on an OpenAI LLM customized specifically for search [54]. There is growing interest in the CHI community regarding different strategies to prompt LLMs [18, 88], interaction designs for mobile user interfaces [83], and applications in various areas such as writing [18, 63], coding [52], supporting public health interventions [41], supporting augmentative and alternative communication [81], and supporting synthetic HCI research [33].\nAuthors\u2019 addresses: Lu Wang, lw823@drexel.edu, College of Computing & Informatics, Drexel University, 3675 Market Street, Philadelphia, Pennsylvania, USA, 19104; Max Song, ms5526@drexel.edu, College of Computing & Informatics, Drexel University, 3675 Market Street, Philadelphia, Pennsylvania, USA, 19104; Rezvaneh Rezapour, sr3563@drexel.edu, College of Computing & Informatics, Drexel University, 3675 Market Street, Philadelphia, Pennsylvania, USA, 19104; Bum Chul Kwon, bumchul.kwon@us.ibm.com, IBM Research, Cambridge, Massachusetts, USA, 02142; Jina Huh-Yoo, jh3767@drexel.edu, College of Computing & Informatics, Drexel University, 3675 Market Street, Philadelphia, Pennsylvania, USA, 19104.\n\u00a9 2024 Association for Computing Machinery. Manuscript submitted to ACM\nManuscript submitted to ACM\nHowever, LLMs have been documented to inherit social bias from training data [23, 97], leading to unjust treatment of marginalized communities including the unjust association of Muslims with violence [1], as well as an excessive representation of gun violence, homelessness, and drug addiction in discussions concerning mental illness [37]. While researchers are actively working on developing evaluations [74, 92, 95] to assess the biases in LLMs, some biases are believed inevitable due to the inherent nature of language and cultural norms [23]. The complexity of sociocultura factors complicates the investigation, the integration of diverse tasks within a single model adds to the challenges faced by LLMs, and the disagreements of individual user opinions and preferences contribute to the complexity of the issue. Researchers also assert that creating a perfectly safe model requires models to deeply understand languages unti Artificial Intelligence (AI) itself becomes a reality [90]. There are controversial conversations among researchers about the efficacy, ethics, and intellectual abilities of LLMs [10, 55]. Various research methodologies employ human evaluation to gain a deeper understanding of LLMs and assess their performance more effectively. Given the extensive range of applications for LLMs, it is crucial to gain insight into how individuals perceive LLMs and what kind of biases they might consider significant. Specifically, we will investigate the following research questions: \u2022 RQ1: In studies examining people\u2019s perceptions of LLMs, what specific biases did researchers explore? \u2022 RQ2: In which settings or contexts were LLMs applied in these studies? \u2022 RQ3: What were the findings regarding people\u2019s perceptions of LLMs as explored in these studies? Through a systematic review, this study informs the different biases of LLMs studied, the existing applications of LLMs evaluated from perspectives of human evaluators, dimensions of people\u2019s perceptions of LLM investigated by researchers, and the gaps as well as the opportunities for future research. Developers and designers will benefit from these findings for future development and applications of user-centered LLMs. The insights could also be generalized to human-centered AI design considering the bias.\nThrough a systematic review, this study informs the different biases of LLMs studied, the existing applications of LLMs evaluated from perspectives of human evaluators, dimensions of people\u2019s perceptions of LLM investigated by researchers, and the gaps as well as the opportunities for future research. Developers and designers will benefit from these findings for future development and applications of user-centered LLMs. The insights could also be generalized to human-centered AI design considering the bias.\n# 2 RELATED WORK\n# 2.1 Natural Language Processing and LLMs\nSince the introduction of LLMs in Natural Language Processing (NLP), the AI space has become increasingly popular due to its capability of successfully \u201cunderstanding\u201d and generating natural language [23, 31, 97]. Paying homage to the early development of language models, recurrent neural networks, and transformer models, LLMs such as ChatGPT and Google Bard have shown significant success in various language-related tasks such as translation [42], text generation [48], summarization[94], question answering [77], and sentiment analysis[23, 31]. LLMs boast a diverse array of applications across two primary scenarios: creative generation and decision-making [97]. In creative generation, LLMs are employed to produce innovative and imaginative content, encompassing tasks like crafting a narrative, composing poetry, or scripting dialogue for a film. For decision-making, LLMs are used to make informed decisions based on natural language instruction, which can be observed in tasks on sentiment analysis, text classification, and question answering [31, 97]. Furthermore, these LLM applications extend across a variety of different fields, like education (e.g., students are engaging with their course material in entirely new ways) [16, 32, 76], finance (e.g., LLMs designed specifically for finance, like BloombergGPT [87], have revolutionized financial NLP tasks like risk assessment, algorithmic trading, market prediction and financial reporting [31]), engineering (e.g., especially in software engineering, LLMs assist with code generation, debugging, software testing, documentation generation, and collaboration [24, 31]), and healthcare Manuscript submitted to ACM\n(e.g., LLMs have been successfully employed in medical education, radiologic decision-making, clinical genetics, and patient care [28, 47, 68]).\n# 2.2 HCI and LLMs\nThere is growing interest in the HCI community regarding different aspects of LLMs, including prompting [18, 88, 91], interaction designs [83], applications for various tasks such as coding [52, 80] and research [33], and supporting different groups in mental health support [41] and communications [81]. Prompt engineering is a growing interest within the CHI community, as many related works show that para-phrasing prompts can lead to better model outputs [18]. For example, chaining prompts for LLMs, a process of breaking up complex tasks into smaller steps to be independently run while the output of one or more steps is used as input for the next, raises the bar of possibility for prompt engineering [88]. Besides, by using a set of prompting techniques designed to adapt LLMs to mobile User Interfaces (UIs), the HCI community is trying to make advancements with machine learning, bridging the gap in NLP and graphical UIs (e.g., web UIs and popular systems like iOS and macOS) to enable conversational interaction [83]. However, researchers found that non-AI experts met challenges in prompt engineering in terms of generating prompts, evaluating prompts, and explaining prompts\u2019 effects [91]. Advancements in LLMs provided the ability to assist users in different tasks. For example, LLMs can automatically generate code in multifarious programming languages, like Python, researchers have been propelled further to study the usability of LLM-based code generation tools, like GitHub Copilot, for real-world programming tasks [80]. Besides, data collection in HCI research can become a hindrance. Since interviews and questionnaires dominate this area, collecting data turns into a slow and arduous process. In essence, LLMs can be viewed as a newfound search engine with the capability of prompt engineering to query for information in many different ways (e.g., querying in the form of a narrative) [33]. In the bigger picture, LLMs have the potential to expand computational user modeling and simulation to increasingly new levels. However, their usefulness relies heavily upon the validity of the generated data [33]. Moreover, advancements in LLMs enhancing the quality of open-ended conversations with chatbots have alternatively fueled other researchers to study the potential of LLM-driven chatbots, like CareCall, to support public health interventions by monitoring populations at scale through empathetic interactions in real-world settings [41]. Furthermore, in the mental health domain, researchers have shown the potential of LLM\u2019s success in therapeutic settings. Teachable self-help techniques, such as mindfulness, can reduce anxiety and improve mental well-being outcomes. Thus, HCI researchers are also exploring is use of LLM\u2019s for improving the awareness of mindfulness [46]. Individuals utilizing augmentative and alternative communication (AAC) devices may encounter challenges in real-time communication due to the time required for composing messages. AI technologies, like LLMs, offer a potential avenue to assist AAC users by enhancing the quality and diversity of text suggestions. Researchers have found that these technologies will radically change how users interact with AAC devices as users transition from typing their own phrases to prompting and selecting AI-generated phrases\u2013highlighting that there are both positive and negative consequences [81].\n# 2.3 Bias of LLMs and Challenges in Mitigating Bias in LLMs\nWhile LLMs are rapidly transforming the way people communicate, create, and work [30], significant advancements made by LLMs give rise to emerging ethical concerns like potential biases and fairness, reliability, and toxicity [97]. Different types of biases can emerge when using LLMs, which usually occur due to either biases from the original training data or biases from different models [5, 23].\nManuscript submitted to ACM\nBias is a ubiquitous challenge across various domains, albeit with nuanced differences in its definitions. In statistics, bias refers to \u201cthe error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model\u201d [39]. In cognition, bias refers to the systematic errors of judgments and choices against rationality, recurring predictably in particular circumstances [19]. In sociology, bias is referred to as \u201cprejudice in favor or against a person, group or thing that is considered to be unfair\u201d [2, 25, 71]. Applying these concepts to LLMs, biases in LLM can be understood as the inaccuracies or limitations LLMs encounter while learning the relationships between words in natural language\u2014in which the LLM might not fully capture the diversity, subtlety, or complexity of natural language as it is used in real-world contexts [11, 23]\u2014or deviations in human-LLM communication that do not align with rational or unbiased language use [5]. It may very well also be the exhibitions of sociologically biased outputs, demonstrating unfair or prejudiced language patterns towards certain groups or topics, that has recently been garnering increased attention from researchers [1, 37, 45, 53, 97]. For example, predicted jobs by GPT-2 are less diverse and more stereotypical for women than for men [45]. English prompts of ChatGPT reduce the variance in model responses and flat out cultural differences and biasing them towards American culture [11]. ChatGPT seemed to create positive Irish Limericks for liberal politicians and negative Limericks for conservative politicians [53]. These biases would bring unintended negative impacts if people apply LLMs without any awareness of such issues. In this paper, we define bias as any deviation from the expected performance, regardless of whether the causes are objective or subjective, and irrespective of inaccuracies or limitations. To detect and measure biases in LLMs, researchers have developed multiple evaluations to assess them in LLMs [74, 92, 95]. For example, researchers tracked how bias transferred from pre-training to tasks after fine-tuning and encouraged practitioners to focus more on dataset quality and context-specific harms [74]. Some researchers proposed benchmarks comprising carefully crafted metrics with datasets [92] or multi-layer networks with tasks and samples [95] to evaluate biases in LLMs. However, some biases are believed inevitable due to inherent biases in language, the ambiguity of cultural norms, the subjectivity of fairness, and continuously evolving language and culture [23]. Completely eliminating bias from LLMs is a complex, challenging, and ongoing task, requiring developers, researchers, and stakeholders to continue working on reducing bias in LLMs, collaborate with diverse communities, and engage in ongoing evaluations and mitigation methods [23]. Some researchers also assert that we can only create a perfectly safe model until models can deeply understand languages and this is an AI-complete problem, implying that the difficulty of these computational problems is equivalent to making computers as intelligent as people [90]. Given the challenges of mitigating biases in LLMs, it is imperative that designers and users exert efforts to ensure the safe use of LLM applications and to mitigate potential risks unaware by users, which also requires a clear understanding of the people\u2019s perceptions of LLMs.\n# 2.4 People\u2019s Perceptions of LLMs\nGenerally, NLP tasks, with a specific goal, favor automatic evaluations due to their cost-effectiveness and efficiency, which are essential for automatic benchmarking and fine-tuning of algorithms [60]. Human evaluations are employed in certain tasks where automatic evaluation may be unsuitable, such as text generation, e.g., ROUGE score for abstractive summarization [51], where the quality of generated content can extend beyond conventional (ground-truth) responses [13, 66]. Various measurements were applied in human evaluations to understand people\u2019s perceptions such as quality [60], accuracy [13], informativeness [60], naturalness [60]. Different tasks require different dimensions of perceptions. For example, researchers analyzed 97 style transfer papers in terms of three aspects: style transfer, meaning preservation, and fluency, and discovered that human evaluation protocols lack specificity and standardization [8]. Through 165 Manuscript submitted to ACM\npapers on natural language generation, researchers discovered that there were more than 200 different terms used to evaluate aspects of quality, indicating a pervasive lack of clarity, extreme diversity in approaches for human evaluations, and the urgent need for standard methods and terminology [36]. A meta-survey of the NLP community revealed that NLP researchers are split almost exactly in half on questions about whether language models understand language, whether the linguistic structure is necessary, and whether expert inductive biases are necessary [55]. This survey also uncovered false sociological beliefs where the community\u2019s predictions on the distributions of the answers don\u2019t match reality [55], indicating the misunderstanding of what others think. The disagreements within the NLP community could slow down communication and lead to wasted effort, missed opportunities, and needless fights [55]. There are differences among people with different expertise in AI, linguistic-related fields, and other fields in terms of adoption rates and willingness to use and trust LLMs [10]. People with expertise in AI were more aware of the limitations of natural language generation and specific ethical concerns, particularly regarding privacy and explainability [10], while people with expertise in linguistics, translators, interpreters, or related areas were more cautious about using natural language generation tools and expressed concerns about their impacts on daily life [10]. Considering the different biases of LLMs with such a variety in people\u2019s perceptions, it is imperative to systematically investigate people\u2019s perceptions toward LLMs. Understanding the potential factors behind these perceptions can also inform researchers in improving the user experience of LLMs, given the proliferation of LLMs in people\u2019s everyday devices.\n# 3 METHOD\nWe used the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) [61] to illustrate our review process (See Fig. 1). Following, we introduce the review process in three parts: 1) literature search and screening, 2) eligibility evaluation and backward snowballing, and 3) data extraction and analysis.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b727/b7276234-a22f-4346-8044-5f8e5d7ffc6a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. Diagram showing the adapted PRISMA of the review.</div>\n# 3.1 Literature Search and Screening\nw, we describe the data source, search query, and filtering and review process.\n3.1.1 Data Source. We used ACM Digital Library and ACL Anthology databases for the sources of literature collection. As a comprehensive bibliographic database focused exclusively on the field of computing, ACM Digital Library provides access to more than 3 million publications in computing [50]. Sponsored by the Association for Computational Linguistics, ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics, representing the NLP community\u2019s most up-to-date and established freely accessible research repository [6]. Since ACL Anthology doesn\u2019t provide advanced search functions, A.1 provides details of how we identified papers from ACL Anthology. 3.1.2 Search Query. On August 7th, 2023, we conducted a query search on the abstract from ACM Digital Library and ACL Anthology. The search query combined three components using \u201cAND\u201d operand between them and \u201cOR\u201d within each component: 1) keywords related to people\u2019s perceptions: experience, perspective*, perception*, acceptability, acceptance*, reaction*, response*, trust*, usability, accountability, transparenc*, participant*, \u201chuman subject*\u201d, \"user stud*\", speaker*, listener*, 2) keywords related to biases: bias*, \u201csocial harm*\u201d, stereotype*, stigma*, fair*, norm*, ethic*, safet* , and 3) keywords related to LLMs: \u201clanguage model*\u201d, GPT, bert, \u201clanguage generation*\u201d, \u201ccontextualized representation*\u201d, \u201csemantic representation*\u201d, \u201cgenerative AI\u201d. The star sign \u201c*\u201d allows matching any number of characters. The quote sign, \u201c\u201d, allows us to search phrases as a whole so that we can get more relevant results. We generated these keywords through brainstorming [65] within the research group, and we acknowledge that this list is not exhaustive. In addition, considering LLMs frequently rely on transformers, a neural network architecture introduced by Vaswani et al. in 2017 [82], we set the publication date as starting from 2017. The search on publications from January 1st, 2017 to August 7th, 2023 resulted in 149 records from the ACM Digital Library and 82 papers from ACL Anthology. Six records from ACM Digital Library were excluded since they were extended abstracts or tutorials. In total, we had 225 papers remaining for eligibility evaluation.\n# 3.2 Eligibility Evaluation and Backward Snowballing\nThe research group held regular meetings to examine the screened papers, determine the inclusion criteria, and validate the criteria. Studies satisfying all the criteria were included: 1) empirical research involving human evaluations, 2) having relevant statements by using the keywords, bias*, \u201csocial harm*\u201d, stereotype*, stigma*, fair*, norm*, ethic*, and safet*, in the full text, indicating what kind of biases the researchers investigated, and 3) reporting people\u2019s direct perceptions of LLMs. Two authors conducted a full-text review of the 225 papers based on the inclusion criteria, and ten papers remained. Examples of studies that did not meet inclusion criteria that were excluded were: 1) solely relying on automatic evaluations, 2) only using the keywords rhetorically such as \u201cto evaluate the models fairly\u201d, and 3) examining perceptions unrelated to LLMs like focusing on different design features of interfaces and evaluating data pre-processing results for LLMs\u2019 training instead of LLMs\u2019 generated responses. We further applied a backward snowballing approach to identify additional relevant papers on people\u2019s perception of LLMs with keywords including bias*, \u201csocial harm*\u201d, stereotype*, stigma*, fair*, norm*, ethic*, and safet* in the full papers through the references provided in the introduction and related work sections by the ten eligible papers and meeting the inclusion criteria. Snowballing refers to using the reference lists of a paper (backward) or the citations to Manuscript submitted to ACM\nthe paper (forward) to identify additional papers [86]. As recommended, snowballing could be applied in addition to the search in the databases for a full systematic review [38, 43]. Using the backward snowballing approach, five more papers were included through the references of the ten eligible papers, resulting in a total number of 15 papers in the final set.\n# 3.3 Data Extraction and Analysis\nTo answer RQ1 and RQ2, we used shared spreadsheets to extract data from the selected papers based on a top-down coding framework. Specifically, for the definitions of biases (RQ1), we extracted the sentences where researchers mentioned the keywords related to bias: bias, \u201csocial harm\u201d, stereotype, stigma, fair, norm, ethic, safety and investigated how these keywords were used by researchers in the context of the included papers. We thematized the bias-related terms based on how the term was used in the included papers to understand what bias-related terms have been investigated by researchers. We further grouped the biases into different themes regarding the sources such as from LLMs or from humans, and biases and related concepts mentioned by researchers in their papers such as bias in datasets or bias of algorithms. For LLM applications (RQ2), we conducted an analysis of the included papers and employed a standardized framework to extract data in order to comprehend the following aspects: 1) the application areas of LLMs, 2) the role of LLMs, 3) algorithms of LLMs, 4) the targeted end-users directly involved in LLMs applications, and 5) the participants recruited for evaluations of LLMs. In order to gain a more detailed understanding of the participants, we also extracted the demographic information of those participants involved in the studies, as presented in the included papers. For RQ3, we conducted a content analysis through open coding to extract people\u2019s perceptions and applied affinity diagrams to analyze the codes and generate the themes of people\u2019s perceptions of LLMs. Guided by grounded theory [75], two researchers coded people\u2019s perceptions within the result sections of included papers using Nvivo software [21]. Each of the generated codes was then transcribed onto a separate digital sticky note on Miro [57], a visual workspace platform for teams. To analyze the codes, the research group used an affinity diagram, a technique to organize ideas and issues into patterns and relationships [69], by continually comparing the transcribed sticky notes, integrating them with other notes, and organizing them based on their thematic connections such as similarities, differences, and hierarchical relationships.\n# 4 FINDINGS\nFollowing our search process and queries, we found that only a few studies investigated people\u2019s perceptions toward bias and related concepts in various applications of LLM. In this review, we only got 15 papers through both database searches and the backward snowballing approach. Below, we present the definitions and examples of how the terms related to bias were used by the researchers, the different LLM application areas, and people\u2019s perceptions of LLMs reported from these studies.\n# 1 Bias-related Terms and Definitions in Studies About Human Evaluat\nOf the 15 papers included, only two papers provided an explicit definition of \u201cbias\u201d or other related terms: \u201cFor this paper, we will use the term \u2018bias\u2019 to refer to any systematic favoring of certain artifacts or behavior over others that are equally valid\u201d [3], and \u201cIn other cases, humans tend to show automation bias, e.g., automatically relying or over-relying on the output produced by a chatbot.\u201d [10]. The rest of the papers used terms related to bias (i.e., bias*, \u201csocial harm*,\u201d stereotype*, stigma*, fair*, norm*, ethic*, and safet*) without explicit elaboration on what they might mean. Table 1\nshows how researchers adopted and used these terms in their papers. Overall, our analysis of the literature shows that researchers investigated three types of biases from two sources: human bias and bias of LLMs. We saw potential opportunities, as well as harm, caused when the two biases interact with each other.\n<div style=\"text-align: center;\">Table 1. Investigated Biases and Related Concepts in the Included Papers</div>\nThemes\nBiases and related concepts\nHow the concepts were used in the papers\nIndividual and\nSocial Bias\nStereotype\n\u201c Previous work has shown that different descriptions of gender-based violence (GBV) in-\nfluence the reader\u2019s perception of who is to blame for the violence, possibly reinforcing\nstereotypes which see the victim as partly responsible, too.\u201d [56]\nStigma\n\u201cAlthough therapy can help people practice and learn this Cognitive Reframing of Negative\nThoughts, clinician shortages and mental health stigma commonly limit people\u2019s access to\ntherapy.\u201d [70]\nCultural Norm\n\u201cWe expected to find variation between educators and students and across countries in line\nwith different educational approaches and cultural norms. Thus far, we have observed\na surprising level of agreement across stakeholders and locals. \u201d [72] \u201cPeople generally\nexperience feelings of shame and guilt when they engage in morally unacceptable behaviors\nor when they violate norms they have internalized\u201d [15] \u201c(The low-quality comments) may\nalso be made by peers who intend to be supportive but have difficulties (e.g., being uncertain\nof the social norms or their own expertise\u201d [62]\nThinking Traps\n\u201cNegative thinking often falls into common patterns, called \u2018thinking traps.\u2019 Also called\ncognitive distortions, these include exaggerated and biased patterns of thinking which\ncause individuals to perceive reality inaccurately.\u201d [70]\nInaccurate, Unfair,\nand Biased Nature\nof LLM Outputs\nDatasets containing harmful,\nbiased, toxic content affecting\nLLM outputs\n\u201cFurther, it is well known that commonly used hate-speech datasets are known to have issues\nwith bias and fairness\u201d [90] \u201cWhen dialogue models are trained to mimic human-human con-\nversations utilizing large preexisting datasets, they will unfortunately also learn undesirable\nfeatures from this human-human data, such as the use of toxic or biased language.\u201d [90] \u201cIt\nis insufficient to merely exclude toxic data from training, as the model would not know how to\nanswer hostile out-of-domain inputs, and positive biases where models tend to agree rather\nthan contradict would lead to undesirable outcomes.\u201d [90] \u201cWe employ 20 annotators to use our\ndesigned evaluation tool in order to lessen the preference bias of various annotators.\u201d [93]\n\u201cLike other LLMs, ChatGPT might have intrinsic biases due to imbalanced training data\u201d [40]\nAlgorithms affecting the re-\nstriction of certain content of\nthe LLM outputs,\n\u201cFor example, respondents are split in half on the importance of artificial general intelligence,\nwhether language models understand language, and the necessity of linguistic structure and\ninductive bias for solving NLP problems.\u201d [55] \u201cWe also describe training and sampling\nalgorithms that bias the generation process with a specific language style restriction or a\ntopic restriction. \u201d [84]\nConcerns or issues caused by\nthe inaccurate, biased LLM out-\nputs\n\u201cLanguage models tend to output repetitive and vague responses. They have no model of\nthe truth; they are learning correlations from large amounts of text and thus are able to\ngenerate falsehoods. Finally, it has been well-documented that these models can generate\noffensive language, have distributional biases, and may copy text from the training data.\u201d [27]\n\u201cLLMs may generate text that is semantically plausible and syntactically correct but factually\nwrong, a phenomenon, known as \u2018hallucination\u2019. The suitability of the term hallucination is\nquestionable as it might imply changes in one\u2019s perceptual experience, which LLMs do not\nhave.\u201d [44]\u201cThere are many ways in which NLG tools can become high-risk AI, e.g., producing\nmisleading or inaccurate information, biasing the user against a particular social group,\nor sharing private information about users.\u201d [10] \u201cappropriate responses to abusive queries\nare vital to prevent harmful gender biases.\u201d [17]\nContinued on next page\n\u201cFurther, it is well known that commonly used hate-speech datasets are known to have issues with bias and fairness\u201d [90] \u201cWhen dialogue models are trained to mimic human-human conversations utilizing large preexisting datasets, they will unfortunately also learn undesirable features from this human-human data, such as the use of toxic or biased language.\u201d [90] \u201cIt is insufficient to merely exclude toxic data from training, as the model would not know how to answer hostile out-of-domain inputs, and positive biases where models tend to agree rather than contradict would lead to undesirable outcomes.\u201d [90] \u201cWe employ 20 annotators to use our designed evaluation tool in order to lessen the preference bias of various annotators.\u201d [93] \u201cLike other LLMs, ChatGPT might have intrinsic biases due to imbalanced training data\u201d [40]\n\u201cFor example, respondents are split in half on the importance of artificial general intelligence, whether language models understand language, and the necessity of linguistic structure and inductive bias for solving NLP problems.\u201d [55] \u201cWe also describe training and sampling algorithms that bias the generation process with a specific language style restriction or a topic restriction. \u201d [84]\n\u201cLanguage models tend to output repetitive and vague responses. They have no model of the truth; they are learning correlations from large amounts of text and thus are able to generate falsehoods. Finally, it has been well-documented that these models can generate offensive language, have distributional biases, and may copy text from the training data.\u201d [27] \u201cLLMs may generate text that is semantically plausible and syntactically correct but factually wrong, a phenomenon, known as \u2018hallucination\u2019. The suitability of the term hallucination is questionable as it might imply changes in one\u2019s perceptual experience, which LLMs do not have.\u201d [44]\u201cThere are many ways in which NLG tools can become high-risk AI, e.g., producing misleading or inaccurate information, biasing the user against a particular social group, or sharing private information about users.\u201d [10] \u201cappropriate responses to abusive queries are vital to prevent harmful gender biases.\u201d [17]\nManuscript submitted to ACM\nContinued on next page\nTable 1 \u2013 continued from previous page\nThemes\nBias and related concepts\nHow the concepts were used in the papers\nOpportunistic,\nSometimes\nProblematic\nInfluence\nof LLM\nAmplified\nDuring\nInteractions\nwith Human\nSpread of bias/misinformation\ndue to confirmation bias and\npeople\u2019s overreliance on the\nLLM outputs\n\u201cSeveral participants expressed concerns about the potential spread of misinformation and\nunintentional plagiarism due to the usage of NLG tools... P77 also mentions the risk of confir-\nmation bias.\u201d [10]\u201cIn other cases, humans tend to show automation bias, e.g., automatically\nrelying or over-relying on the output produced by a chatbot.\u201d [10]\nUnwanted effects of LLMs on\nhumans\n\u201cIntervention in high-risk settings such as mental health necessitates ethical considerations\nrelated to safety, privacy and bias. There is a possibility that, in attempting to assist, AI may\nhave the opposite effect on people struggling with mental health challenges.\u201d [70]\nInadvertent amplification of\nthese biases due to the vulner-\nability of humans to biased in-\nformation\n\u201cWe demonstrate that in at least one domain (writing restaurant reviews), biased system\nbehavior leads to biased human behavior: People presented with phrasal text entry shortcuts\nthat were skewed positive wrote more positive reviews than they did when presented with\nnegative-skewed shortcuts.\u201d [3]\nHuman biases and related concepts include stereotypes [56], stigmas [70], negative thinking biases [70], and cultural norms where researchers expect to find variation across countries in line with different practices [17]. Researchers investigated biases and related concepts in terms of models including three aspects: datasets that contain harmful, biased, and toxic content [90, 93]; algorithms that are integrated with expert inductive bias or tailored according to specific goals such as style transforming, thus affecting the restriction of certain contents of LLM outputs [55, 84]; and concerns or issues caused by the inaccurate, biased LLM outputs such as distributional biases resulting in too generic responses or \u201challucination\u201d\u2014 where LLMs generate outputs that are semantically plausible and syntactically correct but factually wrong [10, 17, 27, 44]. When human biases interact with model biases, researchers investigated related concepts about the spread of bias/misinformation due to confirmation bias and people\u2019s overreliance on the LLM outputs [10], the unwanted opposite effects of LLMs on humans such as LLMs developed for positive impacts turning out to bring negative impacts [70], and the inadvertent amplification and transition of biased behaviors from LLMs to humans due to the vulnerability of humans to biased information [3]. As for the LLMs\u2019 role in these biases in the included papers, two papers investigated potential applications and impacts of LLMs in generating positive effects and their help in mitigating such human biases, instead of considering LLMs as the only bias amplifier [56, 70]. Three papers investigate how LLMs can be applied to generate safer conversations when responding to users\u2019 abusive behaviors [15, 17, 90]. Two papers examined LLMs as bias producers in providing writing assistance for customer reviews writing and simplifying medical reports, expressing concerns regarding how LLMs could introduce biases [3, 40]. The rest papers positioned LLMs in a neutral way and studied LLM applications in different contexts such as applying LLMs in translations, scientific writing, generating knowledge-based dialogue, or generating responses in different personas like Star Wars, Kennedy, Hillary, and Trump bots [10, 27, 44, 55, 62, 84, 93].\n# 4.2 Application Areas of LLMs in Included Studies: Learning About the Role of LLMs, Intended Targe Users, and Evaluators of the Studies\n# 4.2 Application Areas of LLMs in Included Studies: Learning About the Role of LLMs, Intended Target Users, and Evaluators of the Studies\n# 4.2 Application Areas of LLMs in Included Studies: Learning About the Role of LLMs, Intended Target\nIn this section, we describe the application areas of LLMs in the included studies. We further examine the application areas of LLMs, the role of LLMs, the algorithms of LLMs, the targeted end-users directly involved in LLM applications, and the participants recruited for evaluations of LLMs. Table 2 shows the summary of the results. Included studies investigated four broader application areas of LLMs\u2013i.e., content transformation, Question & Answering (Q&A), writing assistance, and responses to users\u2019 abusive behaviors\u2014across various fields such as journalism, Manuscript submitted to ACM\n<div style=\"text-align: center;\">Table 2. Application Areas of LLM in Included Studies</div>\nRoles of LLM\nContexts of the Task\nModels Used\nTarget Users\nMethods | Human Evaluators\nReference\nLLM rewrites news with different perspec-\ntives for gender-based violence news\nGPT-3, mBART\nJournalists\nQuan | 7 females who read the news with different education\nlevel within authors\u2019 network (Mean age:46)\n[56]\nContent\ntransformation\nLLM performs cognitive reframing of neg-\native thoughts through interactive conver-\nsations\nGPT-3, RoBERTa\nSupport seekers\nQuan | 3 mental health practitioners; Quan based on a ran-\ndomized field-study | 2,067 Mental Health America Website\nvisitors\n[70]\nLLM simplifies medical report content\nChatGPT\nPatients\nMix | 15 radiologists with varying levels of experience from\none clinic\n[40]\nLLM influences output style and topic\nthrough model training using small \u2018scent-\ning\u2019 datasets and decoding methods\nNeural Encoder-\nDecoder\nText-based bot users\nQuan | Judges recruited from Amazon Mechanical Turk\n(AMT) with unknown sample size. Workers were selected\nbased on their AMT prior approval rate (>95%). Each ques-\ntionnaire was presented to 3 different workers.\n[84]\nLLM generates knowledge-based dialogue\nwith users\nGLM-Dialog\nText-based bot users\nQuan | 3 annotators for explicit human evaluation and 20\nannotators for implicit human evaluation without reported\ndemographic information in the paper\n[93]\nQ&A\nLLM makes assessments on essays and\ncoding tasks\nChatGPT\nStudents\nMix | Educators and students from two selective institutions\nof higher education, one in Australia (338 students; 26 educa-\ntors) and one in the United States (51 students; 10 educators).\n[72]\nLLM works as cognitive assistants for trou-\nbleshooting e.g., user requesting help with\na machine issue from the cognitive assis-\ntant\nGPT-3.5\nWorkers\nQual | Initial stage: 3 representatives from a detergent fac-\ntory and 2 from a textile factory; feature demonstration and\nfocus group stage: 22 developers, and researchers from three\ndifferent factories(demographic information unavailable)\n[44]\nLLM makes writing recommendations for\nscientific writing\nGPT-2;\nDistil-\nGPT2\nWriters\nMix | 13 PhD students from five different STEM disciplines\n(demographic information unavailable)\n[27]\nWriting\nassistance\nLLM suggests next words for writing cus-\ntomer reviews for restaurants\nScalable Modified\nKneser-Ney Lan-\nguage Model\nWriter\ncomposing\na\nrestaurant review\nMix based on a within-subjects experiment | 38 students\nfrom a university (Demographic information unavailable)\n[3]\nLLM makes recommendations for people\nin writing posts in online communities to\nsupport peers in mental health\nBERT\nSupporter providers\nMix based on a randomized experiment | 30 students from\na local university via word of mouth, having prior experi-\nences of receiving or providing mental support from or to\nothers and that they are willing to offer support to peers in\nonline communities, (13 Females, 15 Males, 2 Not Available;\nages ranging from 20 to 30, Mean = 24.37, SD = 2.72) with\ninclusion criteria regarding fluency in English, depression,\nand experience of online communities\n[62]\nLLM generates safe utterance against\nusers\u2019 abusive behavior\nGPT-2, DialoGPT,\nBST 2.7B\nText-based bot users\nQuan | At least 3 distinct crowd workers from a disjoint set\nwere instructed to annotate a single utterance (Information\non sample size and demographic information unavailable)\n[90]\nResponses\nto users\u2019\nabusive\nbehaviors\nLLMs produces different response styles\nto influence peoples\u2019 behaviors\nAlexa,\nSiri,\nGoogle\nHome,\nCortana.\nConversational\nagent\nusers\nQuan | 190 crowd workers (130 men, 60 women) on the\nFigureEight platform, 62.6% are under age 44\n[17]\nLLM produces various responses to im-\npact people\u2019s behavior by influencing their\nemotions\nSiri,\nBixby,\nGoogle Assistant,\nCortana\nConversational\nagent\nusers\nMix based on a mixed factorial experiment | 94 students (54\nmale; 40 female) with ages ranging from 19 to 31 (M=22.78,\nSD=2.80)\n[15]\nSurvey\nNot participants\ntesting specific\nroles of LLMs\nLLM helps with translation. Examined\nhow different users (e.g., linguists, en-\ngineers) perceive and adopt natural lan-\nguage generation tools and their percep-\ntion of machine-generated text quality.\nAny natural lan-\nguage generation\ntools\nsuch\nas\nChatGPT\nGeneral users of LLM\nMix | 77 respondents from three groups: 1) linguists, trans-\nlators, interpreters, or related, 2) participants whose field\nof expertise is Artificial Intelligence, Natural Language Pro-\ncessing, Computer Science, Software Engineering, or similar\nfields, and 3) participants in any other field (not reported).\n[10]\nNo specific contexts. Examined actively\ndebated issues about LLM (e.g., efficacy,\nethics, and intellectual abilities of lan-\nguage models)\nAny LLMs\nNLP researchers\nQuan | 327 researchers co-authored at least 2 ACL publica-\ntions between 2019 and 2022; Academia 73%, Industry 22%,\nNon-profit 4%; Senior/faculty 41%, Junior/postdoc 23%, PhD\nstudent 33%, Masters student 2%, Undergraduate 1%, NA\n1%; United States 58%, Europe 23%, Asia/Pacific 8%, Middle\nEast/North Africa 5%, Canada 2%, South America/Caribbean\n1%, Sub-Saharan Africa 0.3%, Prefer not to say 2%; Man 67%,\nWoman 25%, Non-binary 3%, Other/Prefer not to say 6%;\nUnderrepresented Minority, Yes 26%, No 63%, NA 11%\n[55]\nhealthcare & medicine, education, manufacturing, science writing, and customer reviews. For instance, researchers evaluated with human evaluators how LLMs could apply therapeutic techniques to reframe people\u2019s cognitive processes for mental health support [70]. In another study, LLM rewrote news about gender violence to influence readers\u2019 perspectives on who should take responsibility for the violence [56]. Researchers also investigated if LLMs can help simplify medical reports for patients [40], or change styles and topics according to different personas for the conversations on different topics [84]. Researchers also investigated how LLMs can conduct Q&A for knowledge-based dialogue [93], help students complete essays and coding [72], and serve as cognitive assistants for knowledge retrieval and troubleshooting and problem-solving [44]. To provide writing assistance, studies examined applying LLMs to provide recommendations for science writing [27], writing customer reviews of restaurants [3], and writing online posts to provide mental health support [62]. To reduce users\u2019 abusive behaviors, researchers investigated the impacts of different styles of responses on humans, such as empathy style, avoidance style, and counterattacking style [15, 17], and aimed to generate safe conversations through human-in-the-loop methodology to evaluate the responses [90]. The models used to test these LLMs include GPT-based models such as GPT-2 [64], GPT-3 [9], GPT-3.5 [29], ChatGPT [14], and DialoGPT [96]. Other LLMs included mBART [78], roBERTa [4], GLM-Dialog [93], BERT [20], BST 2.7B [67], Kneser-Ney language model [34], and neural encoder-decoder model [84]. LLM applications, such as Alexa, Siri, Cortana, and Google assistants, were also used in the studies. Target users of these LLM applications included journalists, patients, students, writers, mental health support seekers and providers, text-based bot users, and conversational agent users as Table 2 shows. In evaluating LLMs, crowd workers and students from universities were the most frequently tested samples. Some studies did not provide details of the demographic statistics of participants and the sample sizes.\n# 4.3 People\u2019s Perceptions of LLMs\nWe found four salient themes around how the reviewed papers studied people\u2019s perception of LLMs about 1) performances and 2) its anthropomorphism. We also found 3) factors that influence people\u2019s perceptions and expectations and 4) concerns for LLMs. Appendix A.2 introduces the structure of the affinity diagram and the organization of our findings.\n4.3.1 Toward LLM\u2019s Performances. The papers shared how people perceived its advantages and biases. However, these perceptions contradicted one another, depending on the tasks, domains, topics, and individual factors. Perceived Advantages. LLMs were perceived as a tool that saves time and gives access to the most up-to-date information quickly. Furthermore, participants revealed that LLMs were robust enough to respond to poorly phrased questions, unlike other information tools (e.g., Google). LLM could also enhance peoples\u2019 confidence in writing and communication across cultures while providing compatibility with various user interfaces. Participants in studies that used LLMs for translations and posting in online communities for peer support perceived that LLMs could help them save time by efficiently completing tedious tasks [3, 10]. Depending on the topic, participants felt that LLMs could offer more accurate information than other internet-based information tools (e.g., Wikipedia). For example, participants in a study applying LLMs for scientific writing discovered that LLM-generated responses were more up-to-date on modern psychology topics when compared to those on Wikipedia [27]. For robustness, factory representatives (e.g., from detergent factories and textile factories) in a study were satisfied with the system\u2019s ability to interpret some of their poorly phrased questions [44]. Participants in the study that used LLMs to provide suggestions in post-writing to support their peers in online communities stated that LLMs could enhance their confidence and satisfaction in writing [62]. Moreover, some participants highlighted the ease of cross-cultural communication, especially Manuscript submitted to ACM\nwhen there is an obvious language barrier [10]. Some participants mentioned that they were able to concentrate better on scientific writing tasks with LLM-integrated user interfaces than using web search. For instance, a participant noted that even though their generations and interactions with the LLM were not as good as Google in terms of accuracy, they were able to maintain focus on their own writing without being distracted by subsequent web searches [27]. Perceived Bias in LLMs. The studies reported that there was a lack of understanding and awareness among the participants on how bias affects LLM results, although a few participants were aware that inherent biases of people can influence AI models [10]. People perceived distribution bias as a disadvantage since it led to a lack of diversity and generated overly generic responses. For example, when using LLMs for scientific writing, most participants found that generated outputs were \u201cless diverse than a Google search\u201d [27]. Moreover, participants in the study that used LLMs for writing restaurant reviews reported that the lack of specificity in the generated responses made it challenging to integrate them into their own writing [3]. The quality of responses could depend on how people prompt LLMs. Although most of the participants agreed that machine-generated texts are, in fact, gender-biased\u2014however, half of them stated that even though natural language generation is inherently gender biased, it is as good as humans, or better, in avoiding gender bias [10]. People tended to perceive biases when they felt out of control of the system and failed to achieve the desired outcomes for their given tasks. For example, some participants in the study that used LLMs for scientific writing reported they eventually gave up after trying different prompts multiple times because they \u201ccould not get the prompts to give me that spark [I wanted]\u201d [27]. On the other hand, some participants from another study that used LLMs for writing restaurant reviews stated that, at times, they felt like the words predicted by LLMs were guiding their own writing [3]. Several users reportedly felt less in control when using LLMs to complete specific tasks, whereas they felt fully in control when using Google for idea generation [27]. These experiences might have been a result of \u201challucination\u201d and/or biased system behaviors, but participants did not report them as biases of LLMs. Conflicting LLM Performances LLM\u2019s performances can be measured through task- and domain-dependent measures, such as accuracy, coherence, and impacts, or subjective measures, such as appropriateness, preference, efficiency, effectiveness, and explainability. The results of these measures have varied, often conflicting across the studies. For task- and domain-dependent measures, variances in LLMs performances across tasks and domains were attributed to the limitations and shortcomings of LLMs. For example, LLMs performed well for Q&A [44] and translation but were not performing well for simplifying medical reports [40]. Also, factory workers applied LLMs as cognitive assistants for troubleshooting and were impressed by the accuracy of the responses [44]. When asked to complete translation tasks, one-third (33%) of a study\u2019s participants reported that AI-generated text accuracy was equally good\u2013if not better\u2013than human-written text. However, 19% of participants expressed uncertainty regarding potential disparities in machine translation performance, especially concerning certain languages, language pairs, or specific domains [10]. However, in medical contexts, radiologists highlighted encountering inaccurate passages. They also identified missing relevant information as well as potentially harmful conclusions, including misinterpretation of medical terms, imprecise language, hallucinations, odd language, missing correct information about crucial patients\u2019 medical information, and missing key medical findings within the simplified medical reports generated [40]. The perceived coherence of LLMs also showed variation across topics, tasks, and different prompts. Participants using LLMs for scientific writing reported that the coherence of these models is effective for specific subjects but less so for others. For instance, they noted high coherence scores for computer security and green marketing topics, while Manuscript submitted to ACM\ndynein and automata theory received low coherence scores [27]. Meanwhile, they also stated that certain prompt templates were more effective for specific topics compared to others such as \u201cOne attribute of X is\u201d prompt templates worked better for source code than old-growth forests topics[27]. People\u2019s perceived impacts of LLMs also varied among tasks and different groups. For example, students and educators felt that LLMs impacted the assessments differently as essays (including reports, literature reviews, case studies, and research papers), computer code (including pseudo-code and mathematical proofs), short-answer and multiple-choice questions were very or at least moderately impacted by LLMs. The assessments that required product design or creative/artistic work were moderately impacted by LLMs, but other assignment types such as presentations, and discussions that were either pre-recorded or live were not impacted by LLMs [72]. For the subjective measures, such as appropriateness, preference, efficiency, effectiveness, and explainability, the variations in perceived performances of LLMs among individuals may be attributed to personal factors, including demographics within social and cultural contexts. For example, variations were observed in how participants rated the appropriateness of LLMs. When using different commercial conversational agents such as Siri, Cortana, etc. to respond in different styles such as empathy, avoidance, and counterattacking to respond to verbal abuse, most participants preferred the empathy style and enjoyed chatting with the conversational agents while some participants rated counterattacking style as the most appropriate, stating that the \u201ceye for an eye\u201d style responses made them regret their actions [15]. Furthermore, Gen Z rated the avoidance strategies adopted by the commercial conversational agents for verbal abuse responses significantly lower than other groups of different ages, whereas older people thought humorous responses to harassment were highly inappropriate [17]. Individual backgrounds could also influence the perception\u2013in the survey study using LLMs for translations, participants with expertise in AI, NLP, Computer Science, Software Engineering, and related fields were generally more positive towards LLMs and their potential applications than participants with expertise in linguistic-related fields and participants with expertise in other fields [10]. Educators and students showed different preferences for prompts, collaborative LLMs\u2019 responses, and response styles in applying LLMs for essays and coding assessments [72]. The perception of LLM\u2019s efficiency varied among people as well. Some participants considered LLMs to be efficient by expressing that \u201c[it] helped me save a lot of time.\u201d [3], however, some others reported concerns of wasting time, stating: \u201cbut I think I didn\u2019t save much time using it, as I was constantly only looking whether the word I was wanting to write appeared in the box\u201d [3]. This holds true in terms of both effectiveness and explainability. For example, when using LLMs in scientific writing, participants\u2019 reports of usefulness varied [27]. Perceptions of LLM\u2019s explainability differ among people with different levels of expertise. In a survey exploring people\u2019s perceptions of LLMs explainability, approximately one-third of respondents believed that ChatGPT clearly communicates its decision-making process to its intended users. Among these participants, those with expertise in AI, NLP, Computer science, software engineering, or similar fields reported a higher percentage of belief in ChatGPT\u2019s explainability compared to those in linguistics, while people outside of computing and linguistic communities had the lowest percentage (26%) of respondents who believed in ChatGPT\u2019s explainability [10]. 4.3.2 People\u2019s Perceptions of LLM\u2019s Perceived Athropomorphism. The perceived anthropomorphism (i.e., being humanlike) of LLMs is a unique performance for technology solutions imitating humans, including likability, perceived intelligence, tone clarity, and distinguishability from humans. In general, participants had a positive experience with existing commercial conversational agents, stating that these models were interesting, likable, and not boring to interact with [15]. The perceived intelligence and tone clarity were not associated with likability. For example,\n4.3.2 People\u2019s Perceptions of LLM\u2019s Perceived Athropomorphism. The perceived anthropomorphism (i.e., being human like) of LLMs is a unique performance for technology solutions imitating humans, including likability, perceived intelligence, tone clarity, and distinguishability from humans. In general, participants had a positive experience with existing commercial conversational agents, stating that these models were interesting, likable, and not boring to interact with [15]. The perceived intelligence and tone clarity were not associated with likability. For example\namongst the different types of commercial conversational agents that responded to verbal abuse, participants rated the counterattacking responses as the same as the empathy style in terms of anthropomorphism and tone clarity [15]. Nowadays, people can still distinguish LLMs from humans in certain tasks [27]. For example, some participants reported seeing distinctions in scientific writing when comparing texts written by humans versus texts generated by LLMs [27]. Moreover, NLP researchers held different opinions on the language comprehension capabilities of LLMS. This division was nearly equal, with half in agreement [55].\n4.3.3 Factors Influencing People\u2019s Perceptions. We found some hidden factors that may help explain the diversity in people\u2019s perceptions of LLMs\u2019 performances. These factors include diverse contextual needs, varying expectations of LLMs, and distinct mental models of LLMs. People\u2019s needs in context showed dynamic patterns. For example, according to 2067 Mental Health American website visitors, they preferred medium-readability, lower-positivity, highly empathic, and highly specific reframes when applying LLMs to reframe negative thoughts for mental health support, showing non-linear patterns across different dimensions. Such needs are challenging to transfer to a mathematical problem with an objective function for models to train since multiple facets of needs should be first investigated. Furthermore, people had different expectations of what LLMs should or should not do. For example, one participant in the included study investigating LLM applications in science writing stated that LLMs should not take over something that is fundamental to humanity\u2019s work [27]. In the survey investigating people\u2019s perceptions of LLMs in translation tasks, some participants added that LLMs should be used for good and not for malicious purposes (e.g., generating knowledge and not the detriment of it) [10]. As people increasingly used LLMs and commercial conversational agents, they began to form their own mental models to conceptualize how LLMs would work. For example, some participants thought that LLMs were impartial, and \u201cassume ChatGPT is an objective entity\u201d [10], which may explain why people thought LLMs were better than humans at dealing with social biases. In other aspects, people had different mental models of whether LLMs were human-like. In the study investigating different styles of conversational agent responses to abusive behaviors, some participants stated that LLMs as machines \u201cshould not display negative emotions toward the user\u201d while some other participants preferred that LLMs should be very human-like and it was totally acceptable and interesting at the same time for LLMs to generate counterattacking responses [15]. Some people even expected LLMs to understand the emotional landscape of the user [10].\n4.3.4 People\u2019s Concerns in LLMs. Considering the existing perceived performances with hidden factors of people\u2019s perceptions, people had concerns about regulation and data protection, and potential negative impacts of LLM applications, but with an overtrust phenomenon in LLMs. With the emergence of generative AI and commercial LLM-based applications, there were evident concerns regarding usage regulation and data protection. When discussing ethics, and LLM\u2019s control of usages, students and educators had ethical concerns about using LLMs\u2013whether or not they should be allowed to use generative AI tools [72]. Similarly, writers and translators had concerns about unintentional plagiarism and authorship [27]. People applying LLMs in post-writing for peer support raise concerns about the lack of sincerity and authorship [62]. Furthermore, people also had concerns over control and regulation for potential misuse. For example, participants expressed the need for transparency and data protection, with some even stating that LLM-based commercial applications should not be publicly released until ethical issues are mitigated [10]. Manuscript submitted to ACM\nFor data protection, people had concerns about privacy issues when using LLMs for mental health support [62] and using personal information and data [10], the monopoly of companies\u2019 singular control of foundational systems such as ChatGPT [55]. Factory representatives expressed concerns related to confidential information and proprietary knowledge being leaked [44]. However, opinions on industry influences varied significantly among job sectors, with 82% of respondents in academia agreeing that private firms have too much influence, compared to only 58% of respondents in industry [55]. People also had concerns about LLMs in terms of potential negative impacts, including lack of fact-checking, unintended harms, and potential decline in people\u2019s capabilities (i.e., compromised work quality and job loss). In terms of fact-checking, participants overtrusted LLMs because they believed them to be impartial, \u201cunfiltered information being taken as truthful\u201d, and \u201cforgetting how to gather information or recognize misinformation due to frequent use of ChatGPT\u201d [10]. However, overtrust issues were influenced by people\u2019s backgrounds and experiences. Participants with more experience in information and communications technology reported lower levels of trust in natural language generation tools compared to those with less experience [10]. Furthermore, people\u2019s concerns about the potential negative impacts of LLMs extended to topics such as misinformation and positive feedback loops, where \u201cthe bias will amplify\u201d [10]. When evaluating potential negative impacts of LLMs, people perceived unintended harms as a significant factor, including topics like safety (LLMs are susceptible to unintended harmful attacks), brand images, and the fact that more needs to be done to address potential risks\u2013and that this gap should be bridged [10]. Factory representatives expressed concerns that prompts made by their employees may negatively impact their company\u2019s brand image [44]. According to crowd workers evaluation, LLMs were susceptible to attack, the safe responses generated by LLMs were no more than 60% [90] and participants expressed concerns that the dimensions assessed by LLMs were insufficient to capture the subtle usage of language when offering peer support for mental health [62]. During creative writing and other goal-oriented tasks, some students reported concerns about a loss of creativity [72] and some participants had concerns that the natural language generation tools would result in less critical thinking, memory development, knowledge, and deductive capability, correlation thinking, and losing writing and communication skills [10]. They also expressed concerns about a decrease in efforts to study different languages and a \u201cdecrease in creativity and an overall trend towards simplistic, repetitive content\u201d [10]. Furthermore, some participants had concerns about the quality of work being sacrificed for time-saving, and immediate results. For example, some participants worried that natural language generation tool users \u201cwould be sacrificing quality translations for immediate results, making translators lose jobs while having subpar machine translations\u201d and \u201cthe use of natural language synthesis might devalue the position of experts\u201d. Moreover, participants in a survey study expressed significant concern about job losses, particularly the potential for LLMs to replace human translators. These concerns are rooted in the growing automation trends since LLMs are increasingly occupying roles that were once held by humans in the business sector [10].\n# 5 DISCUSSION\n# 5.1 Gaps in Studying Human Experiences and Perceptions Toward Biases of LLM\nBiases have been defined and studied in many contexts. For instance, Ferrara reviewed the source of biases for LLMs, including training data, algorithms, labeling and annotation, product design decisions, and policy decisions [23]. For instance, algorithms can assign greater significance to specific features or data points [23], or the subjective preferences Manuscript submitted to ACM\nBiases have been defined and studied in many contexts. For instance, Ferrara reviewed the source of biases for LLMs, including training data, algorithms, labeling and annotation, product design decisions, and policy decisions [23]. For instance, algorithms can assign greater significance to specific features or data points [23], or the subjective preferences\nof human annotators can bias the training data, favoring particular use cases or tailoring user interfaces for specific demographics or industries [23], and developers could establish policies to either restrict or promote specific model behaviors [23]. Blodgett et al., also similarly described how the biases could stem from the source material or selection process for training data [7]. At the same time, in a survey of 146 papers examining \u201cbias\u201d in NLP systems, findings showed the research motivations were frequently unclear, inconsistent, and lacking. Additionally, they pointed out a deficiency in the proper use of quantitative techniques for assessing or mitigating \u201cbias\u201d; and they have limited engagement with relevant literature beyond the scope of NLP [7]. Our results showed similar challenges. The concept of bias and related terms was illdefined, where, out of 15 included papers, only two papers provided the definitions of bias and related terms in their papers [3, 10]. Papers used terms related to bias (i.e., bias, social harm, stereotype, stigma, fair, norm, ethic, and safety) without explicitly defining them. Borrowing the recommendations Blodgett et al. suggested to the NLP community, researchers should be able to articulate their conceptualizations of \u201cbias\u201d or any other related concepts in terms of \u201cwhat kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements\u201d [7]. Because of the lack of articulation on what is considered bias, or other related terms, there is a lack of standardized measurements of these concepts. The studies in this review investigated different LLM performances such as coherence, efficiency, accuracy, appropriateness, likability, efficiency, usefulness, etc. For people\u2019s controversial perceptions we discovered, that some performances were task-and-domain-dependent such as accuracy, coherence, and LLM impacts on tasks, while some performances were individual-dependent such as likability, appropriateness, and efficiency. These measurements need to be investigated in conjunction with researchers\u2019 articulation of the definitions of bias while considering individual factors and application context to establish how these measures can contribute to understanding bias. Although task-and-domain-dependent performance can be measured through metrics and ground truth (i.e., accuracy), for individual-dependent performances, researchers should collect large enough samples of diverse participants regarding demographic distribution. Also, rich qualitative data about individual contexts and reasoning behind perceptions can ensure the successful conceptualization, measurement, and evaluation of biases. Most studies had limited sample sizes and failed to gather demographic information. This finding on the lack of evidence on the involvement of human subjects in LLM research shows that we are still at an early stage of testing people\u2019s perceptions and experiences around LLM. Furthermore, a lack of data on who the human evaluators were shows our limited focus on social-cultural factors that are critical to how people form perceptions.\n# ptions of LLMs due to Imperfect LLM Performance and Hidden Factors\n# ynamic Perceptions of LLMs due to Imperfect LLM Performance and H\nWe found multiple factors influence people\u2019s perceptions. Contexts, tasks, users\u2019 needs, expectations, and mental models to characterize LLMs will influence people\u2019s perceptions of LLMs. However, subjective people\u2019s perceptions of usefulness, efficiency, effectiveness, appropriateness, and preferences of LLMs require more investigations of user experience in interacting with LLMs. There are characteristics some people like while others don\u2019t, such as the extent of anthropomorphism, since people have different opinions of how human-like LLMs should be. For instance, regarding the perceptions of the appropriateness of LLMs in generating abusive behavior responses, some participants thought it was appropriate to apply counterattacking styles to respond to abusive behaviors since it is natural for humans to adopt the \u201ceye for an eye\u201d tactic while some participants thought it was inappropriate and LLMs should always be nice to people. Manuscript submitted to ACM\nptions Toward Bias and Related Concepts in Large Language Models: A System\nSuch a variety of users\u2019 needs, expectations, and mental models will influence how people perceive and interact with technology solutions, impacting user experience. Researchers studied people\u2019s mental models of AI in a cooperative word-guessing game and found that those who won more often had better estimates of the AI agent\u2019s abilities [26]. For gestural interactions with spherical displays, researchers found that children and adults had different mental models in the way they verbalized their perceptions about collaborating around the sphere and the physical affordances of the spherical form factor strongly influenced the way both children and adults conceptualized interaction [73]. Based on the included studies, people conceptualized LLMs as an alternative to Google search [27] or Wikipedia [27], resulting in a problematic mental model considering the \u201challucination\u201d phenomenon. With the integration of LLMs into different interfaces such as keyboards and mobile UIs, further adaptions or transitions of users\u2019 mental models are required in the design process. To deliver transparency of LLMs, education could be delivered to users through engaging ways, such as teaching users about each facet of machine learning through the medium of art. This approach hypothesizes that addressing the challenges related to explainability and accountability in AI systems can be facilitated through art and tangible experiences that bridge the gap between computer code and human comprehension [35].\n# 5.3 Implications for CHI\nLLMs gain growing interest in the CHI community. Researchers investigated how prompt engineering can benefit users during the Human-LLM interaction [88]. Furthermore, Wang et al. investigated adapting LLMs to mobile User Interfaces (UIs), bridging the gap in NLP and graphical UIs [83]. Other LLM research at CHI includes a variety of application areas such as writing [18, 63], coding [52], supporting public health interventions [41], and enhancing augmentative and alternative communication [81]. Despite the potential, successful adoption of these tools would require people\u2019s trust in these systems. Researchers define human-centered AI as supporting its human users \u201cwhile revealing its underlying values, biases, limitations, and the ethics of its data gathering and algorithms to foster ethical, interactive, and contestable use\u201d [12], we are still at an early stage of testing how people perceive and experience bias in LLMs and being able to communicate these limitations and biases to users effectively [49, 89]. Our review found people\u2019s concerns toward LLM and the various aspects of its performance, including coherence, appropriateness, and explainability. Given that LLMs are new and still continuously evolving, continued research should be done to further understand how people\u2019s perceptions of these systems also evolve together, and how these negative perceptions can be addressed through novel design solutions. For instance, prompt engineering requires a complex understanding of the model, and users met challenges in generating, evaluating, and explaining prompts\u2019 effects [91], indicating the necessity of understanding users\u2019 mental models of how LLMs work and devising solutions to address user perceptions that might generate challenges around over- or under-trust of these systems. Meanwhile, such efforts of mitigating the biases stemming from users are meaningful if the biases inherent in LLMs are also actively being addressed\u2013and this line of work is actively being investigated by the researchers (i.e., ACM Conference on Fairness, Accountability, and Transparency )[22]. Our goal here as a CHI community dedicated to understanding people and improving their experiences would be to articulate and further examine the different types of negative effects that LLMs have for people, how people perceive them, and how people\u2019s experiences can be improved and ensured with safety.\nManuscript submitted to ACM\n# 6 LIMITATIONS AND FUTURE WORK\nThough we employed a systematic approach to select keywords related to fairness, bias, privacy, and ethics in the context of using LLMs, this method may not have captured all pertinent literature on human perception of using LLM applications. Articles that include different terminology or focus on nuanced aspects of the related topics may have been inadvertently omitted. Future work can further develop a taxonomy that categorizes diverse human perceptions when using LLM applications in various contexts.\n# 7 CONCLUSION\nThis paper presented a systematic review of the empirical studies on people\u2019s perceptions of LLMs. We screened 15 papers from a total of 231 papers, analyzing different themes around the concept of bias and related terms, and how the concepts were articulated and tested by participants in the included papers. By presenting the application areas, roles of LLMs, LLM models, target users of the LLM application, and the evaluators of the LLMs, we provide insights into what we know about people\u2019s perceptions of bias toward LLMs. We discuss how people\u2019s perceptions of LLMs are influenced by context factors such as tasks, domains, user needs, expectations, and mental models, which are often contradictory. However, much of the work is missing detailed information on who the evaluators were, leaving gaps in defining bias and related concepts, measuring those concepts, and understanding individual factors that might affect these perceptions. More empirical work with humans and taxonomy and methodologies for user interaction with LLMs considering biases are needed for future research.\n# REFERENCES\n[1] Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Large language models associate Muslims with violence. Nature Machine Intelligence 3, 6 (2021), 461\u2013463. [2] Gordon Willard Allport, Kenneth Clark, and Thomas Pettigrew. 1954. The nature of prejudice. (1954). [3] Kenneth C Arnold, Krysta Chauncey, and Krzysztof Z Gajos. 2018. Sentiment Bias in Predictive Text Recommendations Results in Biased Writing. In Graphics interface. ACM, Toronto, Ontario, Canada, 42\u201349. [4] Francesco Barbieri, Jose Camacho-Collados, Leonardo Neves, and Luis Espinosa-Anke. 2020. TweetEval: Unified Benchmark and Comparative Evaluation for Tweet Classification. arXiv:2010.12421 [cs.CL] [5] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big?. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. ACM, Virtual Event, Canada, 610\u2013623. [6] Steven Bird, Robert Dale, Bonnie J Dorr, Bryan R Gibson, Mark Thomas Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir R Radev, Yee Fan Tan, et al. 2008. The ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics.. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908). European Language Resources Association (ELRA), Marrakech, Morocco, 1755\u20131759. [7] Su Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III au2, and Hanna Wallach. 2020. Language (Technology) is Power: A Critical Survey of \"Bias\" in NLP. arXiv:2005.14050 [cs.CL] [8] Eleftheria Briakou, Sweta Agrawal, Ke Zhang, Joel Tetreault, and Marine Carpuat. 2021. A Review of Human Evaluation for Style Transfer. arXiv:2106.04747 [cs.CL] [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33 (2020), 1877\u20131901. [10] Beatriz Cabrero-Daniel and Andrea Sanagust\u00edn Cabrero. 2023. Perceived Trustworthiness of Natural Language Generators. In Proceedings of the First International Symposium on Trustworthy Autonomous Systems (Edinburgh, United Kingdom) (TAS \u201923). Association for Computing Machinery, New York, NY, USA, Article 23, 9 pages. https://doi.org/10.1145/3597512.3599715 [11] Yong Cao, Li Zhou, Seolhwa Lee, Laura Cabello, Min Chen, and Daniel Hershcovich. 2023. Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. arXiv:2303.17466 [cs.CL] [12] Tara Capel and Margot Brereton. 2023. What is Human-Centered about Human-Centered AI? A Map of the Research Landscape. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 359, 23 pages. https://doi.org/10.1145/3544548.3580959 Manuscript submitted to ACM\n[13] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2023. A Survey on Evaluation of Large Language Models. arXiv:2307.03109 [cs.CL] [14] ChatGPT. 2023. ChatGPT. Retrieved August 17, 2023 from https://openai.com/blog/chatgpt [15] Hyojin Chin, Lebogang Wame Molefi, and Mun Yong Yi. 2020. Empathy Is All You Need: How a Conversational Agent Should Respond to Verbal Abuse. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201313. https://doi.org/10.1145/3313831.3376461 [16] Helen Crompton and Diane Burke. 2023. Artificial intelligence in higher education: the state of the field. International Journal of Educational Technology in Higher Education 20, 1 (2023), 1\u201322. [17] Amanda Cercas Curry and Verena Rieser. 2019. A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents. arXiv:1909.04387 [cs.HC] [18] Hai Dang, Sven Goller, Florian Lehmann, and Daniel Buschek. 2023. Choice Over Control: How Users Write with Large Language Models Using Diegetic and Non-Diegetic Prompting. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 408, 17 pages. https://doi.org/10.1145/3544548.3580969 [19] Kahneman Daniel. 2017. Thinking, fast and slow. [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv:1810.04805 [cs.CL] [21] Silvana Di Gregorio. 2000. Using Nvivo for your literature review. In Strategies in qualitative Research: Issues and Results from Analysis Using QSR NVivo and NUD* IST Conference at the institute of Education, London. Institute of Education, London, 29\u201330. [22] ACM FAccT. 2023. ACM Conference on Fairness, Accountability, and Transparency. Retrieved August 30, 2023 from https://facctconference.org/ [23] Emilio Ferrara. 2023. Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models. arXiv:2304.03738 [cs.CY] [24] Mohammad Fraiwan and Natheer Khasawneh. 2023. A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions. arXiv:2305.00237 [cs.CY] [25] Ismael Garrido-Mu\u00f1oz, Arturo Montejo-R\u00e1ez, Fernando Mart\u00ednez-Santiago, and L Alfonso Ure\u00f1a-L\u00f3pez. 2021. A survey on bias in deep NLP. Applied Sciences 11, 7 (2021), 3184. [26] Katy Ilonka Gero, Zahra Ashktorab, Casey Dugan, Qian Pan, James Johnson, Werner Geyer, Maria Ruiz, Sarah Miller, David R. Millen, Murray Campbell, Sadhana Kumaravel, and Wei Zhang. 2020. Mental Models of AI Agents in a Cooperative Game Setting. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201312. https://doi.org/10.1145/3313831.3376316 [27] Katy Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022. Sparks: Inspiration for Science Writing Using Language Models. In Proceedings of the 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia) (DIS \u201922). Association for Computing Machinery, New York, NY, USA, 1002\u20131019. https://doi.org/10.1145/3532106.3533533 [28] Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, et al. 2023. How does ChatGPT perform on the United States medical licensing examination? The implications of large language models for medical education and knowledge assessment. JMIR Medical Education 9, 1 (2023), e45312. [29] GPT3.5. 2023. GPT3.5 API. Retrieved August 17, 2023 from https://platform.openai.com/docs/guides/gpt/chat-completions-api [30] Philipp Hacker, Andreas Engel, and Marco Mauer. 2023. Regulating ChatGPT and Other Large Generative AI Models. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT \u201923). Association for Computing Machinery, New York, NY, USA, 1112\u20131123. https://doi.org/10.1145/3593013.3594067 [31] Muhammad Usman Hadi, Qasem al tashi, Rizwan Qureshi, Abbas Shah, Amgad Muneer, Muhammad Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, and Seyedali Mirjalili. 2023. Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects. https://doi.org/10.36227/techrxiv.23589741.v3 [32] Abid Haleem, Mohd Javaid, and Ravi Pratap Singh. 2022. An era of ChatGPT as a significant futuristic support tool: A study on features, abilities, and challenges. BenchCouncil transactions on benchmarks, standards and evaluations 2, 4 (2022), 100089. [33] Perttu H\u00e4m\u00e4l\u00e4inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating Large Language Models in Generating Synthetic HCI Research Data: A Case Study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 433, 19 pages. https://doi.org/10.1145/3544548.3580688 [34] Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). Association for Computational Linguistics, Sofia, Bulgaria, 690\u2013696. [35] Drew Hemment, Morgan Currie, SJ Bennett, Jake Elwes, Anna Ridler, Caroline Sinders, Matjaz Vidmar, Robin Hill, and Holly Warner. 2023. AI in the Public Eye: Investigating Public AI Literacy Through AI Art. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (Chicago, IL, USA) (FAccT \u201923). Association for Computing Machinery, New York, NY, USA, 931\u2013942. https://doi.org/10.1145/3593013.3594052 [36] David M. Howcroft, Anya Belz, Miruna Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty Years of Confusion in Human Evaluation: NLG Needs Evaluation Sheets and Standardised Definitions. In Proceedings of the 13th International Conference on Natural Language Generation, Brian Davis, Yvette Graham, John Kelleher, and Yaji Sripada (Eds.). Association for Computational Linguistics, Dublin, Ireland, 169\u2013182. Funding Information: Howcroft and Rieser\u2019s contributions were supported\nunder EPSRC project MaDrIgAL (EP/N017536/1). Gkatzia\u2019s contribution was supported under the EPSRC project CiViL (EP/T014598/1). Mille\u2019s contribution was supported by the European Commission under the H2020 contracts 870930-RIA, 779962-RIA, 825079-RIA, 786731-RIA. Publisher Copyright: \u00a9 2020 Association for Computational Linguistics; 13th International Conference on Natural Language Generation 2020, INLG 2020 ; Conference date: 15-12-2020 Through 18-12-2020. [37] Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl. 2020. Social Biases in NLP Models as Barriers for Persons with Disabilities. arXiv:2005.00813 [cs.CL] [38] Samireh Jalali and Claes Wohlin. 2012. Systematic Literature Studies: Database Searches vs. Backward Snowballing. In Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement (Lund, Sweden) (ESEM \u201912). Association for Computing Machinery, New York, NY, USA, 29\u201338. https://doi.org/10.1145/2372251.2372257 [39] Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An introduction to statistical learning. Vol. 112. Springer. [40] Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa St\u00fcber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Ricke, and Michael Ingrisch. 2022. ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports. arXiv:2212.14882 [cs.CL] [41] Eunkyung Jo, Daniel A. Epstein, Hyunhoon Jung, and Young-Ho Kim. 2023. Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 18, 16 pages. https://doi.org/10.1145/ 3544548.3581503 [42] Marzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. arXiv:2304.03245 [cs.CL] [43] Staffs Keele et al. 2007. Guidelines for performing systematic literature reviews in software engineering. [44] Samuel Kernan Freire, Mina Foosherian, Chaofan Wang, and Evangelos Niforatos. 2023. Harnessing Large Language Models for Cognitive Assistants in Factories. In Proceedings of the 5th International Conference on Conversational User Interfaces (Eindhoven, Netherlands) (CUI \u201923). Association for Computing Machinery, New York, NY, USA, Article 44, 6 pages. https://doi.org/10.1145/3571884.3604313 [45] Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic Dreyer, Aleksandar Shtedritski, and Yuki Asano. 2021. Bias out-of-the-box: An empirical analysis of intersectional occupational biases in popular generative language models. Advances in neural information processing systems 34 (2021), 2611\u20132624. [46] Harsh Kumar, Yiyi Wang, Jiakai Shi, Ilya Musabirov, Norman A. S. Farb, and Joseph Jay Williams. 2023. Exploring the Use of Large Language Models for Improving the Awareness of Mindfulness. In Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI EA \u201923). Association for Computing Machinery, New York, NY, USA, Article 129, 7 pages. https://doi.org/10.1145/3544549.3585614 [47] Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepa\u00f1o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. 2023. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLoS digital health 2, 2 (2023), e0000198. [48] Thomas Lancaster. 2023. Artificial intelligence, text generation tools and ChatGPT\u2013does digital watermarking offer a solution? International Journal for Educational Integrity 19, 1 (2023), 10. [49] Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>) (CHI \u201920). Association for Computing Machinery, New York, NY, USA, 1\u201315. https://doi.org/10.1145/3313831.3376590 [50] ACM Digital Library. 2023. About the ACM Digital Library. Retrieved August 17, 2023 from https://dl.acm.org/about [51] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out. Association for Computational Linguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013 [52] Michael Xieyang Liu, Advait Sarkar, Carina Negreanu, Benjamin Zorn, Jack Williams, Neil Toronto, and Andrew D. Gordon. 2023. \u201cWhat It Wants Me To Say\u201d: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI \u201923). Association for Computing Machinery, New York, NY, USA, Article 598, 31 pages. https://doi.org/10.1145/3544548.3580817 [53] Robert W McGee. 2023. Is Chat Gpt Biased Against Conservatives? An Empirical Study. http://dx.doi.org/10.2139/ssrn.4359405 [54] Yusuf Mehdi. 2023. Reinventing search with a new AI-powered Microsoft Bing and Edge, your copilot for the web. https://blogs.microsoft.com/ blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/ [55] Julian Michael, Ari Holtzman, Alicia Parrish, Aaron Mueller, Alex Wang, Angelica Chen, Divyam Madaan, Nikita Nangia, Richard Yuanzhe Pang, Jason Phang, and Samuel R. Bowman. 2022. What Do NLP Researchers Believe? Results of the NLP Community Metasurvey. arXiv:2208.12852 [cs.CL] [56] Gosse Minnema, Huiyuan Lai, Benedetta Muscato, and Malvina Nissim. 2023. Responsibility Perspective Transfer for Italian Femicide News. arXiv:2306.00437 [cs.CL] [57] MIRO. 2023. Miro: The Visual Collaboration Platform for Every Team. https://miro.com/ [58] KK Mueen Ahmed and Bandar E Al Dhubaib. 2011. Zotero: A bibliographic assistant to researcher. Journal of Pharmacology and Pharmacotherapeutics 2, 4 (2011), 304\u2013305. [59] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2023. A Comprehensive Overview of Large Language Models. arXiv:2307.06435 [cs.CL]\n[60] Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser. 2017. Why we need new evaluation metrics for NLG. [61] Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, Jennifer M Tetzlaff, Elie A Akl, Sue E Brennan, et al. 2021. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. International journal of surgery 88 (2021), 105906. [62] Zhenhui Peng, Qingyu Guo, Ka Wing Tsang, and Xiaojuan Ma. 2020. Exploring the Effects of Technological Writing Assistance for Support Providers in Online Mental Health Community. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI \u201920). Association for Computing Machinery, New York, NY",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to understand people's perceptions toward large language models (LLMs) specifically regarding bias, stereotypes, social norms, and safety. It addresses knowledge gaps about how individuals experience LLMs in everyday applications.",
            "scope": "The survey includes empirical studies that assess human evaluators' experiences with LLMs, particularly focusing on biases and related concepts. It excludes studies that do not involve human evaluations or that address other aspects of LLMs outside of bias and perception."
        },
        "problem": {
            "definition": "The survey focuses on the problem of biases inherent in LLMs and how these biases affect user perceptions and experiences.",
            "key obstacle": "Key challenges include the complexity of sociocultural factors, the integration of diverse tasks within a single model, and the varying opinions and preferences of individual users."
        },
        "architecture": {
            "perspective": "The survey categorizes existing research on LLMs into themes related to biases, user perceptions, and the implications of LLM applications in various contexts.",
            "fields/stages": "The survey organizes the current methods and research into four broader application areas: content transformation, question & answering, writing assistance, and responses to user abusive behaviors."
        },
        "conclusion": {
            "comparisons": "The survey compares various studies on user perceptions of LLMs, highlighting differences in effectiveness and biases perceived across different tasks and contexts.",
            "results": "The overarching conclusions indicate that perceptions of LLMs are influenced by context, tasks, user needs, expectations, and individual experiences, leading to often contradictory views."
        },
        "discussion": {
            "advantage": "Current research has achieved insights into the advantages of LLMs, such as efficiency, enhanced communication, and support in various applications.",
            "limitation": "However, significant limitations exist, including a lack of clarity in defining biases, insufficient demographic data on human evaluators, and variability in user experiences.",
            "gaps": "Gaps in research include a need for more empirical studies on human experiences with LLMs and a clearer articulation of bias definitions and measurements.",
            "future work": "Future research should focus on developing a taxonomy of user perceptions, improving methodologies for evaluating LLM interactions, and addressing the biases inherent in LLMs."
        },
        "other info": {
            "additional notes": "The survey emphasizes the importance of understanding user perceptions to inform the design of user-centered LLM applications and to mitigate potential biases."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The survey aims to understand people's perceptions toward large language models (LLMs) specifically regarding bias, stereotypes, social norms, and safety."
        },
        {
            "section number": "2.1",
            "key information": "The paper focuses on the problem of biases inherent in LLMs and how these biases affect user perceptions and experiences."
        },
        {
            "section number": "4.1",
            "key information": "Current research has achieved insights into the advantages of LLMs, such as efficiency, enhanced communication, and support in various applications."
        },
        {
            "section number": "8.2",
            "key information": "Key challenges include the complexity of sociocultural factors, the integration of diverse tasks within a single model, and the varying opinions and preferences of individual users."
        },
        {
            "section number": "10.1",
            "key information": "Significant limitations exist, including a lack of clarity in defining biases, insufficient demographic data on human evaluators, and variability in user experiences."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on developing a taxonomy of user perceptions, improving methodologies for evaluating LLM interactions, and addressing the biases inherent in LLMs."
        }
    ],
    "similarity_score": 0.7518125015948197,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/People's Perceptions Toward Bias and Related Concepts in Large Language Models_ A Systematic Review.json"
}