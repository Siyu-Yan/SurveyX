{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.02239",
    "title": "The Benefits of Label-Description Training for Zero-Shot Text Classification",
    "abstract": "Pretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 17-19% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that performs strongly on multiple text domains for a given label set, even improving over few-shot out-of-domain classification in multiple settings.",
    "bib_name": "gao2023benefitslabeldescriptiontrainingzeroshot",
    "md_text": "# The Benefits of Label-Description Training for Zero-Shot Text Classification\nLingyu Gao1, Debanjan Ghosh2\u2020, and Kevin Gimpel1\u2020 1Toyota Technological Institute at Chicago 2Educational Testing Service {lygao, kgimpel}@ttic.edu, dghosh@ets.org\nAbstract\nPretrained language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 17-19% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model\u2019s vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that performs strongly on multiple text domains for a given label set, even improving over few-shot out-of-domain classification in multiple settings.\n 23 Oct 2023\n[cs.CL]\narXiv:2305.02239v2\n# 1 Introduction\nPretrained language models (PLMs) (Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Brown et al., 2020; Raffel et al., 2020) have produced strong results in zero-shot text classification for a range of topic and sentiment tasks, often using a pattern-verbalizer approach (Schick and Sch\u00fctze, 2021). With this approach, to classify the restaurant review \u201cOverpriced, salty and overrated!\u201d, a pattern like \u201cthe restaurant is [MASK]\u201d is appended to the review and verbalizers are chosen for each label (e.g., \u201cgood\u201d for positive sentiment and \u201cbad\u201d for negative). The text is classified by the pretrained masked language modeling (MLM) head to choose\n\u2020 Co-senior authors.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e9c/7e9cec4d-f117-460d-b38e-4f7e24fe6447.png\" style=\"width: 50%;\"></div>\nLabel\nInput\nBusiness\nbusiness\nfinance\nBusiness is the activity of making one\u2019s living\nor making money by producing or buying and\nselling products. . .\nSports\nsports\nracing\nAn athletic activity requiring skill or physical\nprowess and often of a competitive nature, as\nracing, baseball. . .\n<div style=\"text-align: center;\">(a) Topic classification</div>\nLabel\nInput\nVery\nNegative\nawful\nIt was terrible.\nA horrendous experience.\nVery\nPositive\ngreat\nJust fantastic.\nOverall, it was outstanding.\nTable 1: A few examples of LABELDESC training data for topic and sentiment classification.\nthe most probable verbalizer for the [MASK] position.1 Although effective, the approach is sensitive to the choice of specific pattern/verbalizer pairs, with subtle changes in the pattern, the verbalizer, or both, often having a large impact on performance (van de Kar et al., 2022; Perez et al., 2021). To alleviate these issues, we propose a simple alternative approach of training on small curated datasets intended to describe the labels for a task. Unlike typical training datasets, which consist of input texts annotated by hand with labels, our data contains only the descriptions of the labels. We refer to this data as LABELDESC data and show a few examples for topic and sentiment classification in Table 1. For topic classification, we include a few terms related to the label (e.g., \u201cfinance\u201d for \u201cBusiness\u201d, \u201cracing\u201d for \u201cSports\u201d), a definition of\n1Please refer to Schick and Sch\u00fctze (2021) for more details on the pattern-verbalizer approach.\nthe label from dictionary.com (e.g., \u201cAn athletic activity . . . \u201d for \u201cSports\u201d), and a sentence from the opening paragraph of the label\u2019s Wikipedia article (e.g., \u201cBusiness is the activity of ...\u201d for \u201cBusiness\u201d). For sentiment classification, we simply use related terms that capture the specific sentiment (e.g., \u201cterrible\u201d for \u201cVery Negative\u201d) as well as a few hand-crafted templates (e.g., \u201cIt was t.\u201d where t is a related term). Next, we finetune pretrained models using the pattern-verbalizer approach on LABELDESC data and evaluate them for text classification. For topic classification, we use patterns and verbalizers from Schick and Sch\u00fctze (2022) to train on our LABELDESC examples by finetuning the model as well as the MLM head (see Section 3 for details). We refer to training on LABELDESC data as LABELDESCTRAINING. In experiments, we show that LABELDESCTRAINING consistently improves accuracy (average improvement of 17-19%) over zero-shot classification across multiple topic and sentiment datasets (Table 2). We also show that LABELDESCTRAINING can decrease accuracy variance across patterns compared to zero-shot classification (Table 3), thus being less sensitive to the choice of pattern. We then conduct additional experiments to reveal the value of LABELDESCTRAINING under various circumstances. To study the impact of verbalizer choice, we experiment with uninformative (randomly initialized) and adversarial (intentionally mismatched) verbalizers (Section 4.2.1). While accuracy drops slightly, both settings are still much more accurate than zero-shot classification with its original verbalizers. That is, LABELDESCTRAINING is able to compensate for knowledge-free or even adversarial verbalizer choice. We also compare to finetuning a randomly initialized classifier head without any patterns or verbalizers, again finding accuracy to be higher than zero-shot (Section 4.2.2). Collectively, our results demonstrate that LABELDESCTRAINING leads to strong performance that is less sensitive than zero-shot classification in terms of pattern/verbalizer choice, while also not requiring a pretrained MLM head. Since LABELDESC data focuses entirely on the labels without seeking to capture the input text distribution, we would hope that it would exhibit stable performance across datasets with the same labels. So, we compare LABELDESCTRAINING to the approach of training on a small super-\nvised training set from one domain and testing on another (Section 4.2.4). In multiple cases, LABELDESCTRAINING actually attains higher accuracy than few-shot supervised learning tested on out-of-domain test sets, even when hundreds of manually labeled training examples are used (albeit from a different input domain). In summary, this paper shows several benefits of LABELDESCTRAINING. First, once a practitioner identifies a label set of interest for zero-shot classification, it only requires a few minutes to collect the kind of LABELDESC data shown in Table 1, and training on this data improves over zero-shot by 17-19% absolute. Second, LABELDESCTRAINING leads to greater robustness to pattern/verbalizer choice than zero-shot. Third, LABELDESC data are domain independent with regard to the distribution of the inputs; a single LABELDESC training set can be used for any text classification task as long as it contains the same labels. Our experiments show that this independence to input distribution leads to stable accuracy across domains, even attaining higher accuracy than out-of-domain few-shot learning on a few cases.2\n# 2 Tasks and LABELDESC Datasets\nWe evaluate on two types of tasks: topic classification on AGNews, Yahoo Answers, and DBPedia (Zhang et al., 2015) and sentiment classification on the Stanford Sentiment Treebank (SST) (Socher et al., 2013), Yelp Reviews (Zhang et al., 2015), IMDB (Maas et al., 2011), and Amazon Reviews Polarity (Zhang et al., 2015). We consider both binary and 5-way classification for SST and Yelp datasets (denoted as SST-2, SST-5, Yelp-2, and Yelp-5 henceforth) and only binary for IMDB and Amazon (denoted as IMDB and Amz-2 henceforth).3 Below we describe how we construct LABELDESC data for each label set. Dataset statistics as well as all LABELDESC data are in Section A.5 in the Appendix. Topic Classification. Since labels in topic classification represent general concepts, we use both subjective descriptors of the labels (e.g., related terms) and objective sources of information (e.g., dictionary definition and Wikipedia sentences)\n2Data and code are available at https://github.com/ lingyugao/LabelDescTraining. 3Our method could be adopted for other tasks like natural language inference (NLI) using templates similar to how we approached sentiment classification. We leave a full exploration to future work.\nwhen selecting LABELDESC data. In particular, we create LABELDESC examples for the label term itself, three related terms, a selected definition from dictionary.com, and the leading sentence from the label\u2019s Wikipedia article. As there are typically multiple dictionary.com definitions for our labels, we select a single definition that best aligns with our understanding of the concept underlying the label. We use the leading Wikipedia sentence because it is typically a brief overview/definition of the concept. Most labels in the Yahoo dataset consist of two keywords (e.g., Society & Culture). For these, we use both label terms, definitions for each, and the leading Wikipedia sentences for each. We did not tune any of these decisions experimentally, so these choices in defining LABELDESC data are almost certainly suboptimal. This suboptimality is especially likely for the \u201cWorld\u201d label in the AGNews label set. This label reflects international news, but the dictionary definition and Wikipedia article for the term \u201cWorld\u201d do not capture that sense of the word. Nonetheless, we did not change our procedure for this label because we wanted our results to reflect a real-world implementation of the idea, complete with its limitations for certain labels. The LABELDESC instances we are using do not contain exhaustive information. We could easily extend the lists of related terms for each topic or use WordNet or other semantic knowledge resources (Zhang et al., 2019). However, one of the goals of this research is to demonstrate how simple it is to choose LABELDESC examples to improve zero-shot classification in very little time.\n# Sentiment Classification.\nSentiment Classification. We use a slightly different procedure for sentiment classification. For 5-way sentiment, we use the label verbalizer itself and four synonym terms. In addition, we write four simple templates: \u201cIt was t.\u201d, \u201cA(n) t experience.\u201d, \u201cJust t.\u201d, and \u201cOverall, it was t.\u201d, where t is the label verbalizer or a synonym. For binary sentiment, we remove the neutral instances, combine the two positive labels (\u201cVery Positive\u201d and \u201cPositive\u201d) into one, and combine the two negative labels (\u201cVery Negative\u201d and \u201cNegative\u201d) into one. This procedure produces a total of 25 examples per label (5 terms + 5 terms \u00d7 4 templates) for 5way sentiment and 50 examples per label for binary sentiment. Since these LABELDESC instances are domain-independent, we use the same data for both for 5-way sentiment (Yelp-5 and SST-5) and for bi-\nnary sentiment (Yelp-2, SST-2, IMDB-2, Amz-2).\nHyperparameter Tuning. We adhere to the \u201ctrue\u201d zero-shot setting where hyperparameters cannot be tuned on a development set for the task of interest (Schick and Sch\u00fctze, 2022). Therefore, we use a separate dataset for hyperparameter tuning - the 20 Newsgroups (20NG, henceforth) (Lang, 1995) - a topic classification dataset with twenty labels. We select only four labels from 20NG for our purposes: talk.religion.misc, rec.autos, sci.med, and talk.politics.guns. We chose these four labels because they are sufficiently distinct that we expect tuning to be informative for other real-world classification datasets; many of the other 20NG labels are highly technical or similar to one other, e.g., the pair comp.sys.ibm.pc.hardware and comp.sys.mac.hardware as well as the pair comp.os.ms-windows.misc and comp.windows.x. We follow the same strategy as for topic classification above when constructing LABELDESC data for 20NG. The selected hyperparameters are used for both topic and sentiment classifications.\n# 3 Experimental Settings\nThe following settings are used in our experiments. Unless stated otherwise, we use the pretrained RoBERTa-base (b) and RoBERTa-large (l) models (Liu et al., 2019) for all experiments since RoBERTa is the predominant choice in related zeroshot and dataless research (Schick and Sch\u00fctze, 2021; van de Kar et al., 2022; Gera et al., 2022). Additionally, for every dataset, we use the entire available test sets for evaluation.\n# Zero-shot Classification Baseline\nZero-shot Classification Baseline. We use the standard \u201cpattern-verbalizer\u201d approach for topic and sentiment classification. The set of verbalizers used can be found in Table 10 in the Appendix. For choosing verbalizers, we follow the choices of Schick and Sch\u00fctze (2021) for AGNews, Yahoo, Yelp-5, and SST-5. We follow van de Kar et al. (2022) in choosing verbalizers for Yelp-2, SST-2, IMDB, and Amz-2 and we select verbalizers for DBPedia and 20NG ourselves. Each pattern comprises a prompt including a [MASK] symbol placed before or after the text input, and we aim to predict the masked token. For example, a prompt is added after the input x to frame classification as a question answering task, e.g., \u201cx Question: What is the topic of this newsgroup? Answer: [MASK].\u201d We use RoBERTa-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/040f/040f681f-6699-4f05-9fd9-8ac58b666a03.png\" style=\"width: 50%;\"></div>\nFigure 1: Overview of our proposed method, including the construction of LABELDESC data, the format of the text input, and the target used for both model finetuning and inference during test time. We present text inputs labeled as \u201cSports\u201d from the topic classification task, and use one of our patterns (see Table 11) here as an illustration. Note that all our LABELDESC datasets are balanced, with each pattern being associated with a unique finetuned model checkpoint.\nbase/large with its MLM head for zero-shot experiments. Although the model is able to predict any token within its vocabulary, we choose only among the set of verbalizers, which are designed to be semantically coherent with class labels and tokenized into a single token by the model\u2019s tokenizer. For topic classification tasks, we use the PROMPT and Q&A patterns from Schick and Sch\u00fctze (2022), which amounts to 14 patterns. For AGNews, we use \u201cnews/article\u201d in the pattern templates, while for Yahoo we replace this with \u201cquestion\u201d, and for 20NG we use \u201cnewsgroup\u201d. For the sentiment classification tasks, we create new Q&A patterns such as \u201cx Question: What is the sentiment of this text? Answer: [MASK].\u201d and PROMPT patterns such as \u201cx Sentiment: [MASK].\u201d where x is the input text. There are 14 sentiment patterns in total, presented in the Appendix (Section A.2).\nLABELDESCTRAINING. We use the same settings as the zero-shot baseline except that we finetune the models on LABELDESC data. We do not use any target task data for tuning or early stopping. Instead, we fix hyperparameter values, including number of training steps, by tuning on 20NG following the process described below. We used LABELDESC data for the four selected 20NG labels as our training data and the original 20NG data (training and test sets) as our dev set, restricted to the four selected labels shown in Section 2. We preprocessed the data by removing headers,\nquotes, and footers. We used a batch size of 1 and tuned over a set of five learning rates ({5e-7, 1e-6, 5e-6, 1e-5, 5e-5}). Models were trained for 3500 training steps, evaluating on the dev set after each epoch, i.e., every 24 training steps since it\u2019s the size of LABELDESC dataset for 20NG. Based on tuning accuracies, we chose learning rate 5e-7 and number of training steps 2160 for RoBERTa-base and 1920 for RoBERTa-large. Additionally, we explored variations of parameter freezing, such as freezing certain layers of RoBERTa. The best setting on 20NG was to freeze the lower half of the layers (excluding the embedding layer) during finetuning, so we used this for experiments reported below.4\n# 4 Results and Analysis\nIn this section we first present the results that are obtained via LABELDESCTRAINING and then analyze the benefits of LABELDESC data with a range of additional experiments and analysis.\n# 4.1 Results\nTable 2 compares standard zero-shot classification and LABELDESCTRAINING. LABELDESCTRAINING has higher accuracy across all topic and sentiment classification datasets, outperforming zero-shot by about 17% on average when using\n4Section A.3 in the Appendix provides more details on hyperparameter tuning.\nAGNews\nYahoo\nDBPedia\nYelp-5\nSST-5\nYelp-2\nSST-2\nAmz-2\nIMDB\nAvg.\nzero-shot\nb\n62.7\n41.5\n54.6\n38.0\n35.6\n63.6\n62.6\n64.0\n69.9\n54.7\nl\n68.0\n47.7\n63.9\n38.7\n35.0\n70.6\n63.7\n67.5\n74.1\n58.8\nLABELDESCTRAINING\nb\n77.4\n58.8\n79.5\n43.6\n42.0\n88.3\n84.5\n88.6\n86.9\n72.2\nl\n79.4\n60.8\n86.6\n51.3\n49.2\n94.6\n91.3\n94.1\n92.1\n77.7\nTable 2: Test accuracy (%) comparison between zero-shot classification and LABELDESCTRAINING, b = RoBERTa base, l = RoBERTa-large. For zero-shot, each result is the average over 14 patterns; and for LABELDESCTRAINING each result is the average over 14 patterns and three random seeds per pattern. The \u201cAvg.\u201d column shows the average accuracies across columns.\nAGNews\nYahoo\nDBPedia\nYelp-5\nSST-5\nYelp-2\nSST-2\nAmz-2\nIMDB\nzero-shot\nb\n7.4\n7.0\n18.9\n4.3\n4.3\n10.7\n11.0\n10.3\n13.2\nl\n7.8\n8.2\n9.7\n7.8\n7.7\n15.7\n14.3\n13.7\n17.0\nLDT\nb\n5.0, 5.1, 5.0\n1.7, 1.6, 1.6\n4.5, 4.5, 4.5\n2.0, 2.1, 2.2\n1.8, 1.4, 1.5\n2.1, 2.8, 2.4\n2.5, 2.3, 1.9\n1.3, 1.2, 1.4\n1.8, 2.3, 1.4\nl\n5.3, 6.4, 4.6\n2.1, 2.0, 2.3\n3.2, 2.9, 3.2\n2.4, 2.5, 2.4\n1.6, 1.2, 1.5\n1.1, 2.5, 1.4\n1.2, 2.8, 1.6\n0.9, 1.9, 0.8\n1.1, 1.4, 1.2\nTable 3: Standard deviations of test accuracy (%) across 14 patterns for each test dataset. For LABELDESCTRAINING (LDT in the table), three random seeds were used so we show three standard deviations, one per random seed. All standard deviations over patterns are smaller for LDT than the corresponding values for zero-shot.\nRoBERTa-base and 19% with RoBERTa-large. The results demonstrate that we can greatly improve the performance of zero-shot models with just a few training examples that provide a richer characterization of the label but still without requiring any textual inputs from the task datasets. Table 3 shows that accuracy variances across patterns using LABELDESCTRAINING are much lower than the zero-shot setting, which is known to be unstable (Perez et al., 2021). Finetuning on LABELDESC data not only improves accuracy, but also mitigates sensitivity to pattern selection.\n# Comparisons to the State of the Art. We \nComparisons to the State of the Art. We compare to state-of-the-art (SOTA) results from the literature in Table 4 (we show results using RoBERTabase to better compare to other methods). For this comparison, we use only a single pattern with LABELDESCTRAINING, since doing so reflects more of a real-world use case than averaging over 14 patterns. We choose a single pattern for each of RoBERTa-base and large by tuning on 20NG as we did for other hyperparameters.5 We use three random seeds and report average accuracies and standard deviations over seeds. Chu et al. (2021a) and Chu et al. (2021b) are dataless classification approaches (Chang et al., 2008) that include single-encoder and dual-encoder methods; the latter include the idea of embedding documents and labels and performing classification via semantic retrieval; we report their non-\n5Please refer to A.3 and Table 14 in Appendix for details. We use the same setting for Table 5.\nensemble results in Table 4. Schick and Sch\u00fctze (2022) use labeled training data (10 or 100 examples, see Table 4) for each task, which differs from the domain-independent LABELDESC examples which are agnostic to the domain of the textual inputs.6 From van de Kar et al. (2022), we include the highest accuracies. The results of LABELDESCTRAINING are comparable to other methods across datasets. For sentiment classification, LABELDESCTRAINING performs better than dataless classification (Chu et al., 2021a) by a large margin for all datasets and is competitive with van de Kar et al. (2022) and Schick and Sch\u00fctze (2021). Our method is better than that of van de Kar et al. on topic datasets (AGNews, Yahoo, and DBPedia) but not sentiment datasets except for SST-2. van de Kar et al. (2022) search for naturally occurring data in large corpora; texts expressing sentiment are well-represented in corpora, while texts for topics in a fixed label set may be rarer. LABELDESCTRAINING trains on balanced data from a fixed label set, leveraging available knowledge resources to inform about topics. Although van de Kar et al. (2022) do not report 5-way classification results for Yelp or SST, we report results for both datasets (including base and large models) so that future work can compare to our results in this table. We recommend tuning zero-shot and few-shot methods on datasets that\n6We only include results with PROMPT and Q&A patterns (14 patterns for topic and 16 for sentiment) from Schick and Sch\u00fctze (2022), since those are the pattern types we used for LABELDESCTRAINING.\nAGNews\nYahoo\nDBPedia\nYelp-5\nYelp-2\nSST-5\nSST-2\nAmz-2\nIMDB\nLABELDESCTRAINING\nb\n84.6\u00b10.3\n59.9\u00b10.3\n82.4\u00b11.2\n42.0\u00b10.4\n84.8\u00b10.6\n44.3\u00b10.1\n88.2\u00b10.2\n89.6\u00b10.4\n83.4\u00b10.4\nl\n85.1\u00b11.0\n61.2\u00b10.3\n88.5\u00b10.4\n52.5\u00b11.2\n95.3\u00b10.4\n49.4\u00b11.1\n91.4\u00b10.8\n94.5\u00b10.3\n92.9\u00b10.1\nChu et al. (2021a)\nb\n68.8\n57.8\n81.9\n-\n67.3\n-\n65.0\n66.8\n-\nChu et al. (2021b)\nb\n75.1\n60.0\n88.6\n-\n-\n-\n-\n-\n-\nSchick and Sch\u00fctze (2022)\n10\n79.5\u00b12.2\n58.4\u00b12.7\n-\n44.3\u00b12.5\n-\n-\n-\n-\n-\n100\n87.5\u00b10.8\n65.3\u00b11.0\n-\n54.8\u00b11.5\n-\n-\n-\n-\n-\nvan de Kar et al. (2022)\nb\n79.2\n56.1\n80.4\n-\n92.0\n-\n85.6\n92.0\n86.7\nTable 4: Test accuracy (%) comparison to state-of-the-art methods. 10/100 = # labeled examples used.\nAGNews\nYahoo\nDBPedia\nYelp-5\nYelp-2\nSST-5\nSST-2\nAmz-2\nIMDB\nLABELDESCTRAINING\nb\n84.3\u00b10.1\n57.5\u00b10.7\n82.0\u00b11.5\n41.6\u00b11.2\n83.1\u00b10.5\n45.3\u00b10.6\n86.7\u00b10.6\n90.8\u00b10.4\n83.1\u00b10.6\nl\n85.5\u00b10.6\n57.5\u00b10.7\n88.1\u00b10.6\n53.8\u00b11.9\n95.4\u00b10.4\n51.4\u00b11.3\n90.3\u00b10.7\n94.2\u00b10.3\n94.1\u00b10.2\nAGNews\nYahoo\nDBPedia\nYelp-5\nYelp-2\nSST-5\nSST-2\nAmz-2\nIMDB\nLABELDESCTRAINING\nb\n84.6\u00b10.3\n59.9\u00b10.3\n82.4\u00b11.2\n42.0\u00b10.4\n84.8\u00b10.6\n44.3\u00b10.1\n88.2\u00b10.2\n89.6\u00b10.4\n83.4\u00b10.4\nl\n85.1\u00b11.0\n61.2\u00b10.3\n88.5\u00b10.4\n52.5\u00b11.2\n95.3\u00b10.4\n49.4\u00b11.1\n91.4\u00b10.8\n94.5\u00b10.3\n92.9\u00b10.1\nChu et al. (2021a)\nb\n68.8\n57.8\n81.9\n-\n67.3\n-\n65.0\n66.8\n-\nChu et al. (2021b)\nb\n75.1\n60.0\n88.6\n-\n-\n-\n-\n-\n-\nSchick and Sch\u00fctze (2022)\n10\n79.5\u00b12.2\n58.4\u00b12.7\n-\n44.3\u00b12.5\n-\n-\n-\n-\n-\n100\n87.5\u00b10.8\n65.3\u00b11.0\n-\n54.8\u00b11.5\n-\n-\n-\n-\n-\nvan de Kar et al. (2022)\nb\n79.2\n56.1\n80.4\n-\n92.0\n-\n85.6\n92.0\n86.7\nAGNews\nYahoo\nDBPedia\nYelp-5\nYelp-2\nSST-5\nSST-2\nAmz-2\nIMDB\nLABELDESCTRAINING\nb\n84.3\u00b10.1\n57.5\u00b10.7\n82.0\u00b11.5\n41.6\u00b11.2\n83.1\u00b10.5\n45.3\u00b10.6\n86.7\u00b10.6\n90.8\u00b10.4\n83.1\u00b10.6\nl\n85.5\u00b10.6\n57.5\u00b10.7\n88.1\u00b10.6\n53.8\u00b11.9\n95.4\u00b10.4\n51.4\u00b11.3\n90.3\u00b10.7\n94.2\u00b10.3\n94.1\u00b10.2\ntext-davinci-003 (zero-shot)\n-\n80.2\n58.5\n70.1\n47.2\n92.3\n49.3\n89.3\n93.3\n78.9\ntext-davinci-003 (ICL)\n-\n83.9\n61.1\n84.2\n57.0\n92.9\n51.2\n92.3\n95.1\n88.3\ne 5: Test accuracy (%) comparison to text-davinci-003 on test set s\nare excluded from the final comparison, like 20NG in this paper. Comparisons Involving GPT-3.5. Our method not only works for MLM-style models like RoBERTa, but also for autoregressive models. In Table 5, we show zero-shot and in-context learning (ICL), where we use the entire LABELDESC data for the task as ICL demonstrations, with text-davinci-003 (GPT-3.5; OpenAI, 2022). Due to our restricted budget, we decided to use only 1,000 test instances for each test dataset in GPT-3.5 experiments, while ensuring that the label distribution remains consistent with that of the full test dataset. It is well known that ICL is sensitive to a variety of design choices, including the order of the demonstrations (Fei et al., 2023; Lu et al., 2022). For ICL demonstrations, we included all LABELDESC data for a task to make predictions for each test instance. To avoid the \u201crecency bias\u201d (i.e., the tendency to predict labels that occur towards the end of the prompt; Zhao et al., 2021a), we randomly shuffle the order of demonstrations. We left other parameters untouched. GPT-3.5 with ICL using LABELDESC data outperforms zero-shot GPT-3.5 on all datasets, showing the value of LABELDESC data even if in-domain inputs are unavailable. In comparison to GPT-3.5 flavors, LABELDESCTRAINING (RoBERTa-large) performs better on AGNews, DBPedia, Yelp-2, SST-5, and IMDB, and is competitive across other datasets.\nare excluded from the final comparison, like 20NG in this paper.\n# 4.2 Analysis and Discussion\nOne of the primary requirements of the zero-shot approach is the availability of pattern-verbalizer\npairs (Schick and Sch\u00fctze, 2021, 2022). Here, we study several variations of LABELDESCTRAINING to investigate whether we can simplify or remove components of these pattern-verbalizer pairs. We first experiment with changing verbalizers to gauge the impact of verbalizer choice for LABELDESCTRAINING (Section 4.2.1). Next, we conduct classification experiments that do not use patterns or verbalizers at all (Section 4.2.2). Furthermore, we include one more baseline, i.e., the model finetuned on the 20NG LABELDESC data and patterns to analyze the generalizability (Section 4.2.3). We also report additional experiments in which we measure the multi-domain robustness of LABELDESCTRAINING compared to a standard procedure of training on one domain and testing on an out-of-domain test set (Section 4.2.4). Finally, we take a closer look at label-wise performance to better understand how LABELDESCTRAINING outperforms zero-shot classification (Section 4.2.5).\n# 4.2.1 Impact of Verbalizers\nIn this section we report experiments with LABELDESCTRAINING without meaningful verbalizers and even with adversarially chosen verbalizers. We explore two different verbalizer settings:\n\u2022 RANDOM: We add c new words, i.e., RANDOM1, RANDOM2, ..., RANDOMc, where c is the number of dataset labels, to the model\u2019s vocabulary and randomly initialize their embeddings. This setting prevents the use of any prior knowledge in the verbalizer embeddings. \u2022 MISMATCHED: We shuffle the original mapping\n\u2022 RANDOM: We add c new words, i.e., RANDOM1, RANDOM2, ..., RANDOMc, where c is the number of dataset labels, to the model\u2019s vocabulary and randomly initialize their embeddings. This setting prevents the use of any prior knowledge in the verbalizer embeddings. \u2022 MISMATCHED: We shuffle the original mapping\nAGNews\nYahoo\nDBPedia\nYelp-5\nSST-5\nYelp-2\nSST-2\nAmz-2\nIMDB\nAvg.\nzero-shot\nb\n62.7\u00b17.4\n41.5\u00b17.0\n54.6\u00b118.9\n38.0\u00b14.3\n35.6\u00b14.3\n63.6\u00b110.7\n62.6\u00b111.0\n64.0\u00b110.3\n69.9\u00b113.2\n54.7\u00b19.7\nl\n68.0\u00b17.8\n47.7\u00b18.2\n63.9\u00b19.7\n38.7\u00b17.8\n35.0\u00b17.7\n70.6\u00b115.7\n63.7\u00b114.3\n67.5\u00b113.7\n74.1\u00b117.0\n58.8\u00b111.3\nLDT20NG\nb\n61.8\u00b17.0\n49.4\u00b15.2\n72.9\u00b17.8\n34.6\u00b14.6\n36.5\u00b13.7\n67.7\u00b110.3\n63.4\u00b19.7\n67.2\u00b19.6\n72.5\u00b110.5\n58.4\u00b17.6\nl\n72.4\u00b16.8\n54.4\u00b14.3\n71.9\u00b110.8\n36.3\u00b15.7\n36.6\u00b17.1\n63.4\u00b113.0\n56.9\u00b18.7\n60.9\u00b110.2\n67.5\u00b115.2\n57.8\u00b19.1\nLDT\nb\n77.4\u00b14.9\n58.8\u00b11.6\n79.5\u00b14.4\n43.6\u00b12.1\n42.0\u00b11.6\n88.3\u00b12.5\n84.5\u00b12.2\n88.6\u00b11.4\n86.9\u00b11.8\n72.2\u00b12.5\nl\n79.4\u00b15.0\n60.8\u00b12.1\n86.6\u00b13.0\n51.3\u00b12.4\n49.2\u00b11.6\n94.6\u00b11.8\n91.3\u00b12.0\n94.1\u00b11.3\n92.1\u00b11.2\n77.7\u00b12.3\nMLMr\nb\n77.3\u00b14.0\n54.3\u00b13.9\n81.3\u00b17.3\n38.1\u00b13.8\n37.0\u00b13.2\n78.4\u00b110.0\n73.3\u00b17.9\n80.0\u00b19.9\n73.8\u00b19.6\n65.9\u00b16.6\nl\n75.2\u00b15.0\n58.0\u00b13.0\n85.4\u00b113.0\n46.4\u00b13.3\n43.4\u00b12.9\n90.8\u00b17.6\n84.1\u00b16.8\n90.2\u00b17.1\n87.4\u00b16.2\n73.4\u00b16.1\nMLMm\nb\n73.1\u00b15.6\n50.1\u00b15.4\n72.6\u00b18.1\n36.8\u00b12.8\n35.8\u00b12.5\n80.1\u00b17.2\n75.8\u00b15.0\n81.8\u00b16.8\n76.7\u00b16.0\n64.8\u00b15.5\nl\n66.4\u00b18.6\n44.5\u00b14.9\n73.1\u00b17.3\n41.9\u00b14.0\n38.7\u00b14.2\n83.6\u00b16.5\n78.1\u00b16.0\n85.0\u00b16.0\n77.7\u00b16.9\n65.4\u00b16.0\nclassifier\nb\n72.5\u00b15.5\n57.1\u00b10.7\n87.7\u00b12.6\n40.3\u00b11.3\n39.4\u00b12.5\n86.9\u00b12.9\n79.7\u00b11.1\n89.1\u00b10.9\n80.6\u00b13.6\n70.4\u00b12.3\nl\n77.8\u00b11.5\n50.9\u00b17.3\n78.2\u00b11.0\n42.4\u00b11.6\n35.3\u00b19.2\n93.3\u00b10.9\n86.6\u00b11.4\n93.7\u00b10.5\n85.7\u00b12.0\n71.5\u00b12.8\nTable 6: Test accuracies (%) for several variations of LABELDESCTRAINING. The standard deviations are computed over 14 patterns for zero-shot; 3 random seeds for the classifier (no patterns); and both 14 patterns and 3 random seeds for LABELDESCTRAINING on 20NG, LABELDESCTRAINING, RANDOM, and MISMATCHED (LDT20NG, LDT, MLMr, and MLMm in Table).\nof labels to verbalizers, ensuring that each verbalizer maps to a different label than in the original LABELDESCTRAINING setting. Since we are still finetuning the embeddings, finetuning can help the model recover from this mismatched initialization.\nThe results are shown in Table 6. Since we still use the MLM head for these results, we refer to them as \u201cMLM, RANDOM\u201d and \u201cMLM, MISMATCHED\u201d. While LABELDESCTRAINING performs better than RANDOM, and RANDOM is better than MISMATCHED, both are better than zeroshot on average. These results suggest that LABELDESC data can partially compensate when the quality of the verbalizers is unknown or poor, at least to improve over zero-shot.\n# 4.2.2 Classifiers Without Patterns or Verbalizers\nSince finetuning on LABELDESC data outperforms zero-shot results with RANDOM verbalizers, we also evaluate its performance without patterns, i.e., using a standard randomly initialized softmax classifier. The input is the original text without any patterns and we use a two-layer classification head on top of the [CLS] token representation of the pretrained models. The bottom two rows of Table 6 show the results. The classifiers are close to that of the MLM/RANDOM setting and still much higher than zero-shot on average, suggesting that it is not necessary to use patterns, verbalizers, or even the pretrained MLM head in order to outperform zero-shot classifiers. If it is difficult to select verbalizers or design patterns for a particular classification task,\nusing a classifier that has been finetuned on a small LABELDESC dataset may serve as a strong alternative to the pattern-verbalizer approach.\n# 4.2.3 Cross-Task Generalizability\nWe report results on the model finetuned on the 20NG LABELDESC data and patterns, i.e., LABELDESCTRAINING on 20NG (LDT20NG), in Table 6. While the patterns for the reported datasets are different from those used for 20NG, especially for sentiment datasets, they have similar structures (see Section A.2). For RoBERTa-base, LDT20NG often outperforms zero-shot results, except for AGNews and Yelp-5. However, for RoBERTa-large, while LDT20NG outperforms the zero-shot results on all topic classification datasets, it\u2019s worse on sentiment classification except for SST-5.\n# 4.2.4 Multi-Domain Evaluation\nSince LABELDESC examples are domainindependent, they can be used for multiple datasets that have the same labels. To assess the multidomain performance of LABELDESCTRAINING, we compare it to supervised few-shot learning in which a model is trained on data from one domain and then evaluated on a different domain with the same label set (i.e., training on SST-5 and evaluating on Yelp-5). To create multi-domain test sets for a single topic label set, we keep AGNews as it is and create a new subsampled version of Yahoo as follows: (1) \u201cPolitics & Government\u201d and \u201cSociety & Culture\u201d texts are assigned the label \u201cWorld\u201d, (2) \u201cSports\u201d texts are labeled \u201cSports\u201d, (3) \u201cBusiness & Finance\u201d texts are labeled \u201cBusiness\u201d, and (4) \u201cScience & Mathematics\u201d and \u201cComputers\n& Internet\u201d texts are labeled \u201cSci/Tech\u201d. Other Yahoo texts are removed. We refer to this new version of the Yahoo dataset as YahooAG. For sentiment classification, we choose two dataset pairs that share label sets, i.e., SST-5 and Yelp-5. We do not change anything about the LABELDESCTRAINING configuration for these experiments. We simply evaluate the same model on multiple test sets, reporting average accuracies over patterns. For few-shot setup, we create datasets with 10, 100, and 500 training examples per label. For indomain experiments, train, dev, and test sets are drawn from the same domain/dataset, whereas for out-of-domain experiments, train and dev sets are drawn from one domain and the test set is drawn from another domain. We tune learning rates over the same ranges as mentioned earlier and use batch sizes 1, 2, and 4 for 10, 100, and 500 examples per label, respectively. We train for 15 epochs and select the checkpoint from the best epoch selected by the dev set. The results using RoBERTa-large are shown in Figure 2. For brevity, we only show a subset of results.7 As we would expect, testing on out-ofdomain data leads to accuracy drops but adding more out-of-domain training data reduces this gap. LABELDESCTRAINING, shown as an orange dotted line, outperforms supervised few-shot learning in some cases, such as training on AGNews and testing on YahooAG, even with 500 examples per label (upper-right plot in Figure 2). We see the same trend when the supervised model is trained on Yelp5 and tested on SST-5 (lower-right plot in Figure 2). In 3 out of 4 cases, LABELDESCTRAINING outperforms supervised few-shot out-of-domain learning with 10 examples per label, outperforming 100 in 2 out of 4 cases.\n# 4.2.5 Label-wise Investigation\nTo better understand why LABELDESCTRAINING outperforms zero-shot, we report label-specific F1 scores in Tables 8 and 9. For AGNews, the zeroshot classifiers have low F1 scores for the World label, probably because the verbalizer \u201cWorld\u201d is much less coherent and less representative of the actual label than others like \u201cSports.\u201d LABELDESCTRAINING improves F1 on the World label by roughly 20 points, while the improvement for Sports is only about 4 points. Likewise, the F1 scores for \u201cVery Negative\u201d, \u201cVery Positive\u201d, and\n7Section A.4 in the Appendix shows additional results.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1808/1808e684-9f8b-4d9d-846d-d492a96e3ad5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">in-domain out-of-domain LDT</div>\n<div style=\"text-align: center;\">Figure 2: Domain transfer results, where the X-axis shows the number of training examples per label.</div>\n\u201cNeutral\u201d are very low for the zero-shot models on SST-5, indicating that those labels are being largely ignored. Again, LABELDESCTRAINING shows large improvements in F1 for some of these labels, especially \u201cVery Positive\u201d. These trends are likely due in part to the differences verbalizer probabilities, e.g., \u201cgood\u201d and \u201cbad\u201d occur more frequently than \u201cgreat\u201d and \u201cterrible\u201d. The LABELDESC data is balanced, which helps to mitigate the ignoring of labels, even though the task test sets are not all balanced. Table 7 shows examples that are incorrectly classified by zero-shot models but are correctly classified by the LABELDESCTRAINING models.\n# 5 Related Work\nOne common approach in zero-shot text classification is to transfer knowledge from seen labels (Dauphin et al., 2014), which requires observed labels and a notion of label similarity. Some sources of semantic knowledge used for this purpose include multiple modalities (Lampert et al., 2009), label relationships in knowledge graphs (Wang et al., 2018), and word representations (Song and Roth, 2014; Fei et al., 2022). There are several other approaches to zero-shot classification. To classify documents, Chang et al. (2008) used knowledge-based text representations derived from Wikipedia, and Barak et al. (2009) used both Wikipedia and WordNet. Zhang et al. (2019) combined label descriptions with a label hierarchy and word-to-label paths in ConceptNet, with data augmentation strategies. Yin et al. (2019) used a textual entailment approach with label defi-\ntext ([headline][text body] for AGNews)\nzero-shot\nLABELDESCTRAINING\n[Homeless families total 100,000][The figure for homeless families in England has\ntopped 100,000 for the first time.]\nBusiness\nWorld\n[Shifting signs in North Korea][Kim Jong Il dials back his personality cult as\nprotest activities pick up.]\nSports\nWorld\n[GM, Daimler Go Green][Team-up will help the companies compete and fill gaps\nin both firms\u2019 portfolios.]\nSci/Tech\nBusiness\n(U)nrelentingly stupid.\nPositive\nVery Negative\nStill, I\u2019m not quite sure what the point is...\nPositive\nNegative\nThis 72-minute film does have some exciting scenes, but it\u2019s a tad slow.\nPositive\nNeutral\n<div style=\"text-align: center;\">text ([headline][text body] for AGNews)</div>\nzero-shot\nLABELDESCTRAINING\nWorld\n61.5\u00b115.1\n81.0\u00b14.3\nBusiness\n63.6\u00b17.1\n74.9\u00b14.7\nSports\n88.2\u00b13.9\n92.7\u00b14.5\nSci/Tech\n55.0\u00b111.4\n67.8\u00b19.3\nTable 8: AGNews label-wise F1 (RoBERTa-large).\nzero-shot\nLABELDESCTRAINING\nVery Negative\n11.2\u00b114.9\n25.8\u00b15.7\nNegative\n37.6\u00b121.2\n62.5\u00b12.0\nNeutral\n1.2\u00b12.9\n10.8\u00b15.5\nPositive\n46.0\u00b15.8\n48.2\u00b14.9\nVery Positive\n12.1\u00b115.0\n58.0\u00b14.0\nTable 9: SST-5 label-wise F1 (RoBERTa-large).\nnitions from WordNet. Another approach that has gained popularity is self-training given label names and mining an unlabeled dataset (Meng et al., 2020; Gera et al., 2022). van de Kar et al. (2022) extend the mining-based approach by selecting unsupervised examples (via patterns) for training. Basile et al. (2022) select label descriptions by aggregation. Meng et al. (2022) use language models to generate new training examples. On the contrary, we train on a small set of domain-independent label descriptions. Our setup is influenced by Schick and Sch\u00fctze (2021, 2022), although, instead of finetuning on training examples, we only use our LABELDESC data. Autoregressive language models have also been used for zero-shot text classification; we report zero-shot and ICL results with LABELDESC data using GPT-3.5 (OpenAI, 2022). Zhao et al. (2021b) found it beneficial to \u201ccalibrate\u201d such models for this setting; this idea is not immediately applicable here due to our use of encoder-only models like RoBERTa. Calibration could be extended to encoder-only models, which we plan to explore in future work. Our work is closely related to data-\nless classification (Chang et al., 2008) which involves building classifiers by designing or learning a generic function that scores the compatibility of a document and label defined in natural language. We compared empirically to the dataless classification approaches of Chu et al. (2021a) and Chu et al. (2021b) who used pretrained models, naturally annotated data like that from Wikipedia categories, and unsupervised clustering techniques. There is a wealth of prior work in semi-supervised text classification (Nigam et al., 2000; Xie et al., 2020; Howard and Ruder, 2018). There is also related work on generating label names (Schick et al., 2020) or label descriptions (Chai et al., 2020; Sun et al., 2019) but for supervised text classification.\n# 6 Conclusions\nWe presented LABELDESCTRAINING, a method for improving the accuracy of zero-shot classification by using small, curated datasets that simply describe the labels for a task in natural language. Our method is 17-19% more accurate than zero-shot on average across a range of datasets. LABELDESCTRAINING is also more robust to the choices required for zero-shot classification, such as patterns and verbalizers. Furthermore, LABELDESC data is domain agnostic and therefore can used for any text classification task as long as it contains the same set of labels. LABELDESCTRAINING can even outperform a supervised approach that uses training data from a different domain. One future direction would be to apply the idea to structured prediction, NLI, and natural language generation tasks. Another would be to investigate ways to reduce the dependence of pretrained models on patterns and verbalizers, such as directly calibrating the marginal probabilities of verbalizers with the goal of minimizing biases of pretrained models.\n# 7 Limitations\nWe focus on a simple approach of curating small finetuning datasets that describe the labels for text classification tasks. Although this is beneficial when the task is specific, especially when the data is difficult to obtain, the data curation process is intrinsically intuitive and relies on the practitioner\u2019s understanding of the labels and usage situation. Moreover, since a pretrained model is necessary for this approach, a few curated examples may mitigate, but cannot detect or eliminate, potential biases of the pretrained model. If the labels of a certain classification task are dissimilar from the examples the model was trained on, and the model lacks the knowledge to differentiate among them, it may lead to unsatisfying performance even after finetuning on a few examples of label descriptions.\n# 8 Ethics Statement\nWe use pretrained models for text classification, and curate data with the assistance of data sources such as Wikipedia and dictionary definitions. The large pretrained models are trained on a massive amount of data and have been shown to have issues with bias; however, this is a common challenge when working with pretrained models and would benefit from advances made by the community on this front. While both dictionary.com definitions and Wikipedia are aimed at providing accurate and neutral information for a word/concept, they can be affected by the biases and limitations of their editors, especially for Wikipedia, which is an opensource encyclopedia. Our method is not reliant on specific dictionaries or encyclopedias; others could be used. We chose these resources for simplicity as they are highly accessible and widely used. Since our LABELDESC data is very small in size, we manually examined the data as we selected it for any potential biases or other issues. Finally, we use standard topic and sentiment datasets for evaluation, which are used in a great deal of prior work.\n# References\nLibby Barak, Ido Dagan, and Eyal Shnarch. 2009. Text categorization from category name via lexical reference. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Pa-\npers, pages 33\u201336, Boulder, Colorado. Association for Computational Linguistics.\nAngelo Basile, Marc Franco-Salvador, and Paolo Rosso. 2022. Unsupervised ranking and aggregation of label descriptions for zero-shot classifiers. In Natural Language Processing and Information Systems - 27th International Conference on Applications of Natural Language to Information Systems, NLDB 2022, Valencia, Spain, June 15-17, 2022, Proceedings, volume 13286 of Lecture Notes in Computer Science, pages 119\u2013126. Springer.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\nDuo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei Li. 2020. Description based text classification with reinforcement learning. In International Conference on Machine Learning, pages 1371\u20131382. PMLR.\nMing-Wei Chang, Lev-Arie Ratinov, Dan Roth, and Vivek Srikumar. 2008. Importance of semantic representation: Dataless classification. In Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008, pages 830\u2013835. AAAI Press.\nacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\nYu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023. Mitigating label biases for in-context learning. arXiv preprint arXiv:2305.19148.\nYu Fei, Ping Nie, Zhao Meng, Roger Wattenhofer, and Mrinmaya Sachan. 2022. Beyond prompting: Making pre-trained language models better zeroshot learners by clustering representations. CoRR, abs/2210.16637. Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz, Liat Ein-Dor, and Noam Slonim. 2022. Zeroshot text classification with self-training. CoRR, abs/2210.17541. Jeremy Howard and Sebastian Ruder. 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146. Christoph H. Lampert, Hannes Nickisch, and Stefan Harmeling. 2009. Learning to detect unseen object classes by between-class attribute transfer. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 951\u2013958. IEEE Computer Society. Ken Lang. 1995. Newsweeder: Learning to filter netnews. In Machine Learning, Proceedings of the Twelfth International Conference on Machine Learning, Tahoe City, California, USA, July 9-12, 1995, pages 331\u2013339. Morgan Kaufmann. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086\u20138098, Dublin, Ireland. Association for Computational Linguistics. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 142\u2013150, Portland, Oregon, USA. Association for Computational Linguistics. Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards zero-shot language understanding. arXiv preprint arXiv:2202.04538. Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. 2020. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9006\u20139017, Online. Association for Computational Linguistics.\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han. 2022. Generating training data with language models: Towards zero-shot language understanding. arXiv preprint arXiv:2202.04538.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. 2020. Text classification using label names only: A language model self-training approach. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9006\u20139017, Online. Association for Computational Linguistics.\nKamal Nigam, Andrew Kachites McCallum, Sebastian Thrun, and Tom Mitchell. 2000. Text classification from labeled and unlabeled documents using em. Machine learning, 39(2):103\u2013134.\n# OpenAI. 2022. Openai api [text-davinci-003]. https: //api.openai.com/v1/completions.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 11054\u201311070.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1\u2013 67. Timo Schick, Helmut Schmid, and Hinrich Sch\u00fctze. 2020. Automatically identifying words that can serve as labels for few-shot text classification. arXiv preprint arXiv:2010.13641. Timo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze-questions for few-shot text classification and natural language inference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 255\u2013269, Online. Association for Computational Linguistics. Timo Schick and Hinrich Sch\u00fctze. 2022. True few-shot learning with Prompts\u2014A real-world perspective. Transactions of the Association for Computational Linguistics, 10:716\u2013731. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Yangqiu Song and Dan Roth. 2014. On dataless hierarchical text classification. In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence, July 27 -31, 2014, Qu\u00e9bec City, Qu\u00e9bec, Canada, pages 1579\u20131585. AAAI Press. Chi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence. arXiv preprint arXiv:1903.09588.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1\u2013 67.\nTimo Schick, Helmut Schmid, and Hinrich Sch\u00fctze. 2020. Automatically identifying words that can serve as labels for few-shot text classification. arXiv preprint arXiv:2010.13641.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence. arXiv preprint arXiv:1903.09588.\nChi Sun, Luyao Huang, and Xipeng Qiu. 2019. Utilizing bert for aspect-based sentiment analysis via constructing auxiliary sentence. arXiv preprint arXiv:1903.09588.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5824/5824995d-a3d7-4924-82d6-d941b92925ee.png\" style=\"width: 50%;\"></div>\nMozes van de Kar, Mengzhou Xia, Danqi Chen, and Mikel Artetxe. 2022. Don\u2019t prompt, search! miningbased zero-shot learning with language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7508\u20137520, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Xiaolong Wang, Yufei Ye, and Abhinav Gupta. 2018. Zero-shot recognition via semantic embeddings and knowledge graphs. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6857\u20136866. Computer Vision Foundation / IEEE Computer Society. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. 2020. Unsupervised data augmentation for consistency training. Advances in Neural Information Processing Systems, 33:6256\u20136268.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021a. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021b. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697\u201312706. PMLR.\n# A Appendix\n# A.1 Verbalizers\nDataset\nVerbalizers\n20NG\ntalk.religion.misc \ufffd\u2192religion, rec.autos\n\ufffd\u2192automobile,\nsci.med \ufffd\u2192medicine,\ntalk.politics.guns \ufffd\u2192gun\nAGNews\nWorld \ufffd\u2192World, Sports \ufffd\u2192Sports, Busi-\nness \ufffd\u2192Business, Sci/Tech \ufffd\u2192Tech\nYahoo\nSociety & Culture \ufffd\u2192Society, Science &\nMathematics \ufffd\u2192Science, Health \ufffd\u2192Health,\nEducation & Reference \ufffd\u2192Education, Com-\nputers & Internet \ufffd\u2192Computer, Sports \ufffd\u2192\nSports, Business & Finance \ufffd\u2192Business,\nEntertainment & Music \ufffd\u2192Entertainment,\nFamily & Relationships \ufffd\u2192Relationship,\nPolitics & Government \ufffd\u2192Politics\nDBPedia\nCompany \ufffd\u2192company, Educational institu-\ntion \ufffd\u2192school, Artist \ufffd\u2192artist, Athlete \ufffd\u2192\nsports, Office holder \ufffd\u2192politics, Mean of\ntransportation \ufffd\u2192transportation, Building\n\ufffd\u2192building, Natural place \ufffd\u2192natural, Vil-\nlage \ufffd\u2192village, Animal \ufffd\u2192animal, Plant\n\ufffd\u2192plant, Album \ufffd\u2192album, Film \ufffd\u2192film,\nWritten work \ufffd\u2192book\nYelp-5\nVery Negative \ufffd\u2192terrible, Negative \ufffd\u2192bad,\nNeutral \ufffd\u2192okay, Positive \ufffd\u2192good, Very\nPositive \ufffd\u2192great\nSST-5\nYelp-2\nNegative \ufffd\u2192awful, Positive \ufffd\u2192great\nSST-2\nIMDB\nAmz-2\n<div style=\"text-align: center;\">Table 10: Verbalizers selected for each dataset.</div>\n# A.2 Patterns for MLM A.2.1 Topic Classification\nWe use the patterns shown in Table 11 for AGNews and DBPedia, and replace \u201cnews/article\u201d by \"question\" for Yahoo Question, which follows Schick and Sch\u00fctze (2022)\u2019s practice. We use \"newsgroup\" instead of \"question\" for 20NG.\n# A.2.2 Sentiment Classification\nOur sentiment classification datasets (Yelp-2/5, SST-2/5, Amz-2, and IMDB) share the same patterns listed in Table 12.\n# A.3 Hyperparameters and Best Pattern\nWe selected training batch size as 1 for our experiments on LABELDESC data. After fine-tuning on 20NG, the hyperparameters are selected as shown in Table 13. With the selected hyperparameters, we further examine the dev accuracy on 20NG for all prompt patterns and select the tuned pattern that\ntype\nid\npatterns\nQ&A\n1\nx Question: What is the topic of this\narticle? Answer: [MASK].\n2\nx Question: What is the category of this\narticle? Answer: [MASK].\n3\nx Question: What is the topic of this\narticle? Answer: [MASK]\n4\nx Question: What is the category of this\narticle? Answer: [MASK]\nPROMPT 1\nx Category: [MASK].\n2\nx Class: [MASK].\n3\nx Topic: [MASK].\n4\nx Theme: [MASK].\n5\nx Category: [MASK]\n6\nx Class: [MASK]\n7\nx Topic: [MASK]\n8\nx Theme: [MASK]\n9\n[MASK] News: x\n10\n[MASK] NEWS: x\nTable 11: Patterns for AGNews, where x refers to the given text.\nTable 11: Patterns for AGNews, where x refers to the given text.\nhas the highest dev accuracy. The tuned patterns are listed in Table 14. To our knowledge, this method works well when we adapt to other datasets. However, we also observe that there are fluctuations in the dev accuracy curve for 20NG during training, and we select the training steps in the middle of the flatter part of curves rather than the peak point for robustness. We suggest changing training steps or increasing batch size if this method doesn\u2019t work well. The tuned pattern is not necessarily the best pattern after adapting to other datasets, sometimes even a little lower than the average results over all 14 patterns.\n# A.4 Domain Transfer\nAll results on RoBERTa-base/large are shown in Figure 3.\n# All results on RoBERTa-base/large are shown in Figure 3.\nA.5 LABELDESC Data\nThe statistics of LABELDESC data are shown in Table 15. We use the same set of LABELDESC data for AGNews and YahooAG, Yelp-5 and SST5, Yelp-2 and SST-2, respectively. The data is listed in Table 16 - Table 21. Each term/sentence that is separated by \u201c|\u201d in tables is an independent LABELDESC example during training. For brevity, we list all hand-crafted templates instead of listing all data for sentiment classification.\ntype\nid\npatterns\nQ&A\n1\nx Question: What is the sentiment of\nthis text? Answer: [MASK].\n2\nx Question: What is the writer\u2019s opinion\nin this text? Answer: [MASK].\n3\nx Question: What is the sentiment of\nthis text? Answer: [MASK]\n4\nx Question: What is the writer\u2019s opinion\nin this text? Answer: [MASK]\nPROMPT 1\nx Opinion: [MASK].\n2\nx Feeling: [MASK].\n3\nx Sentiment: [MASK].\n4\nx Summary: [MASK].\n5\nx Opinion: [MASK]\n6\nx Feeling: [MASK]\n7\nx Sentiment: [MASK]\n8\nx Summary: [MASK]\n9\n[MASK] Sentiment: x\n10\n[MASK] SENTIMENT: x\nTable 12: Patterns for sentiment classification, where x refers to the given text.\nlr\nsteps\nMLM\nLDT\nbase\n5e-7\n2160\nlarge\n5e-7\n1920\nMISMATCHED\nbase\n5e-5\n2160\nlarge\n5e-6\n3000\nRANDOM\nbase\n5e-5\n2160\nlarge\n5e-6\n3240\nclassifier\nbase\n1e-5\n1920\nlarge\n1e-6\n2280\nTable 13: Hyperparameters (learning rate, training steps) selected by tuning on 20NG with RoBERTa.\n# A.6 Dataset Preprocessing\nFor 20NG, we remove headers, quotes, and footers. For AGNews, we concatenate the headlines and the text body of the news articles. For Yahoo dataset, we concatenate the title, the question, and the top answer to it. And for IMDB and Amazon Reviews Polarity datasets, we concatenate the title and the content.\n# A.7 Label-wise Metrics\nWe list label-wise precision, recall, and F1 scores for part of our datasets in Table 22 - 29.\npattern\nid\nMLM\nLDT\nbase\nprompt\n9\nlarge\nprompt\n7\nMISMATCHED\nbase\nqa\n3\nlarge\nqa\n1\nRANDOM\nbase\nqa\n3\nlarge\nprompt\n6\n<div style=\"text-align: center;\">Table 14: Tuned pattern and pattern id for each model</div>\ndataset\n#label\nLD\ndev\ntest\n20NG\n4\n24\n3389\n-\nAGNews\n4\n24\n2,000\n7,600\nYahooAG\n3,000\n36,000\nYahoo\n10\n60\n-\n60,000\nDBPedia\n14\n84\n-\n70,000\nYelp-5\n5\n125\n2,500\n50,000\nSST-5\n1,101\n2,210\nYelp-2\n2\n100\n2,000\n38,000\nSST-2\n872\n1,821\nAmz-2\n-\n400,000\nIMDB\n-\n25,000\nTable 15: Statistics of datasets we used, with \u2019#\u2019 denoting the number of labels, LD refers to LABELDESC data.\nLabel\nType\nTraining Data\ntalk.\nreligion.\nmisc\nterms\nreligion | Christian | Buddhist | Jewish\nWiki.\nReligion is usually defined as a social-cultural system\nof designated behaviors and practices, morals, beliefs,\nworldviews, texts, sanctified places, prophecies, ethics,\nor organizations, that generally relates humanity to super-\nnatural, transcendental, and spiritual elements; however,\nthere is no scholarly consensus over what precisely con-\nstitutes a religion.\ndict.\na set of beliefs concerning the cause, nature, and purpose\nof the universe, especially when considered as the creation\nof a superhuman agency or agencies, usually involving\ndevotional and ritual observances, and often containing a\nmoral code governing the conduct of human affairs.\nrec.autos\nterms\nautomobile | truck | car | vehicle\nWiki.\nA car (or automobile) is a wheeled motor vehicle that is\nused for transportation.\ndict.\na passenger vehicle designed for operation on ordinary\nroads and typically having four wheels and a gasoline or\ndiesel internal-combustion engine.\nsci.med\nterms\nmedicine | hospital | symptom | cure\nWiki.\nMedicine is the science and practice of caring for a patient,\nmanaging the diagnosis, prognosis, prevention, treatment,\npalliation of their injury or disease, and promoting their\nhealth.\ndict.\nany substance or substances used in treating disease or\nillness; medicament; remedy.\ntalk.\npolitics.\nguns\nterms\ngun | firearm | weapon | handgun\nWiki.\nA gun is a ranged weapon designed to use a shooting tube\n(gun barrel) to launch projectiles.\ndict.\na weapon consisting of a metal tube, with mechanical\nattachments, from which projectiles are shot by the force\nof an explosive; a piece of ordnance.\nTable 16: LABELDESC data for 20NG. \u201cWiki.\u201d and \u201cdict.\u201d refers to the data source, i.e., Wikipedia or dictionary definition.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/47a9/47a9ce52-c6b6-4b47-b7b0-be346c0a8595.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">in-domain out-of-domain LDT</div>\nFigure 3: Domain transfer results, where X-axis depicts the number of training examples per label. \u201cbase/large\u201d in parenthesis denotes RoBERTa-base/large.\nLabel\nType\nTraining Data\nWorld\nterms\nworld | country | international | politics\nWiki.\nIn its most general sense, the term \u201cworld\u201d refers to the\ntotality of entities, to the whole of reality or to everything\nthat is.\ndict.\nhumankind; the human race; humanity\nSports\nterms\nsport | sports | racing | baseball\nWiki.\nSport pertains to any form of competitive physical activity\nor game that aims to use, maintain or improve physical\nability and skills while providing enjoyment to partici-\npants and, in some cases, entertainment to spectators.\ndict.\nan athletic activity requiring skill or physical prowess and\noften of a competitive nature, as racing, baseball, tennis,\ngolf, bowling, wrestling, boxing, hunting, fishing, etc.\nBusiness\nterms\nbusiness | finance | money | trade\nWiki.\nBusiness is the activity of making one\u2019s living or making\nmoney by producing or buying and selling products (such\nas goods and services).\ndict.\nthe purchase and sale of goods in an attempt to make a\nprofit.\nSci/Tech\nterms\ntechnology | science | computer | biology\nWiki.\nTechnology is the continually developing result of ac-\ncumulated knowledge and application in all techniques,\nskills, methods, and processes used in industrial produc-\ntion and scientific research.\ndict.\nthe branch of knowledge that deals with the creation and\nuse of technical means and their interrelation with life,\nsociety, and the environment, drawing upon such subjects\nas industrial arts, engineering, applied science, and pure\nscience.\nTable 17:\nLABELDESC data for AGNews (and\nYahooAG).\nLabel\nType\nTraining Data\nVery\nNegative\nterms\nawful | terrible | horrendous | horrible | dreadful\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nNegative\nterms\nbad | unpleasant | unsatisfying | lousy | subpar\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nNeutral\nterms\nokay | mediocre | decent | average | alright\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nPositive\nterms\ngood | nice | fine | pleasant | neat\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nVery\nPositive\nterms\ngreat | amazing | excellent | fantastic | outstanding\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nTable 18: LABELDESC data for Yelp-5 and SST-5. \u201cSent.\u201d and \u201ct\u201d refer to hand-crafted sentence templates and terms, respectively.\nLabel\nType\nTraining Data\nNegative\nterms\nawful | terrible | horrendous | horrible | dreadful | bad |\nunpleasant | unsatisfying | lousy | subpar\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nPositive\nterms\ngood | nice | fine | pleasant | neat | great | amazing | excel-\nlent | fantastic | outstanding\nsent.\nIt was t. | A(n) t experience. | Just t. | Overall, it was t.\nTable 19: LABELDESC data for Yelp-2, SST-2, Amz-2 and IMDB.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8e4f/8e4f96f6-d1e0-4d7c-9635-3791c0c66c51.png\" style=\"width: 50%;\"></div>\nLabel\nType\nTraining Data\nSociety\n&\nCulture\nterms\nsociety | culture\nWiki.\nA society is a group of individuals involved in persistent social interaction, or a large social group sharing the same\nspatial or social territory, typically subject to the same political authority and dominant cultural expectations. | Culture\nis an umbrella term which encompasses the social behavior, institutions, and norms found in human societies, as well\nas the knowledge, beliefs, arts, laws, customs, capabilities, and habits of the individuals in these groups.\ndict.\nan organized group of persons associated together for religious, benevolent, cultural, scientific, political, patriotic, or\nother purposes. | the behaviors and beliefs characteristic of a particular group of people, as a social, ethnic, professional,\nor age group (usually used in combination)\nScience\n&\nMathematics\nterms\nscience | mathematics\nWiki.\nScience is a systematic endeavor that builds and organizes knowledge in the form of testable explanations and\npredictions about the universe. | Mathematics is an area of knowledge that includes such topics as numbers, formulas\nand related structures, shapes and the spaces in which they are contained, and quantities and their changes.\ndict.\na branch of knowledge or study dealing with a body of facts or truths systematically arranged and showing the operation\nof general laws | the systematic treatment of magnitude, relationships between figures and forms, and relations between\nquantities expressed symbolically.\nHealth\nterms\nhealth | fitness | medical | diet\nWiki.\nHealth, according to the World Health Organization, is \u201ca state of complete physical, mental and social well-being and\nnot merely the absence of disease and infirmity\u201d.\ndict.\nthe general condition of the body or mind with reference to soundness and vigor\nEducation\n&\nReference\nterms\neducation | reference\nWiki.\nEducation is a purposeful activity directed at achieving certain aims, such as transmitting knowledge or fostering skills\nand character traits. | Reference is a relationship between objects in which one object designates, or acts as a means by\nwhich to connect to or link to, another object.\ndict.\nthe act or process of imparting or acquiring general knowledge, developing the powers of reasoning and judgment,\nand generally of preparing oneself or others intellectually for mature life. | a book or other source of useful facts or\ninformation, such as an encyclopedia, dictionary, etc.\nComputers\n&\nInternet\nterms\ncomputer | internet\nWiki.\nA computer is a digital electronic machine that can be programmed to carry out sequences of arithmetic or logical\noperations (computation) automatically. | The Internet (or internet) is the global system of interconnected computer\nnetworks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices.\ndict.\na programmable electronic device designed to accept data, perform prescribed mathematical and logical operations\nat high speed, and display the results of these operations. Mainframes, desktop and laptop computers, tablets, and\nsmartphones are some of the different types of computers. | Usually the internet (except when used before a noun). a\nvast computer network linking smaller computer networks worldwide. The internet includes commercial, educational,\ngovernmental, and other networks, all of which use the same set of communications protocols\nSports\nterms\nsport | sports | racing | baseball\nWiki.\nSport pertains to any form of competitive physical activity or game that aims to use, maintain or improve physical\nability and skills while providing enjoyment to participants and, in some cases, entertainment to spectators.\ndict.\nan athletic activity requiring skill or physical prowess and often of a competitive nature, as racing, baseball, tennis, golf,\nbowling, wrestling, boxing, hunting, fishing, etc.\nBusiness\n&\nFinance\nterms\nbusiness | finance\nWiki.\nBusiness is the activity of making one\u2019s living or making money by producing or buying and selling products (such as\ngoods and services). | Finance is the study and discipline of money, currency and capital assets.\ndict.\nthe purchase and sale of goods in an attempt to make a profit. | the management of revenues; the conduct or transaction\nof money matters generally, especially those affecting the public, as in the fields of banking and investment.\nEntertainment\n&\nMusic\nterms\nentertainment | music\nWiki.\nEntertainment is a form of activity that holds the attention and interest of an audience or gives pleasure and delight. |\nMusic is generally defined as the art of arranging sound to create some combination of form, harmony, melody, rhythm\nor otherwise expressive content.\ndict.\nthe act of entertaining; agreeable occupation for the mind; diversion; amusement | an art of sound in time that expresses\nideas and emotions in significant forms through the elements of rhythm, melody, harmony, and color.\nFamily\n&\nRelationships\nterms\nfamily | relationship\nWiki.\nFamily is a group of people related either by consanguinity (by recognized birth) or affinity (by marriage or other\nrelationship). | The concept of interpersonal relationship involves social associations, connections, or affiliations\nbetween two or more people.\ndict.\na basic social unit consisting of parents and their children, considered as a group, whether dwelling together or not; a\nsocial unit consisting of one or more adults together with the children they care for. | an emotional or other connection\nbetween people\nPolitics\n&\nGovernment\nterms\npolitics | government\nWiki.\nPolitics is the set of activities that are associated with making decisions in groups, or other forms of power relations\namong individuals, such as the distribution of resources or status. | A government is the system or group of people\ngoverning an organized community, generally a state.\ndict.\nthe science or art of political government. | the political direction and control exercised over the actions of the members,\ncitizens, or inhabitants of communities, societies, and states; direction of the affairs of a state, community, etc.; political\nadministration\nTable 20: LABELDESC data for Yahoo Answers.\n<div style=\"text-align: center;\">Table 20: LABELDESC data for Yahoo Answers.</div>\nLabel\nType\nTraining Data\nCompany\nterms\ncompany | firm | corporation | business\nWiki.\nA company, abbreviated as co., is a legal entity representing an association of people, whether natural, legal or a\nmixture of both, with a specific objective.\ndict.\na number of persons united or incorporated for joint action, especially for business\nEducational\nInstitution\nterms\neducational institution | university | college | school\nWiki.\nAn educational institution is a place where people of different ages gain an education, including preschools, childcare,\nprimary-elementary schools, secondary-high schools, and universities.\ndict.\nan institution for instruction in a particular skill or field.\nArtist\nterms\nartist | writer | actor | singer\nWiki.\nAn artist is a person engaged in an activity related to creating art, practicing the arts, or demonstrating an art.\ndict.\na person who produces works in any of the arts that are primarily subject to aesthetic criteria.\nAthlete\nterms\nathlete | sports | footballer | weightlifter\nWiki.\nAn athlete (also sportsman or sportswoman) is a person who competes in one or more sports that involve physical\nstrength, speed, or endurance.\ndict.\na person trained or gifted in exercises or contests involving physical agility, stamina, or strength; a participant in a sport,\nexercise, or game requiring physical skill.\nOffice\nHolder\nterms\noffice-holder | politics | mayor | president\nWiki.\nA person who\u2019s been appointed to a position by a company or organisation but doesn\u2019t have a contract or receive regular\npayment may be an office-holder.\ndict.\na person filling a governmental position; public official.\nMean\nof\nTransportation\nterms\nmean of transportation | car | bus | train\nWiki.\nTransport (in British English), or transportation (in American English), is the intentional movement of humans, animals,\nand goods from one location to another.\ndict.\na means of transporting or conveying, as a truck or bus.\nBuilding\nterms\nbuilding | apartment | skyscraper | tower\nWiki.\nA building or edifice, is an enclosed structure with a roof and walls standing more or less permanently in one place,\nsuch as a house or factory (although there\u2019s also portable buildings).\ndict.\na relatively permanent enclosed construction over a plot of land, having a roof and usually windows and often more\nthan one level, used for any of a wide variety of activities, as living, entertaining, or manufacturing.\nNatural\nPlace\nterms\nnatural place | forest | mountain | river\nWiki.\nThe natural environment or natural world encompasses all living and non-living things occurring naturally, meaning in\nthis case not artificial.\ndict.\nexisting in or formed by nature (opposed to artificial)\nVillage\nterms\nvillage | town | countryside | rural\nWiki.\nA village is a clustered human settlement or community, larger than a hamlet but smaller than a town (although the\nword is often used to describe both hamlets and smaller towns), with a population typically ranging from a few hundred\nto a few thousand.\ndict.\na small community or group of houses in a rural area, larger than a hamlet and usually smaller than a town, and\nsometimes (as in parts of the U.S.) incorporated as a municipality.\nAnimal\nterms\nanimal | insect | bird | fish\nWiki.\nAnimals are multicellular, eukaryotic organisms in the biological kingdom Animalia.\ndict.\nany member of the kingdom Animalia, comprising multicellular organisms that have a well-defined shape and usually\nlimited growth, can move voluntarily, actively acquire food and digest it internally, and have sensory and nervous\nsystems that allow them to respond rapidly to stimuli: some classification schemes also include protozoa and certain\nother single-celled eukaryotes that have motility and animallike nutritional modes.\nPlant\nterms\nplant | flower | tree | grass\nWiki.\nPlants are predominantly photosynthetic eukaryotes, forming the kingdom Plantae.\ndict.\nBotany. any member of the kingdom Plantae, comprising multicellular organisms that typically produce their own\nfood from inorganic matter by the process of photosynthesis and that have more or less rigid cell walls containing\ncellulose, including vascular plants, mosses, liverworts, and hornworts: some classification schemes may include\nfungi, algae, bacteria, and certain single-celled eukaryotes that have plantlike qualities, as rigid cell walls or the use of\nphotosynthesis.\nAlbum\nterms\nalbum | soundtrack | mixtape | CD\nWiki.\nAn album is a collection of audio recordings issued on compact disc (CD), vinyl, audio tape, or another medium such\nas digital distribution.\ndict.\na record or set of records containing several musical selections, a complete play or opera, etc.\nFilm\nterms\nfilm | movie | comedy | drama\nWiki.\nA film \u2013 also called a movie, motion picture, moving picture, picture, photoplay or (slang) flick \u2013 is a work of visual art\nthat simulates experiences and otherwise communicates ideas, stories, perceptions, feelings, beauty, or atmosphere\nthrough the use of moving images.\ndict.\na sequence of consecutive still images recorded in a series to be viewed on a screen in such rapid succession as to give\nthe illusion of natural movement; motion picture.\nWritten\nWork\nterms\nwritten work | novel | newspaper | book\nWiki.\nA book is a medium for recording information in the form of writing or images, typically composed of many pages\n(made of papyrus, parchment, vellum, or paper) bound together and protected by a cover.\ndict.\na handwritten or printed work of fiction or nonfiction, usually on sheets of paper fastened or bound together within\ncovers.\n<div style=\"text-align: center;\">Table 21: LABELDESC data for DBPedia.</div>\nTable 21: LABELDESC data for DBPedia.\ndataset\nRoBERTa\nlabel\nprecision(%)\nrecall(%)\nF1(%)\nzero-shot\nLDT\nzero-shot\nLDT\nzero-shot\nLDT\nAGNews\nbase\nWorld\n58.7\u00b112.8\n80.2\u00b17.3\n29.1\u00b127.8\n62.0\u00b118.2\n33.7\u00b121.2\n68.0\u00b112.0\nBusiness\n60.6\u00b18.1\n71.0\u00b16.3\n66.9\u00b114.0\n77.0\u00b14.6\n63.0\u00b19.7\n73.7\u00b13.9\nSports\n72.9\u00b114.7\n94.1\u00b11.5\n92.5\u00b19.6\n94.4\u00b16.2\n80.1\u00b18.7\n94.1\u00b13.3\nSci/Tech\n65.3\u00b115.8\n69.9\u00b19.2\n62.4\u00b114.8\n76.4\u00b16.0\n60.6\u00b18.1\n72.4\u00b14.3\nlarge\nWorld\n81.6\u00b110.0\n78.8\u00b16.3\n53.1\u00b121.3\n84.1\u00b16.8\n61.5\u00b115.1\n81.0\u00b14.3\nBusiness\n53.1\u00b113.7\n67.4\u00b19.9\n84.6\u00b19.2\n86.4\u00b15.8\n63.6\u00b17.1\n74.9\u00b14.7\nSports\n86.8\u00b1",
    "paper_type": "method",
    "attri": {
        "background": "Pretrained language models (PLMs) have produced strong results in zero-shot text classification for a range of topic and sentiment tasks, often using a pattern-verbalizer approach. However, this approach is sensitive to the choice of specific pattern/verbalizer pairs, leading to a need for a new method that can improve accuracy and robustness in zero-shot classification.",
        "problem": {
            "definition": "The problem addressed in this paper is the sensitivity of zero-shot text classification methods to the choice of patterns and verbalizers, which can significantly affect model performance.",
            "key obstacle": "The main challenge is that subtle changes in patterns or verbalizers can lead to large performance variations, making it difficult to maintain consistent accuracy across different datasets."
        },
        "idea": {
            "intuition": "The idea stems from the observation that providing richer descriptions of labels can enhance model understanding and classification accuracy without relying heavily on input text.",
            "opinion": "The proposed idea involves training on small curated datasets that describe the labels in natural language, referred to as LABELDESC data, instead of using typical annotated input texts.",
            "innovation": "The key innovation is the introduction of LABELDESC data, which allows for improved zero-shot classification accuracy by providing a more stable and robust training approach that is less sensitive to specific patterns and verbalizers."
        },
        "method": {
            "method name": "LABELDESC Training",
            "method abbreviation": "LABELDESCTRAINING",
            "method definition": "LABELDESCTRAINING is a method that involves using small datasets containing natural language descriptions of labels to finetune pretrained language models for improved zero-shot classification.",
            "method description": "This method leverages curated label descriptions to enhance the performance of zero-shot classifiers without requiring the original input texts.",
            "method steps": [
                "Curate small datasets that describe the labels for the classification task.",
                "Finetune pretrained language models using the LABELDESC data.",
                "Evaluate the model's performance on various text classification tasks."
            ],
            "principle": "This method is effective because it focuses on enriching the semantic understanding of the labels, which helps the model generalize better across different datasets, leading to improved classification accuracy."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on multiple topic and sentiment datasets, including AGNews, Yahoo Answers, DBPedia, SST, Yelp Reviews, IMDB, and Amazon Reviews, comparing LABELDESCTRAINING against standard zero-shot classification methods.",
            "evaluation method": "The performance of the method was assessed by measuring accuracy and standard deviation across various datasets and patterns, using pretrained RoBERTa models."
        },
        "conclusion": "LABELDESCTRAINING significantly improves zero-shot classification accuracy by 17-19% across various datasets, demonstrating greater robustness to pattern and verbalizer choices compared to traditional zero-shot methods. It also shows potential for domain independence, allowing the same training set to be used across different text classification tasks.",
        "discussion": {
            "advantage": "The main advantages include improved accuracy, reduced sensitivity to pattern and verbalizer selection, and the ability to generalize across different domains with the same label set.",
            "limitation": "The method relies on the quality of the curated label descriptions and may not fully mitigate biases present in the pretrained models, especially if the labels are dissimilar from the training examples.",
            "future work": "Future research could explore applying the LABELDESC approach to other tasks, such as structured prediction and natural language inference, and investigate ways to reduce reliance on patterns and verbalizers."
        },
        "other info": {
            "data availability": "The datasets used for LABELDESC training and evaluation are publicly available on GitHub.",
            "ethics statement": "The research employs pretrained models and curated data from resources like Wikipedia and dictionary definitions, acknowledging potential biases in these sources."
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The method LABELDESCTRAINING involves using small datasets containing natural language descriptions of labels to finetune pretrained language models for improved zero-shot classification."
        },
        {
            "section number": "4.3",
            "key information": "The main advantages of the proposed method include improved accuracy, reduced sensitivity to pattern and verbalizer selection, and the ability to generalize across different domains with the same label set."
        },
        {
            "section number": "6",
            "key information": "The method relies on the quality of the curated label descriptions and may not fully mitigate biases present in the pretrained models, especially if the labels are dissimilar from the training examples."
        },
        {
            "section number": "7.1",
            "key information": "Future research could explore applying the LABELDESC approach to other tasks, such as structured prediction and natural language inference, and investigate ways to reduce reliance on patterns and verbalizers."
        },
        {
            "section number": "2.2",
            "key information": "Pretrained language models have produced strong results in zero-shot text classification for a range of topic and sentiment tasks, often using a pattern-verbalizer approach."
        }
    ],
    "similarity_score": 0.543557805411578,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-12-0928_,arti/papers/The Benefits of Label-Description Training for Zero-Shot Text Classification.json"
}