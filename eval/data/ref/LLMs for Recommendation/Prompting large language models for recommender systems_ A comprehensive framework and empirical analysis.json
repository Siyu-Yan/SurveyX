{
    "from": "google",
    "scholar_id": "VHRRjLlKROoJ",
    "detail_id": null,
    "title": "Prompting large language models for recommender systems: A comprehensive framework and empirical analysis",
    "abstract": " Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2018 Association for Computing Machinery. Manuscript submitted to ACM\n\nand academia [23, 28, 29, 84]. Over the past decade, various recommendation algorithms have been proposed to solve\nrecommendation tasks by capturing the personalized interaction patterns from user behaviors [39, 144]. Despite the\nprogress of conventional recommenders, the performance is highly dependent on the limited training data from a few\ndatasets and domains, and there are two major drawbacks. On the one hand, traditional models lack the general world\nknowledge beyond interaction sequences. For complex scenarios that need to think or plan, existing methods do not\nhave commonsense knowledge to solve such tasks [27, 71, 112, 119]. On the other hand, traditional models cannot\ntruly understand intentions and preferences of users. The recommendation results do not have explainability, and\nrequirements expressed by users in explicit forms such as natural languages are difficult to consider [47, 52, 126].\nRecently, Large Language Models (LLMs) such as ChatGPT have demonstrated impressive abilities in solving general\ntasks [24, 118], showing their potential in developing next-generation recommender systems. The advantages of\nincorporating LLMs into recommendation tasks are two-fold. Firstly, the excellent performance of LLMs in complex\nreasoning tasks indicates the rich world knowledge and superior inference ability, which can effectively compensate\nfor the local knowledge of traditional recommenders [1, 75, 85]. Secondly, the language modeling abilities of LLMs\ncan seamlessly integrate massive textual data, enabling them to extract features beyond IDs and even understand user\npreferences explicitly [30, 50]. Therefore, researchers have attempted to leverage LLMs for recommendation task",
    "bib_name": "xu2024prompting",
    "md_text": "# Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis\n\nLANLING XU and JUNJIE ZHANG, Gaoling School of Artificial Intelligence, Renmin University of China, China\nBINGQIAN LI, Beijing Key Laboratory of Big Data Management and Analysis Methods, China\nJINPENG WANG and MINGCHEN CAI, Meituan Group, China\n\nWAYNE XIN ZHAO \u2217 and JI-RONG WEN, Gaoling School of Artificial Intelligence, Renmin University, China\n\nRecently, large language models such as ChatGPT have showcased remarkable abilities in solving general tasks, demonstrating the\npotential for applications in recommender systems. To assess how effectively LLMs can be used in recommendation tasks, our study\nprimarily focuses on employing LLMs as recommender systems through prompting engineering. We propose a general framework for\nutilizing LLMs in recommendation tasks, focusing on the capabilities of LLMs as recommenders. To conduct our analysis, we formalize\nthe input of LLMs for recommendation into natural language prompts with two key aspects, and explain how our framework can be\ngeneralized to various recommendation scenarios. As for the use of LLMs as recommenders, we analyze the impact of public availability,\ntuning strategies, model architecture, parameter scale, and context length on recommendation results based on the classification of\nLLMs. As for prompt engineering, we further analyze the impact of four important components of prompts, i.e., task descriptions, user\ninterest modeling, candidate items construction and prompting strategies. In each section, we first define and categorize concepts in\nline with the existing literature. Then, we propose inspiring research questions followed by experiments to systematically analyze the\nimpact of different factors on two public datasets. Finally, we summarize promising directions to shed lights on future research.\n\nCS Concepts: \u2022 Information systems \u2192 Recommender systems\n\ndditional Key Words and Phrases: Large Language Models, Recommender Systems, Empirical Study\n\nACM Reference Format:\n\n# ACM Reference Format:\n\nLanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao, and Ji-Rong Wen. 2018. Prompting Large\nLanguage Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis. In. ACM, New York, NY, USA,\n46 pages. https://doi.org/XXX.XXX\n\n# 1 INTRODUCTION\n\nIn order to alleviate the problem of information overload [31, 76], recommender systems explore the needs of users and\nprovide them with recommendations based on their historical interactions, which are widely studied in both industry\n\n\u2217 Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author.\n\n\u2217 Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author.\n\n\u2217 Wayne Xin Zhao (batmanfly@gmail.com) is the corresponding author.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a9 2018 Association for Computing Machinery. Manuscript submitted to ACM\n\nand academia [23, 28, 29, 84]. Over the past decade, various recommendation algorithms have been proposed to solve\nrecommendation tasks by capturing the personalized interaction patterns from user behaviors [39, 144]. Despite the\nprogress of conventional recommenders, the performance is highly dependent on the limited training data from a few\ndatasets and domains, and there are two major drawbacks. On the one hand, traditional models lack the general world\nknowledge beyond interaction sequences. For complex scenarios that need to think or plan, existing methods do not\nhave commonsense knowledge to solve such tasks [27, 71, 112, 119]. On the other hand, traditional models cannot\ntruly understand intentions and preferences of users. The recommendation results do not have explainability, and\nrequirements expressed by users in explicit forms such as natural languages are difficult to consider [47, 52, 126].\nRecently, Large Language Models (LLMs) such as ChatGPT have demonstrated impressive abilities in solving general\ntasks [24, 118], showing their potential in developing next-generation recommender systems. The advantages of\nincorporating LLMs into recommendation tasks are two-fold. Firstly, the excellent performance of LLMs in complex\nreasoning tasks indicates the rich world knowledge and superior inference ability, which can effectively compensate\nfor the local knowledge of traditional recommenders [1, 75, 85]. Secondly, the language modeling abilities of LLMs\ncan seamlessly integrate massive textual data, enabling them to extract features beyond IDs and even understand user\npreferences explicitly [30, 50]. Therefore, researchers have attempted to leverage LLMs for recommendation tasks.\nTypically, there are three ways to employ LLMs to make recommendations: (1) LLMs can serve as the recommender to\nmake recommendation decisions, encompassing both discriminative and generative recommendations [3, 12, 20, 32, 133].\n(2) LLMs can be leveraged to enhance traditional recommendation models by extracting semantic representations of\nusers and items from text corpora. The extensive semantic information and robust planning capabilities of LLMs are\nintegrated into traditional models [1, 16, 27, 31, 71, 107, 112, 119]. (3) LLMs are utilized as the recommendation simulator\nto execute external generative agents in the recommendation process, where users and items may be empowered by\nLLMs to stimulate the virtual environment [18, 100, 101, 130, 132]. We mainly focus on the first scenario in this paper.\nConsidering the gap between the general knowledge from large language models and the domain knowledge from\nrecommendation models [3, 136], there are two key factors for prompting LLMs as recommenders, i.e., how to select a\nLLM as the foundation model and how to construct a prompt as the prompting text. As for LLMs, a growing number of\nopen-source and closed-source models have emerged, and the same model also has different variants due to settings such\nas parameter scales and context lengths [140]. There are notable variations in the performance of different LLMs when\nit comes to general language tasks such as generation and reasoning [24, 118]. However, the performance differences of\nLLMs in recommendation tasks have not been fully explored. It is worth discussing how to select corresponding LLMs\nfor specific scenarios and develop corresponding training strategies. As for the prompt, it is an important medium for\ninteractions between humans and language models, and a well-designed prompt can better stimulate the powerful\ncapabilities of LLMs [43, 70]. To stimulate the recommendation ability of language models, prompt engineering should\ninvolve not only task description and prompting strategies for general tasks, but also the incorporation of user interest\nmodeling and the creation of candidate items in recommender systems [17, 70, 125].\nAlthough existing studies have made initial attempts to explore the recommendation capabilities of LLMs like Chat\nGPT [12, 20, 32, 68, 89], and some studies have used paradigms such as fine-tuning and instruction tuning to train\nLLMs in the field of recommender systems [2, 3, 133, 141], they focus on exploring the performance of a certain task\nrather instead of constructing a comprehensive framework to formalize the potential applications of LLM-powered\nrecommender systems. There are also systematic reviews concentrating on the progress of LLMs [140] and surveys of\n2\n\nTable 1. An overview of the primary discoveries presented in our work. We summarize new findings in the second column as \u201cn findings\u201d, and conduct experiments to verify findings discussed in existing literature as \u201cre-validated findings\u201d.\n\nTable 1. An overview of the primary discoveries presented in our work. We summarize n findings\u201d, and conduct experiments to verify findings discussed in existing literature as \u201cr\n\n\u2022 Fine-tuning all parameters of LLMs for recommen-\ndation is more effective than parameter-efficient fine-\ntuning, but more training time is required.\n\u2022\ning LLMs as recommenders, such as position bias [32,\n74] and lack of domain knowledge [125, 141].\nPrompts\nTask description\n\u2022 Our framework can be adapted to point-wise, pair-\nwise and list-wise recommendation tasks.\n\u2022 Different recommendation tasks can be imple-\nmented by LLMs through prompts [21, 108].\nUser interest Modeling\n\u2022 For short-term interest, it is preferable to summa-\nrize recent interactions into text and recommend.\n\u2022 For long-term interest, it is useful to maintain a\npersonalized memory to store and retrieve.\n\u2022 Long-term preferences and short-term intentions\nshould be effectively combined to model users.\n\u2022 For short-term interest, only truncating the most\nrecent items is not the optimal strategy [62].\n\u2022 Increasing the number of historical items to repre-\nsent users brings insignificant gains for LLMs [32].\n\u2022 Personalized user profiles and customized item\ndescriptions assist in the user interest modeling for\nLLM-based recommendations [89, 125].\nCandidate items construction\n\u2022 Candidate items construction for LLMs is crucial\nto the final recommendation results.\n\u2022 Retrieving candidate items by traditional recom-\nmendation models first, and then re-ranking items\nby LLMs can further improve the results, but the per-\nformance varies on specific methods and datasets.\n\u2022 LLMs can select from a given candidate item\nlist [12, 68], as well as directly generate recommen-\ndation results [71, 103].\n\u2022 Indexing methods and grounding strategies of\nitems are important factors affecting the effective-\nness of LLM-based recommendations [54, 60, 63].\nPrompting strategies\n\u2022 In chain-of-thought prompting, specific problem\ndecomposition is required for recommendation tasks\nrather than general prompts.\n\u2022 The few-shot prompting strategy has insignificant\nadvantage in recommendation scenarios.\n\u2022 Although provided with historical items in chrono-\nlogical order, LLMs still needs explicit guidance to\nunderstand the importance of recent items [32, 74].\n\u2022 Role-playing and expert-like prompts can leverage\nthe capabilities of LLMs in specific fields [38, 105].\nOverall\n\u2022 Leveraging LLMs as recommenders lies in stimulat-\ning the general knowledge of LLMs and integrating\nthe domain knowledge with the user interest.\n\u2022 LLMs are restricted by the unacceptable inference\ntime [54], expensive memory cost [96], limited con-\ntext length [62] and black-box abilities [55].\n<div style=\"text-align: center;\">Task description\n</div>\n<div style=\"text-align: center;\">Candidate items construction\n</div>\nrecommender systems empowered by LLMs [51, 61, 116]. However, previous surveys generally use specific criteria to\nclassify existing work and introduce them separately. They mainly focus on showcasing related work and summarizing\nadvantages and limitations, rather than conducting additional experiments to validate existing results and explore new\n\n<div style=\"text-align: center;\">Re-validated findings\n</div>\ndiscoveries. Our work focuses on the ability of LLMs to directly serve as recommenders, aiming to establish a general\nframework of Pro mpting L arge L anguage M odels for Rec ommendation (ProLLM4Rec).\n\nIn order to conduct our analysis for ProLLM4Rec, we formalize the input of LLMs for recommendation into natural\nlanguage prompts with two key aspects: LLMs and prompts, and explain how our framework can be generalized to\nvarious recommendation scenarios and tasks. As for the use of LLMs as recommenders, we analyze the impact of\nthe public availability, tuning strategies, model architecture, parameter scale, and context length on recommendation\nresults based on the classification of LLMs. As for prompt engineering, we further analyze the impact of four important\ncomponents of prompts, i.e., task description, user interest modeling, candidate items construction, and prompting\nstrategies. Given personalized prompts that include task description and user interest, the LLM selects, generates, or\nexplains candidate items based on general world knowledge and personalized user profiles. For each module, we first\ndefine and categorize concepts in line with the existing literature. Then, we propose inspiring research questions,\nfollowed by detailed experiments to systematically analyze the impact of different conditions on the recommendation\nperformance. Based on the empirical analysis, we finally summarize empirical findings for future research.\n\n# In general, the contributions of our work can be summarized as follows:\n\n\u2022 We derive a general framework ProLLM4Rec to sum up existing work of utilizing LLMs as foundation models fo\nrecommendation, which can be generalized to multiple scenarios and tasks by different LLMs and prompts.\n\nIn what follows, we first review the related work in Section 2. In Section 3, we present our proposed general framework\nand its instantiation, and introduce overall settings of the following experiments. As the core components of this\npaper, we discuss two main aspects of ProLLM4Rec, i.e., LLMs and prompts in Section 4 and Section 5, respectively. For\neach aspect, we generalize key factors that affect recommendation results, and conduct corresponding experiments to\nsummarize empirical findings. At last, Section 6 concludes this paper and sheds lights on future directions.\n\n# 2 RELATED WORK\n\n# 2.1 Recommender Systems\n\nFor tackling the challenge of information overload [76, 93, 127], recommender systems have become pivotal tools for\ndelivering personalized contents for users across various domains. In line with previous studies, recommendation\nalgorithms aim to derive user preferences and behavioral patterns from their historical interactions. The most common\ntechnique for the interaction-based recommendation is Collaborative Filtering (CF) [86, 90], which recommends items\nbased on preferences of similar users. Matrix Factorization (MF) [42] is a prevalent approach in collaborative filtering,\n\nand it constructs embedding representations for users and items from the interaction matrix, facilitating the algorithm\nto calculate similarity scores efficiently. Furthermore, Neural Collaborative Filtering (NCF) [29], integrating deep\nneural networks, replaces the inner product used in MF with a neural architecture, thereby demonstrating better\nperformance than previous methods. Contemporary advancements in deep neural network architectures have enhanced\nthe integration of user and item embeddings [66]. For example, since recommendation data can be represented as\ngraph-structured data, Graph Neural Network (GNN) [117] can be utilized to encode the information of the interaction\ngraph (nodes consist of users and items), and generate meaningful representations via message propagation and\ncontrastive learning strategies [28, 64, 104, 115]. As Pre-trained Language Models (PLM) gain prominence, there is a\ngrowing interest in pre-trained large-scale recommendation models powered by PLMs [31, 135, 144]. In addition to\nuser-item pairs and IDs, content-based recommendation algorithms leverage auxiliary modalities such as textual and\nvisual information to augment user and item representations in recommendation tasks [80, 110, 127].\n\n# 2.2 Large Language Models for Recommender Systems\n\nLarge Language Models (LLMs) are a cutting-edge advancement in artificial intelligence that excel in understanding and\ngenerating human-like texts [81, 95, 129]. LLMs are usually transformer-based models and trained on vast amounts of\ntextual data with billions of parameters, allowing them to comprehend contexts, generate coherent sentences, and even\nmimic human conversations [24, 118]. Through this process, LLMs have shown prominent potentials in the field of\nNatural Language Processing (NLP), and have demonstrated various incredible capabilities in dealing with complex NLP\ntasks, including but not limited to In-Context Learning (ICL) [4], instruction following [96] and step-by-step reasoning\nabilities [140]. Recently, LLMs have been increasingly integrated into recommender systems to provide personalized\nrecommendations [51, 116]. Recent studies have explored the fusion of LLMs with recommender systems, which can be\ndivided into the three paradigms, i.e., LLM as recommendation model (Section 2.2.1), LLM improves recommendation\nmodels (Section 2.2.2) and LLM as recommendation simulator (Section 2.2.3) as follows.\n2.2.1 LLM as Recommendation Model. This paradigm takes the LLM as a recommender system. Employing diverse\nstrategies like pre-training, fine-tuning, or prompting, LLMs can combine general knowledge with input data to yield per\nsonalized recommendations for users [12, 32, 37]. Due to the variety of recommendation tasks, LLM as recommendation\nmodel can be categorized into two types: discriminative recommendation and generative recommendation.\n\u2022 Discriminative recommendation instructs LLMs to make recommendation decisions on the given candidate items,\nusually focusing on item scoring [19] and re-ranking tasks [12]. For Click-Through Rate (CTR) prediction tasks, Liu et\nal. [68] designed specific zero-shot and few-shot prompts to evaluate abilities of LLMs on rating predictions. LLMs\nwere required to assign a score for the item according to the previous rating history of users and the score range\ngiven in prompts, while the result indicated that LLMs can outperform classical rating methods (e.g., MF and MLP)\nin few-shot conditions [68]. Kang et al. [40] further formulated the rating prediction task as multi-class classification\nand regression task, investigating the influence of model size on recommendation performance. Different from these\nmethods, Hou et al. [32] structured a re-ranking task, employing in-context learning approaches for LLMs to rank\nitems in the candidate pool. Previous studies highlighted the sensitivity of LLMs to the sequence of interaction histories\nprovided in prompts [74], which can be alleviated by strategies such as recency-focused prompting [32].\n\u2022 Generative recommendation requires LLMs to generate items recommended to users, either from candidate item\nlists within prompts or from LLMs with general knowledge [51]. GenRec [37] leveraged the contextual comprehension\n5\n\nability of LLMs to transform interaction histories into formulated prompts for next-item predictions. To address instances\nwhere GenRec might propose items absent in candidate lists, GPT4Rec [48] came up with the method that used BM25\nalgorithm to retrieve the most similar item in candidate item lists with the item generated by LLMs. In addition to\ntop-n recommendations, LLMs can be leveraged for generative tasks such as explainable recommendations [11, 44, 49,\n73, 124] and review summarization [21, 69, 108]. Moreover, with the incredible abilities in dialogue comprehension\nand communication, LLMs are naturally considered as the backbone of conversational and interactive recommender\nsystems. ChatRec [20] designed an interactive recommendation framework based on ChatGPT, which can comprehend\nrequirements of users through multi-turn dialogues and traditional recommendation models. Moreover, RecLLM [18]\ncombined the dialogue management module with a ranker module and a controllable LLM-based user simulator to\ngenerate synthetic conversations for tuning system modules. Apart from these methods, InteRecAgent [35] employed\nLLMs as the brain and recommender models as tools, combining their respective strengths to create an interactive\nrecommender system [35]. As a conversational recommender system, InteRecAgent enabled traditional recommender\nsystems to become interactive systems with a natural language interface through the integration of LLMs.\nThere are mainly two paradigms for adapting LLMs as recommenders, i.e., non-tuning paradigm and tuning paradigm.\n\u2022 Non-tuning paradigm keeps parameters of LLMs fixed and extracts the general knowledge of LLMs with prompting\nstrategies. Existing work of non-tuning paradigm focuses on designing appropriate prompts to stimulate recommenda\ntion abilities of LLMs [58, 108, 125]. Liu et al. [68] proposed a prompt construction framework to evaluate abilities of\nChatGPT on five common recommendation tasks, each type of prompts contained zero-shot and few-shot versions.\nHou et al. [32] not only used prompts to evaluate abilities of LLMs on sequential recommendation, but also introduced\nrecency-focused prompting and in-context learning strategies to alleviate order perception and position bias issues of\nLLMs. ChatRec [20] and InteRecAgent [35] mentioned above are also within the classic non-tuning paradigm.\n\u2022 Tuning paradigm aims to update parameters of LLMs to inject recommendation capabilities into LLM itself. The\ntuning strategies include fine-tuning [33, 141] and instruction tuning [72, 79]. P5 [21] proposed five types of instructions\ntargeting at different recommendation tasks to fine-tune a T5 [81] model. The instructions were formulated based\non conventional recommendation datasets with designed templates, which equipped LLMs with generation abilities\nfor unseen prompts or items [21]. InstructRec [133] further designed abundant instructions for tuning, including 39\nmanually designed templates with preference, intention, task form and context of a user. Compared with these methods,\nTallRec [3] used LoRA [33], a parameter-efficient tuning method, to handle the two-stage tuning for LLMs. It was first\nfine-tuned on general data of Alpaca [92], and then further fine-tuned with the historical information of users.\nAlthough LLMs as recommendation models present a way of utilizing the common knowledge of LLMs, it still encounters\nsome problems to be coped with. Due to the high computational cost [60, 96] and slow inference time [54], LLMs are\nstruggled to be efficient enough compared to traditional recommendation methods [20, 32]. Additionally, constraints\non input sequence length will limit the amount of external information (e.g., candidate item lists) [62], leading to the\ndegrading performance of LLMs in scenarios such as sequential recommendation. Furthermore, since information in\nrecommendation tasks is challenging to be expressed in natural language [63, 126], it is hard to formulate appropriate\nprompts that make LLMs truly understand what they are required to do, leading to unexpected performance.\n2.2.2 LLM Improves Recommendation Models. This method mainly utilizes LLMs to generate auxiliary information\nto enhance the performance of recommendation models [16, 107, 112], based on the reasoning abilities and common\n\nknowledge. The research on how to improve recommendation models with LLMs can be divided into three categori\ni.e., LLM as feature encoder, LLM for data augmentation and LLM co-optimized with domain-specific models.\n\n\u2022 LLM as feature encoder. The representation embeddings of users and items are important factors in classical\nrecommender systems [28, 84]. LLMs serving as feature encoders can generate related textual data of users and items,\nand enrich their representations with semantic information. U-BERT [80] injected user representations with user\nreview texts, item review texts and domain IDs, augmenting the contextual semantic information in user vectors. Wu et\nal. [114], on the other hand, employed language models to generate item representations for news recommendation.\nWith the development of LLMs and prompting strategies, BDLM [134] constructed the prompt consisting of interaction\nand contextual information into LLMs, and obtained top-layer feature embeddings as user and item representations.\n\u2022 LLM for data augmentation. For this paradigm, LLMs are required to generate auxiliary textual information for\ndata augmentation [1, 66, 71, 112]. By using prompting or in-context learning strategies, the related knowledge will be\nextracted out in different text forms to facilitate recommendation tasks [16, 107, 119]. One form of auxiliary textual\ninformation is summarization or text generation, enabling LLMs to enrich representations of users or items [110]. For\nexample, Du et al. [16] proposed a job recommendation model which utilized the capability of LLMs for summarization\nto extract user information and job requirements. Considering item descriptions and user reviews, KAR [119] extracted\nthe reasoning knowledge on user preferences and the factual knowledge on items through specifically designed prompts,\nwhile SAGCN [66] utilized a chain-based prompting strategy to generate semantic information. Another form of\nusing the textual features generated from LLMs is for graph augmentation in the recommendation field. LLMRG [107]\nleveraged LLMs to extend nodes in recommendation graphs. The resulting reasoning graph was encoded using GNN,\nwhich served as additional input to enhance sequential models. LLMRec [112] adopted three types of prompts to\ngenerate information for graph augmentation, including implicit feedback, user profile and item attributes.\n\u2022 LLM co-optimized with domain-specific models. The categories mentioned above mainly focus on the impact of\ncommon knowledge for domain-specific models [110]. However, LLM itself often struggles to handle domain-specific\ntasks due to the lack of task-related information [40, 125]. Therefore, some studies conducted experiments to bridge the\ngap between LLMs and domain-specific models. BDLM [134] proposed an information sharing module serving as an\ninformation storage mechanism between LLMs and domain-specific models. The user embeddings and item embeddings\nstored in the module were updated in turn by the LLM and the domain-specific model, enhancing the performance\nof both sides. CoLLM [136] combined LLMs with a collaborative model, which formed collaborative embeddings for\nLLM usage. By tuning LLM and collaborative module, CoLLM showed great improvements in both warm and cold-start\nscenarios. In conversational recommender systems, approaches such as ChatRec [20] and InteRecAgent [35] considered\nLLMs as the backbone, and leveraged traditional recommendation models for candidate item retrieval.\nIn addition to the context limitation and computational cost of LLMs [96], the paradigm that LLM improves recom\nmendation models also encounters other problems. (1) Although LLMs can enhance offline recommender systems to\navoid online latency, this paradigm also limits the ability of LLMs to model real-time collaborative filtering information,\nneglecting the key factor for recommendation [110, 112, 119]. (2) Feature encoding, data augmentation, and collaborative\ntraining inevitably expose the user data to LLMs, which may bring privacy, security and ethical issues [6, 87, 113].\n2.2.3 LLM as Recommendation Simulator.  Due to the gap between offline metrics and online performance of recom\nmendation methods [29, 77], it is necessary for the designed approach to get intents of users by simulating real-world\n\nelements. In this way, LLM as the recommendation simulator is introduced by taking LLMs as the foundational archi\ntecture of generative agents, and agents simulate the virtual users in the recommendation environment [101, 130, 132].\n\nRecently, there emerged a lot of work studying the performance of LLMs as the recommendation simulator. Agent4rec [130\nwas a movie simulator consisting of two core fractions: LLM-empowered generative agents and recommendation\nenvironment. The work equipped each agent with user profile, memory and actions modules, mapping basic behaviors\nof real-world users. AgentCF [132], on the other hand, considered not only users but also items as agents. It captured\nthe two-sided relations between users and items, and optimized these agents by prompting them to reflect on and adjust\nthe misleading simulations collaboratively [132]. Moreover, in addition to behaviors within the recommender system,\nRecAgent [100, 101] took external influential factors of user agent simulation into account, such as friend chatting and\nsocial advertisement. In order to describe users accurately, RecAgent applied five features for users, and implemented\ntwo global functions including real-human playing and system intervention to operate agents flexibly.\nAlthough LLM as recommendation simulator aims to imitate real-world recommendation behaviors to enhance the\nrecommendation performance, it still has deficiencies in some aspects. Firstly, since current work is mainly demo\nsystems that operate a few agents [100, 101], there still exists a gap between virtual agent environment and real-world\npractical recommendation applications, which requires further research and development. Additionally, LLMs may\narise privacy and safety concerns. Many studies take ChatGPT as the architecture of agents, presenting security risks to\nthe recommended information for users [132]. Moreover, Zhang et al. [130] have explored that hallucination in LLMs\ncan exert huge impact on recommendation simulations. The LLM sometimes fails to accurately simulate human users,\nsuch as providing inconsistent score for an item and fabricating non-existent items for rating.\n\n# 2.3 Differences with Existing Work\n\nPrevious surveys of LLMs for recommender system usually categorized existing work with a classification standard\nand introduced studies respectively. Lin et al. [61] categorized existing work into the targets and the methods of\nadapting LLMs to recommendation tasks. Wu et al. [116] mainly focused on the form of information between LLMs and\nrecommender systems, including LLM embeddings, LLM tokens and LLMs. Fan et al. [17] summarized the framework\ninto four sections, involving deep representation learning, pre-training, fine-tuning and prompting of LLMs. These\nresearch mainly concentrated on demonstrating related work and summarizing the advantages and limitations.\nCompared to previous work, we concentrate on the ability of LLMs leveraged for recommendation tasks, and provide a\nsystematic empirical analysis on prompting LLM-based recommendations by devising a general framework ProLLM4Rec.\nWe mainly focus on two aspects, i.e., LLMs and prompt engineering, providing definitions and solutions from both\nconceptual and methodological perspectives. Furthermore, we conduct experiments to discover new findings and\nvalidate results previously discussed in existing research, serving as an inspiration for future research efforts.\n\n# 3 GENERAL FRAMEWORK AND OVERALL SETTINGS\n\nIn this section, we present our proposed ProLLM4Rec with two important components, namely LLMs and prompts.\nGenerally speaking, the rich world knowledge and general capabilities of LLMs demonstrate the potential to develop LLM\npowered recommender systems. Nevertheless, it is essential to introduce appropriate prompting engineering to provide\ndomain-specific knowledge on recommendation tasks for LLMs to serve as personalized recommenders. In the following,\nwe first describe our general framework by defining several key elements (Section 3.1). Then, we explain how our\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9099/90999ae4-717c-43fa-b8b7-0ede3f658e81.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. The overall framework of our proposed ProLLM4Rec.\n</div>\nframework can be generalized to various recommendation scenarios and tasks by framework instantiation (Sectio\nFinally, we introduce the details of the overall experimental settings for further analysis (Section 3.3).\n\n# 3.1 Overview of the Approach\n\nWhen it comes to the use of LLMs in recommender systems, the first step involves choosing suitable LLMs tailored\nto specific scenarios, with an emphasis on their distinct capabilities. Subsequently, it is important to conduct prompt\nengineering to instruct LLMs to perform effective recommendations. Typically, in the realm of recommender systems\nusing LLMs, despite the task settings in existing studies vary a lot, the overall prompting format remains relatively\nconsistent with minor variations [12, 20, 32, 108]. Therefore, to unify existing prompting approaches for different recom\nmendation purposes, as illustrated in Fig. 1, we establish a general framework for prompt engineering in ProLLM4Rec,\nconsisting of four key factors in the prompts. Firstly, task description is required to clearly express the specific goal of\nrecommendation tasks. Secondly, prompts need to be carefully designed, to express user interest and meanwhile enable\nLLMs to provide personalized recommendations with both world and domain knowledge. Thirdly, the purpose of a\nrecommender system is to provide users with candidate items, and potential candidate items should be constructed\nfor LLMs to facilitate the recommendation with the understanding of domain-specific item information. Furthermore,\nspecial prompting strategies can be further employed to enhance the specialized recommendation capabilities of LLMs.\nIn what follows, we will thoroughly examine the impact of each factor on the recommendation performance.\n3.1.1 Key Elements in Our Framework. To carry out the experiments, we first describe the key elements in ProLLM4Rec,\nto clarify their definitions and scope in this work. Specially, we introduce the following five elements for our framework:\n\u2022 Large language models. As proposed in previous research [2, 32, 53, 136], there exists a large gap between general\nlanguage modeling and personalized user behavioral modeling, making it non-trivial to utilize LLMs in recommender\nsystems. In this work, we investigate the efficacy of LLMs from perspectives of public availability, tuning strategies,\nmodel architecture, parameter scale, and context length, aiming to gain insights into the selection of appropriate LLMs\nfor performing recommendation tasks. Observations and discussions on the use of LLMs are presented in Section 4.\n\u2022 Task description. To adapt LLMs to the scenario of recommendation, it is necessary to clearly express the context\nand target of recommender systems for LLMs in the prompt, i.e., task description. With different prompt descriptions of\ntasks, large language models can be utilized to various recommendation scenarios and tasks such as click-through rate\npredictions [2, 3, 40], sequential recommendation [12, 32, 141] and conversational recommender systems [20, 30, 105].\n\u2022 User interest modeling. The modeling of user interest is the key to recommendation tasks [29, 84]. When leveraging\nLLMs for recommendation, users are generally expressed in natural language text, which is different from traditional\n9\n\nLLMs\nTask description\nUser interest\nCandidate items\nPrompting strategies\nRelated Work\nNot tuning setting\nCTR\npredictions,\nrating, re-ranking\nrecent and relevant\nitems (with attributes),\nuser profile\npointwise, pairwise,\nlistwise item(s)\nchain of thoughts, in-\ncontext learning, role\nprompting\n[12, 14, 32, 40, 58, 65,\n68, 69, 74, 89, 99, 108,\n109, 125, 142]\nChatGPT,\nGPT-4\nconversational rec-\nommender systems\nuser explicit interest,\ninteractive feedback\nrecalled from tradi-\ntional models\nrole prompting\n[18, 20, 30, 35, 38,\n105, 145]\ngenerative\nrecom-\nmendation\nrecent items (with at-\ntributes), user profile\n(not provided, gen-\neration methods)\nbasic prompts\n[71, 103]\n(Parameter-efficient) Fine-tuning setting\nLLaMA,\nLLaMA2,\nVicuna,\nChatGPT\nCTR\npredictions,\nrating, re-ranking\nrecent and relevant\nitems (with attributes),\nuser profile, collabora-\ntive embedding\npointwise, listwise\nitem(s)\nchain of thoughts, role\nprompting, soft prompt-\ning\n[3, 14, 19, 40, 55, 60,\n62, 88, 98, 122, 128,\n136]\nLLaMA,\nLLaMA2,\nBART, GPT\nrecall, retrieving\nrecent and relevant\nitems (with attributes)\n(not provided, item\ngrounding methods)\nbasic prompts\n[2, 37, 48, 54, 63, 78,\n135, 141]\nInstruction tuning setting\n(Flan-)T5,\nLLaMA2\nrating,\nranking,\nretrieving, explana-\ntion, summarization\nrecently\ninteracted\nitems,\nuser\nprofile,\nshort-term intentions\npointwise, pairwise,\nlistwise item(s)\nbasic prompts\n[9, 21, 57, 72, 79,\n133]\napproaches capturing user preference from ID-based behavior sequences [39, 115]. In this paper, we mainly consider\nthe reflected user interest based on his or her interaction behaviors with interacted items. Especially, as detailed in\nSection 5.2, we employ item description texts, user profiles, and historical interactions between users and items to\nreveal the underlying user interest using natural languages [62, 89, 108, 125].\n\u2022 Candidate items construction. The purpose of recommender systems is to provide users with items to choose from,\nso candidate items construction is a crucial step in our framework [12, 32, 133]. A simple approach is to provide several\ncandidate items in prompts for the LLM, e.g., the items recalled by traditional recommendation models [62, 128]. Due\nto the input length limitation of LLMs, it is not possible to include all items in the prompts. In addition to selecting\nsuitable candidate sets, there are also methods that directly generate candidate items by LLMs, utilizing strategies such\nas output probability distribution [128] and vector quantization [141] for item indexing and grounding. Section 5.3 will\nfocus on the construction strategies of candidate items, including selection and generation.\n\u2022 Prompting strategies. Despite the impressive capabilities of LLMs, they tend to exhibit unsatisfactory performance\nin providing personalized recommendations [12, 32, 40, 68]. The reason may stem from the significant semantic gap\nbetween the general knowledge encoded in LLMs and the domain-specific behavioral pattern and item catalogs of\nrecommender systems [53, 136]. To specialize LLMs to recommender systems, we summarize and propose several\nprompting strategies specialized for recommendation tasks. Details will be discussed in Section 5.4.\n\napproaches capturing user preference from ID-based behavior sequences [39, 115]. In this paper, we mainly consider\nthe reflected user interest based on his or her interaction behaviors with interacted items. Especially, as detailed in\nSection 5.2, we employ item description texts, user profiles, and historical interactions between users and items to\nreveal the underlying user interest using natural languages [62, 89, 108, 125].\n\n<div style=\"text-align: center;\">Table 3. Statistics of two public datasets for ProLLM4Rec.\n</div>\nDataset\n#User\n#Item\n#Interaction\nSparsity\nItem Attributes\nMovieLens-1M\n6,040\n3,706\n1,000,209\n4.4642%\nrelease year, title, genre\nAmazon-Books\n13,469\n12,984\n1,142,940\n0.6536%\ntitle, categories, brand, price, description\n# 3.2 Instantiation of ProLLM4Rec\n\nBy combining the key elements mentioned above, we can instantiate various types of recommender systems in our\nframework with the following five steps. Specifically, (1) we can employ LLMs with varying levels of public availability,\ndifferent tuning strategies, model architectures, parameter scales, and context lengths. (2) We can define a range\nof task description, such as retrieving, rating, recalling, and ranking. (3) Regarding user interest modeling, we can\nemploy different types of interest, representation forms, and modeling methods. (4) When collecting candidate items,\nwe take into account their different representation types, sources, and grounding methods. (5) Additionally, we can\nintroduce several well-designed prompting strategies to effectively guide the recommendation capabilities of LLMs.\nTo demonstrate the compatibility and versatility of our framework, we summarize previous work on LLM-powered\nrecommender systems in Table 2 based on various settings in our framework ProLLM4Rec.\n\n# 3.3 Experimental Settings\n\nIn this section, we introduce the overall settings of the following experiments. We first describe the bas\nof the two public datasets, and then present the configurations and implementations to conduct our stu\n\nIn this section, we introduce the overall settings of the following exp\nof the two public datasets, and then present the configurations and i\n\nIn this section, we introduce the overall settings of the following experiments. We first describe the basic information\nof the two public datasets, and then present the configurations and implementations to conduct our study in detail.\n3.3.1 Datasets. The domain characteristics of movies and books are closer to the general knowledge of LLMs, which\nfacilitates the further analysis. Considering the scale, popularity and side information of public datasets, we select two\nrepresentative datasets to conduct our study, i.e., MovieLens-1M [25] and Amazon Books (2018) [76] as follows.\n\u2022 MovieLens-1M [25] is one of the most widely used benchmark datasets in the field of recommender systems, covering\nmovie ratings and attributes on the website movielens.org. We use the one million version from the MovieLens datasets.\n\u2022 Amazon Books (2018) [76] is an updated version of the Amazon review dataset released in 2014. At first, Amazon\nonly operated online book sales business, so the data in the book field is the most abundant. To improve the data quality,\nwe filter out inactive users and unpopular products, and remove dirty data without necessary attributes.\nIn our research, we are concerned about how LLMs can fully utilize the domain knowledge to make recommendations,\nand use the title of items as the input for prompts. However, titles are not enough to describe items, and there are\ndeviations between the text of the title and the content of the item itself (e.g., the movie Twelve Monkeys). Therefore,\nwe further investigate the benefits of detailed item descriptions on the recommendation effect. As shown in Table 3,\nthere are no item descriptions in the original dataset of MovieLens, only the release year, title, and genre. To enrich the\nmovie dataset, we use the general knowledge of ChatGPT 1 to generate text descriptions for movies.\n3.3.2 Configuration and Implementation. As for ProLLM4Rec, we can directly evaluate the cold-start recommendation\nability of large language models in the zero-shot setting, as well as evaluate the fine-tuned ability with few or full\n\n1 The URL of ChatGPT API: https://chat.openai.com/. Note that there are multiple versions of the ChatGPT API, and OpenAI has released interfaces on March 1 and June 13, 2023, respectively. In the absence of clear annotations, the ChatGPT used in this article is \u201cgpt3.5-turbo-4k-0613\u201d.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1154/1154baf1-95c2-4670-9bc7-31bd625089fc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a160/a160800e-28e4-4146-8afe-6d06d35bfde8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. The basic prompts used in the MovieLens-1M dataset for the following experiments.\n</div>\n# Fig. 2. The basic prompts used in the MovieLens-1M dataset for the following experiments\n\nrecommendation samples in the fine-tuning setting. Considering the typical scenarios of LLMs in recommender systems,\nwe conduct experiments on two representative task settings, i.e., (1) the zero-shot ranking task without modifying\nparameters of LLMs [12, 20, 32, 74] and (2) the click-through rate prediction task with LLMs tuned [3, 19, 40, 88].\n\u2022 Zero-shot ranking task. On the one hand, we evaluate the zero-shot recommendation performance of LLMs\nfor cold-start scenarios to study the effect of LLMs and the design of prompts. In this paper, our approach mainly\nconcentrates on ranking tasks that better reflect the capabilities of LLMs [12, 20, 32, 74]. As shown in Fig. 1, information\nof users and items is encoded into the prompt as inputs for LLMs. In this setting, we do not modify the parameters of\nLLMs, so the evaluated models are closed-source large language models or open-source models without fine-tuning. To\nconduct experiments on the impact of each factor on recommendation results with LLMs, we implement the overall\narchitecture based on the open-source recommendation library RecBole [121, 138, 139] and the zero-shot re-ranker\nLLMRank [32]. Our basic prompt used in the MovieLens-1M dataset for the zero-shot ranking task is shown in Fig. 2(a).\n\u2022 Click-through rate prediction task with LLMs tuned. On the other hand, we evaluate the fine-tuned recommen\ndation performance of LLMs to explore how LLMs adapt to recommendation scenarios with data provided [3, 19, 40, 88].\nAlthough our framework can be generalized to various recommendation tasks, we concentrate on exploring the fine\ntuning performance of LLMs with point-wise Click-Through Rate (CTR) prediction tasks to reduce selection bias. In this\nsetting, we not only consider fine-tuning LLMs using recommendation data, but also devise a two-stage approach of\nusing instruction data to fine-tune LLMs first, and then implement recommendation fine-tuning for further adaptation.\nSpecifically, we compare the fine-tuned recommendation performance of the original LLM and the LLM after instruction\ntuning, respectively. LLaMA-7B [95] and LLaMA2-7B [96] are the original models, while Alpaca-lora-7B [92] and\nLLaMA2-chat-7B [96] are LLMs after instruction tuning. In terms of the tuning strategies of LLMs, we report results\nwith both parameter-efficient fine-tuning and complete fine-tuning. We implement the fine-tuning framework based on\nthe open-source library transformers and the instruction tuning code of LLaMA with Stanford Alpaca data 2. Our\ninstruction data used in the MovieLens-1M dataset for the click-through rate prediction task is illustrated in Fig. 2(b).\n3.3.3 Evaluation Metrics. As for the zero-shot ranking task, considering economic and efficiency factors, we refer to\nexisting literature [32, 89] to randomly sample 200 users instead of evaluating results on the whole dataset. For each\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/116f/116fc8f6-314c-4ba7-b075-d1d0fbc40794.png\" style=\"width: 50%;\"></div>\nuser from the sample set, we sort all items that the user interacts with in the chronological order. Then, we evaluate\nthe results based on the leave-one-out strategy and treat the last interacted item as the ground truth. For performance\ncomparison, we fix the length of candidate items to 20 as in [32], and mix the other 19 items from the ground truth in\nrandom positions by default. As for evaluation, we utilize two widely used ranking metrics in recommender systems,\ni.e., Recall [5] and Normalized Discounted Cumulated Gain (NDCG) [36]. Since 20 items are selected for candidate\ngeneration, we set \ud835\udc58 = 20 for Recall@ \ud835\udc58 to measure whether all ground-truth items have been recalled. In existing\nliterature, the re-generation method of multiple results until the format requirements are met can be employed to obtain\nthe final response [56], while we consider the recall capability within one inference for fair evaluation. Furthermore,\nwe set \ud835\udc58 = 1, 10, 20 for NDCG to explore the detailed recommendation performance in terms of ranking abilities. To\nassure a scientific research, we repeat each experiment three times and take the average value as the final result.\nAs for the CTR prediction task, we first sort the original dataset by timestamp, use the latest 10,000 records for\ntraining and evaluation, and regard other previous data as the interaction history of users. Then, we split the ten\nthousand interactions into the training, validation and test sets in ratio of 8:1:1. For each interaction, we retain 10\nhistorical interacted items as user representations, and set thresholds based on rating data to obtain user preferences [3].\nInteractions with a rating higher than or equal to the threshold will be considered items that the user likes, while the\nopposite indicates dislikes. For the MovieLens-1M dataset, the rating threshold is 4, and we set 5 for the Amazon-Books\ndataset. We employ the training data to fine-tuning LLMs and evaluate the recommendation performance on the test\nset. The validation set is used for selecting best checkpoints during the training process. As for evaluation of prediction,\nwe utilize the widely used metric for CTR predictions, i.e., accuracy. In a random state, the accuracy is around 0.5.\n3.3.4 Discussion on Variable Factors. To conduct our analysis, we mainly focus on two key aspects, i.e., LLMs and\nprompts. As for the effects of LLMs, we analyze the impact of public availability, tuning strategies, model architecture,\nparameter scale, and context length on recommendation results based on the classification of LLMs. As for prompt\nengineering, we further analyze the impact of four important components of prompts, i.e., task description, user interest\nmodeling, candidate items construction and prompting strategies. Due to the fact that all factors have an impact on\nthe final result, it is also crucial to select the other aspects when focusing on one aspect. Limited by resources and\nefficiency, it is neither necessary nor feasible to exhaust all possibilities. When not explicitly specified, the LLM uses\nChatGPT released in June 2023 to ensure the consistent recommendation quality. For the design of prompts, we refer to\nthe template in [32] and emphasize the most recent items to re-rank the random 20 candidate items. The default method\nfor modeling user interest is to concatenate the title sequence of recently interacted items using natural languages.\n\n# 4 THE IMPACT OF LARGE LANGUAGE MODELS AS RECOMMENDER SYSTEMS\n\nLLMs are the core of our framework ProLLM4Rec, and directly determine the performance of LLMs as recommender\nsystems [32, 40, 68, 69]. Therefore, it is worth exploring how to choose a suitable LLM as the foundation for recom\nmendation. In this section, we compare the differences between LLMs and traditional models on the recommendation\nperformance, discuss how the different properties and tuning strategies of LLMs affect the recommendation results,\npresent the limitations of LLMs as recommenders, and draw empirical conclusions through systematic experiments.\n\n# 4.1 Classification of Large Language Models\n\nIn this paper, we consider language models that have a size larger that one billion as LLMs. In line with existing\nresearches [17, 51, 61, 116, 140], LLMs can be categorized into different classes from several perspectives. Most typically,\n\nLLMs can be divided into open-source and closed-source models in terms of the public availability. When it comes to\nleveraging LLMs as the foundation model in recommender systems, tuning strategies can adjust LLMs towards specific\nrecommendation tasks. From the perspective of the model architecture, various LLMs can also be categorized into\ntypes of encoder-decoder, causal decoder, and prefix decoder [140]. For the same framework of a LLM, it is widely\nacknowledged that the parameter scale and context length are two key factors that jointly determine abilities of\nLLMs [32, 62]. To explore the recommendation performance with respect to different variants of LLMs, we focus on\nfive aspects, i.e., public availability, tuning strategies, model architecture, parameter scale and context length as follows.\n4.1.1 Public Availability. According to whether the model checkpoints can be publicly obtained, existing LLMs can be\ndivided into open-source models and closed-source models, and both can be leveraged as recommender systems.\n\u2022 Open-source models refer to LLMs whose model checkpoints can be publicly accessible. As shown in Table 2,\nresearchers often use recommendation data to fine-tune open-source models for performance improvement. As the\nrepresentative of open-source models, LLaMA [95] and its variants like Vicuna [8] are widely used when leverag\ning LLMs for recommender systems [54, 141]. Parameter-Efficient Fine-Tuning (PEFT) strategies such as Low-Rank\nAdaptation (LoRA) [33] are frequently adopted for recommendation data considering the trade-off between effect and\nefficiency [3, 60]. Other models such as the Flan-T5 [10] series from Google Inc. and ChatGLM [129] from Tsinghua\nUniversity are also popular in the field of recommender systems. The publicly available checkpoints of open-source\nmodels provide flexibility for LLMs to modify parameters tailored for recommendation tasks.\n\u2022 Closed-source models refer to LLMs whose model checkpoints can not be publicly accessible. For the closed-source\nLLMs utilized for recommenders, researchers generally study the zero-shot recommendation ability in cold-start\nscenarios. The most typical closed-source model is the ChatGPT series from OpenAI. The subsequent GPT-4 has\nstronger capabilities compared to ChatGPT, but it is still not open-source. In this paper, ChatGPT refers to the API\nfunction of \u201cgpt3.5-turbo-4k-0613\u201d unless specified. Without checkpoints, OpenAI provides several ways for researchers\nand users to improve the model performance on specific tasks, such as plugins for website browsing [45] and interfaces\nfor fine-tuning ChatGPT [55]. However, the flexibility of closed-source models as recommender systems is still limited\ndue to the expensive price and black-box parameters. Faced with this challenge, existing literature has explored to\ninject knowledge of recommender systems into closed-source models by means of prompt design [99, 125], retrieval\nenhancement [35, 89, 108] and combination of traditional recommendation models [20, 105].\n4.1.2 Tuning Strategies. During the deployment of LLMs in recommender systems, we can also classify existing work\ndepending on whether LLMs are fine-tuned, as well as the various fine-tuning strategies employed, i.e., not-tuning\nsetting, fine-tuning setting and instruction tuning setting.\n\u2022 Not-tuning setting means evaluating the zero-shot recommendation ability of LLMs, which is generally used\nfor closed-source models such as ChatGPT. By designing different prompt templates, LLMs without fine-tuning can\nbe directly used for recommendation tasks such as click-through rate predictions [14, 40], sequential recommen\ndation [32, 108], and conversational recommender systems [30, 38, 105]. In this case, the user interest is expressed\nexplicitly (e.g., ratings and reviews) or implicitly (interacted items of users), and the limited candidate items can be\nrecalled by traditional models, while prompting strategies such as role prompting and chain of thoughts are used.\nInspired by Artificial Intelligence Generated Content (AIGC), it is worth noting that the excellent generation ability of\nLLMs provides opportunities for generative recommendation [71, 103]. Without providing candidate items, generative\n14\n\n\u2022 Not-tuning setting means evaluating the zero-shot recommendation ability of LLMs, which is generally used\nfor closed-source models such as ChatGPT. By designing different prompt templates, LLMs without fine-tuning can\nbe directly used for recommendation tasks such as click-through rate predictions [14, 40], sequential recommen\ndation [32, 108], and conversational recommender systems [30, 38, 105]. In this case, the user interest is expressed\nexplicitly (e.g., ratings and reviews) or implicitly (interacted items of users), and the limited candidate items can be\nrecalled by traditional models, while prompting strategies such as role prompting and chain of thoughts are used.\nInspired by Artificial Intelligence Generated Content (AIGC), it is worth noting that the excellent generation ability of\nLLMs provides opportunities for generative recommendation [71, 103]. Without providing candidate items, generative\n\nlanguage models can directly generate the desired items that users need based on recommendation requirements, and\nthey can also be generalized into our framework ProLLM4Rec as shown in Table 2.\n\n\u2022 Fine-tuning setting means that using recommendation data to fine-tune LLMs as recommender systems. Considering\ncost and efficiency, researchers often use parameter-efficient fine-tuning (e.g., Low-Rank Adaptation of Large Language\nModels, LoRA [33]) to quickly adapt to recommendation scenarios [3, 60, 141]. As for LLMs, open-source models based\non LLaMA [95] are widely used, including but not limited to LLaMA, LLaMA2 and Vicuna [8] with different parameter\nsizes. Based on whether candidate items are provided, existing work on fine-tuning LLMs can be further divided into two\nkinds of recommendation tasks. On the one hand, researchers have explored fine-tuning models for recommendation\nthat provide candidate items such as rating, re-ranking and predictions. Specifically, the fine-tuning interface of the\nclosed-source model ChatGPT has brought new breakthroughs to the research of LLMs, and there have been attempts\nto fine-tune ChatGPT for recommendation tasks [55]. On the other hand, LLMs can also be fine-tuned for recall tasks\nof recommender systems by retrieving candidates from the whole item pool [2, 54, 63, 78]. Through well-designed\nindexing, alignment, and retrieval strategies, directly generating recommendation items without providing lengthy\ncandidate sequences is more suitable for practical application scenarios, which has not yet been fully explored.\n\u2022 Instruction tuning setting  means providing template instructions of recommenders as prompts to tuning tar\ngeted LLMs, generally involving multiple recommendation tasks [9, 21, 79, 133]. However, some fine-tuning meth\nods (e.g., TALLRec [3]) also involve the form of instructions. In order to avoid ambiguity, we consider the instructions\nof a single task template as fine-tuning settings, and the instructions of multiple tasks as instruction-tuning settings. As\nthe backbone of recommender systems, researchers generally use T5 or Flan-T5 for various recommendation scenarios\nsuch as rating, ranking, retrieving, explanation generation and news recommendation. Besides Flan-T5, RecRanker [72]\nis also proposed to instruction tuning LLaMA2 as the ranker for top\ud835\udc58 recommendation. By instantiating appropriate\ninstructions for each recommendation task, this setting can also be summarized into our framework.\n4.1.3 Model Architecture. For the architecture design of LLMs leveraged as recommender systems, we consider\nthree mainstream architectures as summarized in [140], i.e., encoder-decoder, prefix decoder, and causal decoder. The\nrecommendation scenarios for each framework are introduced in what follows.\n\u2022 The encoder-decoder architecture adopts two Transformer blocks as the encoder and decoder, which is the basis of\nBERT [97]. In line with existing literature on LLM-based recommender systems, the bidirectional property of the encoder\ndecoder architecture allows LLMs to easily customize encoders and decoders towards recommendation (e.g., dual-encoder\nconsidering both ids and text [79]), and conveniently adapt to multiple recommendation tasks [9, 21, 57, 79, 133]. A few\nLLMs use the encoder-decoder architecture, and the typical one is the series of T5 and its variant Flan-T5 [10].\n\u2022 The prefix decoder architecture is also the decoder-only architectures, which is known as non-causal decoder. It\ncan bidirectionally encode the prefix tokens like the encoder-decoder architecture, and perform unidirectional attention\non the generated tokens like the causal decoder architecture. One of the representative LLMs based on the prefix\ndecoder architecture is ChatGLM [129] and its variants, and researchers have attempted to explore the recommendation\nperformance of ChatGLM as one of the benchmarks in related work [69].\n\u2022 The causal decoder architecture has been widely adopted in various LLMs, and the series of GPT [4] and LLaMA [95]\nare the most representative models. It uses the unidirectional attention mask, and only decoders are deployed to process\nboth the input and output tokens. Due to the popularity of the causal decoder architecture, most LLM-based recommender\n15\n\n\u2022 The causal decoder architecture has been widely adopted in various LLMs, and the series of GPT [4] and LLaMA [95]\nare the most representative models. It uses the unidirectional attention mask, and only decoders are deployed to process\nboth the input and output tokens. Due to the popularity of the causal decoder architecture, most LLM-based recommender\n\nsystems employ this framework to adapt to different recommendation tasks such as click-through rate predictions,\nsequential recommendation and conversational recommender systems.\n\n4.1.4 Parameter Scale. To meet the diverse needs of different users, the most typical variant of LLMs is the parameter\nscale. In general, open-source models have multiple parameter sizes to choose from, and larger parameter sizes\ngenerally mean better capabilities [95, 140]. But meanwhile, the corresponding computational and spatial complexity\nwill also increase. Considering the memory and efficiency issues for experiments, researchers in the field of LLM-based\nrecommender systems generally use LLMs with parameters no more than 10B, while the performance of LLMs with\nlarger parameters remains to be further explored in the field of recommender systems [40].\n4.1.5 Context Length. Another property closely related to user needs is the length of the input context. The inability\nto handle inputs with longer contexts means that decision-making cannot be accurately developed, thereby limiting the\nmodel capabilities [96, 140]. When the user input exceeds the limited length, the input will be truncated, so sufficient\ncontext length is crucial for the user experience [12, 32, 74]. However, different lengths of context inputs imply different\nmodel architectures and parameters. When expanding the context length of a model, it often leads to higher time and\nmemory complexity. To address the length limitation of LLMs, existing methods either selectively discard previous\ncontexts using sliding windows [120], or only sample a portion of the context for retrieval augmentation [45, 62], or\nemploy small models without emergence ability. Despite recent strategies, the limitation of context length has not yet\nbeen truly resolved. Considering economic and efficiency issues, existing LLMs including open-source and closed-source\nmodels only provide a limited number of context length options. In this paper, we mainly focus on several classic\nlengths, i.e., 2K, 4K, 8K, 16K and 32K (K is the abbreviation for one thousand, similarly hereinafter).\n\n# 4.2 Research Questions and Experimental Setup\n\n4.2.1 Research Questions. In this section, we conduct experiments to verify the impact of different factors of LLMs on\nrecommendation results. Specifically, we focus on the following four research questions.\n\u2022 RQ1: What are the differences between the recommendation ability of LLMs and traditional recommenders?\n\u2022 RQ2: How do the different attributes of LLMs, including the public availability, model architectures, parameter\nscales, and context lengths affect the recommendation performance and inference time?\n\u2022 RQ3: What are the similarities and differences in recommendation results of LLMs with different tuning strategies?\nIs the LLM after instruction tuning more suitable for recommendation tasks?\n\u2022 RQ4: What are the limitations of leveraging LLMs as recommender systems?\n4.2.2 Evaluated Models. As for the experimental settings, we consider the following baselines and LLMs.\n\u2022 Random: Random baseline recommends the \ud835\udc58 (k=20 in this section) candidate items in a random order, which is\nthe basic situation to evaluate the metric values of each dataset.\n\u2022 Pop: Pop method always ranks the candidate items based on their interaction times with users in the training set.\nWe consider it as the fully-trained method since it uses the statistical information of datasets.\n\u2022 BPR [84]: BPR is one of the typical traditional models that utilize matrix factorization for recommendation. It is\ntrained in the pair-wise paradigm a.k.a., BPR training loss without considering temporal information.\n16\n\n\u2022 SASRec [39]: SASRec is a sequential recommendation model based on the backbone of the classic self-attention\nnetwork a.k.a., Transformer [97], and achieves comparable performance among sequential models.\n\u2022 ChatGPT: ChatGPT is a closed-source large-scale pre-trained language model developed by OpenAI. Note that\nOpenAI has released interfaces of ChatGPT on March 1 and June 13, 2023, respectively. Considering the up-to-date\nrequirements, we adopt the version on June 13 for recency, the same as GPT-4.\n\u2022 GPT-4: GPT-4 is the latest generation of closed-source natural language processing model launched by the OpenAI\ncompany. Experiments have shown that GPT-4 is significantly superior to ChatGPT in multiple tasks.\n\u2022 Flan-T5 [10]: Flan-T5 is an open-source language model based on the encoder-decoder architecture T5 released by\nGoogle [81]. Flan-T5 is extended from T5 by a multi-task fine-tuning paradigm i.e., instruction tuning to enhance\nthe generalization of different tasks. There are multiple variants of Flan-T5 in terms of parameters, including\nFlan-T5-Small (80M), Flan-T5-Base (250M), Flan-T5-Large (780M), Flan-T5-XL (3B) and Flan-T5-XXL (11B). Since\nthe first three models are too small to meet the requirements of LLMs discussed in this paper (1B, B is short for\nbillion and the same below), we consider the Flan-T5-XL and Flan-T5-XXL for comparison.\n\u2022 ChatGLM [129]: ChatGLM is an open-source bilingual dialogue LLM that supports both Chinese and English,\nbased on the General Language Model (GLM) [129] with the prefix decoder architecture. The team released the\nsecond version ChatGLM2 and the third version ChatGLM3 in June 2023 and October 2023, respectively.\n\u2022 LLaMA [95]: LLaMA is an open-source language model introduced by MetaAI from the causal decoder architecture\nwith four sizes (6B, 13B, 35B and 65B). Due to its outstanding performance and low computational cost, LLaMA has\nreceived much attention from researchers so far, and Vicuna [8] is one of the most popular variants by extending\nLLaMA. To further improve abilities of LLaMA, MetaAI released LLaMA2 as the next generation of open-source\nlarge language models in July, 2023. In addition to the regular version, MetaAI also provide the chatting version of\nLLaMA2 (i.e., LLaMA2-chat) [96], and it is specifically tuned for the dialogue scenario by Reinforcement Learning\nwith Human Feedback (RLHF).\n\n# 4.3 Observations and Discussion\n\n4.3.1 LLMs Compared to Traditional Recommenders (RQ1). Compared to traditional models based on collaborative\nfiltering of interacted data in fully-trained settings, we provide the cold-start recommendation performance of LLMs\nin zero-shot settings. In what follows, we introduce the empirical findings on the recommendation effect of different\nmodels from three aspects, i.e., recommendation performance, the impact of historical item sequences and inference time.\n\u2022 Recommendation performance of LLMs. As shown in Table 4, we provide the fully-trained results of four traditional\nmethods, as well as the zero-shot recommendation performance of various LLMs. For traditional recommenders, BPR [84]\nbased on collaborative filtering is significantly better than Pop based on popularity, and the sequential recommendation\n\n4.3.1 LLMs Compared to Traditional Recommenders (RQ1). Compared to traditional models based on collaborative\nfiltering of interacted data in fully-trained settings, we provide the cold-start recommendation performance of LLMs\nin zero-shot settings. In what follows, we introduce the empirical findings on the recommendation effect of different\nmodels from three aspects, i.e., recommendation performance, the impact of historical item sequences and inference time.\n\nin zero-shot settings. In what follows, we introduce the empirical findings on the recommendation effect of different\nmodels from three aspects, i.e., recommendation performance, the impact of historical item sequences and inference time.\n\u2022 Recommendation performance of LLMs. As shown in Table 4, we provide the fully-trained results of four traditional\nmethods, as well as the zero-shot recommendation performance of various LLMs. For traditional recommenders, BPR [84]\nbased on collaborative filtering is significantly better than Pop based on popularity, and the sequential recommendation\nmodel SASRec [39] combined with attention mechanism and temporal information is significantly better than BPR,\nwhich is consistent with the results in existing literature [39, 84, 144]. It is worth noting that for the 20 candidate\nitems, LLMs that rely on natural languages cannot completely recall all items in most cases, and several models with\npoor abilities can only output a dozen items, greatly limiting the accuracy of recommendation results. Therefore,\nrecall@20 indicates the ability of LLMs to memorize, re-rank and output candidate items, and several approaches such\nas the re-generation method [56] and probability distribution outputs [128] have been proposed to improve the recall\n\nTable 4. Overall performance of different models on recommendation. We consider both the fully-trained settings for traditiona models and zero-shot settings for LLMs. Note that there are always ground-truth items in the randomly selected 20 candidates, s the ideal recall@20 equals to 1. \u201cIT\u201d is for \u201cInference Time\u201d, and we record the average inference time for each user measured i seconds (s). \u201cN/A\u201d is the abbreviation for \u201cNot Applicable\u201d since the inference time of closed-source models is unknown.\n\nMovieLens-1M\nAmazon-Books\nmodel\ncontext\nlength\nparam.\nsize\nrecall@20\nndcg@1\nndcg@10\nIT (s)\nrecall@20\nndcg@1\nndcg@10\nIT (s)\nFully-trained settings for traditional models\nRandom\n-\n0\n1.0000\n0.0300\n0.2081\n0.01\n1.0000\n0.0350\n0.2628\n0.01\nPop\n-\n1\n1.0000\n0.1800\n0.4841\n0.03\n1.0000\n0.1000\n0.2672\n0.03\nBPR\n-\n<1M\n1.0000\n0.2550\n0.5743\n0.04\n1.0000\n0.2950\n0.6236\n0.04\nSASRec\n-\n<1M\n1.0000\n0.6400\n0.7916\n1.07\n1.0000\n0.6800\n0.8305\n1.49\nZero-shot settings for LLMs\nClosed-source LLMs\n4K\n-\n0.9583\n0.1817\n0.3985\nn/a\n0.9850\n0.2467\n0.4276\nn/a\nChatGPT\n16K\n-\n0.9600\n0.1500\n0.3735\nn/a\n0.9800\n0.2400\n0.4032\nn/a\nGPT-4\n8K\n-\n0.9900\n0.3100\n0.5828\nn/a\n1.0000\n0.3300\n0.5631\nn/a\nOpen-source LLMs with the encoder-decoder architecture\n3B\n0.0050\n0.0000\n0.0016\n3.51\n0.0000\n0.0000\n0.0000\n4.33\nFlan-T5\n0.5k\n11B\n0.0050\n0.0000\n0.0016\n5.21\n0.0000\n0.0000\n0.0000\n8.28\nOpen-source LLMs with the prefix decoder architecture\nChatGLM\n2K\n6B\n0.7750\n0.0300\n0.1945\n19.12\n0.7000\n0.0350\n0.2026\n19.52\nChatGLM2\n32K\n6B\n0.1900\n0.0450\n0.0885\n11.06\n0.1600\n0.0250\n0.0680\n13.62\n2K\n6B\n0.6900\n0.0950\n0.2762\n7.95\n0.6550\n0.0450\n0.2273\n10.79\nChatGLM3\n32K\n6B\n0.7750\n0.0700\n0.2579\n14.06\n0.7050\n0.0550\n0.2068\n16.89\nOpen-source LLMs with the causal decoder architecture\n7B\n0.2700\n0.0350\n0.1068\n9.11\n0.2650\n0.0200\n0.0992\n9.35\n13B\n0.2500\n0.0300\n0.1028\n9.51\n0.2250\n0.0250\n0.0867\n9.94\n33B\n0.3900\n0.0400\n0.1328\n92.88\n0.2950\n0.0350\n0.1015\n106.61\nLLaMA\n2K\n65B\n0.5300\n0.0450\n0.1913\n171.39\n0.3750\n0.0500\n0.1253\n182.51\n7B\n0.2650\n0.0500\n0.1078\n8.17\n0.4400\n0.0550\n0.1559\n11.12\nVicuna\n2K\n13B\n0.4100\n0.0550\n0.1507\n9.26\n0.4800\n0.0700\n0.1813\n12.72\n7B\n0.2350\n0.0500\n0.0888\n9.97\n0.5600\n0.0650\n0.1648\n10.01\n13B\n0.4500\n0.0700\n0.1215\n15.64\n0.5100\n0.0750\n0.2150\n14.44\nLLaMA2\n4K\n70B\n0.7600\n0.1250\n0.2918\n24.38\n0.6600\n0.1150\n0.2912\n24.63\n7B\n0.7950\n0.0900\n0.2744\n9.80\n0.7050\n0.1500\n0.3217\n9.86\n13B\n0.8050\n0.1650\n0.3866\n14.80\n0.7500\n0.1650\n0.3411\n12.25\nLLaMA2\n(chat)\n4K\n70B\n0.9550\n0.2430\n0.4344\n23.07\n0.8850\n0.2230\n0.3827\n25.01\nperformance. While for the traditional models, all candidate items can be recalled (i.e., recall@20 equals to 1). Because\nthe candidate items are not recalled completely, the recommendation effect of a few LLMs (e.g., Flan-T5 and ChatGLM)\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b04/9b0453bc-5891-4df6-b82a-e9d131e1ae4c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f70/8f709133-0e0b-4607-a14e-f2e3ccf69816.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a3c/4a3c7cd5-0c76-432d-913b-93c8ef907894.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) MovieLens-1M\n</div>\nis not even as good as the random baseline. For most LLMs, the zero-shot recommendation performance is not as good\nas the baseline method Pop based on popularity of interactions in the dataset [12, 32, 68]. However, the powerful LLMs\nlike ChatGPT and LLaMA-70B (chat) [96] can achieve better results than Pop in zero-shot settings. Furthermore, GPT-4\ncan even perform better than the fully-trained matrix factorization model BPR on two datasets, indicating the potential\nof LLMs to serve as the backbone of recommender systems. In addition, the significant differences between the results\nof LLMs demonstrate the importance of selecting appropriate LLMs for downstream recommendation tasks [40, 68, 69].\n\u2022 The impact of historical item sequences. As for ranking tasks in Fig. 2(a), the recently interacted historical items\nare used as the user representations. However, there is no standard value for the number of items that represent users.\nTo analyze the impact of historical item sequences on the recommendation performance, we conduct experiments to\nexplore the recommendation effect of the sequential recommender SASRec [39] and the closed-source LLM ChatGPT\nwith different numbers of historical interactions. For the fully-trained SASRec, the maximum length of the historical\nitem sequence will affect the model framework and prediction results [31, 39]. In order to ensure the model uniformity,\nwe fix the model checkpoint with the historical item sequence of 50 as the \u201cSASRec (fixed)\u201d for comparison, and evaluate\nthe recommendation performance (NDCG@10 [36]) with the number of historical items at 1, 5, 10, 20, 30, 40 and 50,\nrespectively. For ChatGPT and GPT-4, we get zero-shot results with different items to verify whether the powerful\nLLMs can deal with the long context for recommendation. As illustrated in Fig. 3, with the increasing number of\nhistorical items, the results of SASRec improve steadily, while the the recommendation performance of LLMs changes\nlittle. Consistent conclusions on two datasets can be drawn that even if LLMs can accept more historical items for user\nrepresentations, increasing the number of historical items does not bring significant gains in the recommendation\nperformance. The performance trends of LLMs show that the increased historical item sequence is not fully utilized\nby the language model, indicating the importance of selecting appropriate item sequences to represent users, and the\ninadequacy of LLMs for user interest mining. To improve the mining of user interest for LLMs, approaches such as\nretrieval augmentation [62] and prompting strategies [108, 125] can be used, which will be analyzed in Section 5.\n\u2022 Inference time of LLMs. In the actual deployment of recommender algorithms, the inference efficiency is the\ndecisive factor for industrial applications [91, 98]. In general, the inference time of models is closely related to the size\nof parameters. As shown in the last column of Table 4, for the lightweight traditional recommenders, the inference\ntime of SASRec is about 1 second for each user. However for LLMs, except that closed-source models cannot accurately\n\n<div style=\"text-align: center;\">(b) Amazon-Books\n</div>\nobtain inference time due to limitations of the API, the inference time of open-source models takes ne\nseconds for one prediction, leading to an unacceptable time delay in practical applications.\n\nObservations on the Overall Recommendation Performance of LLMs\n\u2022 In zero-shot scenarios, LLMs have cold-start capabilities, and GPT-4 even surpasses collaborative filtering\nmodels. However, all LLMs are inferior to fully-trained sequential recommendation models.\n\u2022 Even if LLMs can accept more historical items for user representations, increasing the number of historical\nitems does not bring significant gains in the recommendation performance.\n\u2022 Compared to recommenders, the inference time of LLMs is unacceptable to be used for real applications.\n\n4.3.2 LLMs on Recommendations w.r.t. Four Aspects (RQ2). For different LLMs, differences in public availability and\nmodel architecture will lead to different recommendation scenarios, results and inference time [40, 69]. For the same\nLLM, the parameter scale and context length also affect the efficiency and effectiveness of language models [140].\nTherefore, we explore the impact of different LLMs on recommendations from four aspects, namely public availability,\nmodel architecture, parameter scale and context length as follows.\n\u2022 Public availability. As shown in Table 4, closed-source models achieve significantly better results than the open\nsource models in the cold-start scenario, but they cannot outperform fully-trained sequential models. In terms of LLMs,\nChatGPT with zero-shot settings has comparable recommendation performance with the fully-trained Pop especially on\nthe sparse Amazon-Books dataset, indicating the fundamental ability of LLMs on recommendation tasks. Furthermore,\nthe upgraded GPT-4 exceeds ChatGPT by a large margin due to its strong zero-shot generalization ability. The superior\nzero-shot performance of GPT sheds lights on leveraging LLMs for recommendation. However, the open-source models\nalways get poor results compared to GPT-4 in the zero-shot settings, while LLaMA2-chat-70B has the comparable\nrecommendation performance with ChatGPT. The reason is that open-source models lack comprehensive cold-start\ncapabilities, and their strength lies in the ability to integrate domain knowledge through strategies such as prompt\ntuning. In line with previous studies on LLMs [32, 40, 68, 74], employing a closed-source model in cold-start scenarios\nyields better results, while an open-source model is more flexible and easy to use when tuning is needed [3, 54, 72, 141].\n\u2022 Model architecture. For the model architecture, Flan-T5 [10] based on the encoder-decoder architecture has almost\nno ability to recommend items in the cold-start setting, as its training corpus does not involve specialized instructions of\nour task. Trained with prompts of recommendations, the encoder-decoder architecture is suitable for prompt tuning and\ninstruction tuning [9, 21, 133]. Similarly, the first and second versions of ChatGLM [129] based on the prefix decoder\nperform poorly on the zero-shot ranking task, and are not as good as Vicuna and LLaMA2 based on the causal decoder.\nHowever, the third version of ChatGLM, i.e., ChatGLM3 has comparable recommendation performance with Vicuna [8]\nand LLaMA2 [96], which further indicates the importance of selecting an advanced foundation model. In terms of the\nseries of LLaMA models [95], LLaMA2 is better than Vicuna, and Vicuna is better than LLaMA, which is related to\ntheir training data and release time. Furthermore, the chat version of LLaMA2, i.e., LLaMA2-chat is a series that uses\nconversational dialog instructions to fine-tune LLaMA2 [96], which is more suitable for our tasks in ranking settings.\nTherefore, the results of LLaMA2-chat are significantly improved compared with LLaMA2, and LLaMA2-70B-chat even\nachieves better performance than the closed-source model ChatGPT. Generally speaking, researchers prefer to study\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3e9/b3e9c0aa-0db9-4fb4-b8f8-cee19408523d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) MovieLens-1M\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d34/9d348003-a437-4ce7-b2ee-8276f3fbd677.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0543/0543e44e-7245-4eef-9de7-d393795f8e1b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) MovieLens-1M\n</div>\n<div style=\"text-align: center;\">Fig. 5. The recommendation performance of LLMs w.r.t. the context len\n</div>\n<div style=\"text-align: center;\">recommendation tasks based on the causal decoder framework such as LLaMA, and the second version of LLaMA has\nbetter generalization ability than the first version in recommendation tasks.\n</div>\nrecommendation tasks based on the causal decoder framework such as LLaMA, and the second version of LLaMA has a\nbetter generalization ability than the first version in recommendation tasks.\n\u2022 Parameter scale. It is widely recognized that the larger the parameter size, the more powerful the LLMs [32, 40, 140],\nthe same applies in the field of recommender systems. To compare the effect and efficiency of LLMs w.r.t. the parameter\nscale, we compare the recommendation performance and inference time of LLaMA [95], Vicuna [8], LLaMA2 [96],\nand LLaMA2-chat at different parameter scales in Fig. 4. As the scale of parameters enlarges, the recommendation\nperformance and inference time of LLMs steadily increases, and both datasets (Fig. 4(a) and Fig. 4(b)) have consistent\nconclusions. Therefore, it is necessary to consider the trade-off between performance and efficiency when choosing the\nparameter scale. Moreover, the performance improvement on the increasing scale of LLaMA2 is more significant than\nthat of LLaMA, indicating that the scale effect of LLMs has to do with the capabilities of the base model.\n\u2022 Context length. Different LLMs have different maximum input limitations [95, 96, 140], and a longer context input\nmeans LLMs can accommodate more historical items for recommendation. However, it remains to be explored whether\nthe maximum input length of LLMs will affect the recommendation results when the length limitation is not exceeded.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7230/7230b1de-cf31-434c-bf46-760fd10cce22.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Amazon-Books\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78e7/78e70252-a678-4a85-91b5-3f63fbe619d9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Amazon-Books\n</div>\n<div style=\"text-align: center;\">Table 5. Overall performance of LLMs on CTR predictions. There are three settings, i.e., zero-shot setting without fine-tuning parameter-efficient fine-tuning (PEFT) setting with a few parameters tuned, and fine-tuning (FT) setting with all parameters tuned\n</div>\ndataset\nml-1m\nAmazon-Books\nmodel\nzero-shot\nPEFT\nFT\nzero-shot\nPEFT\nFT\nLLaMA-7B\n0.4683\n0.5479\n0.6658\n0.6488\n0.8262\n0.8469\nAlpaca-LoRA-7B\n0.5264\n0.5767\n0.6702\n0.6558\n0.8440\n0.8533\nLLaMA2-7B\n0.5284\n0.6133\n0.6457\n0.6754\n0.8550\n0.8542\nLLaMA2-chat-7B\n0.5255\n0.6275\n0.6731\n0.7174\n0.8560\n0.8660\nTherefore, we conduct experiments to investigate the differences in the recommendation performance between the\ntwo length versions of ChatGPT (4K and 16K) and ChatGLM3 (2K and 32K). As shown in Fig. 5, expanding the length\nlimitation of LLMs does not necessarily mean the better recommendation performance, while there is a slight decrease\nin NDCG@10. Furthermore, when the maximum input of LLMs remains unchanged, increasing the historical input of\nusers results in insignificant gains as shown in Fig. 3. Therefore, the key to the recommendation problem is to enable\nLLMs to effectively utilize the information within the limited context [62, 125], and a suitable context length selection\nfor LLMs as recommender systems is worthy of deep consideration.\n\n\u2022 As for the public availability, closed-source models outperform the open-source models in terms of the\nrecommendation performance, but have poorer flexibility.\n\u2022 As for the model architecture, different frameworks are adapted to different recommendation tasks and\nfine-tuning strategies, while LLMs with the casual decoder architecture are still mainstream.\n\u2022 As for the parameter scale, the larger the parameter scale, the better the recommendation ability.\n\u2022 As for the context length, a longer maximum context length leads to worse recommendation results.\n\n4.3.3 Comparisons of Tuning Strategies for LLMs (RQ3). Due to the fact that LLMs are not customized to recommender\nsystems during the training process, it is insufficient to only consider the zero-shot recommendation performance in cold\nstart scenarios [24, 40, 69, 72]. In order to explore the impact of different training strategies of LLMs on recommendations,\nwe compare the click-through rate prediction performance of four LLaMA-based LLMs on two datasets. Specifically,\nwe consider three training settings of LLMs, i.e.,  the zero-shot setting without fine-tuning, Parameter-Efficient Fine\nTuning (PEFT) setting (we use the LoRA [33] here) with a few parameters tuned, and Fine-Tuning (FT) setting with all\nparameters tuned, and summarize empirical conclusions from three aspects: overall performance of different settings, the\nimpact of instruction tuning, and the impact of training data.\n\u2022 Overall performance of different settings. As shown in Table 5, we can see that the results of fine-tuning\nLLMs (PEFT and FT) on only 256 samples are significantly better than the zero-shot performance in cold-start scenarios,\nand empirical findings are consistent across four LLMs (LLaMA, Alpaca-LoRA, LLaMA2, LLaMA2-chat) on both datasets.\nFurthermore, considering the two kinds of fine-tuning strategies, the performance of the fine-tuning setting is even\nbetter than that of the PEFT setting since more parameters are tuned [33]. In addition to recommendation effects,\ntraining efficiency is also a performance that deserves attention. Therefore, we compare the training time of the two\nfine-tuning strategies on two datasets. As shown in Fig. 6, the time for parameter-efficient fine-tuning is significantly\n22\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e78b/e78bf74f-ade7-4eec-9aaa-95affd496a0b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6341/634153c1-ddcb-4b04-822a-626abd5ac6e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) MovieLens-1M\n</div>\n<div style=\"text-align: center;\">Fig. 6. The comparison of training time w.r.t. the parameter-efficient fine-tuning (PEFT) and fine-tuning (FT) strategies.\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to assess the effectiveness of large language models (LLMs) in recommendation tasks, addressing knowledge gaps regarding their capabilities as recommenders through prompting engineering.",
            "scope": "The survey encompasses the utilization of LLMs in various recommendation scenarios, specifically focusing on prompting strategies, model architectures, and the impact of different factors on recommendation performance. It excludes the exploration of LLMs beyond recommendation tasks and does not cover non-LLM based recommendation methods."
        },
        "problem": {
            "definition": "The core issue explored in this survey is the integration of LLMs into recommender systems and how to effectively use them to provide personalized recommendations.",
            "key obstacle": "Primary challenges include the lack of explainability in traditional models, difficulties in integrating user preferences expressed in natural language, and the efficiency and performance limitations of LLMs in real-time applications."
        },
        "architecture": {
            "perspective": "The survey introduces a comprehensive framework called ProLLM4Rec, which categorizes existing research on LLMs in recommendation tasks and emphasizes the importance of prompt engineering to adapt LLMs for specific recommendation scenarios.",
            "fields/stages": "The survey organizes the current methods into different categories based on LLM types (open-source vs closed-source), tuning strategies (not-tuning, fine-tuning, instruction tuning), and model architectures (encoder-decoder, causal decoder)."
        },
        "conclusion": {
            "comparisons": "The comparative analysis indicates that while traditional recommendation models outperform LLMs in fully-trained settings, LLMs like GPT-4 can surpass some traditional models in zero-shot scenarios, showcasing their potential in cold-start recommendations.",
            "results": "Key takeaways suggest that LLMs can effectively serve as recommenders, especially when fine-tuned or instructed appropriately. However, challenges remain regarding their efficiency and the need for suitable prompt designs."
        },
        "discussion": {
            "advantage": "Current research demonstrates that LLMs can leverage vast world knowledge and reasoning abilities, enhancing the quality of recommendations compared to traditional models.",
            "limitation": "Limitations include the high computational costs, slow inference times, and the inability of LLMs to fully understand user intentions without well-structured prompts.",
            "gaps": "Unanswered questions include the optimal ways to design prompts for diverse recommendation scenarios and the need for further exploration into real-time collaborative filtering.",
            "future work": "Future research should focus on improving prompt engineering, exploring hybrid models that combine LLMs with traditional methods, and addressing privacy and efficiency concerns in LLM-based recommendations."
        },
        "other info": {
            "additional findings": "The survey emphasizes the need for empirical validation of LLMs' performance in recommendation tasks and suggests that different LLM architectures and tuning methods can significantly impact recommendation outcomes."
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "This survey aims to assess the effectiveness of large language models (LLMs) in recommendation tasks, addressing knowledge gaps regarding their capabilities as recommenders through prompting engineering."
        },
        {
            "section number": "2.3",
            "key information": "The survey organizes the current methods into different categories based on LLM types (open-source vs closed-source), tuning strategies (not-tuning, fine-tuning, instruction tuning), and model architectures (encoder-decoder, causal decoder)."
        },
        {
            "section number": "4.1",
            "key information": "Current research demonstrates that LLMs can leverage vast world knowledge and reasoning abilities, enhancing the quality of recommendations compared to traditional models."
        },
        {
            "section number": "4.2",
            "key information": "The core issue explored in this survey is the integration of LLMs into recommender systems and how to effectively use them to provide personalized recommendations."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on improving prompt engineering, exploring hybrid models that combine LLMs with traditional methods, and addressing privacy and efficiency concerns in LLM-based recommendations."
        },
        {
            "section number": "11",
            "key information": "Key takeaways suggest that LLMs can effectively serve as recommenders, especially when fine-tuned or instructed appropriately. However, challenges remain regarding their efficiency and the need for suitable prompt designs."
        }
    ],
    "similarity_score": 0.8110768549045267,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9099/90999ae4-717c-43fa-b8b7-0ede3f658e81.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1154/1154baf1-95c2-4670-9bc7-31bd625089fc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a160/a160800e-28e4-4146-8afe-6d06d35bfde8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/116f/116fc8f6-314c-4ba7-b075-d1d0fbc40794.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b04/9b0453bc-5891-4df6-b82a-e9d131e1ae4c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8f70/8f709133-0e0b-4607-a14e-f2e3ccf69816.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4a3c/4a3c7cd5-0c76-432d-913b-93c8ef907894.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b3e9/b3e9c0aa-0db9-4fb4-b8f8-cee19408523d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9d34/9d348003-a437-4ce7-b2ee-8276f3fbd677.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0543/0543e44e-7245-4eef-9de7-d393795f8e1b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7230/7230b1de-cf31-434c-bf46-760fd10cce22.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78e7/78e70252-a678-4a85-91b5-3f63fbe619d9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e78b/e78bf74f-ade7-4eec-9aaa-95affd496a0b.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6341/634153c1-ddcb-4b04-822a-626abd5ac6e8.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/712f/712fd8fc-5e4c-4d30-9dfc-db9ee6cf38dc.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cdd2/cdd2980f-05ee-480b-8f27-9d487f5d0df2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3a87/3a875d84-7150-4f42-a4d7-deb15fb68a02.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2cbd/2cbd6864-4a04-488f-b32e-a32e92c8e5eb.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Prompting large language models for recommender systems_ A comprehensive framework and empirical analysis.json"
}