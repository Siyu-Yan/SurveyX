{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2209.11895",
    "title": "In-context Learning and Induction Heads",
    "abstract": "\"Induction heads\" are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] -> [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all \"in-context learning\" in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in in-context learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence.",
    "bib_name": "olsson2022incontextlearninginductionheads",
    "md_text": "# In-context Learning and Induction Heads\nCatherine Olsson , Nelson Elhage , Neel Nanda , Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah \u2217 \u2217 \u2217 \u2020 \u2020 \u2020 \u2020 \u2021\nAbstract: \u201cInduction heads\u201d are attention heads that implement a simple algorithm to complete token sequences like [A][B] \u2026 [A] \u2192 [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all \u201cincontext learning\u201d in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. \nAs Transformer generative models continue to scale and gain increasing real world use, addressing their associated safety problems becomes increasingly important. Mechanistic interpretability \u2013 attempting to reverse engineer the detailed computations performed by the model \u2013 offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models.  [1, 2, 3, 4, 5] 1\nIn the past, mechanistic interpretability has largely focused on CNN vision models , but recently, we presented some very preliminary progress on mechanistic interpretability for Transformer language models\u200b . Specifically, in our prior work we developed a mathematical framework for decomposing the operations of transformers, which allowed us to make sense of small (1 and 2 layer attention-only) models and give a near-complete account of how they function. Perhaps the most interesting finding was the induction head, a circuit whose function is to look back over the sequence for previous instances of the current token (call it A ), find the token that came after it last time (call it B ), and then predict that the same completion will occur again (e.g. forming the sequence [A][B] \u2026 [A] \u2192 [B] ). In other words, induction heads \u201ccomplete the pattern\u201d by copying and completing sequences that have occurred before. Mechanically, induction heads in our models are implemented by a circuit of two attention heads: the first head is a \u201cprevious token head\u201d which copies information from the previous token into the next token, while the second head (the actual \u201cinduction head\u201d) uses that information to find tokens preceded by the present token. For 2-layer attention-only models,  we were able to show precisely that induction heads implemen this pattern copying behavior and appear to be the primary source of in-context learning. [6] [7] 2 Ultimately, however, our goal is to reverse-engineer frontier language models (which often contain hundreds of layers and billions or trillions of parameters), not merely 2-layer attention-only models. Unfortunately, both the presence of many layers, and the presence of MLPs, makes it much more difficult to mathematically pin down the precise circuitry of these models. However, a different approach is possible: by empirically observing, perturbing, and studying the learning process and the formation of various structures, we can try to assemble an indirect case for what might be happening mechanistically inside the network. This is somewhat similar to how a neuroscientist might gain understanding of how part of the brain functions by looking at neural development over time, studying patients with an injury to that part of the brain, perturbing brain function in animals, or looking at a select small number of relevant neurons. In this paper, we take the first preliminary steps towards building such an indirect case. In particular we present preliminary and indirect evidence for a tantalizing hypothesis: that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transforme models. Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a \u201cfuzzy\u201d or \u201cnearest neighbor\u201d version of pattern completion, completing [A*][B*] \u2026 [A] \u2192 [B]  , where  A* \u2248 A  and B* \u2248 B are similar in some space; and furthermore, that these circuits implement most in-context learning in large models. The primary way in which we obtain this evidence is via discovery and study of a phase change that occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of incontext learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it.\nThe primary way in which we obtain this evidence is via discovery and study of a phase change tha occurs early in training for language models of every size (provided they have more than one layer) and which is visible as a bump in the training loss. During this phase change, the majority of incontext learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it.\nTogether the claims establish a circumstantial case that induction heads might be responsible for the majority of in-context learning in state-of-the-art transformer models. We emphasize that our results here are only the beginnings of evidence for such a case, and that like any empirical or interventional study, a large number of subtle confounds or alternative hypotheses are possible \u2013 which we discuss in the relevant sections. But we considered these results worth reporting, both because future work could build on our results to establish the claim more firmly, and because this kind of indirect evidence is likely to be common in interpretability as it advances, so we\u2019d like to establish a norm of reporting it even when it is not fully conclusive.\nFinally, in addition to being instrumental for tying induction heads to in-context learning, the phase change may have relevance to safety in its own right. Neural network capabilities \u2014 such as multidigit addition \u2014 are known to sometimes abruptly form or change as models train or increase in scale , and are of particular concern for safety as they mean that undesired or dangerous behavior could emerge abruptly. For example reward hacking, a type of safety problem, can emerge in such a phase change . Thus, studying a phase change \u201cup close\u201d and better understanding its internal mechanics could contain generalizable lessons for addressing safety problems in future systems. In particular, the phase change we observe forms an interesting potential bridge between the microscopic domain of interpretability and the macroscopic domain of scaling laws and learning dynamics. [8, 1] [9]\nThe rest of the paper is organized as follows. We start by clarifying several key concepts and definitions, including in-context learning, induction heads, and a \u201cper-token loss analysis\u201d method we use throughout. We then present the 6 arguments one by one, drawing on evidence from analysis of 34 transformers over the course of training, including more than 50,000 attention head ablations (the data of which is shown in the Model Analysis Table). We then discuss some unexplained \u201ccuriosities\u201d in our findings, as well as reviewing related work.\n# Key Concepts\nIn modern language models, tokens later in the context are easier to predict than tokens earlier in the context. As the context gets longer, loss goes down. In some sense this is just what a sequence model is designed to do (use earlier elements in the sequence to predict later ones), but as the ability to predict later tokens from earlier ones gets better, it can increasingly be used in interesting ways (such as specifying tasks, giving instructions, or asking the model to match a pattern) that suggest it can usefully be thought of as a phenomenon of its own. When thought of in this way, it is usually referred to as in-context learning.3 Emergent in-context learning was noted in GPT-2  and gained significant attention in GPT-3 . Simply by adjusting a \u201cprompt\u201d, transformers can be adapted to do many useful things without retraining, such as translation, question-answering, arithmetic, and many other tasks. Using \u201cprompt engineering\u201d to leverage in-context learning became a popular topic of study and discussion . [10] [1] [11, 12] At least two importantly different ways of conceptualizing and measuring in-context learning exist in the literature. The first conception, represented in Brown et al., focuses on few-shot learning  of specific tasks. The model is prompted with several instances of some \u201ctask\u201d framed in a nexttoken-prediction format (such as few-digit addition, or English-to-French translation). The second conception of in-context learning, represented in Kaplan et al. , focuses on observing the loss at different token indices, in order to measure how much better the model gets at prediction as it receives more context. The first conception can be thought of as a micro perspective (focusing on specific tasks), where as the second conception can be seen as a macro perspective (focusing on general loss, which on average correlates with these tasks). [1] [13] The \u201cfew-shot learning\u201d conception of in-context learning has tended to receive greater community attention. The ability to do many different tasks with one large model, even without further finetuning, is a notable change to the basic economics of model training. Moreover, it gives evidence of wide-ranging general capabilities and the ability to adapt on the fly, which nudges us to re-examine what it means for a model to \u201cunderstand\u201d or to \u201creason\u201d. However, for the purposes of this work, we focus instead on the Kaplan et al. conception: decreasing loss at increasing token indices. We do so because it's a more general framing of the phenomenon than \u201cfew-shot learning\u201d. A drawback of this definition is it fails to isolate specific behaviors of interest. At the same time, it allows us to measure models\u2019 overall ability to learn onthe-fly from the context, without depending on our specific choices of \u201ctask\u201d.  We\u2019ll also see that, starting from this definition, we are also able to study a couple classic few-shot learning examples (see Argument 4).\n# Throughout this work we compute a simple heuristic measure of in-context learning:\nIn-context learning score: the loss of the 500th token in the context minus the average loss of the 50th token in the context, averaged over dataset examples.\nWe chose the 500th and 50th token indices somewhat arbitrarily. The 500th token is near the end of a length-512 context, and the 50th token is far enough into the context that some basic properties of the text have been established (such as language and document type) while still being near the start. We will also show that picking different numbers here does not change our conclusions.\nFinally, it is worth noting that in-context learning is of potentially special relevance to safety. Incontext learning makes it harder to anticipate how a model might behave after a long context. In the longer run, concepts such as mesa-optimization or inner-alignment  postulate that meaningful learning or optimization could occur at test time (without changing the weights). In-context learning would be an obvious future mechanism for such hidden optimization to occur, whether or not it does so today. Thus, studying in-context learning seems valuable for the future. [14]\n(See Related Work for more on in-context learning, and Discussion for more on the connection to safety.)\n# Induction Heads\nIn our previous paper, we discovered a special kind of attention head \u2013 which we named induction heads \u2013 in two layer attention-only models. Induction heads are implemented by a circuit consisting of a pair of attention heads in different layers that work together to copy or complete patterns. The first attention head copies information from the previous token into each token. This makes it possible for the second attention head to attend to tokens based on what happened before them, rather than their own content. Specifically, the second head (which we call the \"induction head\") search for a previous place in the sequence where the present token A  occurred and attends to the next token (call it B ), copying it and causing the model to be more likely to output B  as the next token. That is, the two heads working together cause the sequence \u2026[A][B]\u2026[A]  to be more\nIn our previous paper, we discovered a special kind of attention head \u2013 which we named induction heads \u2013 in two layer attention-only models. Induction heads are implemented by a circuit consisting of a pair of attention heads in different layers that work together to copy or complete patterns. The first attention head copies information from the previous token into each token. This makes it possible for the second attention head to attend to tokens based on what happened before them, rather than their own content. Specifically, the second head (which we call the \"induction head\") search for a previous place in the sequence where the present token A  occurred and attends to the next token (call it B ), copying it and causing the model to be more likely to output B  as the next token. That is, the two heads working together cause the sequence \u2026[A][B]\u2026[A]  to be more likely to be completed with [B] . Induction heads are named by analogy to inductive reasoning. In inductive reasoning, we might infer that if A  is followed by B  earlier in the context, A  is more likely to be followed by B  again later in the same context. Induction heads crystallize that inference. They search the context for previous instances of the present token, attend to the token which would come next if the pattern repeated, and increase its probability. Induction heads attend to tokens that would be predicted by basic induction (over the context, rather than over the training data). Notice that induction heads are implementing a simple algorithm, and are not memorizing a fixed table of n-gram statistics. The rule [A][B] \u2026 [A] \u2192 [B]  applies regardless of what A  and B are. This means that induction heads can in some sense work out of distribution, as long as local statistics early in the context are representative of statistics later. This hints that they may be capable of more general and abstract behavior. 4 Our previous paper focused on a few explorations of induction heads, including showing that these heads occur in 2-layer attention-only models (but not 1-layer models); tracking down how they operate mechanistically as part of our mathematical decomposition of transformers; and making an eigenvalue-based test for detecting their presence. However, we were a bit vague on the exact definition of induction heads: it was more like we found a cluster of behaviors and mechanisms that tended to occur together, and called heads in that cluster \u201cinduction heads\u201d. In this paper our goal is to provide evidence for something more expansive: that induction heads play a major role in general in-context learning (not just literal [A][B]...[A]\u2192[B]  copying), for large models and not just for small 2-layer attention only models.  To do this clearly and coherently, we need a more precise definition of induction heads. Mechanistic analysis of weights and eigenvalue analysis are much more complicated in large models with MLP\u2019s, so for this paper we choose to define induction heads by their narrow empirical sequence copying behavior (the [A][B]...[A]\u2192[B] ), and then attempt to show that they (1) also serve a more expansive function that can be tied to in-context learning, and (2) coincide with the mechanistic picture for small models.\nInduction heads are named by analogy to inductive reasoning. In inductive reasoning, we might infer that if A  is followed by B  earlier in the context, A  is more likely to be followed by B  again later in the same context. Induction heads crystallize that inference. They search the context for previous instances of the present token, attend to the token which would come next if the pattern repeated, and increase its probability. Induction heads attend to tokens that would be predicted by basic induction (over the context, rather than over the training data).\nNotice that induction heads are implementing a simple algorithm, and are not memorizing a fixed table of n-gram statistics. The rule [A][B] \u2026 [A] \u2192 [B]  applies regardless of what A  and B are. This means that induction heads can in some sense work out of distribution, as long as local statistics early in the context are representative of statistics later. This hints that they may be capable of more general and abstract behavior. 4\nOur previous paper focused on a few explorations of induction heads, including showing that these heads occur in 2-layer attention-only models (but not 1-layer models); tracking down how they operate mechanistically as part of our mathematical decomposition of transformers; and making an eigenvalue-based test for detecting their presence. However, we were a bit vague on the exact definition of induction heads: it was more like we found a cluster of behaviors and mechanisms that tended to occur together, and called heads in that cluster \u201cinduction heads\u201d.\nIn this paper our goal is to provide evidence for something more expansive: that induction heads play a major role in general in-context learning (not just literal [A][B]...[A]\u2192[B]  copying), for large models and not just for small 2-layer attention only models.  To do this clearly and coherently, we need a more precise definition of induction heads. Mechanistic analysis of weights and eigenvalue analysis are much more complicated in large models with MLP\u2019s, so for this paper we choose to define induction heads by their narrow empirical sequence copying behavior (the [A][B]...[A]\u2192[B] ), and then attempt to show that they (1) also serve a more expansive function that can be tied to in-context learning, and (2) coincide with the mechanistic picture for small models.\nFormally, we define an induction head as one which exhibits the following two properties on a repeated random sequence of tokens: Prefix matching: The head attends back to previous tokens that were followed by the current and/or recent tokens. That is, it attends to the token which induction would suggest comes next. Copying: The head\u2019s output increases the logit corresponding to the attended-to token. 6 7 In other words, induction heads are any heads that empirically increase the likelihood of [B]  given [A][B]...[A]  when shown a repeated sequence of completely random tokens. An illustration of induction heads\u2019 behavior is shown here:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8b23/8b23c116-5545-47c9-9df4-c7c0653f017a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\nNote that, as a consequence, induction heads will tend to be good at repeating sequences wholesale. For example, given \u201cThe cat sat on the mat. The cat \u2026\u201d, induction heads will promote the continuation \u201csat on the mat\u201d. This gives a first hint of how they might be connected to general incontext learning and even few-shot learning: they learn to repeat arbitrary sequences, which is a (simple) form of few-shot learning. One of things we\u2019ll be trying to establish is that when induction heads occur in sufficiently large models and operate on sufficiently abstract representations, the very same heads that do this sequence copying also take on a more expanded role of analogical sequence copying or in-context nearest neighbors. By this we mean that they promote sequence completions like [A*][B*] \u2026 [A] \u2192 [B]  where A*  is not exactly the same token as A  but similar in some embedding space, and also B  is not exactly the same token as B* . For example, A  and A*  (as well as B and B* ) might be the same word in different languages, and the induction head can then translate a sentence word by word by looking for \u201csomething like A \u201d, finding A*  followed by B* , and then completing with \u201csomething like B* \u201d (which is B ). We are not yet able to prove mechanistically that induction heads do this in general, but in Argument 4 we show empirical examples of induction heads behaving in this way (including on translation), and in Argument 5 we point out that the known copying mechanism of induction heads in small models can be naturally adapted to function in this way.\n# Per-Token Loss Analysis\nTo better understand how models evolve during training, we analyze what we call the \"per-token loss vectors.\" The core idea traces back to a method used by Erhan et al. , and more generally to the idea of \"function spaces\" in mathematics. [15] 8\nWe start with a collection of models. (In our use, we'll train several different model architectures, saving dozens of \u201csnapshots\u201d of each over the course of training. We\u2019ll use this set of snapshots as our collection of models.) Next, we collect the log-likelihoods each model assigns to a consistent set of 10,000 random tokens, each taken from a different example sequence. We combine these log-likelihoods into a \"per-token loss vector\" and apply Principal Component Analysis (PCA):\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/85c6/85c6e67c-37c3-4f7b-8a2d-d5827a667edd.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\">A more detailed discussion of technical details can be found in the Appendix.</div>\nBy applying this method to snapshots over training for multiple models, we can visualize and compare how different models' training trajectories evolve in terms of their outputs. Since we're using PCA, each direction can be thought of as a vector of log-likelihoods that models are moving along. We particularly focus on the first two principal components, since we can easily visualize those. Of course, models also move in directions not captured by the first two principal components, but it's a useful visualization for capturing the highest-level story of training.\n# Arguments that induction heads are the mechanism for the majority of in-context\n# Arguments that induction heads are the mechanism for the majority of in-context learning.\nNow we\u2019ll proceed to the main part of the paper, which makes the case that induction heads may provide the primary mechanism for the majority of in-context learning for transformer models in general. As stated in the introduction, this is a very broad hypothesis and much of our evidence is indirect, but nevertheless we believe that all the lines of evidence together make a relatively strong though not conclusive, case.\nBefore we go through the arguments, it\u2019s useful to delineate where the evidence is more conclusive vs. less conclusive. This is shown in the table below. For small, attention-only models, we believe we have strong evidence that attention heads are the mechanism for the majority of in-context learning, as we have evidence supported by ablations and mechanistic reverse engineering. Conversely, for all models, we can make a strong case that induction heads play some role in incontext learning, as we can demonstrate examples and show suggestive correlations. However, the larger the models get, the harder it is to establish that induction heads account for the actual majority of in-context learning. For large models with MLP\u2019s, we must therefore rely on mainly correlational evidence, which could be confounded. We explore alternate hypotheses throughout, including at the end of Argument 1 and again briefly in \u200bArgument 6.\n<div style=\"text-align: center;\">ARY OF EVIDENCE FOR SUB-CLAIMS (STRONGEST ARGUMENT FOR EAC</div>\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nContributes Some\nStrong, Causal\nStrong, Causal\nMedium, Correlational &\nMechanistic\nContributes Majority\nStrong, Causal\nMedium, Causal\nMedium, Correlational\nArgument 6 (Continuity from small to large models): In the previous 5 arguments, the case for induction heads explaining in-context learning is stronger for small models than for large ones.  However, many behaviors and data related to both induction heads and in-context learning are smoothly continuous from small to large models, suggesting the simplest explanation is that mechanisms are the same.\nFor each argument, we\u2019ll have a similar table to the one in this section, showing the strength of the evidence provided by that claim as it applies to large/small models and some/most of context learning. The table above is the sum of the evidence from all six lines of reasoning.\nArgument 1: Transformer language models undergo a \u201cphase change\u201d during training, during which induction heads form and simultaneously in-context learning improves dramatically.\nThe table below summarizes the quality of this evidence for the models we have studied: it applies to both large and small models, and is the expected outcome if induction heads were responsible for the majority of in-context learning, but it is only correlational and so could be confounded (discussed more below).\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSTRENGTH OF ARGUMENT FOR SUB-CLAIMS\nSmall Attention-Only\nSmall with MLPs\nLarge Models\ntributes Some\nMedium, Correlational\nMedium, Correlational\nMedium, Correlational\ntributes Majority\nMedium, Correlational\nMedium, Correlational\nMedium, Correlational\nOur first observation is that if we measure in-context learning for transformer models over the course of training (defined as the 50th token loss minus the 500th token loss as described in Key Concepts), it develops abruptly in a narrow window early in training (roughly 2.5 to 5 billion tokens) and then is constant for the rest of training (see figure below).  Before this window there is less than 0.15 nats of in-context learning, after it there is roughly 0.4 nats, an amount that remains constant for the rest of training and is also constant across many different model sizes (except for the one layer model where not much in-context learning ever forms).  This seems surprising \u2013 naively, one might expect in-context learning to improve gradually over training, and improve with larger model sizes, as most things in machine learning do. 9\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e0cd/e0cd636c-49dd-4a54-bea3-79bdde983536.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nAlthough we only show three models above, the pattern holds true very generally: many example are shown in the Model Analysis Table later in the paper, including models of varying model architecture and size.\nOne might wonder if the sudden increase is somehow an artifact of the choice to define in-context learning in terms of the difference between the 500th and 50th tokens. We'll discuss this in more depth later. But for now, an easy way to see that this is a robust phenomena is to look at the derivative of loss with respect to the logarithm token index in context. You can think of this as measuring something like \"in-context learning per \u03b5% increase in context length.\" We can visualize this on a 2D plot, where one axis is the amount of training that has elapsed, the other is the token index being predicted. Before the phase change, loss largely stops improving around token 50, but after the phase change, loss continues to improve past that point.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ff2/5ff25a70-e6a1-402f-947c-e5a421f8fd52.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nIt turns out that a sudden improvement in in-context learning isn't the only thing that changes in this window. If we go through the attention heads of a model and and score them for whether they are induction heads (using a prefix matching score which measures their ability to perform the task we used to define induction heads in Key Concepts), we find that induction heads form abruptly during exactly the same window where in-context learning develops (figure below). Again we show only a few models here, but a full set is shown in the Model Analysis Table. The exception is the one-layer model, where induction heads never form \u2013 just as in-context learning never substantially develops for the one-layer model.\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8b9a/8b9a9a00-3e35-48fd-9da8-c8c73ee687e8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nThis already strongly suggests some connection between induction heads and in-context learning, but beyond just that, it appears this window is a pivotal point for the training process in general: whatever's occurring is visible as a bump on the training curve (figure below). It is in fact the only place in training where the loss is not convex (monotonically decreasing in slope). That might not sound significant, but the loss curve is averaging over many thousands of tokens. Many behaviors people find interesting in language models, such as the emergence of arithmetic, would be microscopic on the loss curve. For something to be visible at that scale suggests it's a widespread, major change in model behavior. This shift also appears to be the first point where, at least for small models, the loss curve diverges from a one-layer model \u2013 which does not display the bump, just as it does not display the other abrupt changes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c811/c811ac57-7901-4202-9bf8-ec8d78e3830c.png\" style=\"width: 50%;\"></div>\nWe can also apply principal components analysis (PCA) to the per-token losses, as described in per-token-loss analysis, which allows us to summarize the main dimensions of variation in how several models' predictions vary over the course of training. Below we show the first two principal components of these models\u2019 predictions, with the golden outline highlighting the same interval shown above, when in-context learning abruptly improved. We see that the training trajectories pivot during exactly the same window where the other changes happen. In some sense, whatever is occurring when in-context learning improves is the primary deviation from the basic trajectory our transformers follow during the course of their training.  Once again the only exception is the one-layer model \u2013 where induction heads cannot form and incontext learning does not improve.\n# \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/216b/216bc0af-c844-49f3-9181-fc0d477f4d49.png\" style=\"width: 50%;\"></div>\nCapacity for in-context learning sharply improves (as measured via the in-context learning score). Induction heads form. Loss undergoes a small \u201cbump\u201d (that is, the loss curve undergoes a period of visibly steeper improvement than the parts of the curve before and after). The model\u2019s trajectory abruptly changes (in the space of per-token losses, as visualized with PCA).\nCollectively these results suggest that some important transition is happening during the 2.5e9 to 5e9 token window early in training (for large models this is maybe 1-2% of the way through training). We call this transition \u201cthe phase change\u201d, in that it\u2019s an abrupt change that alters the model\u2019s behavior and has both macroscopic (loss and in-context learning curves) and microscopic (induction heads) manifestations, perhaps analogous to e.g. ice melting. 10\n# Looking at the Phase Change More Closely\nA natural explanation would be that for all these models, the induction heads implement in-context learning: their formation is what drives all the other changes observed. To strengthen this hypothesis a bit, we check a few things. First, the window where the phase change happens doesn\u2019t appear to correspond to a scheduled change in learning rate, warmup, or weight decay; there is not some known exogenous factor precipitating everything. Second, we tried out training some of the small models on a different dataset, and we observed the phase change develop in the same way (see Model Analysis Table for more details). 11\nThird, to strengthen the connection a little more, we look qualitatively and anecdotally at what\u2019s going on with the model\u2019s behavior during the phase change. One way to do this is to look at specific tokens the model gets better and worse at predicting. The model's loss is an average of billions of log-likelihood losses for individual tokens. By pulling them apart, we can get a sense for what's changed.\nConcretely, let's pick a piece of text \u2013 for fun, we'll use the first paragraph of Harry Potter \u2013 and look at the differences in log-likelihoods comparing the start and the end of the phase change. We'll notice that the majority of the changes occur when tokens are repeated multiple times in the text. If a sequence of tokens occurs multiple times, the model is better at predicting the sequence the second time it shows up. On the other hand, if a token is followed by a different token than it previously was, the post-phase-change model is worse at predicting it: 12\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1bda/1bda6d9d-bc96-4a68-bde6-0525fff95ff1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n\nWe can also do this same analysis over the course of model training. The loss curve is the average of millions of per-token loss curves. We can break this apart and look at the loss curves for individual tokens. In particular, let\u2019s take a look at per-token loss trajectories for two tokens in the first paragraph of Harry Potter. In red, we show a token whose prediction gets dramatically better during the phase change: it\u2019s the last of four tokens in \u201c The Dursleys\u201d, a sequence that appears several times in the text. In blue, we show a token that gets meaningfully worse during the phase change: it\u2019s the firstever appearance of \u201c Mrs Potter\u201d, after both previous instances of \u201c Mrs\u201d were followed by \u201c Dursley\u201d instead.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/256d/256d7a62-f7b6-4f4f-8fe6-4ac333be2688.png\" style=\"width: 50%;\"></div>\n# \n<div style=\"text-align: center;\"></div>\nAll of this shows that during the phase change, we see exactly the behaviors we\u2019d expect to see if induction heads were indeed contributing the majority of in-context learning.\n# Assessing the Evidence\nIn large models, we have low time resolution on our analysis over training. Co-occurrence when one only has 15 points in time is less surprising and weaker evidence.\nPerhaps other mechanisms form in our models at this point, that contribute to not only induction heads, but also other sources of in-context learning. (For example, perhaps the phase change is really the point at which the model learns how to compose layers through the residual stream, enabling both induction heads and potentially many other mechanisms that also require composition of multiple heads) Put another way, perhaps the co-occurrenc is primarily caused by a shared latent variable, rather than direct causality from induction heads to the full observed change in in-context learning.\nThe fact that in-context learning score is roughly constant (at 0.4 nats) after the phase change doesn't necessarily mean that the underlying mechanisms of in-context learning are constant after that point. In particular, the metric we use measures a relative loss between the token index 500 and 50, and we know that the model\u2019s performance at token 50 improves over training time. Reducing the loss a fixed amount from a lower baseline is likely harder, and so may be driven by additional mechanisms as training time goes on.  13 14\nOne point worth noting here is that the argument that induction heads account for most in-context learning at the transition point of the phase change is more solid than the argument that they account for most in-context learning at the end of training \u2013 a lot could be changing during training even as the in-context learning score remains constant.\nArgument 2: When we change the transformer architecture in a way that shifts when induction heads form or whether they can form, the dramatic improvement in incontext learning shifts in a precisely matching way.\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nContributes Some\nMedium, Interventional\nMedium, Interventional\nWeak, Interventional\nContributes Majority\nMedium, Interventional\nMedium, Interventional\nWeak, Interventional\nOf course, the observation about one-layer models is pretty weak evidence by itself. (One could imagine one-layer models being different from models with more layers in all sorts of ways!) But it suggests a more general line of attack. If induction heads are the mechanism behind the large improvement in in-context learning, that makes predictions about the minimum architectural requirements in order to achieve the observed improvement. For a standard transformer, the important thing is to have two attention layers. But that's only because the key vectors need to be a function of the attended token and the token before it.\nWe define a \u201csmeared key\" architecture with a very simple modification that should make it easy for transformers of any depth to express induction heads. In our modified models, for each head , we introduce a trainable real parameter   which we use as   to interpolate between the key for the current token and previous token : h \u03b1h \u03c3(\u03b1 ) \u2208[0, 1] h 16\nk \u200b  =  \u03c3(\u03b1 )k \u200b  +  (1 \u2212\u03c3(\u03b1 ))k j h h j h h j\u22121 h\nThe hypothesis that induction heads are the primary mechanism of in-context learning predicts tha the phase change will happen in one-layer models with this change, and perhaps might happen earlier in models with more layers. If they're one of several major contributing factors, we might expect some of the in-context learning improvement to happen early, and the rest at the same time as the original phase change.\nThe results (figure below) are in line with the predictions: when we use the smeared-key architecture, in-context learning does indeed form for one-layer models (when it didn\u2019t before), and it forms earlier for two-layer and larger models.  More such results can be seen in the model analysis table.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/eb35/eb355078-648d-49f2-9a34-91afddb8135f.png\" style=\"width: 50%;\"></div>\nHowever, we probably shouldn't make too strong an inference about large models on this evidence. This experiment suggests that induction heads are the minimal mechanism for greatly increased incontext learning in transformers. But one could easily imagine that in larger models, this mechanism isn\u2019t the whole story, and also this experiment doesn\u2019t refute the idea of the mechanism of incontext learning changing over the course of training.\nArgument 3: When we directly \u201cknock out\u201d induction heads in small models at test-time the amount of in-context learning greatly decreases.\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSTRENGTH OF ARGUMENT FOR SUB-CLAIMS\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nes Some\nStrong, Causal\nStrong, Causal\nes Majority\nStrong, Causal\nMedium, Causal\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9e34/9e34c645-48db-4a1a-a07b-79bfc58ae27b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/282e/282eeb8d-032f-4ba5-a317-00eb65921a6f.png\" style=\"width: 50%;\"></div>\nOur ablations measure the marginal effects of removing attention heads from the model. To the extent two heads do something similar and the layer norm before the logits rescales things, the importance of individual heads may be masked.\nAll things considered, we feel comfortable concluding from this that induction heads are the primary mechanism for in-context learning in small attention-only models, but see this evidence as only suggestive for the MLP case.\nnarrowly as copying random sequences, induction heads can implement surprisingly abstract types of in-context learning.\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSTRENGTH OF ARGUMENT FOR SUB-CLAIMS\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nContributes Some\nPlausibility\nContributes Majority\nPlausibility\nAll of our previous evidence (in Arguments 1-3) focused on observing or perturbing the connection between induction head formation and macroscopic in-context learning. A totally different angle is to just find examples of induction heads implementing seemingly-difficult in-context learning behaviors; this would make it plausible that induction heads account for the majority of in-context learning. This evidence applies even to the very largest models (and we study up to 12B parameter models), but since it shows only a small number of tasks, it\u2019s only suggestive regarding in-context learning in general. Recall that we define induction heads as heads that empirically copy arbitrary token sequences using a \u201cprefix matching\u201d attention pattern. Our goal is to find heads that meet this definition but also perform more interesting and sophisticated behaviors, essentially showing that induction heads\nRecall that we define induction heads as heads that empirically copy arbitrary token sequences using a \u201cprefix matching\u201d attention pattern. Our goal is to find heads that meet this definition but also perform more interesting and sophisticated behaviors, essentially showing that induction heads in large models can be \u201cgeneralizable\u201d.\nIn this argument, we show some anecdotal examples of induction heads from larger transformers (our 40-layer model with 13 billion parameters) that exhibit exactly such behaviors \u2013 namely literal copying, translation, and a specific type of abstract pattern matching. The behaviors are all of the form [A*][B*]...[A][B] , aka the \u201cfuzzy nearest neighbor match\u201d or \u201cfind something similar early in the sequence and complete the sequence in analogy\u201d. We verify that these heads score highly on our \u201ccopying\u201d and \u201cprefix matching\u201d evaluations (that is, they increase the probability of the token they attend to, and attend to tokens where the prefix matches the present token on random text), and are thus \u201cinduction heads\u201d by our strict empirical definition, at the same time as they also perform these more sophisticated tasks.\nHead\nLayer Depth\nCopying score \nPrefix matching score \nLiteral copying head\n21 / 40\n0.89\n0.75\nTranslation head\n7 / 40\n0.20\n0.85\nPattern-matching head\n8 / 40\n0.69\n0.94\n(?)\n(?)\nHead\n# Behavior 1: Literal sequence copying\nWe'll start with the simplest case of a head that literally copies repeated text, to get familiar with the visualization interface we're using and the basic dynamics of these heads. We've selected an induction head which seems to perform very basic copying behavior and will look at how it behaves on the first paragraph of Harry Potter. We've repeated the first few sentences afterwards to show the head's behavior on longer segments of repeated text. For the following interactive visualizations, we recommend visiting the HTML article.\nThe visualization will show two different things:\nIn red, \"Attention\" lets you see where the head is attending to predict the next token. In blue, \"Logit attr\" shows the earlier tokens that contributed to the prediction of the current token, using \"direct-path\" logit attribution.21\nTo start exploring the visualization, we suggest you visit the HTML article and try hovering your cursor over the second paragraph.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54f5/54f5d058-c499-45c4-9242-ea8b0164fcfa.png\" style=\"width: 50%;\"></div>\nIf you explore the visualization, you'll see that the head predicts repeating the names \"Dursley\" and \"Potters\"; the phrase \"a small son\u201d; and then the entire repeated sentences at the end. In all cases, these successful predictions are made by attending back to a previous instance where this phrase was present in the text.\n# Behavior 2: Translation\nIt's a well-known result that language models can translate between languages. Intriguingly, we've encountered many examples of induction heads that can do translation. Here, we explore a head w found in layer 7 of our 40-layer model, showcasing translation between English, French, and German. (As with the others in this section, this head is also an \u201cinduction head\u201d by the same definition we\u2019ve been using all along, because when shown repeated random tokens, it uses a \u201cprefix matching\u201d attention pattern to copy the sequence verbatim.)\n# \nNote that the overall attention pattern (in red, top left) is more or less an \u201coff-diagonal\u201d, but it meanders a little bit away from a sharp diagonal. The meandering is because different languages have different word order and token lengths. As this head attends sequentially to past tokens that will semantically come next, the attended token position in the earlier sentences jumps around. The logit attribution patterns for this head are not perfectly sharp; that is, even in cases where the attention head is attending to the matching word in an earlier language, it does not always directly increase the logit of the corresponding prediction. We would guess that this is because this head\u2019s output needs to be further processed by later layers. However, taken overall, the direct logit attributions show clear evidence of contributing on net to the correct translation.\n# Behavior 3: Pattern matching\nIn this final example, we show an attention head (found at layer 26 of our 40-layer model) which does more complex pattern matching. One might even think of it as learning a simple function in context! (Again, this head also scores highly on our  of \u201cbasic\u201d induction behavior when shown repeated random sequences, so it is an induction head by that definition.) measurements\nTo explore this behavior, we've generated some synthetic text which follows a simple pattern. Eac line follows one of four templates, followed by a label for which template it is drawn from. The template is random selected, as are the words which fill in the template:\n(month) (animal): 0 (month) (fruit): 1 (color) (animal): 2 (color) (fruit): 3\nBelow, we show how the attention head behaves on this synthetic example. To make the diagram easier to read, we've masked the attention pattern to only show the \":\" tokens are the destination, and the logit attribution to only show where the output is the integer tokens.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b00b/b00b1025-ecd4-4054-9fdb-7f318eabf4d2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1c06/1c06fa5e-d721-4078-ae7c-c2eec843f26c.png\" style=\"width: 50%;\"></div>\nThis head attends back to a previous instance of the correct category more often than not. It often knows to skip over lines where one of the words is identical but the pattern is wrong (such as \u201cJanuary bird\u201d primarily attending to \u201cApril fish\u201d and not \u201cgrey bird\u201d). This head isn\u2019t perfect at this, but empirically it allocates about 65% of its attention from the colons to the correct positions, when tested on a range of similar problems.\n# What\u2019s going on with more abstract heads that are also induction heads?\n# What\u2019s going on with more abstract heads that are also induction\nWe emphasize again that the attention heads that we described above simultaneously implement both the abstract behaviors that we described, and these very same attention heads (as in, the exact same head in the same layer) also satisfy the formal definition of induction head (literal copying of random sequences using prefix matching).  The comparison is not a metaphor or a blurring of the definition: induction heads which are defined by their ability to copy literal sequences turn out to also sometimes match more abstract patterns.  This is what the table at the beginning of the section shows empirically.\nBut this still leaves the question: why do the same heads that inductively copy random text also exhibit these other behaviors?  One hint is that these behaviors can be seen as \u201cspiritually similar\u201d to copying.  Recall that where an induction head is defined as implementing a rule like [A][B] \u2026 [A] \u2192 [B] , our empirically observed heads also do something like [A*][B*] \u2026 [A] \u2192 [B]  where A*  and B*  are similar to A  and B  in some higher-level representation.  There are several ways these similar behaviors could be connected.  For example, note that the first behavior is a special case of the second, so perhaps induction heads are implementing a more general algorithm that reverts to the special case of copying when given a repeated sequence  .  Another possibility is that induction heads implement literal copying when they take a path through the residual stream that includes only them, but implement more abstract behaviors when they process the outputs of earlier layers that create more abstract representations (such as representations where the same word in English and French are embedded in the same place). 22\nIn Argument 5 we\u2019ll strengthen this argument by giving a mechanistic account of how induction heads (when doing simple copying with prefix-matching) attend back to the token that comes next in the pattern, and observe that the actual mechanism they use could naturally generalize to more abstract pattern matching.  Our point in this section is just that it's actually quite natural for these more abstract induction heads to also exhibit the basic copying behaviors underlying our definition.\nArgument 5: For small models, we can explain mechanistically how induction heads work, and can show they contribute to incontext learning. Furthermore, the actual mechanism of operation suggests natural ways in which it could be re-purposed to perform more general in-context learning.\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSTRENGTH OF ARGUMENT FOR SUB-CLAIMS\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nContributes Some\nStrong, Mechanistic\nStrong, Mechanistic\nMedium, Mechanistic\nContributes Majority\nWeak, Mechanistic\nOne of the main reasons we care about whether induction heads drive in-context learning is that we can understand them and so have a path to understanding in-context learning. But we can also turn this around: we can use our understanding of induction heads to make a purely logical argument that they should contribute to in-context learning. We begin with a semi-empirical argument. Let's take for granted that induction heads behave the way we've described and empirically seen, searching the context for previous examples and copying what happened next. We should expect such a procedure to improve a model's ability to predict tokens later in its context. We're essentially using the previous context as data points for a nearest neighbor algorithm, and nearest neighbors improves as one gives it more data points. Therefore, if induction heads exist as described, they would contribute to in-context learning as we've defined it. In some sense, this argument is quite strong if we're only arguing that there exist some cases where induction heads contribute to in-context learning. We've seen concrete examples above where induction heads improve token predictions by copying earlier examples. If nothing else, they must help in those cases! And more generally, our definition of induction heads (in terms of their behavior on repeated random sequences) suggests they behave this way quite generally. This argument doesn't say anything about what fraction of in-context learning is performed by induction heads, but it seems like a very strong argument that some is, both in large and small models.\nBut the really satisfying thing about this line of attack \u2014 namely, using our understanding of induction heads to anticipate their impact on in-context learning \u2014 is that we can actually drop the dependency on empirical observations of induction head behavior, at the cost of needing to make a more complex argument. In our previous paper, we were able to reverse engineer induction heads, showing from the parameter level how they implement induction behavior (and that they should). If we trust this analysis, we can know how induction heads behave without actually running them, and the argument we made in the previous paragraph goes through. Of course, there are some limitations. In the previous paper, we only reverse engineered a single induction head in a small attention-only model, although we can reverse engineered others (and have done so). A bigger issue is that right now we're unable to reverse engineer induction heads in models with MLP layers. But at least in some cases we\u2019ve observed, we can look at the parameters of a transformer and identify induction heads, just as a programmer might identify an algorithm by reading through\nWe can actually push this argument a bit further in the case of the two-layer attention only transformer we reverse engineered in the previous paper. Not only do we understand the induction heads and know that they should contribute to in-context learning, but there doesn't seem to really be an alternative mechanism that could be driving it. This suggests that induction heads are the primary driver of in-context learning, at least in very small models. 23\nThe following section will briefly summarize reverse engineering induction heads. Note that it relies heavily on linking to our previous paper. We do not expect it to be possible to follow without reading the linked portions. After that, we briefly discuss how the described mechanism could also implement more abstract types of induction head behavior.\n# ummary of Reverse Engineering Induction Heads\nNote: This section provides a dense summary with pointers to our previous paper; please see the previous paper for more information.\nRecall from Key Concepts that induction heads are defined as heads that exhibit both copying and prefix matching.\nCopying is done by the OV (\"Output-Value\") circuit. One of the defining properties of an induction head is that it copies. Induction heads are not alone in this! Transformers seem to have quite a number of copying heads, of which induction heads are a subset. This is done by having a \"copying matrix\" OV circuit, most easily characterized by its positive eigenvalues.\nPrefix matching is implemented with K-composition (and to a lesser extent Q-composition) in the QK (\"Query-Key\") Circuit. In order to do prefix matching, the key vector at the attended token needs to contain information about the preceding tokens \u2014 in fact, information about the attended token itself is quite irrelevant to calculating the attention pattern for induction. In the models we study, \u201ckey shifting\u201d occurs primarily using what we call K-composition. That is to say that an induction head\u2019s   reads from a subspace written to by an earlier attention head. The most basic form of an induction head uses pure K-composition with an earlier \u201cprevious token head\u201d to create a QK-Circuit term of the form   where   has positive eigenvalues. This term causes the induction head to compare the current token with every earlier position's preceding token and look for places where they're similar. More complex QK circuit terms can be used to create induction heads which match on more than just the preceding token. 24 W \u200b K Id \u2297h \u200b \u2297W prev W\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4600/46000009-6939-4da0-8532-28c76fa3d98f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c6ad/c6ad589a-b0c2-4b3b-8797-5778bac6f3b1.png\" style=\"width: 50%;\"></div>\nSome models use a different mechanism to implement induction heads. In GPT-2 , we've seen evidence of a second \"pointer-arithmetic\" mechanism for induction heads. This mechanism makes use of the positional embedding and \u201cQ-composition\u201d. In GPT-2, the earlier attention head attends to previous copies of the current token, and its   circuit copies their positional embedding into a subspace in the present token. The induction head then uses Q-composition to rotate that position embedding one token forward, and thereby attend to the following token. This mechanism isn't available to the models we study here, since they do not add positional informatio into the residual stream. [10] W \u200bV O 25\n# What About More Complex Induction Heads?\nWhat about the induction heads we saw in Argument 2 with more complex behavior? Can we also reverse engineer them? Do they operate on the same mechanisms? Presently, fully reverse engineering them is beyond us, since they exist in large models with MLPs, which we don't have a strong framework for mechanistically understanding.  However, we hypothesize they're different in two ways: (1) using more complex QK terms rather than matching on just the previous token; and (2) matching and copying more abstract and sophisticated linguistic features, rather than precise tokens.\nWhen we first introduced induction heads, we observed that they could be seen as a kind of \"incontext nearest neighbor\" algorithm. From this perspective, it seems natural that applying the same mechanism to more abstract features can produce more complex behavior.\nArgument 6: Extrapolation from small models suggests induction heads are responsible for the majority of in-context learning in large models.\n<div style=\"text-align: center;\">STRENGTH OF ARGUMENT FOR SUB-CLAIMS</div>\nSTRENGTH OF ARGUMENT FOR SUB-CLAIMS\nSmall Attention-Only\nSmall with MLPs\nLarge Models\nContributes Some\nAnalogy\nContributes Majority\nAnalogy\n# If things change from the small model case to the large model case, where do they change? And why is there no visible sign of the change in all our measurements?\nOn the flip side, there are many cases where large models behave very differently than small models (see discussion of phase changes with respect to model size in Related Work). Extrapolating from small models to models many orders of magnitude larger is something one should do with caution.\nThe most compelling alternative possibility we see is that other composition mechanisms may also form during the phase change. Larger models have more heads, which gives them more capacity for other interesting Q-composition and K-composition mechanisms that small models can\u2019t afford to express. If all \u201ccomposition heads\u201d form simultaneously during the phase change, then it\u2019s possible that above some size, non-induction composition heads could together account for more of the phase change and in-context learning improvement than induction heads do.\n# Model Analysis Table\nThe arguments above are based on analysis of 34 decoder-only Transformer language models, with different snapshots saved over the course of training, for one run of training per model. The models are drawn from four different model series as follows:\nThe dataset used for training the small models and smeared key models was an earlier version of the dataset described in Askell et al. , consisting of filtered common crawl data  and internet books, along with several other smaller distributions , including approximately 10% python code. The full-scale models were trained on an improved version of roughly the same data distribution. Additionally, another set of the small models was trained on a different dataset, consisting of just internet books, to explore the impact of varying the dataset. All models trained on a given dataset saw the same examples in the same order. Models never saw the same training data twice. [18] [19] [20]\n# \n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1af8/1af816a2-bb73-47eb-9629-ef1a43fd5f7b.png\" style=\"width: 50%;\"></div>\n\n\n# \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f14c/f14c8f4a-a7c9-47de-b07d-9a6c5232c8d2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b5fd/b5fd63d0-4c78-40a1-be47-a61d53c1d7a6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d5cc/d5cc7d00-74a5-4cb3-9ebb-0aab50adff07.png\" style=\"width: 50%;\"></div>\n# \n\n# \n# \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5d9e/5d9e107e-0272-4045-824e-e4a3d9023633.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b76d/b76da040-2950-4332-b794-5362f81a832b.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/22fd/22fd0086-1029-41e2-bfe7-d6ad35ae5a07.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/acc3/acc3aaaf-fb61-4e16-b6f4-d734b898c797.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f197/f1975fc3-081a-473d-9d61-915699709156.png\" style=\"width: 50%;\"></div>\nThe small models are 1- through 6-layer Transformers. These include models both with MLPs and without MLPs (i.e. \u201cattention only\u201d models). They have a context window of 8192 tokens, a 216 token vocabulary, an activation dimension dmodel = 768, and 12 attention heads per layer regardless of total model size. They were trained for 10,000 steps (~10 billion tokens), saving 200 snapshots at intervals of every 50 steps. Their positional embeddings are implemented with a variant on the standard positional embeddings (similar to Press et al. ). The training dataset is described earlier at the start of the Model Analysis Table. [17]\nWe observe a \u201cphase change\u201d phenomenon that appears at approximately 1-3 billion tokens in the small models. It might be reasonable to ask whether these phenomena are driven by scheduled changes to the hyperparameters, such as learning rate or weight decay. Weight decay was reduced at 4750 steps (approximately 5 billion tokens), the effects of which can be seen as a slight deviation about halfway through the displayed loss curves, occurring at the exact same point for all models; this is not related to the phase change, as this step number is notably beyond the range in which the phase change occurs. The only other hyperparameter change that occurs within the range of the phase change is the learning rate warm-up, which ramps up over the first 1.5e9 tokens.\n# FULL-SCALE MODELS\nThe \u201cfull-scale models\u201d are from the same set of models as described in Askell et al.  The context window and vocabulary size are the same as the small models (that is, 8192 tokens and 216 tokens respectively). Unlike the small models, their dimensions are adjusted to scale up with increasing size, with an activation dimension  , and a varying number of attention heads (See \u200bAppendix for full details). The models have both dense and local attention heads. In a local attention head, each token may only attend to earlier tokens within a fixed window of relative positions. Dense heads are the standard head, where a token may attend to any earlier token (including itself). The training dataset is described earlier at the start of the Model Analysis Table. [18] d \u200b = 128 \u2217n \u200b model layer\nSnapshots from these models were saved at exponential step numbers, at an interval of 2x. For our analyses we use snapshots at steps from 2^5 through 2^17, plus one or two final saves thereafter, for a total of 15 saved snapshots (except the 40L which has 14 saved snapshots). This corresponds to a consistent number of tokens across all models up through 2^11 steps (= 2.15E+09 tokens), after which adjustments to the training schedule cause the number of tokens per step to increase for the 24L and 40L models. 26\nNon-embedding\nparameter counts\nActivation dimension \nAttention heads\nper layer\nAttention\ndimension \n4\n13M\n512\n8\n64\n6\n42M\n768\n12\n64\n10\n200M\n1280\n20\n64\n16\n810M\n2048\n32\n64\n24\n2.7B\n3072\n48\n64\n40\n13B\n5120\n40\n128\nn\n\u200bayer\nl\nd\n\u200b = 128 \u2217n\n\u200b\nmodel\nlayer\nd\n\u200b\nhead\n# SMEARED KEY MODELS\nThe \u201csmeared key\u201d architecture modification described in Argument 2 is as follows: we introduce a trainable real parameter  used as   that interpolates between the key for the current token and previous token: \u03b1 \u03c3(\u03b1) \u2208[0, 1]\nk \u200b = \u03c3(\u03b1)k \u200b + (1 \u2212\u03c3(\u03b1))k j j j\u22121\n(In the case of the very first token in the context, no interpolation happens). These models were otherwise proportioned and trained exactly the same as the small models. We present these only at one-layer and two-layer sizes.\n# Unexplained Curiosities\nAs with all scientific investigations, in the course of this work we\u2019ve encountered a few unexplained phenomena. In this section, we discuss these and provide very preliminary investigations of a few that were especially surprising.\n# Seemingly Constant In-Context Learning Score\nOne of the stranger observations in this paper is that in-context learning score (as we've defined it: the loss of the 500th token in the context minus the loss of the 50th token in the context) is more or less the same for all models after the phase change. It appears to not matter whether the model is a tiny two layer model or a fairly large 13 billion parameter model, nor whether the model has just gone through the phase change or trained much longer. The only thing that matters, seemingly, is whether the model has gone through the phase change at all.\nA natural question is whether this might be an artifact of the relatively arbitrary definition. Afterall, there's no reason to privilege token index 50 or 500 in the context. But it appears that varying these doesn't matter. In the following plot, we show how the large models' \"in-context learning score\" varies if we define it instead as the difference between the loss at the final token in the context (8192) and other indices. While there are small differences between models \u2013 for some definitions, small models would do slightly more \"in-context learning\"! \u2013 all definitions appear to show that the amount of in-context learning varies only slightly between models. 27\nHow can this be? First, it's important to be clear that large models still predict tokens at all indice better than small models, and they're best at predicting later tokens. What's going on is that the large models gain all their advantage over small models very early in the context. In fact, the majority of the difference forms in the first ten tokens:\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c67/7c67c8ed-f4bc-4be7-876f-469e825292a3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af78/af78fc5f-8cc1-48d2-b926-77b13b2ed5b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c1f5/c1f5befe-d406-40e7-a034-88c8e145eb65.png\" style=\"width: 50%;\"></div>\nIt seems that large models are able to pull a lot of information out of the very early context. (This might partly be, as an example, because their increased world knowledge mean they don't need to gain as much information from the context.) They then further decrease their loss by a roughly fixed amount over the remainder of the context. It seems likely this fixed amount is in some sense \"more difficult in-context learning\" for large models, since they're starting from a lower loss baseline. While it still seems mysterious to us why models should have the same in-context learning score, this perspective makes it \"strange\" rather than \"shocking\". 28\n# Phase Change Effect on Loss Derivatives\nAnother observation we find quite striking is that if one looks at the derivative of the loss curves of models of different sizes, it appears that their order switches at the phase change. This is most easily seen by plotting the derivative of loss with respect to the log of elapsed tokens (since loss curves are often most easily reasoned about on a log x-axis). The key observation is that the loss decreases more slowly for small models than large models before the phase change, but the opposite is true after.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b90f/b90f7c0b-311c-4b2a-9ab4-a09646fe66c9.png\" style=\"width: 50%;\"></div>\nWhile it doesn't seem that surprising that small models learn more quickly in early training, it is striking that this inversion seems to coincide with the phase change. It's another piece of evidence that suggests the phase change is an important transition point in the training of transformers.\n# Additional Curiosities\nIn the model analysis table:\nThe 6-layer attention-only model has an unusual head that develops in the later half of training. This head is not an induction head, and yet ablating it has an effect similar to reversing the phase change (in the \u201cbefore-and-after vector\u201d attribution plot). What is this head?\nThe 4-layer MLP model ablations are nowhere near as \u201cpeaky\u201d as those of any other model. What is different about this model\u2019s development?\nThe 6-layer MLP model has one lone induction head whose ablation has the opposite effect on the in-context learning score. What is this head?\nFull-scale models above 16 layers start to show a small number of heads that score well on \u201cprefix search\u201d, but get a negative score on copying, which means they are not induction heads. What can we learn about these \u201canti-copying prefix-search\u201d heads?\n# Safety Implications\nThe ultimate motivation of our research is the theory that reverse engineering neural networks migh help us be confident in their safety. Our work is only a very preliminary step towards that goal, but it it does begin to approach several safety-relevant issues:\nPhase changes: If neural network behavior discontinuously changes from one scale to the next, this makes it more challenging for researchers and society to prepare for future problems.\nIn-Context Learning: In-context learning has been a topic of concerned speculation in the safety community. With less-capable neural networks, one might be tempted to treat their behavior as relatively fixed after training. (That said, demonstrations of adversarial reprogramming  shed some doubt on this assumption.) In-context learning highlights that model behavior can in some sense \u201cchange\u201d during inference, without further training. Even if we think of in-context learning as \u201clocating\u201d an already-learned behavior , rather than learning something new, the behavior could be a surprising and unwanted off-distribution generalization. [21] [22]\nMesa-Optimization: There have been some concerns that the underlying mechanism of in-context learning might be mesa-optimization , a hypothesized situation where models develop an internal optimization algorithm. Our work suggests that the primary mechanism of in-context learning, at least in small models, is induction heads. We did not observe any evidence of mesaoptimizers. [14]\n# Linking Learning Dynamics, Scaling Laws, and Mechanistic Interpretability\n# Linking Learning Dynamics, Scaling Laws, and Mechanistic\nThe in-context-learning phase change may be a useful \"Rosetta stone\" linking mechanistic interpretability, learning dynamics , and statistical physics-like empirical properties of neural networks (e.g. scaling laws or phase changes). If one wants to investigate the intersections of these lines of work, the phase change seems like an ideal starting point: a concrete example where these lines of inquiry are intertwined, which can be explored in small models, bounded in a small sliver of the training process, and is linked to a capability (in-context learning) the community is excited about. [23]\n# Related Work\nThe general approach of this paper to reverse engineering transformers is based heavily on our previous paper, A Mathematical Framework for Transformer Circuits. There is much to be said about how that framework relates to other work in interpretability. Rather than repeating it, we refer readers to Related Work in our previous paper, especially discussion of the relationship to circuits , to analysis of attention heads (e.g.  ), and to related mathematical analysis (e.g.  ). [6] [24, 25, 26, 27, 28] [29]\nBuilding on that perspective, we here focus on how aspects of this paper raise new connections to the machine learning literature, separate from the connections raised simply by the underlying framework.\n# IN-CONTEXT LEARNING\nEmergent in-context learning was compellingly demonstrated in GPT-3  . A number of papers have studied how to effectively leverage in-context learning, especially with \"prompt engineering\" . But of particular importance to us, several papers have tried to study how and when incontext learning occurs (e.g. ). [1] [11, 12] [13, 30, 31, 32]\n# Some of the findings of these papers are consistent with the induction head hypothesis, or support our methodology:\nKaplan et al.  is the origin of our approach for using loss at different token indices as a formalism for studying in-context learning. O\u2019Connor & Andreas   find that preserving word order in contexts is important, as the induction head hypothesis would suggest. [13] [30]\nHowever, there are also places where experiments in these papers seem in tension with the induction head hypothesis:\nO\u2019Connor & Andreas  have some experiments suggesting that removing all words except nouns can improve loss. This seems inconsistent with the induction head hypothesis. However, they only find this for experiments where they retrain the model on modified data. This seems both less directly related to our work (because we aim to study models trained on natural data) and subtle to interpret (because retraining models introduces the possibility of run-to-run loss variation, and the measured loss differences are small). The experiments where they don't retrain models on modified data seem consistent with the induction head hypothesis. Xie et al.  finds that LSTMs outperform Transformers when fit to synthetic data generated by a Hidden Markov Model (HHM) designed to isolate a particular theoretical model of incontext learning. We generally expect Transformers to outperform LSTMs at in-context learning on natural text (as seen in Kaplan et al. ), with induction heads as a major explanation. But in the case of the Xie et al. experiments (which don\u2019t use natural text), we suspect that the structure of the synthetic data doesn't benefit from Transformers, and that LSTMs are perhaps better at simulating HMMs. [30] [31] [13]\nO\u2019Connor & Andreas  have some experiments suggesting that removing all words except nouns can improve loss. This seems inconsistent with the induction head hypothesis. However, they only find this for experiments where they retrain the model on modified data. This seems both less directly related to our work (because we aim to study models trained on natural data) and subtle to interpret (because retraining models introduces the possibility of run-to-run loss variation, and the measured loss differences are small). The experiments where they don't retrain models on modified data seem consistent with the induction head hypothesis. Xie et al.  finds that LSTMs outperform Transformers when fit to synthetic data generated by a Hidden Markov Model (HHM) designed to isolate a particular theoretical model of incontext learning. We generally expect Transformers to outperform LSTMs at in-context [30] [31]\nNote that we are using a broader conception of \u201cin-context learning\u201d, rather than something as specific as \u201cfew-shot learning\u201d. This is in contrast with Brown et al. , which describes that a language model \u201cdevelops a broad set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize the desired task,\u201d with examples of tasks such as few-digit addition and typo correction. In our conception of \u201cin-context learning\u201d, we refer to all the ways that a model rapidly adapts to or recognizes what is going on in the context, even if \u201cwhat is going on in the context\u201d isn\u2019t well-conceived-of as multiple \u201cshots\u201d of some other specific repeated task. [1]\n# SCALING LAWS\nOver the last few years, the observation that machine learning models change in smooth, predictable ways described by scaling laws   has emerged as a useful tool for modelling the properties of models before training them. [13]\nThe relationship between scaling laws and mechanistic interpretability might be seen as analogous to the relationship between thermodynamics and the physics of individual particles. For both thermodynamics and scaling laws, even though the underlying system is very complicated, we're able to find simple relationships between the variables \u2013 for thermodynamics: entropy, temperature, volume and pressure; for neural networks: loss, compute, parameters, and data. In contrast, mechanistic interpretability studies the individual circuits underlying our models, vaguely analogous to how one might carefully study individual particles in physics. In physics, these two layers of abstraction were bridged by statistical physics.\nCan we bridge these two levels of abstraction in machine learning? The induction head phase change is the first time we're aware of a bridge. They give us a phenomenon at the level of macroscopic properties at loss which can be explained at the level of circuits and mechanistic\nIn fact, induction heads may be able to explain previous observed exceptions to scaling laws. In our work, 1-layer transformers seem very different from deeper transformers. This was previously observed by Kaplan et al.  who found that 1-layer transformers do not follow the same scaling laws as larger transformers. It seems quite plausible that the reason why the scaling laws are different for one-layer models is that they don't have induction heads. [13]\n# PHASE CHANGES & DISCONTINUOUS MODEL BEHAVIOR\nIn the previous section, we discussed how scaling laws describe smooth predictable relationships between model loss and properties like scale. However, a more recent set of results have made the situation seem more subtle. While models\u2019 losses often scale in predictable ways, there are cases where behavior is more complex:\nFor more general discussion of these phase change phenomena, see a recent blog post by Steinhardt  . [35]\nThe discontinuous phase change behavior we observe with induction heads is most analogous to Power et al. 's \"grokking\", in that it occurs over the course of training. We think our main contribution to this literature is linking the changes we observe to the formation of induction heads and a parameter-level understanding of the circuits involved. As far as we know, induction heads are the first case where a mechanistic account has been provided for a phase change in machine learning. [8]\nIf neural networks can genuinely be understood mechanistically, in terms of circuits, it seems like there almost has to be some way to understand the learning process in terms of the dynamics of circuits changing. Induction heads offer an interesting preliminary bridge between these topics, and are a source of optimism for such a connection. This section will briefly review some strands of work on the learning dynamics side, which seem particularly promising to think about if one wanted to pursue such a connection further.\nOne remarkable result in learning dynamics, by Saxe et al , has been the discovery of closed form solutions to learning dynamics for linear neural networks without activation functions. The exciting thing about this work is that it actually provides a simple way to conceptually think about neural network learning in a simplified case. (In follow up work, Saxe et al also explore connections between this framework and models learning to represent semantic information  .) We're unable to provide a detailed review of this work here, but we note that Saxe et al's framework could naturally suggest a circuit lens for thinking about learning dynamics. Very roughly, they find that linear neural networks can be understood in terms of the evolution of independent paths through the network, with each path corresponding to a principal component of the data. These paths might be thought of as circuits. [23] [36] Another interesting line of work has been the study of the geometry of neural network loss surfaces (e.g. ). Here, our thoughts on the connection are more superficial, but it seems like there must be some way in which aspects of the loss surface connect to the formation of circuits. Very concretely, it seems like the phase change we've described in this paper must correspond to some very large feature in the loss landscape of transformers. [37, 38, 39, 40]\n# UNIVERSALITY\nIn the context of interpretability and circuits, \"universality\"  or \"convergent learning\"  is when multiple models develop the same features and circuits. Universality might seem like an intellectual curiosity, but the circuits thread argues that universality plays a critical role in what kind of interpretability makes sense: [41] [42]\n[I]magine the study of anatomy in a world where every species of animal had a completely unrelated anatomy: would we seriously study anything other than humans and a couple domestic animals? In the same way, the universality hypothesis determines what form of circuits research makes sense. If it was true in the strongest sense, one could imagine a kind of \u201cperiodic table of visual features\u201d which we observe and catalogue across models. On the other hand, if it was mostly false, we would need to focus on a handful of models of particular societal importance and hope they stop changing every year.    [41]\nResearch on universality began with Li et al.  who showed that many neurons are highly correlated with neurons in retrained versions of the same model. More recently, a number of papers have shown that in aggregate, neural networks develop representations with a lot of shared information (e.g. ). The Circuits thread tried to extend this notion of universality from features to circuits, finding that not only do at least some families of well-characterized neurons reoccur across multiple networks of different architectures and that the same circuits  , but the same circuits appear to implement them  . [42] [43, 44] [41] [45]\nCertain kinds of universality are often implicitly assumed in the language model attention head interpretability literature. For example, it seems widely accepted that \"previous token\" attention heads form across many transformer language models (e.g.  ). The implicit hypothesis of universal attention heads \u2013 that is, attention heads with the same attention patterns in different models \u2013 isn't exactly the same thing as the kind of feature universality studied in the vision context, but is kind of analogous. [26, 27]\nOur work in this paper has analogies to many of these strands of prior work. Like the previous attention head papers, we describe the induction head pattern as a universal attention pattern. However, our analysis of these heads\u2019 OV and QK circuits extends this claim of universality to the circuit level, similar to the original Circuits thread. And a corollary of our analysis of the OV circuit is a claim about what feature the attention head computes (roughly: the token embedding of the token following a previous copy of the present token) which is more similar to the traditional work on\nSeparate from all of this, it's worth mentioning that increasingly there's evidence for a particularly extreme kind of universality at the intersection of neuroscience and deep learning. Increasingly, research suggests that biological and artificial neural networks learn similar representations (e.g.  ). In fact, Goh et al.  find that multimodal \"concept\" neurons found in humans (such as the famous \"Jennifer Anniston neuron\") occur in neural networks. [46, 47, 48] [49]\n# ATTENTION PATTERNS IN TRANSLATION-LIKE TASKS\nIn Argument 4, we saw an induction head that helps implement translation. Although we're not aware of anything quite so general in the prior literature, there are reports of attention patterns which, in retrospect, seem somewhat similar. Often, in translation-like tasks, we see attention attend to the token which is about to be translated. We see this in literal translation (e.g. ) and also in voice recognition (e.g.   where the model attends to the portion of the audio about to be transcribed). Visualizations of this in the encoder-decoder context often slightly obscure the induction-like nature of the attention patterns, because the decoder is visualized in terms of the output tokens predicted per time step rather than its input tokens. [50] [51]\n# Comments & Replications\nInspired by the original Circuits Thread and Distill's Discussion Article experiment, the authors invited several external researchers who were also investigating induction heads to comment on this work. Their comments are included below.\nRedwood Research has been working on language model interpretability, inspired in part by Anthropic\u2019s work. We\u2019ve found induction heads reliably appearing in two-layer attention-only transformers. Their structure roughly matches the description in \u201cAnalyzing a Two-Layer Model\u201d from Anthropic\u2019s previous paper, with previous-token heads in layer 0 and induction heads in layer 1 (often one of each). These each have the expected attention behaviors. We tested this by replacing attention-score matrices with idealized versions and comparing the change in loss to the change from ablating that head. Replacing the previous-token head\u2019s scores with exact previous-token attention recovered 99% of the loss difference. Replacing the induction head\u2019s scores with a simple approximation (attending to the first token and to exact [A][B]...[A]  matches) recovered about 65% of the loss difference. Our induction heads also match patterns of the form [A][B][C]...[A][B]\u2192[C] ; including this in the substituted attention scores recovered an additional 10% of loss difference. The induction head\u2019s OV circuit copies tokens, including some fuzzy matching of semantically similar tokens. Its QK circuit is dominated by K-composition with the previous-token head; the previous-token head\u2019s OV matrix copies information to a new subspace and the induction head\u2019s QK matrix copies it back to the usual token embedding. We\u2019ve also cataloged a few other kinds of attention heads, including skip-trigram heads.\nI used the empirical criterion for induction heads presented in this paper to look for induction heads in publicly available models. To reiterate: on a sequence of tokens [A][B] .... [A] \u2192 [B] , a head is called an induction head if it attends to the previous [B]  when reading the last [A] , and if the previous [B]  increased the logit of the last [B] .\n[B]  when reading the last [A] , and if the previous [B]  increased the logit of the last [B] . Under this definition, I found potential induction heads in GPT2 and GPT-Neo mostly starting in the mid-depth region. I made an interactive version to explore attention and logit attribution across all layers and heads on the long Harry Potter prompt with the repetition of the first paragraph. It can be accessed here. For example, for GPT2-XL, head 20 in layer 21 seems to be an induction head, as well as head 0 in layer 12 of GPT-Neo-2.7B. For these heads we can see that virtually every token in the repetition of the first paragraph attends to its following token in the original paragraph. Thanks to EleutherAI for providing the compute resources for this project.\nAt one point the paper speculates on the minimal context length necessary to form induction heads. On a synthetic dataset, I have already found induction to be easily learned with a context length of 4. However, I believe that this is, in general, a property of the dataset/data distribution rather than the model itself, i.e. \"how useful is induction for this task, on this dataset?\". While it is intriguing to think about the statistics of language in this way, I am not sure how useful this line of research would be to the overall project of interpretability.\n2022-09-20: Corrects the definition of Prefix matching in two places, which incorrectly stated \u201ctokens that were preceded by the current token\u201d instead of \u201cpreceded\u201d or \u201cwere followed by\u201d. Adds detail to the definition of the copying head evaluator to explain that the effect on \u201craising the logits\u201d is calculated using a ReLU.\n# Footnotes\nReferences\n1. Language models are few-shot learners [PDF] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. and others,, 2020. arXiv preprint arXiv:2005.14165.\n18. A General Language Assistant as a Laboratory for Alignment Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N. and others,, 2021. arXiv preprint arXiv:2112.00861.\nResearch: The bulk of this line of work, including many iterations of experiments that didn\u2019t make it into the final paper, was conducted by Catherine Olsson with mentorship from Chris Olah. Chris first discovered the \u201cinduction bump\u201d in a 2layer model. He had the idea of investigating what aspects of the per-token loss changed most over the bump, and made the connection to in-context learning. Catherine led data collection and analysis throughout the project. Data collection made extensive use of Garcon infrastructure by Nelson Elhage. Nelson collected data and ran analyses for the smeared key models. Chris Olah, Neel Nanda, and Catherine Olsson wrote the head activation evaluators. Research ideas were significantly influenced and improved by others at Anthropic.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4f18/4f18a59a-b96e-4d4c-bed2-3ab66d318114.png\" style=\"width: 50%;\"></div>\nCitation Information\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8a86/8a8617b4-8909-4640-801a-33a174db57e9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/739c/739c5810-",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning in large transformer models, specifically exploring the role of 'induction heads' as a potential mechanism for this phenomenon. As transformer generative models scale, understanding their internal workings becomes crucial for addressing associated safety problems.",
        "problem": {
            "definition": "The challenge lies in understanding how large transformer models exhibit in-context learning, particularly the mechanisms behind this behavior.",
            "key obstacle": "The complexity of transformer architectures with many layers and the presence of MLPs complicates the identification of the precise circuitry responsible for in-context learning."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation of a sudden increase in in-context learning ability during training, coinciding with the formation of induction heads.",
            "opinion": "Induction heads may constitute a significant mechanism for in-context learning across various transformer models.",
            "innovation": "The primary improvement over previous methods is the identification and analysis of induction heads as a distinct mechanism for facilitating in-context learning."
        },
        "Theory": {
            "perspective": "The theory posits that induction heads perform a 'fuzzy' or 'nearest neighbor' version of pattern completion, enhancing the model's ability to predict subsequent tokens based on earlier occurrences.",
            "opinion": "Induction heads are critical for understanding the learning dynamics within transformers and their capacity for in-context learning.",
            "proof": "Evidence is presented through the observation of a phase change in training that correlates with the emergence of induction heads and improved in-context learning abilities."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved analyzing 34 transformer models over the course of training, including more than 50,000 attention head ablations.",
            "evaluation method": "The method included tracking the development of in-context learning scores and correlating these with the formation of induction heads during training."
        },
        "conclusion": "The experiments suggest that induction heads are a primary mechanism for in-context learning in transformer models, although further research is needed to confirm this across larger models.",
        "discussion": {
            "advantage": "The paper provides a systematic approach to understanding in-context learning through the lens of induction heads, potentially aiding in the development of safer AI systems.",
            "limitation": "The evidence for larger models remains correlational rather than causal, and the complexities of their architectures pose challenges for definitive conclusions.",
            "future work": "Future research should aim to further investigate the mechanisms of induction heads in larger models and explore alternative mechanisms that may also contribute to in-context learning."
        },
        "other info": [
            {
                "info1": "The study includes a detailed analysis of the training dynamics of transformer models and their implications for safety."
            },
            {
                "info2": {
                    "info2.1": "The paper emphasizes the importance of mechanistic interpretability in understanding neural network behavior.",
                    "info2.2": "A focus on phase changes in model training may provide insights into the emergence of complex behaviors in AI systems."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of in-context learning in large transformer models, specifically exploring the role of 'induction heads' as a potential mechanism for this phenomenon."
        },
        {
            "section number": "1.3",
            "key information": "As transformer generative models scale, understanding their internal workings becomes crucial for addressing associated safety problems."
        },
        {
            "section number": "3.2",
            "key information": "The theory posits that induction heads perform a 'fuzzy' or 'nearest neighbor' version of pattern completion, enhancing the model's ability to predict subsequent tokens based on earlier occurrences."
        },
        {
            "section number": "3.1",
            "key information": "Induction heads may constitute a significant mechanism for in-context learning across various transformer models."
        },
        {
            "section number": "6.1",
            "key information": "The complexities of transformer architectures with many layers and the presence of MLPs complicate the identification of the precise circuitry responsible for in-context learning."
        },
        {
            "section number": "6.4",
            "key information": "Future research should aim to further investigate the mechanisms of induction heads in larger models and explore alternative mechanisms that may also contribute to in-context learning."
        }
    ],
    "similarity_score": 0.7013306517430175,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/In-context Learning and Induction Heads.json"
}