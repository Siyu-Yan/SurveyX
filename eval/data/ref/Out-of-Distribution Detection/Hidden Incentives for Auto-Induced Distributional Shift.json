{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2009.09153",
    "title": "Hidden Incentives for Auto-Induced Distributional Shift",
    "abstract": "Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users\u2019 perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce \u2018unit tests\u2019 and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.",
    "bib_name": "krueger2020hiddenincentivesautoinduceddistributional",
    "md_text": "# Hidden Incentives for Auto-induced Distributional Shift\nDavid Scott Krueger 1 2 3 Tegan Maharaj 1 4 Jan Leike 3\n# Abstract\nDecisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users\u2019 perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce \u2018unit tests\u2019 and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/178f/178f7fef-38c8-4abb-8b1f-08377cde63e3.png\" style=\"width: 50%;\"></div>\narXiv:2009.09153v1\n# 1. Introduction\nConsider a content recommendation system whose performance is measured by accuracy of predicting what users will click. This system can achieve better performance by either 1) making better predictions, or 2) changing the distribution of users such that predictions are easier to make. We propose the term auto-induced distributional shift (ADS) to describe this latter kind of distributional shift, caused by the algorithm\u2019s own predictions or behaviour (Figure 1).\n1Montr\u00e9al Institute for Learning Algorithms, Canada 2Universit\u00e9 de Montr\u00e9al, Canada 3work begun while author was at DeepMind 4Polytechnique Montr\u00e9al, Canada. Correspondence to: David Krueger <davidscottkrueger@gmail.com>.\nADS are not inherently bad, and are sometimes even desirable. But they can cause problems if they occur unexpectedly. It is typical in machine learning (ML) to assume (e.g. via the i.i.d. assumption) that (2) will not happen. However, given the increasing real-world use of ML algorithms, we believe it is important to model and experimentally observe what happens when assumptions like this are violated. This is the motivation of our work.\nFigure 1. Distributions of users over time. Left: A distribution which remains constant over time, following the i.i.d assumption. Right: Auto-induced Distributional Shift (ADS) results in a change in the distribution of users in our content recommendation environment. (see Section 7.3 for details).\n<div style=\"text-align: center;\">Figure 1. Distributions of users over time. Left: A distribution which remains constant over time, following the i.i.d assumption. Right: Auto-induced Distributional Shift (ADS) results in a change in the distribution of users in our content recommendation environment. (see Section 7.3 for details).</div>\nIn many cases, including news recommendation, we would consider (2) a form of cheating\u2014the algorithm changed the task rather than solving it as intended. We care which means the algorithm used to solve the problem (e.g. (1) and/or (2)), but we only told it about the ends, so it didn\u2019t know not to \u2019cheat\u2019. This is an example of a specification problem (Leike et al., 2017; Ortega et al., 2018): a problem which arises from a discrepancy between the performance metric (maximize accuracy) and \u201cwhat we really meant\u201d: to maximize accuracy via (1), which is difficult to encode as a performance metric.\nIdeally, we\u2019d like to quantify the desirability of all possible means, e.g. assign appropriate rewards to all potential strategies and \u201cside-effects\u201d, but this is intractable for real-world settings. Using human feedback to learn reward functions which account for such impacts is a promising approach to specifying desired behavior (Leike et al., 2018; Christiano et al., 2017). But the same issue can arise whenever human feedback is used in training: one means of improving performance could be to alter human preferences, making them easier to satisfy.Thus in this work, we pursue a complementary approach: managing learners\u2019 incentives.\nA learner has an incentive to behave in a certain way when doing so can increase performance (e.g. accuracy or reward). Informally, we say an incentive is hidden when the learner behaves as if it were not present. But we note that changes to the learning algorithm or training regime could cause previously hidden incentives to be revealed, resulting in unexpected and potentially undesirable behaviour. Managing incentives (e.g. controlling which incentives are hidden/ revealed) can allow algorithm designers to disincentivize broad classes of strategies (such as any that rely on manipulating human preferences) without knowing their exact instantiation.1\nA learner has an incentive to behave in a certain way when doing so can increase performance (e.g. accuracy or reward). Informally, we say an incentive is hidden when the learner behaves as if it were not present. But we note that changes to the learning algorithm or training regime could cause previously hidden incentives to be revealed, resulting in unexpected and potentially undesirable behaviour. Managing incentives (e.g. controlling which incentives are hidden/ revealed) can allow algorithm designers to disincentivize broad classes of strategies (such as any that rely on manipulating human preferences) without knowing their exact instantiation.1 Our goal in this work is to provide insight and practical tools for understanding and managing learners\u2019 incentives, specifically hidden incentives for auto-induced distributional shift: HI-ADS. To study the conditions which cause HI-ADS to be revealed, we present unit tests for detecting HI-ADS in supervised learning (SL) and in reinforcement learning (RL). We also create an environment which models ADS in news recommendation, to illustrate the potential effects of revealing HI-ADS in this setting. The unit tests both have two means by which the learner can improve performance: one which creates ADS and one which does not. The intended method of improving performance is one that does not induce ADS; the other is \u2019hidden\u2019 and we want it to remain hidden. A learner \"fails\" the unit test if it nonetheless pursues the incentive to increase performance via ADS. The SL unit test provides an illustrative example. It is a prediction problem with two targets, mean-zero Gaussians. The intended means of improving performance is to make good predictions (i.e. predict {0, 0}). However we create an incentive for ADS: a prediction of >0.5 for the first target will reduce the variance of the second target, reducing future loss. A learner fails the unit test to the extent it predicts >0.5 for the first target. In both the RL and SL unit tests, we find that \u2018vanilla\u2019 learning algorithms (e.g. minibatch SGD) pass the test, but introducing an outer-loop of meta-learning (e.g. PopulationBased Training (PBT) (Jaderberg et al., 2017)) can lead to high levels of failure. We find results consistent with our unit tests in the content recommendation environment: recommenders trained with PBT create earlier, faster, and larger drift in user interests, and for the same level of performance, create larger changes in the user base. These results suggest that failure of our unit tests indicates that an 1 Note removing or hiding an incentive for a behavior is different from prohibiting that behavior, which may still occur incidentally. In particular, not having a (revealed) incentive for behaviors that change a human\u2019s preferences, is not the same as having a (revealed) incentive for behaviors that preserve a human\u2019s preferences. The first is often preferable; we don\u2019t want to prevent changes in human preferences that occur \u201cnaturally\u201d, e.g. as a result of good arguments or evidence.\nOur goal in this work is to provide insight and practical tools for understanding and managing learners\u2019 incentives, specifically hidden incentives for auto-induced distributional shift: HI-ADS. To study the conditions which cause HI-ADS to be revealed, we present unit tests for detecting HI-ADS in supervised learning (SL) and in reinforcement learning (RL). We also create an environment which models ADS in news recommendation, to illustrate the potential effects of revealing HI-ADS in this setting.\n1 Note removing or hiding an incentive for a behavior is different from prohibiting that behavior, which may still occur incidentally. In particular, not having a (revealed) incentive for behaviors that change a human\u2019s preferences, is not the same as having a (revealed) incentive for behaviors that preserve a human\u2019s preferences. The first is often preferable; we don\u2019t want to prevent changes in human preferences that occur \u201cnaturally\u201d, e.g. as a result of good arguments or evidence.\nalgorithm is prone to revealing HI-ADS in other settings. Finally, we propose and test a mitigation strategy we call context swapping. The strategy consists of rotating learners through different environments throughout learning, so that they can\u2019t see the results or correlations of their actions in one environment over longer time horizons. This effectively mitigates HI-ADS in our unit test environments, but did not work well in content recommendation experiments.\n# 2. Background\n# 2.1. Meta-learning and population based training\nMeta-learning is the use of machine learning techniques to learn machine learning algorithms. This involves instantiating multiple learning scenarios which run in an inner loop (IL), while an outer loop (OL) uses the outcomes of the inner loop(s) as data-points from which to learn which learning algorithms are most effective (Metz et al., 2019). The number of IL steps per OL step is called the interval. Many recent works focus on multi-task meta-learning, where the OL seeks to find learning rules that generalize to unseen tasks by training the IL on a distribution of tasks (Finn et al., 2017; Ren et al., 2018; Andrychowicz et al., 2016). Single-task meta-learning includes learning an optimizer for a single task (Gong et al., 2018), and adaptive methods for selecting models (Kalousis, 2000) or setting hyperparameters (Snoek et al., 2012). For simplicity in this initial study we focus on single-task meta-learning. Population-based training (PBT; Jaderberg et al., 2017) is a meta-learning algorithm that trains multiple learners L1, ..., Ln in parallel, after each interval (T steps of IL) applying an evolutionary OL step which consists of: (1) Evaluate the performance of each learner, (2) Replace both parameters and hyperparameters of 20% lowest-performing learners with copies of those from the 20% high-performing learners (EXPLOIT). (3) Randomly perturb the hyperparameters (but not the parameters) of all learners (EXPLORE). Two distinctive features of PBT are notable because they give the OL more control than most meta-learning algorithms (e.g. Bayesian optimization (Snoek et al., 2012)) over the dynamics and outcome of the learning process: (1) OL applies optimization to parameters, not just hyperparameters. This means the OL can directly select for parameters which lead to ADS, instead of only being able to influence parameter values via hyperparameters (2) Multiple OL steps per training run.\n# 2.2. Distributional shift and content recommendation\nIn general, distributional shift refers to change of the data distribution over time. In supervised learning with data x and labels y, this can be more specifically described as\ndataset shift: change in the joint distribution of P(x, y) between the training and test sets (Moreno-Torres et al., 2012; Quionero-Candela et al., 2009). As identified by MorenoTorres et al. (2012), two common kinds of shift are: (1) Covariate shift: changing P(x). In the example of content recommendation, this corresponds to changing the user base of the recommendation system. For instance, a media outlet which publishes inflammatory content may appeal to users with extreme views while alienating more moderate users. This self-selection effect (Kayhan, 2015) may appear to a recommendation system as an increase in performance, leading to a feedback effect, as previously noted by Shah et al. (2018). This type of feedback effect has been identified as contributing to filter bubbles and radicalization (Pariser, 2011; Kayhan, 2015). (2) Concept shift: changing P(y|x). In the example of content recommendation, this corresponds to changing a given user\u2019s interest in different kinds of content. For example, exposure to a fake news story has been shown to increase the perceived accuracy of (and thus presumably future interest in) the content, an example of the illusory truth effect (Pennycook et al., 2019). For further details on these and other effects in content recommendation, see Appendix 1.\n# 3. Auto-induced Distribution Shift (ADS)\nAuto-induced distribution shift (ADS) is distributional shift caused by an algorithm\u2019s behaviour. This is in contrast to distributional shift which would happen even if the learner were not present - e.g. for a crash prediction algorithm trained on data from the summer, encountering snowy roads is an example of distributional shift, but not auto-induced distributional shift (ADS).\nWe emphasize that ADS are not inherently bad or good; often ADS can even be desirable: consider an algorithm meant to alert drivers of imminent collisions. If it works well, such a system will help drivers avoid crashing, thus making selfrefuting predictions which result in ADS. What separates desirable and undesirable ADS? The collision-alert system alters its data distribution in a way that is aligned with the goal of fewer collisions, whereas the news manipulation results in changes that are misaligned with the goal of better predicting existing users\u2019 interests (Leike et al., 2018).\nIn reinforcement learning (RL), ADS are typically encouraged as a means to increase performance. On the other hand, in supervised learning (SL), the i.i.d. assumption precludes ADS in theory. In practice, however, the possibility of using ADS to increase performance (and thus an incentive to do so) often remains. For instance, this occurs in online learning. In our experiments, we explicitly model such situations where i.i.d. assumptions are violated: We study the behavior of SL and myopic RL algorithms, in environments designed to include incentives for ADS, in order to understand when\nincentives are effectively hidden. Fig. 2 contrasts these settings with typical RL and SL.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3488/348830d6-0da3-4ab9-b39f-9c797a06ec49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c457/c457ac3f-dc8c-4755-8aee-fa33949379a7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Myopic RL: Incentives for ADS are present and pursuing them is undesirable</div>\n<div style=\"text-align: center;\">(a) RL: Incentives for ADS are present and pursuing them is desirable</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/08e7/08e7a5cb-3eba-41a3-b1a2-38f2543fcf7c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d694/d694054a-69a9-4f35-9fae-72309b99e174.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(d) SL with ADS: Incentives for ADS are present and pursuing them is undesirable</div>\n<div style=\"text-align: center;\">(c) SL with i.i.d. data: Incentives for ADS are absent</div>\nFigure 2. The widely studied problems of reinforcement learning (RL) with state s, action s, reward s tuples, and i.i.d. supervised learning (SL) with inputs x, predictions \u02c6y and loss l (a,c) are free from incentive problems. We focus on cases where there are incentives present which the learner is not meant to pursue (b,d). Lines show paths of influence. The learner may have incentives to influence any nodes descending from its action, A, or prediction, \u02c6y. Which incentives are undesirable (orange) or desirable (cyan) for the learner to pursue is context-dependent.\n# 4. Incentives\nFor our study of incentives, we use the following terminology: an incentive for a behavior (e.g. an action, a classification, etc.) is present (not absent) to the extent that the behaviour will increase performance (e.g. reward, accuracy, etc.) (Everitt & Hutter, 2019). This incentive is revealed to (not hidden from) a learner if it would, at higher than chance levels, learn to perform the behavior given sufficient capacity and training experience. The incentive is pursued (not eschewed) by a learner if it actually performs the incentivized behaviour. Note even when an incentive is revealed, it may not be pursued, e.g. due to limited capacity and/or data, or simply chance. See Fig 3.\nFor example, in content recommendation, the incentive to drive users away is present if some user types are easier to predict than others. But this incentive may be hidden from the learner by using a myopic algorithm, e.g. one that does not see the effects of its actions on the distribution of users. The incentive might instead be revealed to the outer loop of a meta-learning algorithm like PBT, which does see the\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7be5/7be52116-0191-4b55-a0ff-fbf8e48dc807.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 3. Types of incentives, and their relationship to ADS.\neffects of learner\u2019s actions.\nEven when this incentive is revealed, however, it might not end up being pursued. For example, this could happen if predicting which recommendations will drive away users is too difficult a learning problem, or if the incentive to do so is dominated by other incentives (e.g. change individual users\u2019 interests, or improve accuracy of predictions). In general, it may be difficult to determine empirically which incentives are revealed, because failure to pursue an incentive can be due to limited capacity, insufficient training, and/or random chance. To address this challenge, we devise extremely simple environments (\u201cunit tests\u201d), where we can be confident that revealed incentives will be pursued.\n# 5. Hidden Incentives for Auto-induced Distributional shift (HI-ADS)\nFollowing from the definitions in Sections 3 and 4, HIADS are incentives for behaviors that cause Auto-induced Distributional Shift that are hidden from the learner, i.e. the learner would not learn to perform the incentivized behaviors at higher than chance levels, even given infinite capacity and training experience.\nLike ADS, HI-ADS are not necessarily problematic. Indeed, hiding incentives can be an effective method of influencing learner behavior. For example, hiding the incentive to manipulate users from a content recommendation algorithm could prevent it from influencing users in a way they would not endorse. However, if machine learning practitioners are not aware that incentives are present, or that properties of the learning algorithm are hiding them, then seemingly innocuous changes to the learning algorithm may reveal HI-ADS, and lead to significant unexpected changes in behavior.\nHiding incentives for ADS may seem counter-intuitive and counter-productive in the context of reinforcement learning (RL), where moving towards high-reward states is typically desirable. However, for real-world applications of RL, the ultimate goal is not a system that achieves high reward, but rather one that behaves according to the designer\u2019s intentions. And as we discussed in the introduction, it can be intractable to design reward functions that perfectly specify intended behavior. Thus managing learners incentives can\nstill provide a useful tool for specification.\nWe have several reasons for focusing on HI-ADS: (1) The issue of HI-ADS has not yet been identified, and thus is likely to be neglected (at least sometimes) in practice. Our \u201cunit tests\u201d are the first published empirical methodology for assessing whether incentives are hidden or revealed by different learning algorithms. (2) Machine learning algorithms are commonly deployed in settings where ADS are present, violating assumptions used to analyze their properties theoretically. This means learners could exploit ADS in unexpected and undesirable ways if incentives for ADS are not hidden. Hiding these incentives heuristically (e.g. via off-line training) is a common approach, but potentially brittle (if practitioners don\u2019t understand how HI-ADS could become revealed). In particular, meta-learning algorithms can reveal HI-ADS, and are increasingly popular. (3) Substantial real-world issues could result from improper management of learner\u2019s incentives. Examples include tampering with human-generated reward signals (Everitt & Hutter, 2018) (e.g. selecting news articles which manipulate user interests), and creating \u201cself-fulfilling prophecies\u201d (e.g. driving up the value of a held asset by publicly predicting its value will increase (Armstrong & O\u2019Rorke, 2017)).\n# 6. Removing HI-ADS via Context Swapping\nWe propose a technique called context swapping for removing incentives for ADS revealed by changes to the learning algorithm (e.g. introducing meta-learning). The technique trains N learners in parallel, and shuffles the learners through N different copies of the same (or similar) environments; which copy a given learner inhabits can change at any (or every) time-step. We use a deterministic permutation of learners in environment copies, so that the i-th learner inhabits the j-th environment on time-steps t where j = (i + t) mod N, makes an observation, takes an action, and receives a reward before moving to the next environment.\nWhen N is larger than the interval of the OL optimizer, each learner inhabits each copy for at most a single time-step before an OL step is applied. Under the assumption that different copies of the environment do not influence each other, this technique can address HI-ADS in practice, as we show in Sec. 7.2.1.\n# 7. Experiments\nIn Sections 7.2 and 7.1, we introduce \u2018unit tests\u2019 for HIADS. Our primary goal with these experiments is to convey a crisp understanding of potential issues caused by revealing HI-ADS. Put simply, our experiments show that you can have a learner which behaves as intended, and just by using meta-learning (e.g. PBT), without changing the performance metric (e.g. loss or rewards), the learner\u2019s behavior\n\nFigure 4. (a) No context swapping (b) Context swapping. The proposed technique rotates learners through different environments. This removes the incentive for a learner to\u201cinvest\u201d in a given environment, since it will be swapped out of that context later and not be able to reap the benefits of its investment.\ncan change completely.\nWe also show that context swapping is an effective mitigation technique in these environments. On the practical side, the unit tests can be used to compare learning algorithms and diagnose their propensity to reveal incentives.\nIn Section 7.3, we model a content recommendation system. The goal of these experiments is to demonstrate how HI-ADS could create issues for real-world content recommendation systems such as news feeds, search results, or automated suggestions. They also validate the usefulness of the unit tests: algorithms failed the unit tests also reveal HI-ADS in this setting. We emphasize that ADS takes place in this environment by construction. The point of our experiments is that meta-learning can increase the rate and/or extent of ADS, by revealing this incentive. We find that context swapping is not effective in this environment, highlighting the need for alternative mitigation strategies.\n# 7.1. HI-ADS Unit Test 1: Supervised Learning\nThis unit test consists of a simple prediction problem. There are no inputs, only an underlying state s \u2208{0, 1}, and targets y \u2208R2 with y1, y2 \u223cN(0, s \u2217\u03c32), N(0, 1), with corresponding predictions \u02c6y1, \u02c6y2. Additionally, st+1 = 0 iff \u02c6y2 > .5. We use Mean Squared Error as the loss function, so the optimal predictor is \u02c6y1, \u02c6y2 = (0, 0). However, predicting \u02c6y2 > .5 reduces the variance of \u02c6y1, i.e. reduces future loss. The baseline/IL predictor learns \u02c6y1, \u02c6y2 as parameters using SGD with a learning rate of 0.001. For experiments with meta-learning, PBT is the OL (with default settings, see Section 2.2), used to tune the learning rate, with negative loss on the final time-step of the interval as the performance measure for PBT.\n# 7.2. HI-ADS Unit Test 2: Myopic RL\nThis unit test is based on a version of the prisoner\u2019s dilemma (Prisner, 2014) where an agent plays each round against its past self. The reward function is presented in Table 1. An agent in this environment has a long-term, non-myopic, incentive for cooperation (with its future self), but a current-\ntime-step, myopic, incentive for defection (from its future self). The unit test evaluates whether a agent reveals the non-myopic incentive even when the agent is meant to optimize for the present reward only (i.e. uses discount rate \u03b3 = 0). Naively, we\u2019d expect the non-myopic incentive to be hidden from the agent in this case, and for the agent to consistently defect; learning algorithms that do so pass the test. But some learning algorithms also fail the unit test, revealing the incentive for the agent to cooperate with its future self. While aiming for myopic behavior may seem odd, myopic learners have no incentives to cause distributional shift, since it can only improve future performance. And while making learners myopic may seem like a \u2019brute-force\u2019 guaranteed way to manage HI-ADS, we show it is in fact non-trivial to implement. Formally, this environment is not a 2x2 game (as the original prisoner\u2019s dilemma); it\u2019s a partially observable Markov Decision Process (\u00c5str\u00f6m, 1965; Kaelbling et al., 1998):\nFormally, this environment is not a 2x2 game (as the original prisoner\u2019s dilemma); it\u2019s a partially observable Markov Decision Process (\u00c5str\u00f6m, 1965; Kaelbling et al., 1998):\nwhere I is an indicator function, and \u03b2 = \u22121/2 is a parameter controlling the alignment of incentives (see Appendix 3.2 for an exploration of different \u03b2 values.). The initial state is sampled as s0 \u223cU(defect, cooperate). Policies are represented by a single real-valued parameter \u03b8 (initialized as \u03b8 \u223cN(0, 1)) passed through a sigmoid whose output represents P(at = defect). We use REINFORCE (Williams, 1992) with discount factor \u03b3 = 0 as the baseline/IL optimizer. PBT (with default settings, see Section 2.2) is used to tune the learning rate, with reward on the final time-step of the interval as the performance measure for PBT.\nTable 1. Rewards for the RL unit test. Note that the myopic (defect) action always increases reward at the current time-step, but decreases reward at the next time-step - the incentive is hidden from the point of view of a myopic learner. A supposedly myopic learner \u2018fails\u2019 the unit test if the hidden incentive to cooperate is revealed, i.e. if we see more cooperate (C) actions than defect (D).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3c96/3c969b41-e1e5-4e59-8619-cc33dfe80dd8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">7.2.1. HI-ADS UNIT TESTS EXPERIMENTAL RESULTS AND DISCUSSION</div>\nWe first show that agents trained with PBT fail the unit tests more often than \u201cvanilla\u201d algorithms which do not use meta-learning. We initialize the learning rate log-uniformly between 0.01 and 1.0 for all experiments (whether using\nPBT or not). We expect and confirm that the following two factors lead to higher rates of unit test failure: (1) Shorter intervals: These give the OL more opportunities to influence the population. (2) Larger populations: These make outliers with exceptional non-myopic performance more likely, and OL makes them likely to survive and propagate. The baseline (no meta-learning) algorithms all pass the unit tests: hidden incentives are almost never revealed - see blue curves in Fig. 5. However, agents trained with metalearning and large populations often fail the unit tests: see orange curves in top rows of Fig. 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2661/2661d8b0-0671-4103-bf58-30d17294b1d5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(B2) Myopic RL Unit Test. OL=REINFORCE</div>\nFigure 5. (A) Values of \u02c6y2 in the supervised learning (SL) unit test. Larger values mean sacrificing present performance for future performance (i.e. non-myopic exploitation of ADS). (B) Average level of non-myopic cooperate behavior observed in the RL unit test for HI-ADS, with two meta-learning algorithms (B1) PBT and (B2) REINFORCE. Lower is better, since the goal is for nonmyopic incentives to remain hidden. Despite the inner loop being fully myopic (simple MLP in the SL test, \u03b3 = 0 in RL test), in all cases outer-loop (OL) optimizers reveal HI-ADS (top rows). Context swapping significantly mitigates HI-ADS (bottom rows).\nmitigates the effect of HI-ADS in both unit tests, decreasing undesirable behaviour to near-baseline levels - see bottom rows of Fig. 5. This effect can be explained as follows: Because context swapping transfers the benefits of one learner\u2019s action to the next learner to inhabit that environment, it increases the second learner\u2019s fitness, and thereby reduces the relative fitness (as evaluated by PBT\u2019s EXPLOIT step) of the non-myopic cooperate behaviour. We observe some interesting exceptions with the combination of small populations and short PBT intervals: Although context swapping still significantly decreases the effect of HI-ADS, non-myopic cooperate behaviour is observed as much as 20% of the time (for #learners=10, T = 1; see bottom-left plot). We also observe that PBT reveals HI-ADS even when T = 1, where the explanation that PBT operates on a longer time horizon than the inner loop does not apply. We provide a detailed explanation for how this might happen in Appendix 3, but in summary, we hypothesize that there are at least 2 mechanisms by which PBT is revealing HI-ADS: (1) optimizing over a longer time-scale, and (2) picking up on the correlation between an agent\u2019s current policy and the underlying state. Mechanism (2) can be explained informally as reasoning as: \u201cIf I\u2019m cooperating, then I was probably cooperating on the last time-step as well, so my reward should be higher\u201d. As support for these hypotheses, we run control experiments identifying two algorithms (each sharing only one of these properties) that can fail the unit test. Context swapping remains effective. (1) Optimizing over a longer time-scale: replacing PBT with REINFORCE as an outer-loop optimizer. The outerloop optimizes the parameters to maximize the summed reward of the last T time-steps. As with PBT, we observe non-myopic behavior, but now only when T > 1. This supports our hypothesis that exploitation of HI-ADS is due not to PBT in particular, but just to the introduction of sufficiently powerful meta-learning. See Fig. 5 B2. (2) Exploiting correlation: Q-learning with \u03b3 = 0 an \u03f5 = 0.1-greedy behavior policy and no meta-learning. If either state was equally likely, the Q-values would be the average of the values in each column in Table 1, so the estimated Q(defect) would be larger. But the \u03f5-greedy policy correlates the previous action (i.e. the current state) and current action (so long as the policy did not just change), so the top-left and bottom-right entries carry more weight in the estimates, sometimes causing Q(defect) \u2248Q(cooperate) and persistent nonmyopic behavior. See Fig. 6 for results,\n(2) Exploiting correlation: Q-learning with \u03b3 = 0 an \u03f5 = 0.1-greedy behavior policy and no meta-learning. If either state was equally likely, the Q-values would be the average of the values in each column in Table 1, so the estimated Q(defect) would be larger. But the \u03f5-greedy policy correlates the previous action (i.e. the current state) and current action (so long as the policy did not just change), so the top-left and bottom-right entries carry more weight in the estimates, sometimes causing Q(defect) \u2248Q(cooperate) and persistent nonmyopic behavior. See Fig. 6 for results, Appendix 4.2 for more results, and Appendix 4.1 for experimental details.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/11d2/11d25b3c-3b8f-4817-92df-280a352f7b64.png\" style=\"width: 50%;\"></div>\nFigure 6. Q-learning fails the unit test for some random seeds; empirical p(cooperate) stays around 80-90% in 3 of 5 experiments (bottom row). Each column represents an independent experiment. Q-values for the cooperate and defect actions stay tightly coupled in the failure cases (col. 1,2,5), while in the cases passing the unit test (col. 3,4) the Q-value of cooperate decreases over time.\n# 7.3. HI-ADS in content recommendation\nWe now present a toy environment for modeling content recommendation of news articles, which includes the potential for ADS by incorporating the mechanisms mentioned in Sec. 2.2, discussed as contributing factors to the problems of fake news and filter bubbles. Specifically, the environment assumes that presenting an article to a user can influence (1) their interest in similar articles, and (2) their propensity to use the recommendation service. These correspond to modeling auto-induced concept shift of users, and auto-induced covariate shift of the user base, respectively (see Sec. 2.2). This environment includes the following components, which change over (discrete) time: User type: xt, Article type: yt, User interests: Wt (propensity for users of each type to click on articles of each type), and User loyalty: gt (propensity for users of each type to use the platform). At each time step t, a user xt is sampled from a categorical distribution, based on the loyalty of the different user types. The recommendation system (a classifier) selects which type of article to present in the top position, and finally the user \u2018clicks\u2019 an article yt, according to their interests. User loyalty for user type xt undergoes covariate shift: in accordance with the self-selection effect, gt increases or decreases proportionally to that user type\u2019s interest in the top article. The interests of user type xt (represented by a column of Wt) undergoing concept shift; in accordance with the illusory truth effect, interest in the topic of the top article chosen by the recommender system always increases. Formally, this environment is similar to a POMDP\\R, i.e. a POMDP with no reward function, also known as a world model (Armstrong & O\u2019Rourke, 2017; Hadfield-Menell et al., 2017); the difference is that the learner observes the input (ot pre) before acting and only observes the target (ot post) after acting. The states s, observations o, and actions\na are computed as follows:\nFor further details on this environment, including the state transition function, see Appendix 2.\nOur recommender system is a 1-layer MLP trained with SGD-momentum. Actions are sampled from the MLP\u2019s predictive distribution. For PBT, we use T = 10 and 20 agents, and use accuracy to evaluate performance. We run 20 trials, and match random seeds for trials with and without PBT. See Appendix 3 for full experimental details.\nWe find that PBT yields significant improvements in training time and accuracy, but also greater distributional shift ( Fig. 7). User base and user interests both change faster with PBT, and user interests change more overall. We observe that the distributions over user types typically saturate (to a single user type) after a few hundred time-steps (Fig 1 and Fig. 7, Right). We run long enough to reach such states, to demonstrate that the increase in ADS from PBT is not transitory. The environment has a number of free parameters, and our results are qualitatively consistent so long as (1) the initial user distribution is approximately uniform, and (2) the covariate shift rate (\u03b11) is faster than the concept shift rate (\u03b12). See Appendix 2 for details. We measure concept shift (change in P(y|x)) as the cosine distance between each user types\u2019 initial and current interest vectors. And we measure covariate shift (change in P(x)) as the KL-divergence between the current and initial user distributions, parametrized by g1 and gt, respectively. In Figure 8, we plot concept shift and covariate shift as a function of accuracy. We observe that for both types of ADS, at low levels of accuracy PBT actually causes less shift than occur in baseline agents; HI-ADS are only observed for accuracies above 60%. This suggests that only relatively strong performers are able to pick up on the HI-ADS revealed by PBT (Fig. 8).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8ca/c8ca85f2-8f69-4c84-8278-cf6360ec5c36.png\" style=\"width: 50%;\"></div>\nFigure 7. Content recommendation experiments. Left: using Population Based Training (PBT) increases accuracy of predictions faster, leads to a faster and larger drift in users\u2019 interests, P(y|x), (Center); as well as the distribution of users, P(x), (Right). Shading shows std error over 20 runs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f300/f300f2a8-bbc2-4471-99ee-0d07dccdd4dd.png\" style=\"width: 50%;\"></div>\nFigure 8. Amount of auto-induced covariate shift (left) and autoinduced concept shift (right) as a function of performance (accuracy) averaged over all trials, learners, and time-steps. Only relatively strong learners (those which achieve accuracy > 60%) exhibit HI-ADS.\n# 8. Related work\nADS in practice: We introduce the term ADS, but we are far from the first to study it. Caruana et al. (2015) provide an example of asthmatic patients having lower predicted risk of pneumonia. Treating asthmatics with pneumonia less aggressively on this basis would be an example of harmful ADS; the reason they had lower pneumonia risk was because they had received more aggressive care already. Schulam & Saria (2017) note that such predictive models are commonly used to inform decision-making, and propose modeling counterfactuals (e.g. \u201chow would this patient fare with less aggressive treatment\u201d) to avoid making such selfrefuting predictions. While their goal is to make accurate predictions in the presence of ADS, our goal is to identify and manage incentives for ADS. Goodfellow (2019) argues that adversarial defenses that do not account for ADS are critically flawed.\nNon-i.i.d bandits: Contextual bandits (Wang et al., 2005; Langford & Zhang, 2008) are frequently discussed as an approach to content recommendation (Li et al., 2010). While bandit algorithms typically make the i.i.d. assumption, counter-examples exist (Gheshlaghi Azar et al., 2014; Shah et al., 2018); most famously, adversarial bandits (Auer et al., 1995). Closest to our work is Shah et al. (2018), who consider covariate shift caused by multi-armed bandits. Our\ntask in Sec. 7.3 is similar to their problem statement, but more general in that we include user features, thus disentangling covariate shift and concept shift. Our motivation is also different: Shah et al. (2018) seek to exploit ADS, whereas we aim to avoid hidden incentives for ADS. Safety and incentives: Emergent incentives to influence the world (such as HI-ADS) are at the heart of many concerns about the safety of advanced AI systems (Omohundro, 2008; Bostrom, 2014). Understanding and managing the incentives of learners is also a focus of Armstrong & O\u2019Rourke (2017); Everitt (2018); Everitt et al. (2019); Cohen et al. (2019). While Everitt et al. (2019) focus on identifying which incentives are present, we note that incentives may be present and yet not be revealed or pursued - for example, in supervised learning, there is an incentive to make predictions that are over-fit to the test set, but we typically hide the test set from the learner, which effectively hides this incentive. While Carey et al. (2020); Everitt et al. (2019); Armstrong & O\u2019Rourke (2017) discuss methods of removing problematic incentives, we note in practice incentives are often hidden rather than removed. Our work addresses the efficacy of this approach and ways in which it can fail. HI-ADS and meta-learning: As far as we know, our work is the first to consider the problem of HI-ADS, or its relation to meta-learning. A few previous works have some relevance or resemblance. Rabinowitz (2019) documents qualitative differences in learning behavior when meta-learning is applied. MacKay et al. (2019) and Lorraine & Duvenaud (2018) view meta-learning as a bilevel optimization problem, with the inner loop playing a bestresponse to the outer loop. In our work, the inner loop is unable to achieve such best-response behavior; the outer loop is too powerful (see Fig. 5). Finally, Sutton et al. (2007) note that meta-learning can change learning behavior in a way that improves performance by preventing convergence of the inner loop.\n# 9. Discussion and Conclusion\nWe have identified the phenomenon of auto-induced distributional shift (ADS), and the problems that can arise when there are hidden incentives for learners to induce distributional shift (HI-ADS). And our experiments demonstrate that using meta-learning can reveal HI-ADS and lead learners to use ADS as a means of increasing performance.\nOur work highlights the interdisciplinary nature of issues with real-world deployment of ML systems - we show how HI-ADS could play a role in important technosocial issues like filter bubbles and the propagation of fake news. There are a number of potential implications for our work: (1) When HI-ADS are a concern, our methodology and environments can be used to help diagnose whether and to what\nextent the final performance/behavior of a learner is due to ADS and/or incentives for ADS, i.e. to quantify their influence on that learner. (2) Comparing this quantitative analysis for different algorithms could help us understand which features of algorithms affect their propensity to reveal HI-ADS, and aid in the development of safer and more robust algorithms. (3) Characterizing and identifying HI-ADS in these tests is a first step to analyzing and mitigating other (problematic) incentives, as well as to developing theoretical understanding of incentives. Broadly speaking, our work emphasizes that the choice of machine learning algorithm plays an important role in specification, independently of the choice of performance metric. A learner can use ADS to increase performance according to the intended performance metric, and yet still behave in an undesirable way, if we did not intend the learner to improve performance by that method. In other words, performance metrics are incomplete specifications: they only specify our goals or ends, while our choice of learning algorithm plays a role in specifying the means by which we intend an learner to achieve those ends. With increasing deployment of ML algorithms in daily life, we believe that (1) understanding incentives and (2) specifying desired/allowed means of improving performance are important avenues of future work to ensure fair, robust, and safe outcomes.\n# 10. Acknowledgements\nThanks to the DeepMind and Future of Humanity Institute AI safety teams who gave lots of feedback on these ideas. Thanks to Valentin Dalibard for help with Population Based Training, and Toby Pohlen for help with using Google infrastructure. Thanks to Owain Evans, Audrey Durand, Jacob Buckman, Michael Noukhovitch and Emmanuel Bengio for feedback on drafts.\n# References\nGroshek, J. and Koc-Michalska, K. Helping populism win? Social media use, filter bubbles, and support for populist presidential candidates in the 2016 us election campaign. Information, Communication & Society, 20(9):1389\u20131407, 2017. doi: 10.1080/ 1369118X.2017.1329334. URL https://doi.org/ 10.1080/1369118X.2017.1329334.\nHadfield-Menell, D., Milli, S., Abbeel, P., Russell, S., and Dragan, A. Inverse reward design. In Neural Information Processing Systems, 2017.\nJaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W. M., Donahue, J., Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., Fernando, C., and Kavukcuoglu, K. Population Based Training of Neural Networks. ArXiv preprint, 2017.\nAppendices\n# 1. Content recommendation in the wild\n1. Content recommendation in the wild\nFilter bubbles, the spread of fake news, and other technosocial issues are widely reported to be responsible for the rise of populism (Groshek & Koc-Michalska, 2017), increase in racism and prejudice against immigrants and refugees (Noble, 2018), increase in social isolation and suicide (Luxton et al., 2012), and, particularly with reference to the 2016 US elections, are decried as threatening the foundations of democracy (El-Bermawy, 2016). Even in 2013, well before the 2016 American elections, a World Economic Forum report identified these problems as a global crisis (Lee Howell, 2013).\nWe focus on two related issues in which content recommendation algorithms play a role: fake news and filter bubbles.\n# 1.1. Fake news\nFake news (also called false news or junk news) is an extreme version of yellow journalism, propaganda, or clickbait, in which media that is ostensibly providing information focuses on being eye-catching or appealing, at the expense of the quality of research and exposition of factual information. Fake news is distinguished by being specifically and deliberately created to spread falsehoods or misinformation (Merriam-Webster, 2017; Mihailidis & Viotty, 2017). Why does fake news spread? It may at first seem the solution is simply to educate people about the truth, but research tells us the problem is more multifaceted and insidious, due to a combination of related biases and cognitive effects including confirmation bias (people are more likely to believe things that fit with their existing beliefs), priming (exposure to information unconsciously influences the processing of subsequent information, i.e. seeing something in a credible context makes things seem more credible) and the illusory truth effect (i.e. people are more likely to believe something simply if they are told it is true). Allcott & Gentzkow (2017) track about 150 fake news stories during the 2016 US election, and find the average American adult saw 1-2 fake news stories, just over half believed the story was true, and likelihood of believing fake news increased with ideological segregation (polarization) of their social media. Shao et al. (2018) examine the role of social bots in spreading fake news by analyzing 14 million Twitter messages. They find that bots are far more likely than humans to spread misinformation, and that success of a fake news story (in terms of human retweets) was heavily dependent on whether bots had shared the story. Pennycook et al. (2019) examine the role of the illusory truth effect in fake news. They find that even a single exposure to a news story makes people more likely to believe that it is\ntrue, and repeat viewings increase this likelihood. They find that this is not true for extremely implausible statements (e.g. \u201cthe world is a perfect cube\u201d), but that \u201conly a small degree of potential plausibility is sufficient for repetition to increase perceived accuracy\u201d of the story. The situation is further complicated by peoples\u2019 inability to distinguish promoted content from real news - Amazeen & Wojdynski (2018) find that fewer than 1/10 people were able to tell when content was an advertisement, even when it was explicitly labelled as such. Similarly, Fazio et al. (2015) find that repeated exposure to incorrect trivia make people more likely to believe it, even when they are later able to identify the trivia as incorrect.\n# 1.2. Filter bubbles\nFilter bubbles, a term coined and popularized by Pariser (2011) are created by positive or negative feedback loops which encourage users or groups of users towards increasing within-group similarity, while driving up between-group dissimilarity. The curation of this echo chamber is called self-selection (people are more likely to look for or select things that fit their existing preferences), and favours what Techopedia (2018) calls intellectual isolation. In the context of social and political opinions, this is often called the polarization effect (Wikipedia contributors, 2018).\nFilter bubbles can be encouraged by algorithms in two main ways. The first is the most commonly described: simply by showing content that is similar to what a user has already searched for, search or recommender systems create a positive feedback loop of increasingly-similar content (Pariser, 2011; Kayhan, 2015). The second way is similar but opposite - if the predictions of an algorithm are good for a certain group of people, but bad for others, the algorithm can do better on its metrics by driving hard-to-predict users away. Then new users to the site will either be turned off entirely, or see an artificially homogenous community of like-minded peers, a phenomena Shah et al. (2018) call positive externalities.\nIn a study of 50,000 US-based internet users, Flaxman & Goel (2015) find that two things increase with social media and search engine use: (1) exposure of an individual to opposing or different viewpoints, and (2) mean ideological distance between users. Many studies cite the first result as evidence of the benefits of internet and social media (Robson, 2018; Bakshy et al., 2015), but the correlation of exposure with ideological distances demonstrates that exposure is not enough, and might even be counterproductive.\nFacebook\u2019s own study on filter bubbles results show that the impact of the news feed algorithm on filter bubble \u201csize\u201d (a measure of homogeneity of posts relative to a baseline) is almost as large as the impact of friend group composition (Bakshy et al., 2015). Kayhan (2015) specifically study\nthe role of search engines in confirmation bias, and find that search context and the similarity of results in search engine results both reinforce existing biases and increase the likelihood of future biased searches. Nguyen et al. (2014) similarly study the effect of recommender systems on individual users\u2019 content diversity, and find that the set of options recommended narrows over time. Filter bubbles create an ideal environment for the spread of fake news: they increase the likelihood of repeat viewings of similar content, and because of the illusory truth effect, that content is more likely to be believed and shared (Pennycook et al., 2019; DiFranzo & Gloria-Garcia, 2017; Pariser, 2011). We are not claiming that HI-ADS are entirely or even mostly responsible for these problems, but we do note that they can play a role that is worth addressing.\nThis section presents an exploration of the parameter \u03b2, which controls the alignment of incentives in the HI-ADS unit tests (see Table 2). To clarify the interpretation of experiments, we distinguish between environments in which myopic (defect) vs. nonmyopic (cooperate) incentives are opposed, orthogonal, or compatible. Note that in this unit test myopic behaviour (defection) is what we want to see.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9b0c/9b0c6019-ac26-41f4-85ac-1dbe77ed6edf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 9. Average level of non-myopic (i.e. cooperate) behavior learned by agents in the unit test for HI-ADS. Despite making the inner loop fully myopic (\u03b3 = 0), population-based training (PBT) can cause HI-ADS, leading agents to choose the cooperate action (top row). context swapping successfully prevents this (bottom row). Columns (from left to right) show results for populations of 10, 100, and 1000 learners. In the legend, \u201cinterval\u201d refers to the interval (T) of PBT (see Sec. 2.2). Sufficiently large populations and short intervals are necessary for PBT to induce nonmyopic behavior.\n# 2. Extra experiments and reproducibility details\n2.1. HI-ADS unit test\n2.1.1. ALIGNMENT OF INCENTIVES EXPLORATION\nThis section presents an exploration of the parameter \u03b2, which controls the alignment of incentives in the HI-ADS unit tests (see Table 2).\nTo clarify the interpretation of experiments, we distinguish between environments in which myopic (defect) vs. nonmyopic (cooperate) incentives are opposed, orthogonal, or compatible. Note that in this unit test myopic behaviour (defection) is what we want to see.\n1. Incentive-opposed: Optimal myopic behavior is incompatible with optimal nonmyopic behavior (classic prisoner\u2019s dilemma; these experiments are in the main paper).\n2. Incentive-orthogonal: Optimal myopic behavior may or may not be optimal nonmyopic behavior.\n# 2. Incentive-orthogonal: Optimal myopic behavior may or may not be optimal nonmyopic behavior.\n3. Incentive-compatible: Optimal myopic behavior is necessarily also optimal nonmyopic behavior.\nWe focused on incentive-opposed environment (\u03b2 = \u22121/2) in the main paper in order to demonstrate that HI-ADS can be powerful enough to change the behavior of the system in an undesirable way. Here we also explore incentivecompatible and incentive-orthogonal environments because they provide useful baselines, helping us distinguish a systematic bias towards nonmyopic behavior from other reasons (such as randomness or optimization issues) for behavior that does not follow a myopically optimal policy.\n# 2.1.2. WORKING THROUGH A DETAILED EXAMPLE FOR PBT WITH T = 1\n2.1.2. WORKING THROUGH A DETAILED EXAMPLE FOR PBT WITH T = 1\nTo help provide intuition on how (mechanistically) PBT could lead to persistent levels of cooperation, we walk through a simple example (with no inner loop). Consider PBT with T = 1 and a population of 5 deterministic agents A1, ..., A5 playing cooperate and receiving reward of r(Ai) = 0. Now suppose A1 suddenly switches to play defect. Then r(A1) = 1/2 on the next time-step (while the other agents\u2019 reward is still 0), and so PBT\u2019s EXPLOIT step will copy A1 (without loss of generality to A2). On the following time-step, r(A2) = 1/2, and r(A1) = \u22121/2, so PBT will clone A2 to A1, and the cycle repeats. Similar reasoning applies for larger populations, and T > 1.\nTable 2. \u03b2 controls the extent to which myopic and nonmyopic incentives are aligned.\n\u03b2\nEnvironment\nCooperating\n< 0\nincentive-opposed\nyields less reward on the current time-step (myopically detrimental)\n= 0\nincentive-orthogonal\ndoes not affect the current reward (myopically indifferent)\n> 0\nincentive-compatible\nyields more reward on the current time-step (myopically beneficial)\nyields less reward on the current time-step (myopically detrimental) does not affect the current reward (myopically indifferent) yields more reward on the current time-step (myopically beneficial)\nWe show that, under certain conditions, Q-learning can learn to (primarily) cooperate, and thus fails the HI-ADS unit test. We estimate Q-values using the sample-average method, which is guaranteed to converge in the fully observed, tabular case (Sutton & Barto, 1998). The agent follows the \u03f5-greedy policy with \u03f5 = 0.1. In order to achieve this result, we additionally start the agent off with one synthetic memory where both state and action are defect and therefor R(defect) = \u2212.5, and we hard-code the starting state to be cooperate (which normally only happens 50% of the time). Without this kind of an initialization, the agent always learns to defect. However, under these conditions, we find that 10/30 agents learned to play cooperate most of the time, with Q(cooperate) and Q(defect) both hovering around \u22120.07, while others learn to always defect, with Q(cooperate) \u2248\u22120.92 and Q(defect) \u2248\u22120.45. context swapping, however, prevents majority-cooperate behavior from ever emerging, see Figure 12.\n# 2.1.4. Q-LEARNING: FURTHER RESULTS\nTo give a more representative picture of how often Qlearning fails the unit test, we run a larger set of experiments with Q-learning, results are in Figure 11. It\u2019s possible that the failure of Q-learning is not persistent, since we have not proved otherwise, but we did run much longer experiments and still observe persistent failure, see Figure 10.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/497b/497b29f4-3bb0-4055-bcee-61e2b5c03ae4.png\" style=\"width: 50%;\"></div>\nFigure 10. The same experiments as Figures 6, 11, run for 50,000 time-steps instead of 3000, to illustrate the persistence of nonmyopic behavior.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/df41/df41add5-4418-45be-9408-8c2307a74f05.png\" style=\"width: 50%;\"></div>\nFigure 11. More independent experiments with Q-learning, exactly following Figure 6. Q-learning fails the unit test in a total of 10/30 experiments (including those from Figure 6).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/63c7/63c74cdb-ee1a-4c76-a6f2-9dcbfb189337.png\" style=\"width: 50%;\"></div>\nFigure 12. More independent experiments with Q-learning, exactly following Figure 6, except also using context swapping. This leads to a 100% success rate on the unit test.\n# 2.2. Content recommendation\n2.2.1. ENVIRONMENT DETAILS\nThe evironment has the following components:\n1. User type, xt: categorical variable representing different types of users. The content recommender conditions its predictions on the type of the current user.\n2. User loyalty, gt: the propensity for users of each type to use the platform. User xt is sampled from a categorical distribution with parameters given by softmax(gt).\n3. Article type, yt: a categorical variable (one-hot encoding) representing the type of article selected by the user.\n4. User interests, Wt: a matrix whose entries W t x,y represent the average interest user of type x have in articles of type y.\nAt each time step t, a user xt is sampled from a categorical distribution (based on the loyalty of the different user types), then the recommendation system selects which type of article to present in the top position, and finally, the user selects an article. The goal of the recommendation system is to predict the likelihood that the user would click on each of the available articles, in order to select the one which is most interesting to the user. User loyalty for xt then changes in accordance with the self-selection effect, increasing or decreasing proportionally to their interest in the top article. The interests of user type xt (represented by a column of Wt) also change; in accordance with the illusory truth effect, their interest in the topic of the top article (as chosen by the recommender system) always increases. Overall, this environment is an extremely crude representation of reality, but it allows us to incorporate both the effects of self-selection (via covariate shift), and the illusory truth effect (via concept shift). Formally, this environment is similar to a POMDP\\R, i.e. a POMDP with no reward function, also known as a world model (Armstrong & O\u2019Rourke, 2017; Hadfield-Menell et al., 2017); the difference is that the learner observes the input before acting and only observes the target after acting. The states, observations, and actions given below.\nst = (gt, Wt, xt, yt) ot pre, at, ot post = (xt, \u02c6yt, yt)\nThe state transition function is defined by:\nWhere \u02c6yt is the top article as chosen by the recommender, and \u03b11, \u03b12 represent the rate of covariate and concept shift (respectively). The update for Wt+1 merely increases the interest of user type xt in article type \u02c6yt, then normalizes the interests for that user type.\n# 2.2.2. REPRODUCIBILITY DETAILS\nFor these experiments, the recommendation system is a ReLU-MLP with 1 hidden layer of 100 units, trained via supervised learning with SGD (learning rate = 0.01) to predict which article a user will select. Actions are sampled from the MLP\u2019s predictive distribution. We apply PBT without any hyperparameter selection (this amounts to just doing the EXPLOIT step), and an interval of 10, selecting on accuracy. We use a population of 20 learners (whether applying PBT or not), and match random seeds for the trials with and without PBT. We initialize g1 and W1 to be the same across the 20 copies of the environment (i.e. the learners start with the same user population), but these values diverge throughout learning. For the environment, we set the number of user and article types both to 10. Initial user loyalties are randomly sampled from N(0, 0.03), \u03b11 = 0.03, and \u03b12 = 0.003.\n# 2.2.3. CONTEXT SWAPPING IN CONTENT RECOMMENDATION\nWe believe context swapping is not appropriate for the content recommendation environment, since when the environments diverge, optimal behavior may differ across environments. Nevertheless, we ran experiments with it for completeness. The main effect appears to be to hamper learning when PBT is not used, see Figure 13. Notably, it does not appear to significantly influence the rate or extent of ADS when combined with PBT.\n# 2.2.4. EXPLORATION OF ENVIRONMENT PARAMETERS\n2.2.4. EXPLORATION OF ENVIRONMENT PARAMETERS\nIn Figure 14, we examine the effect of the rate-of-change parameters (\u03b11, \u03b12) of the content recommendation environment on the results provided in the paper. As noted there, our results are qualitatively consistent so long as (1) the initial user distribution is approximately uniform, and (2) the covariate shift rate (\u03b11) is faster than the concept shift rate (\u03b12). These distributions are updated by different mechanisms, and are not directly comparable. Concept\nshift changes the task more radically, requiring a learner to change its predictions, rather than just become accurate on a wider range of inputs. We conjecture that changes in P(y|x) must therefore be kept smooth enough for the outer loop to have pressure to capitalize on HI-ADS.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/70ab/70ab2df9-a38a-4aa5-a6b9-499c9ac96f4c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13. Context swapping doesn\u2019t have the desired effect in the content recommendation environment.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4748/47488658-8741-4567-8081-1ab5df7d4cb4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 14. Content recommendation results for different values of \u03b11, \u03b12.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of auto-induced distributional shift (ADS) in machine learning algorithms, particularly in content recommendation systems, where the algorithm's behavior can change user preferences and perceptions, leading to unintended distributional shifts that can impact performance and ethical considerations.",
        "problem": {
            "definition": "The problem is the hidden incentives for auto-induced distributional shift (HI-ADS) that arise when machine learning systems inadvertently influence the distribution of their inputs, leading to potential ethical concerns and performance issues.",
            "key obstacle": "The main difficulty is that existing learning algorithms often do not account for the consequences of their actions on user preferences, which can lead to unexpected behaviors that deviate from intended outcomes."
        },
        "idea": {
            "intuition": "The idea is inspired by the observation that machine learning systems can create hidden incentives that lead to undesirable changes in user behavior and distributional shifts, which are not accounted for in traditional performance metrics.",
            "opinion": "The proposed idea involves developing unit tests and mitigation strategies to reveal and manage these hidden incentives, ensuring that machine learning systems operate within desirable ethical boundaries.",
            "innovation": "The primary innovation lies in the introduction of unit tests for detecting HI-ADS and a mitigation strategy called context swapping, which rotates learners through different environments to prevent the revelation of undesirable incentives."
        },
        "method": {
            "method name": "Hidden Incentives for Auto-induced Distributional Shift (HI-ADS)",
            "method abbreviation": "HI-ADS",
            "method definition": "HI-ADS refers to the hidden incentives that cause machine learning algorithms to induce changes in the distribution of their inputs without the learners being aware of these incentives.",
            "method description": "The method involves identifying and managing hidden incentives in machine learning systems to prevent unintended distributional shifts.",
            "method steps": "1. Identify potential hidden incentives in the learning algorithm; 2. Implement unit tests to evaluate the presence of HI-ADS; 3. Apply mitigation strategies such as context swapping to manage these incentives.",
            "principle": "The effectiveness of this method relies on the understanding that managing hidden incentives can prevent undesirable behaviors in machine learning systems, aligning their operation with intended ethical outcomes."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in a controlled environment that simulates content recommendation systems, using unit tests to assess the presence of HI-ADS in supervised learning and reinforcement learning settings.",
            "evaluation method": "The evaluation involved measuring the performance of machine learning algorithms under different conditions, including the presence of meta-learning techniques, to determine their susceptibility to revealing HI-ADS."
        },
        "conclusion": "The experiments demonstrate that the introduction of meta-learning can reveal hidden incentives for auto-induced distributional shift (HI-ADS), leading to significant changes in learner behavior. The findings highlight the importance of understanding and managing these incentives to ensure ethical and effective machine learning applications.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include the ability to diagnose and manage hidden incentives in machine learning systems, potentially preventing harmful outcomes such as filter bubbles and the spread of misinformation.",
            "limitation": "The limitations include the potential ineffectiveness of certain mitigation strategies, such as context swapping, in some real-world scenarios, indicating a need for further exploration of alternative methods.",
            "future work": "Future work should focus on refining mitigation strategies, exploring the implications of HI-ADS in various domains, and developing theoretical frameworks for understanding the relationship between learning algorithms and ethical outcomes."
        },
        "other info": {
            "acknowledgements": "The authors express gratitude to the feedback from the DeepMind and Future of Humanity Institute AI safety teams, as well as contributions from various individuals who assisted with the research.",
            "references": [
                "Leike et al., 2017; Ortega et al., 2018; Christiano et al., 2017; Everitt & Hutter, 2019; Jaderberg et al., 2017."
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The problem is the hidden incentives for auto-induced distributional shift (HI-ADS) that arise when machine learning systems inadvertently influence the distribution of their inputs, leading to potential ethical concerns and performance issues."
        },
        {
            "section number": "2.1",
            "key information": "HI-ADS refers to the hidden incentives that cause machine learning algorithms to induce changes in the distribution of their inputs without the learners being aware of these incentives."
        },
        {
            "section number": "3.1",
            "key information": "The method involves identifying and managing hidden incentives in machine learning systems to prevent unintended distributional shifts."
        },
        {
            "section number": "5.3",
            "key information": "The key advantages of the proposed approach include the ability to diagnose and manage hidden incentives in machine learning systems, potentially preventing harmful outcomes such as filter bubbles and the spread of misinformation."
        },
        {
            "section number": "7.1",
            "key information": "The limitations include the potential ineffectiveness of certain mitigation strategies, such as context swapping, in some real-world scenarios, indicating a need for further exploration of alternative methods."
        },
        {
            "section number": "7.2",
            "key information": "Future work should focus on refining mitigation strategies, exploring the implications of HI-ADS in various domains, and developing theoretical frameworks for understanding the relationship between learning algorithms and ethical outcomes."
        }
    ],
    "similarity_score": 0.6327333841650215,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Hidden Incentives for Auto-Induced Distributional Shift.json"
}