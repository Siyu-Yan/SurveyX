{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2206.08111",
    "title": "On Private Online Convex Optimization: Optimal Algorithms in $\\ell_p$-Geometry and High Dimensional Contextual Bandits",
    "abstract": "Differentially private (DP) stochastic convex optimization (SCO) is ubiquitous in trustworthy machine learning algorithm design. This paper studies the DP-SCO problem with streaming data sampled from a distribution and arrives sequentially. We also consider the continual release model where parameters related to private information are updated and released upon each new data, often known as the online algorithms. Despite that numerous algorithms have been developed to achieve the optimal excess risks in different $\\ell_p$ norm geometries, yet none of the existing ones can be adapted to the streaming and continual release setting. To address such a challenge as the online convex optimization with privacy protection, we propose a private variant of online Frank-Wolfe algorithm with recursive gradients for variance reduction to update and reveal the parameters upon each data. Combined with the adaptive differential privacy analysis, our online algorithm achieves in linear time the optimal excess risk when $1<p\\leq 2$ and the state-of-the-art excess risk meeting the non-private lower ones when $2<p\\leq\\infty$. Our algorithm can also be extended to the case $p=1$ to achieve nearly dimension-independent excess risk. While previous variance reduction results on recursive gradient have theoretical guarantee only in the independent and identically distributed sample setting, we establish such a guarantee in a non-stationary setting. To demonstrate the virtues of our method, we design the first DP algorithm for high-dimensional generalized linear bandits with logarithmic regret. Comparative experiments with a variety of DP-SCO and DP-Bandit algorithms exhibit the efficacy and utility of the proposed algorithms.",
    "bib_name": "han2022privateonlineconvexoptimization",
    "md_text": "# On Private Online Convex Optimization: Optimal Algorithms in \u2113p-Geometry and High Dimensional Contextual Bandits\nYuxuan Han*\u2020, Zhicong Liang*\u2020, Zhipeng Liang*\u2021, Yang Wang\u2020\u2021, Yuan Yao\u2020\ufffd, and Jiheng Zhang\u2020\u2021\ufffd\n\u2020Department of Mathematics \u2021Department of Industrial Engineering and Decision Analytics The Hong Kong University of Science and Technology \ufffdEmails: yuany@ust.hk, jiheng@ust.hk\nAbstract\nDifferentially private (DP) stochastic convex optimization (SCO) is ubiquitous in trustworthy machine learning algorithm design. This paper studies the DP-SCO problem with streaming data sampled from a distribution and arrives sequentially. We also consider the continual release model where parameters related to private information are updated and released upon each new data, often known as the online algorithms. Despite that numerous algorithms have been developed to achieve the optimal excess risks in different \u2113p norm geometries, yet none of the existing ones can be adapted to the streaming and continual release setting. To address such a challenge as the online convex optimization with privacy protection, we propose a private variant of online Frank-Wolfe algorithm with recursive gradients for variance reduction to update and reveal the parameters upon each data. Combined with the adaptive differential privacy analysis, our online algorithm achieves in linear time the optimal excess risk when 1 < p \u22642 and the state-of-the-art excess risk meeting the non-private lower ones when 2 < p \u2264\u221e. Our algorithm can also be extended to the case p = 1 to achieve nearly dimension-independent excess risk. While previous variance reduction results on recursive gradient have theoretical guarantee only in the independent and identically distributed sample setting, we establish such a guarantee in a non-stationary setting. To demonstrate the virtues of our method, we design the first DP algorithm for high-dimensional generalized linear bandits with logarithmic regret. Comparative experiments with a variety of DP-SCO and DP-Bandit algorithms exhibit the efficacy and utility of the proposed algorithms.\nKey words and phrases: Differential privacy, Online Convex Optimization, Stochastic Conve Optimization, High Dimensional Contextual Bandits\n# Contents\n# 1 Introduction\n# 3 DP Online Frank-Wolfe Algorithms\n3.1 \u2113p-setup for 1 < p \u2264\u221e. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 3.2 \u2113p-setup for p = 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 3.3 Conversion from Excess Risk to Regret Bounds . . . . . . . . . . . . . . . . . . . 15\n4.1 Introduction to Generalized Linear Bandit Problem . . . . . . . . . . . . . . . . . 17 4.2 Private High Dimensional Bandit Algorithm . . . . . . . . . . . . . . . . . . . . . 18\n# 5 Experiments\n5.1 Generation of Generalized Gaussian Noise . . . . . . . . . . . . . . . . . . . . . . 21 5.2 Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 5.3 Comparison with DP-SCO Algorithms . . . . . . . . . . . . . . . . . . . . . . . . 22 5.4 Comparison with DP-Bandit Algorithms . . . . . . . . . . . . . . . . . . . . . . . 22\n# 6 Conclusions\n# A Proofs of Section 3\nA.1 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 A.2 Proof of Lemma 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 A.3 Proof of Proposition 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 A.4 Proof of Theorem 3.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 A.5 Proof of Theorem 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 A.6 Proof of Theorem 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 A.7 Proof of Theorem 3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 A.8 Proof of Theorem 3.6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n# B Proofs of Section 4\nB.2 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Proof of Theorem 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n# References\n# 1 Introduction\nStochastic convex optimization (SCO) is a fundamental problem widely studied in machine learning, statistics, and operations research. The goal of SCO is to minimize a population loss function FP (\u03b8) = Ex\u223cP [f(\u03b8, x)] over a d-dimensional support set C, with only access to the independent and identically distributed (i.i.d.) or exchangeable samples {xt}n t=1 from some distribution P. The performance of an algorithm is often measured in terms of the excess population risk of its solution \u03b8, i.e., FP (\u03b8) \u2212minv\u2208C FP (v). In practice, samples related to users\u2019 profiles might contain sensitive information; thus, it is important to solve stochastic convex optimization problems with differential privacy guarantees (DP-SCO) [Bassily et al., 2014, 2019, 2021a]. In this paper, we consider the DP-SCO with streaming data, where samples arrive sequentially and cannot be stored in memory for long, often known as online algorithms in literature. Streaming data has been studied in the context of online learning [Smale and Yao, 2006, Yao, 2010, Tarres and Yao, 2014], online statistical inference [Vovk, 2001, 2009, Steinhardt et al., 2014, Fang et al., 2018], and online optimization [Zinkevich, 2003, Cesa-Bianchi and Lugosi, 2006, Hazan, 2019, Hoi et al., 2021]. Moreover, data release is concerned due to privacy requirements. Our online method can also accommodate continual release [Jain et al., 2021, Dwork et al., 2010, Chan et al., 2011], i.e., receiving sensitive data as a stream of input and releases an output of it immediately after processing while satisfying differential privacy requirements. Such a private online algorithm can be formulated as a recursive update process \u03b8t = \u0398t(\u03b8t\u22121, xt, \u03b5t) where \u03b5t encodes the differential privacy noise and \u0398t is the update mapping, e.g. the online Frank-Wolfe algorithm considered in this paper. A closely related setting considered as an extension in this paper is so-called online decision making [Slivkins, 2019, Lattimore and Szepesv\u00b4ari, 2020], where a decision needs to be made at each time, and the performance is measured in terms of accumulative regret, the gap between actual reward and the best possible reward, over the time. Recent work starts introducing streaming algorithms in (private) SCO into the solution of the online decision-making [Ding et al., 2021, Han et al., 2021] to enjoy high computational efficiency and flexibility to handle different reward structures. In particular, Han et al. [2021] propose to solve private contextual bandits with stochastic gradient descent (SGD). However, the extension of other streaming algorithms, including the Frank-Wolfe and the stochastic mirror descent, remains elusive in this setting. Compared with non-private SCO, private SCO has an inherent dependence on the dimension d [Agarwal et al., 2012, Bassily et al., 2021b]. Therefore, in DP-SCO, the optimal convergence rate also has a crucial dependence on the space metric. Remarkable progress has been made in achieving optimal rate in \u2113p norm with various 1 \u2264p \u2264\u221eas shown in Table 1.1. However, no existing rate-optimal DP-SCO algorithms can be adopted in the streaming and continual release\nTable 1.1: Bounds for excess population risk of (\u03b5, \u03b4)-DP-SCO. \u2020 denotes bounds in expectation while \u2021 denotes bounds with high probability. And \u2217denotes bounds without smoothness assumption. Here \u03ba = min{ 1 p\u22121, 2 log d}. Most of the DP-SCO algorithms require large batch size and thus fail to be adopted in streaming data, except for [Feldman et al., 2020]. However their algorithm can only release the last variable for privacy protection and thus contradict to the requirements of continual release.\nLoss\n\u2113p\nTheorem\nGradient Queries\nRate\nBatch Size\nConvex\np = 2\nThm. 3.2 [Bassily et al., 2019]\nO(min{n3/2, n5/2\nd })\nO(\n\ufffd\n1\nn +\n\u221a\nd\n\u03f5n )\u2020\nO(\u221a\u03b5n)\nThm. 3.5 [Feldman et al., 2020]\nO(min{n, n2\nd })\nO(\n\ufffd\n1\nn +\n\u221a\nd\n\u03f5n )\u2020\nO(\n\u221a\nd\n\u03b5 )\np = 1\nThm. 7 [Asi et al., 2021]\nO(n)\n\u02dcO(\n\ufffd\nlog d\nn\n+ ( log d\n\u03b5n )2/3)\u2020\nO(\nn\nlog2 n)\nThm. 3.2 [Bassily et al., 2021b]\nO(n)\n\u02dcO( log d\n\u03b5\u221an)\u2020\nO( n\n2 )\nTheorem 3.5\nO(n)\n\u02dcO( log d\n\u03b5\u221an)\u2021\n1\n1 < p < 2\nThm. 5.4 [Bassily et al., 2021b]\nO(n)\n\u02dcO( \u03ba\n\u221an + \u03ba\n\u221a\nd\n\u03b5n3/4 )\u2020\nO( n\n2 )\n1 < p \u22642\nThm. 13 [Asi et al., 2021]\nO(n3/2)\n\u02dcO(\n1\n\u221a\n(p\u22121)n +\n\u221a\nd\n(p\u22121)n\u03b5)\u2020\u2217\nO( n\n2 )\nTheorem 3.2\nO(n)\n\u02dcO(\ufffd\u03ba\nn +\n\u221a\n\u03bad\nn\u03b5 )\u2021\n1\n2 < p \u2264\u221e\nProp. 6.1 [Bassily et al., 2021b]\nO(n2)\n\u02dcO( d1/2\u22121/p\n\u221an\n+ d1\u22121/p\n\u03b5n\n)\u2020\u2217\nO(n)\nTheorem 3.2\nO(n)\n\u02dcO( d1/2\u22121/p\n\u221an\n+ d1\u22121/p\n\u03b5n\n)\u2021\n1\nStrongly Convex\np = 1\nThm. 9 [Asi et al., 2021]\nO(n)\n\u02dcO( log d\nn\n+ ( log d\n\u03b5n )4/3)\u2020\nO(\nn\n2 log n)\nTheorem 3.6\nO(n)\n\u02dcO( log2 d\n\u03b52n )\u2021\n1\n1 < p \u22642\nTheorem 3.3\nO(n)\n\u02dcO( \u03ba\nn +\n\u03bad\n\u03b52n2 )\u2021\n1\n2 < p \u2264\u221e\nTheorem 3.3\nO(n)\n\u02dcO( d1\u22122/p\nn\n+ d2\u22122/p\n\u03b52n2 )\u2021\n1\nsetting since they rely on either Frank-Wolfe or mirror descent with batched-gradient estimator. Algorithms relying on Frank-Wolfe require a batch size of \u02dc\u2126(n) [Bassily et al., 2021b, Asi et al., 2021] for variance reduction, which is unacceptable in the streaming setting. Algorithms based on mirror descent require the same batch size and need a superlinear number of gradient query of \u02dc\u2126(n3/2) [Asi et al., 2021].\n# 1.1 Our Contributions\nNote that in the online setting, the total time step T equals the sample size n. So we will use n instead of T for the total number of iterations. Excess population risk bounds denoted by t hold for every time step t \u2208[n], while those denoted by n only hold after \u2126(n) time steps.\nCase of 1 < p \u22642. We present a systematic study on a differentially private online Frank-Wolfe algorithm with recursive gradient in various \u2113p geometries, which is rate-optimal for 1 < p \u22642. Our algorithm is based on the observation that the non-private recursive variance reduction scheme used in Xie et al. [2020] can be written as a normalized incremental summation of gradients. According to this observation, we can apply the tree-based mechanism in Guha Thakurta and\nSmith [2013], and utilize an adaptive argument to show that our noise accumulates logarithmically as the total number of iteration grows, comparing with the polynomial grow rate in Bassily et al. [2021b] and Asi et al. [2021]. In this case, our algorithm can fit in the online setting where a large number of updates is required. Such an analysis leads to a variance reduced gradient error bound of \u02dcO( 1 \u221a t + \u221a d t\u03b5 ) with high probability The recursive gradient method we used here is closely related to Bassily et al. [2021b], while their algorithm uses n 2 samples for variance reduction, and their gradient error is of the order \u02dcO( 1 \u221an + \u221a d \u03b5n3/4 ) in the worst-case. Our improvement on the variance reduction reduces their \u02dcO( 1 \u221an + \u221a d n3/4\u03b5) excess risk to \u02dcO( 1 \u221a t + \u221a d t\u03b5 ), which is optimal up to a logarithmic factor. Asi et al. [2021] achieves the optimal rates in terms of n and d at the cost of O(n3/2) gradient queries while we achieve the same rate with only O(n) gradient queries. Moreover, their rate will explode to +\u221ewhen p approaches 1, while our dependency on p is upper bounded by log d. One thing we need to mention here is that, Theorem 13 of Asi et al. [2021] achieves the optimal rate without smoothness assumption, as mentioned in Table 1.1. As shown in [Bassily et al., 2021b] and [Asi et al., 2021], smooth and non-smooth settings of DP-SCO share the same optimal rate of excess risk for 1 < p < 2. The benefit of smoothness mainly lies in the complexity of gradient query. Smoothness enables us to use variance reduction to achieve linear gradient query time, while the complexity of [Asi et al., 2021] is supper-linear. Case of 2 < p \u2264\u221e. The analysis above can be generalized to the case of 2 < p \u2264\u221e. We achieve a \u02dcO \ufffd d1/2\u22121/p\u221a T +d1\u22121/p/\u03b5 \ufffd regret bound and a convergence rate of \u02dcO \ufffdd1/2\u22121/p \u221a t + d1\u22121/p t\u03b5 \ufffd , which matches the non-private lower bound \u2126 \ufffdd1/2\u22121/p \u221an \ufffd in non-private SCO and is thus optimal when d = \u02dcO(n\u03b52). Previously, Bassily et al. [2021b] achieve the same convergence rate by reducing their 2 < p \u2264\u221ecase to p = 2 by bounding the \u21132 diameter and Lipschitz constant for the \u2113p-setup. Case of p = 1. The challenge of this case is that the tree-based mechanism is no longer applicable to achieve a logarithmic dependence on d because the tree-based method will lead to an O( \u221a d) factor. To overcome the difficulty, we combine the analysis of adaptive composition and Report Noisy Max mechanism [Dwork et al., 2014] to show that the noise with variance O( log d t\u03b5 ) is enough to protect the privacy. Such a result then leads to O( \ufffd log d t + log d \u221a t\u03b5 ) convergence rate. Comparing with the rate-optimal DP-SCO algorithm with excess risk O \ufffd\ufffd log d n + \ufffdlog d n\u03b5 \ufffd2/3\ufffd proposed in Asi et al. [2021], ours SCO result is sub-optimal. The gap is not due to our technique of the variance reduction analysis but the difficulty of the online setting. The optimal rates in Asi et al. [2021] rely on the privacy amplification via shuffling the dataset. However, accessing all information at beginning is impossible in the online setting. Privacy-Preserving Online Decision Making. A salient feature of our algorithm is that we provide \u02dcO(1/t) convergence guarantee for each time step while previous works (e.g., [Asi et al., 2021, Feldman et al., 2020]) can only hold after observing \u02dc\u2126(n) samples. Such a convergence result is not of purely intellectual interest. Instead, it is one of the foundations for extending our\nalgorithm to the online decision-making setting. Despite the adaptivity of our algorithm to the streaming nature, it is highly non-trivial to extend the SCO guarantee to the online decision setting. The recursive gradient variance reduction method needs the stationary distribution assumption of coming data xt. In contrast, the distribution of collected sample xt depends on the decision before and at time t, and thus our previous SCO results would fail in this non-stationary setting. By carefully analyzing the structure of bandit problems, we establish a novel variance reduction guarantee that involves a total-variation term to describe the non-stationarity. Then we show that under suitable assumptions, such total-variation term decays at a favorable rate to ensure the desired estimation error guarantee. While our results can be generalized easily in the case of 1 < p \u2264\u221eand various reward structures, we consider the high-dimensional (where p = 1) online decision-making problem with generalized linear reward [Bastani and Bayati, 2020], which has received lots of recent attention, to illustrate the virtue of our method. While several remarkable progress has been made on the low-dimensional setting with DP guarantee, [Chen et al., 2020, Shariff and Sheffet, 2018], no existing work provides sub-linear regret bound in the high-dimensional setting with DP protection even for linear rewards. Instead, we provide the first logarithmic regret bound ( Theorems 4.3) based on our private online Frank-Wolfe based bandit algorithms. This paper is a journal extension of [Han et al., 2022] that reports the main theoretical results above. Our main extensions in this version are as follows.\n1. Complete proofs of all the theoretical statements are provided in details, with furthe discussions on related literature.\n3. We provide an algorithm to generate the generalized Gaussian noise based on Lemma 3.2 in Han et al. [2022] (Lemma 3.1 in this paper), which will be used by NosiySFW and our algorithms in experiment.\nRecently we also noticed that a new arXiv preprint [Bassily et al., 2022] widely extended their previous results in [Bassily et al., 2021b] in the following three aspects. (a) In 1 < p \u22642 regime, they combined the binary-tree based variance reduction technique in [Asi et al., 2021] with Frank-Wolfe based algorithm to improve their previous \u02dcO( 1 \u221an + \u221a d \u03b5n3/4 ) risk bounds and achieve the same optimal excess risk in linear time as ours; (b) In 2 < p \u2264\u221eregime, they replace the multi-pass SGD in [Bassily et al., 2020] by phased SGD in [Feldman et al., 2020] to achieve the same risk as ours and [Bassily et al., 2021b] in linear time. They also explore the concentration property of generalized Gaussian distribution via developing similar results as our Lemma 3.1 and improved the in-expectation risk bound in [Bassily et al., 2021b] to high probability bounds.\nTable 1.2: Regret bounds (defined in (3.7)) of (\u03b5, \u03b4)-DP algorithms. \u2020 denotes bounds in expectation while \u2021 denotes bounds with high probability. And \u2217denotes bounds without\nTable 1.2: Regret bounds (defined in (3.7)) of (\u03b5, \u03b4)-DP algorithms. \u2020 denotes bounds in expectation while \u2021 denotes bounds with high probability. And \u2217denotes bounds without smoothness assumption.\nLoss\nType\n\u2113p\nTheorem\nRegret\nLinear\nAdversarial\np = 2\nCor. 3.2 [Agarwal and Singh, 2017]\nO(\n\u221a\nT + d/\u03b5)\u2020\nConvex\nAdversarial\np = 2\nThm. 2 [Jain et al., 2012]\nO(\n\u221a\ndT 2/3/\u03b5)\u2020\u2217\nThm. 11 [Guha Thakurta and Smith, 2013]\nO(\n\u221a\ndT/\u03b5)\u2020\u2217\nStochastic\n1 < p \u22642\nThm. 3.2\nO(\n\u221a\nT +\n\u221a\nd/\u03b5)\u2021\nA crucial difference between our results and theirs lies in that, while our algorithms are adapted to the online setting, the algorithms in [Bassily et al., 2022] for 1 < p \u22642 and 2 < p \u2264\u221e need the same batch size as Theorem 7 in [Asi et al., 2021] and Theorem 3.5 in [Feldman et al., 2020], respectively. Thus their algorithms cannot be applied to online setting with streaming data and the continual release.\n# 1.2 Other Related Work\nOur paper is most related to the DP-SCO community. In addition, there are two streams of literature that are related to ours: online convex optimization with differential privacy and DP bandits. Below we present a review on them. Online Convex Optimization and Privacy Preserving: Online convex optimization (OCO) algorithms [Zinkevich, 2003], learning from a stream of data samples and releasing an output upon new data, provide some of the most successful solutions for many machine learning problems, both in terms of the speed of optimization and the ability of generalization [Hazan, 2019]. Similar to the streaming setting, developing OCO algorithms under DP constraint is harder than the DP guarantee in the offline learning setting since the whole sequence of outputs along the time horizon is required to be protected [Jain et al., 2012, Guha Thakurta and Smith, 2013, Agarwal and Singh, 2017]. Jain et al. [2012] provide a generic framework to convert proper online convex programming algorithm into a private one while maintaining \u02dcO( \u221a dT/\u03b5) regret for Lipshitz-bounded strongly convex functions and \u02dcO( \u221a dT 2/3/\u03b5) for general Lipshitz convex functions. Guha Thakurta and Smith [2013] propose algorithms with \u02dcO( \u221a dT/\u03b5) regret bound for Lipschtiz convex functions. In contrast to the DP SCO works, all above bounds paid a price of privacy factor in the leading order term. The only existing work with privacy-free regret bounds \u02dcO( \u221a Td + 1/\u03b5) is given by Agarwal and Singh [2017] for linear losses, while their results and arguments cannot be generalized to more general convex losses. Our results contribute to the literature by showing privacy-for-free bounds \u02dcO( \u221a T + 1/\u03b5) are also available for general convex functions under stochastic setting. We provide a summary about the comparison with them in Table 1.2 and the derivation is in Section 3.3. DP-SCO in \u2113p Geometry: In the case of p = 2, Bassily et al. [2014] give the first excess population risk of \u02dcO( d1/4 \u221an\u03b5) by adding a strongly convex regularizer to control the gap between\nexcess population risk and empirical risk. Bassily et al. [2019] further show that with min-batch and multi-pass SGD, the optimal rate \u02dcO( \ufffd 1 n + \u221a d n\u03b5 ) is achievable. And they further relax the smoothness assumption by applying the smoothing technique based on Moreau-Yosida envelope operator. Bassily et al. [2021a] consider non-smooth DP-SCO with generalized linear losses (GLL). In p = 2, their algorithm achieves optimal excess risk in O(n log n) time. In p = 1, they bypass the lower bound in non-smoothing setting given by Asi et al. [2021] and achieve the optimal risk in non-private case when \u03b5 = \u0398(1). Wang et al. [2020] consider the heavy-tailed data where the Lipschitz condition of the loss function no longer holds and the the gradient can be unbounded. They achieved excess population risk of \u02dcO( d n1/3\u03b52/3 ) given that each coordinate of the gradient has bounded second-order moment. Hu et al. [2021] further extend their results to high dimensional space. And Kamath et al. [2022] improve the rates in [Wang et al., 2020] and extends to their results are applicable to bounded moment conditions of all orders. DP-Bandits: Designing bandits algorithm with DP guarantee is an emerging topic in the recent years and we only mention the work which utilize the side-information (context). Shariff and Sheffet [2018] propose the notion of joint differential privacy (JDP) under which bandits algorithm can achieve nontrivial regret and then they design a scheme to convert the classic linear-UCB algorithm into a joint differential private counterpart to match the non-private regret bound. Dubey and Pentland [2020] extend Shariff and Sheffet [2018] algorithms to the federated learning setting. Chen et al. [2021b] tackle private dynamic pricing problem under generalized linear demand model by combining the tree-based mechanism, differentially private empirical risk minimization and UCB algorithm and obtain both excellent DP and performance guarantee for oblivious adversarial and stochastic settings. Chen et al. [2021a] develop two algorithms which make pricing decisions and learn the unknown non-parametric demand on the fly, while satisfying the DP and LDP gurantees respectively.\n# 2 Preliminaries\nNotations. Let (E, \u2225\u00b7\u2225) be a normed space of dimension d, and C \u2286E is a compact convex set of diameter D. Let \u27e8\u00b7\u27e9be an arbitrary inner product over E (not necessarily inducing the norm \u2225\u00b7\u2225). The dual norm over E is defined as \u2225y\u2225\u2217:= max\u2225x\u2225\u22641\u27e8x, y\u27e9. With this definition, (E, \u2225\u00b7\u2225\u2217) is also a d-dimensional normed space. We use [K] to denote {1, 2, \u00b7 \u00b7 \u00b7 , K} and for any Z \u2208Rd we denote Z1:t = {Z1, Z2, \u00b7 \u00b7 \u00b7 , Zt}. We denote 0 as an all-zero matrix whose size is adjusted according to the context. We adopt the standard asymptotic notations. For two non-negative sequences {an} and {bn}, we denote {an} = O({bn}) or {an} \u2272{bn} iff lim supn\u2192\u221ean/bn < \u221e, an = \u2126(bn) iff bn = O(an), and an = \u0398(bn) iff an = O(bn) and bn = O(an). We also use \u02dcO(\u00b7), \u02dc\u2126(\u00b7) and \u02dc\u0398(\u00b7) to denote the respective meanings within multiplicative logarithmic factors in n and \u03b4.\n# 2.1 SCO with Streaming Data\nWe formally introduce the excess risk below. Given a parameter set C \u2282Rd, and an unknown distribution P over X and a function f : C \u00d7 X \u2192R, we consider the following optimization\n# min \u03b8\u2208C FP (\u03b8) := Ex\u223cP [f(\u03b8, x)],\nWe will abbreviate FP as F when the context is clear for simplicity. In practice, the population loss F(\u00b7) is unknown and one can only access it via empirical approximation from a set of i.i.d. samples {xi}n i=1. In the literature, the study of such SCO problems focuses on designing efficient algorithms to find a parameter \u03b8 over the samples {xi}n i=1 such that the excess population risk is acceptable. In this work, we consider SCO under streaming and continual release setting. In each time step t, one sample xt \u223cP arrives, and our algorithm needs to output a parameter \u03b8t with convergence guarantee regarding F. We list the following standard assumptions under a general norm \u2225\u00b7\u2225and its dual \u2225\u00b7\u2225\u2217for future reference. Assumption 2.1 (Strongly-convex). For any \u03b81, \u03b82 \u2208C, the population loss F is said to be \u00b5-strongly convex if F(\u03b81) \u2265F(\u03b82) + \u27e8\u2207F(\u03b82), \u03b81 \u2212\u03b82\u27e9+ \u00b5 2 \u2225\u03b81 \u2212\u03b82\u22252 for some \u00b5 \u22650. Assumption 2.2 (Smoothness). For any \u03b81, \u03b82 \u2208C and x \u2208X, the loss function f is saied to be \u03b2-smooth if \u2225\u2207f(\u03b81, x) \u2212\u2207f(\u03b82, x)\u2225\u2217\u2264\u03b2\u2225\u03b81 \u2212\u03b82\u2225. Assumption 2.3. For any \u03b8 \u2208C and x \u2208X, the loss function f satisfies: \u2225\u2207f(\u03b8, x)\u2212\u2207F(\u03b8)\u2225\u2217\u2264 G. Assumption 2.4 (Lipschitz). For any \u03b8 \u2208C and x \u2208X, the loss function f satisfies: \u2225\u2207f(\u03b8, x)\u2225\u2217\u2264L.\n# 2.2 Differential Privacy\nOur work also extends to the privacy-preserving setting, where the sequence (\u03b81, . . . , \u03b8n) satisfies the differential privacy constraint (see Definition 2.1) with respect to the data. Here we recall the definition of (\u03b5, \u03b4)-differential privacy.\nOur work also extends to the privacy-preserving setting, where the sequence (\u03b81, . . . , \u03b8n) satisfies the differential privacy constraint (see Definition 2.1) with respect to the data. Here we recall the definition of (\u03b5, \u03b4)-differential privacy. Definition 2.1 (Differential Privacy [Dwork et al., 2014], (\u03b5, \u03b4)-DP). A randomized algorithm A is said to be (\u03b5, \u03b4) differentially private if for any pair of datasets D and D\u2032 differing in one entry and any event E in the range of A it holds that P[A(D) \u2208E] \u2264e\u03b5P[A(D\u2032) \u2208E] + \u03b4. To design the DP-SCO algorithm under \u2113p norm with 1 < p \u22642, we recall the generalized Gaussian mechanism proposed in [Bassily et al., 2021b] that leverages the regularity of the dual normed space.\n# Definition 2.2 (Regular Normed Space). For a normed space (E, \u2225\u00b7\u2225), we say that the norm \u2225\u00b7\u2225is \u03ba-regular associated with \u2225\u00b7\u2225+, if there exists 1 \u2264\u03ba+ \u2264\u03ba so that \u2225\u00b7\u2225+ is \u03ba+-smooth and \u2225\u00b7\u2225and \u2225\u00b7\u2225+ are equivalent with constant \ufffd \u03ba/\u03ba+:\n\ufffd \u2225x\u22252 \u2264\u2225x\u22252 + \u2264\u03ba \u03ba+ \u2225x\u22252, \u2200x \u2208E.\n\u2113q norm for q \u22651 is a important class of regular norms, we specify the regularity constant \u03baq and the associated smooth norm \u2225\u00b7\u2225q,+ later in Lemma 3.2 and Lemma 3.3.\n\u2113q norm for q \u22651 is a important class of regular norms, we specify the regularity constant \u03baq and the associated smooth norm \u2225\u00b7\u2225q,+ later in Lemma 3.2 and Lemma 3.3. Lemma 2.1 (Generalized Gaussian Distribution and Mechanism [Bassily et al., 2021b]). Given a \u03ba-regular norm \u2225\u00b7\u2225associated with smooth norm \u2225\u00b7\u2225+ in d-dimensional space, and the generalized Gaussian distribution G\u2225\u00b7\u2225+(\u00b5, \u03c32) with density:\nLemma 2.1 (Generalized Gaussian Distribution and Mechanism [Bassily et al., 2021b]). Given a \u03ba-regular norm \u2225\u00b7\u2225associated with smooth norm \u2225\u00b7\u2225+ in d-dimensional space, and the generalized Gaussian distribution G\u2225\u00b7\u2225+(\u00b5, \u03c32) with density:\nwhere C(\u03c3, d) = \ufffd Area{\u2225x\u2225+ = 1}(2\u03c32)d/2 2 \u0393(d/2) \ufffd\u22121, and Area is the (d \u22121)-dimension surface measure on Rd, then for any function f with \u2225\u00b7\u2225sensitivity s > 0, we have that the mechanism output:\nis (\u03b5, \u03b4)-differentially private.\n# 3 DP Online Frank-Wolfe Algorithm\nIn this section, we present the DP online Frank-Wolfe algorithm framework in solving the \u2113 DP-SCO problem as well as the corresponding excess risk and the regret bounds.\n# 3.1 \u2113p-setup for 1 < p \u2264\u221e\nIn this section, we provide a unified design and analysis for optimization in \u2113p geometry with 1 < p \u2264\u221e. As a consequence of the H\u00a8older\u2019s inequality, the dual of \u2113p norm is \u2113q norm, where q satisfies 1 p + 1 q = 1, i.e., q := p p\u22121.\nAlgorithm 1 Private Tree-Based Online Frank-Wolfe (DP-TOFW).\n1: Input: privacy parameters (\u03b5, \u03b4), {\u03c1t}n\nt=1 = {\u03b7t}n\nt=1 =\n1\n1+t, p considered in \u2113p, and its dual\nnorm \u2225\u00b7\u2225q associated with regular norm \u2225\u00b7\u2225q,+, initial point \u03b80 = \u03b81 = 0 \u2208C\n2: for t = 1 to n do\n3:\nCompute and pass gt in Eq. (3.1) and \u03c3+(q, \u03b5, \u03b4) according to Theorem 3.1 into the\ntree-based mechanism (Algorithm 5).\n4:\nGet noisy summation \u02dcGt = noisy(\ufffdt\ni=1 gi) from the tree-based mechanism (Algorithm 5).\n5:\nSet dt =\n1\nt+1 \u00b7 \u02dcGt\n6:\nvt = arg minv\u2208C\u27e8dt, v\u27e9.\n7:\n\u03b8t+1 \u2190\u03b8t + \u03b7t(vt \u2212\u03b8t).\n8: end for\n# Our proposed algorithm is shown in Algorithm 1. At iteration t, we consider the following recursive gradient estimator dt [Xie et al., 2020] as an unbiased estimator of the population gradient \u2207F(\u03b8t):\ndt = \u2207f(\u03b8t, xt) + (1 \u2212\u03c1t)(dt\u22121 \u2212\u2207f(\u03b8t\u22121, xt)),\n \u2207 A similar recursive gradient scheme is also used in Bassily et al. [2021b] for 1 < p \u22642. However, their algorithms use additive noise to ensure the privacy of dt at each iteration, which accumulate linearly in t. To alleviate the influence of the noise induced by DP, they initialize d1 with the first n 2 samples and begin to take mini-batch updates with batch size \u221an 2 for \u221an iterations, which helps control the sensitivity of dt and maintain a lower number of noise accumulations. However, this strategy leads to a gradient estimation error of O( 1 n1/2 + \u221a d \u03b5n3/4 ). And it also fails in the streaming setting where only one sample is available in initialization. To improve the error rate and fit the streaming setting, our key observation is that the recursive gradient estimation dt can be represented as the following summation of empirical gradients,\n\ufffd \ufffd\ufffd \ufffd Now we reduce the problem of privately releasing dt in every step t to the problem of privately releasing the incremental summation of gi in Eq. (3.1), which motivates us to apply the tree-based mechanism in Guha Thakurta and Smith [2013]. In the tree-based mechanism, the leave nodes store the vectors gi. Each internal node stores a private version of the summation of all the leaves in its sub-tree. In this case, any partial summation over gi can be represented by at most \u2308log2 n\u2309nodes. This critical property ensures that the DP noise on dt would not accumulate linearly in t. In this case, our algorithm fits in the streaming setting, where a relatively large number of iterations is required. One difficulty of applying the tree-based mechanism is the sensitivity analysis. Suppose without loss of generality that for adjacent datasets D \u223cD\u2032, we have x1 \u0338= x\u2032 1. Such difference will affect the whole trajectory of the parameters: \u03b8i \u0338= \u03b8\u2032 i, \u2200i \u22652. In other words, the sensitivity will be very large. Fortunately, we can show that such sensitivity can be dramatically reduced by the adaptive analysis similar to Guha Thakurta and Smith [2013]. It turns out that noise with variance \u02dcO( \u03ba2 q t2\u03b52 ) is enough to maintain (\u03b5, \u03b4)-differential privacy guarantee when reporting the t-th recursive gradient over the whole time horizon. With the tree-based mechanism and the adaptive analysis mentioned above, we achieve a gradient error rate of \u02dcO(\ufffd\u03ba1 n + \u221a d\u03baq \u03b5t ) (see Proposition 3.1). Furthermore, to report private incremental summation \ufffdt i=1 gi for all t \u2208[n], the amount of space required by the tree-based mechanism is O(log2 n). Detailed description can be found in Algorithm 5 in the Appendix. In the following theorem, we characterize the privacy guarantee of Algorithm 1. The proof can be found in Section A.1.\n(3.1)\n\u03c32  = 8(\u2308log2 n\u2309+ 1)2\u03baq log((\u2308log2 n\u2309+ 1)/\u03b4)(\u03b2D + L)2 .\nExisting results only concern the excess population risk in expectation [Bassily et al., 2021b], thus the moment information of generalized Gaussian mechanism is enough for their derivation. While in our high-probability analysis, the tail behaviour of generalized Gaussian mechanism is characterized. Lemma 3.1 (Gamma Distribution). Assume that Z \u223cG\u2225\u00b7\u2225+(0, \u03c32 +) in d-dimensional space, then \u2225Z\u22252 + follows Gamma distribution \u0393(d/2, 2\u03c3+). Furthermore, \u2225Z\u22252 + \u2212E[\u2225Z\u22252 +] follows sub-Gamma(2\u03c34 +d, 2\u03c32 +), which implies that for any \u03bb > 0, we have\nExisting results only concern the excess population risk in expectation [Bassily et al., 2021b], thus the moment information of generalized Gaussian mechanism is enough for their derivation. While in our high-probability analysis, the tail behaviour of generalized Gaussian mechanism is characterized. Lemma 3.1 (Gamma Distribution). Assume that Z \u223cG\u2225\u00b7\u2225+(0, \u03c32 +) in d-dimensional space,\nP(\u2225Z\u22252 + > E[\u2225Z\u22252 +] + 2 \ufffd \u03c34 +d\u03bb + 2\u03c32 +\u03bb) \u2264exp(\u2212\u03bb).\nAs a result, we have the following high-probability variance reduction guarantee for the recursive gradient estimator. The proof can be found in Section A.2. Proposition 3.1. Under Assumption 2.2 and 2.3, with probability at least 1 \u2212\u03b1, for t \u2208[n], Algorithm 1 satisfies:\nOne known drawback of Frank-Wolfe is that its convergence rate is slow when the solution lies at the boundary, and it cannot be improved in general even the objection function is strongly convex [Lacoste-Julien and Jaggi, 2015, Garber and Hazan, 2015]. In this case, additional assumption is necessary to improve the convergence rate of Frank-Wolfe in the strongly convex setting. In the following, we introduce a geometric assumption, which is typical for Frank-Wolfe in the strongly convex setting, even for the non-private case [Gu\u00b4elat and Marcotte, 1986, Lafond et al., 2015]. Denoted by \u2202C the boundary set of C. Assumption 3.1. [Lafond et al., 2015] There is a minimizer \u03b8\u2217of F that lies in the interior of C, i.e., \u03b3 := inf \u2225v \u2212\u03b8\u2217\u2225> 0.\n(3.2)\nTheorem 3.3 (Convergence Guarantee for Strong Convexity). Consider Algorithm 1 with Assumptions 2.1, to 2.4 and 3.1, for 1 < p \u2264\u221eand t \u2208[n], we have with probability at least\nDiscussions about \u2113p-setup for 1 < p \u22642\nThe bound in equation (3.3) is optimal, up to a logarithmic factor, comparing with the \u2126( 1 \u221a t + \u221a d t\u03b5 ) lower bound shown in Bassily et al. [2021b] in the case of 1 < p \u22642. In strongly convex case, equation (3.4) is tight comparing with the \u2126(1 t + d t2\u03b52 ) lower bound shown in Bassily et al. [2014] in the case of p = 2. And we conjecture that such bound is also tight for general 1 < p \u22642, developing the corresponding lower bound is leaved in the future work.\nDespite noticing that regularity constant of \u2113q norm has a worse dependence on d, we can still get a satisfactory convergence rate by plugging the constants in Lemma 3.3 to Theorem 3.2 and Theorem 3.3: Excess-Risk: Convex: F(\u03b8t) \u2212F(\u03b8\u2217) \u2272d1/2\u22121/p \ufffd log(n/\u03b1) t + d1\u22121/p log(log(n)/\u03b4) t\u03b5 . (3.5) Strongly Convex: F(\u03b8t) \u2212F(\u03b8\u2217) \u2272d1\u22122/p log(n/\u03b1) t + d2\u22122/p log2(log(n)/\u03b4) t2\u03b52 . (3.6)\nDespite noticing that regularity constant of \u2113q norm has a worse dependence on d, we c still get a satisfactory convergence rate by plugging the constants in Lemma 3.3 to Theorem  and Theorem 3.3: Excess-Risk:\n(3.3) (3.4)\n(3.6)\nComparing with the optimal non-private lower bound \u2126(d1/2\u22121/p \u221an ) [Agarwal et al., 2012] in convex setting when 2 < p \u2264\u221e, our result (3.5) nearly matches the optimal non-private rate and is optimal when d = \u02dcO(n\u03b52). The same private-SCO rate is also attained by Bassily et al. [2021b] using the the multi-pass noisy SGD in Bassily et al. [2020] for \u21132-setup. While the multi-pass SGD has super-linear complexity. Remark 3.3. one may ask whether there exists other linear-time algorithm achieve the same rate as ours in smooth setting. The answer is \u2018YES\u2019: One may replace the multi-pass SGD by the snow-ball SGD[Feldman et al., 2020], which achieves optimal rate under \u21132 setting in linear time: Lemma 3.4. Under the same assumption as in Theorem 3.2 when p > 2 and assume moreover \u03b2 \u2272 n\u03b5 d1\u22121/p , the last iteration output of Algorithm 2 in [Feldman et al., 2020] satisfies\nComparing with the optimal non-private lower bound \u2126(d1/2\u22121/p \u221an ) [Agarwal et al., 2012] in convex setting when 2 < p \u2264\u221e, our result (3.5) nearly matches the optimal non-private rate and is optimal when d = \u02dcO(n\u03b52). The same private-SCO rate is also attained by Bassily et al. [2021b] using the the multi-pass noisy SGD in Bassily et al. [2020] for \u21132-setup. While the multi-pass SGD has super-linear complexity. Remark 3.3. one may ask whether there exists other linear-time algorithm achieve the same\nAs stated above, to achieve the same optimal bound when p > 2, the snow-ball SGD need more that the smoothness constant \u03b2 \u2272 n\u03b5 d1\u22121/p , while ours result make no additional assumption on \u03b2.\n# 3.2 \u2113p-setup for p = 1\nAlgorithm 2 Private Polyhedral Online Frank-Wolfe (DP-POFW)\n1: Input: praivacy parameters (\u03b5, \u03b4), {\u03c1t}n\nt=1 = {\u03b7t}n\nt=1 =\n1\n1+t, and initial point \u03b80 = \u03b81 = 0 \u2208\nC.\n2: for t = 1 to n do\n3:\nif t=1 then\n4:\ndt = \u2207f(\u03b8t, xt).\n5:\nelse\n6:\ndt = \u2207f(\u03b8t, xt) + (1 \u2212\u03c1t)(dt\u22121 \u2212\u2207f(\u03b8t\u22121, xt)).\n7:\nend if\n8:\n\u2200v \u2208C, sample nt\nv \u223cLap\n\ufffd4D(\u03b2D+L)\u221a\nlog n\u00b7log(1/\u03b4)\n\u03b5\n\u221a\nt\n\ufffd\n.\n9:\nvt = arg minv\u2208C(\u27e8dt, v\u27e9+ nt\nv).\n10:\n\u03b8t+1 \u2190\u03b8t + \u03b7t(vt \u2212\u03b8t).\n11: end for\nIn this section, we consider the \u2113p-setup for p = 1. In Algorithm 2, we combine the analysis of the adaptive composition, and the Report Noisy Max mechanism [Dwork et al., 2014] to ensure differential privacy, which reduces the O( \u221a d) factor in the excess population risk incurred by the tree-based mechanism in Section 3.1. In the following, we characterize the privacy guarantee of Algorithm 2. The proof can be found in Section A.6. Theorem 3.4 (Privacy Guarantee). Algorithm 2 is (\u03b5, \u03b4)-differentially private.\nTheorem 3.5 (Convergence Guarantee for General Convexity). Consider Algorithm 2 with convex function F, Assumptions 2.2-2.4 and 3.1, for t \u2208[n], we have with probability at least 1 \u2212\u03b1, F(\u03b8t) \u2212F(\u03b8\u2217) \u2264 3 \u221at + 1(\u03b2D2 + A),\nF(\u03b8t) \u2212F(\u03b8\u2217) \u2264 3 \u221at + 1(\u03b2D2 + A),\nA = 8D(\u03b2D + G) \ufffd log(8dn/\u03b1) + 16D(\u03b2D + L) log(4dn/\u03b1) \ufffd log n \u00b7 log(1/\u03b4) \u03b5 .\nThe gradient error in our algorithm (see Lemma A.2) is of the same rate O( 1 n) as the one in Asi et al. [2021]. Comparing with their excess population risk of \u02dcO( \ufffd log d n + ( log d n\u03b5 )2/3), our bound achieves the rate of \u02dcO( \ufffd log d t + log d \u221a t\u03b5 ). However, the analysis in Asi et al. [2021] relies on the privacy amplification via shuffling the dataset, which is unacceptable in streaming setting. The proof of the above theorem can be found in Section A.7. Theorem 3.6 (Convergence Guarantee for Strong Convexity). Consider Algorithm 2 with Assumptions 2.1-2.4 and 3.1, for t \u2208[n], we have with probability at least 1 \u2212\u03b1,\nwhere A is defined in Theorem 3.5.\nThe above theorem achieves a rate of \u02dcO( log d t + log2 d t\u03b5 ) comparing with the rate of \u02dcO( log d n + ( log d n\u03b5 )4/3) in Asi et al. [2021], which relies on the privacy amplification via shuffling the dataset as we mentioned in the comment under Theorem 3.5. The proof of this theorem can be found in Section A.7.\nRemark 3.4. Our results can be generalized to the case that the population loss is strongly convex. Although it is appealing to use a folklore reduction from convex setting to strongly convex setting as in Asi et al. [2021] and Feldman et al. [2020] to attain the same \u02dcO( 1 n) convergence rate, the reduction relies on the batch splitting. Specifically, a batch size of the order O(n/ log n) is required. However, in practice, the ground-truth time horizon n\u2217can hardly be known in advance. Thus, one may need to overestimate the time horizon to ensure sufficient privacy protection. Once the estimated time horizon n \u2273n\u2217/ log n\u2217, the batch-based method will fail, and the last iteration only has the same guarantee as in the convex setting.\n# 3.3 Conversion from Excess Risk to Regret Bounds\nDP online convex optimization considers the learning algorithms design with continual release feature and privacy guarantee and thus is comparable with our algorithms. We formally introduce the online stochastic convex optimization problem: for a given time horizon T, at each time one single sample xt \u223cP in X comes and the player choose a point \u03b8t from a set C. Then the player observes a random cost/reward f(xt, \u03b8t) and try to minimize/maximize her population cumulative cost/reward FP (\u03b8t) := Ex\u223cP [f(\u03b8t, x)] in the whole time horizon. The objective of the decision is to minimize the population cumulative regret, which is the absolute difference between\nFor 2 < p \u2264\u221e, we plug the constants in Lemma 3.3 to Corollary 3.1, Corollary 3.2 and formula 3.2\n\ufffd \ufffd In conclusions, our algorithm improves the DP online general convex optimization, i.e., [Guha Thakurta and Smith, 2013], to a privacy-free rate under the stochastic and smooth setting.\n(3.7)\n(3.8) (3.9)\n(3.10) (3.11)\n(3.12) (3.13)\n# DP High Dimensional Generalized Linear Bandits\nIn this section, we consider the generalized contextual bandits with stochastic contexts, where a decision is made upon each new data [Li et al., 2017]. Our proposed private Frank-Wolfe algorithm is promising to derive a satisfying estimator for smart decisions under a wide range of reward structures while providing sufficient privacy protection in this setting due to the streaming and continual release feasibility. However, we face some non-stationarity incurred by the decision process, which leads to a highly non-trivial difficulty when applying the recursive gradient for variance reduction. For the fluency of the presentation, we first formulate the contextual bandits model and further explain the difficulty and our novel contributions in-depth.\n# 4.1 Introduction to Generalized Linear Bandit Problem\nConsider the following generalized linear bandit problem. At each time t, with individual-specific context Xt sampled from some distribution P on X, the decision maker can take an action at from a finite set (arms) of size K to receive a reward depending on the context Xt and the chosen arm at through its parameter \u03b8\u2217 at via a generalized linear model (GLM): rt = \u03b6(X\u22a4 t \u03b8\u2217 at) + \u03f5t, where \u03b6(\u00b7) is an inverse link function. We further assume that the context Xt, the underlying parameters {\u03b8\u2217 i }i\u2208[K] and the reward rt are all bounded. We assume the noise \u03f5t is sub-Gaussian [Wainwright, 2019] and conditional mean zero, i.e., Ft = \u03c3(X1:t, r1:t\u22121) and E[\u03f5t|Ft] = 0. We use the standard notion of pseudo regret, i.e., the difference between expected rewards obtained by the algorithm and the best achievable expected rewards, across the time:\nRegret(T) = T \ufffd t=1 \u03b6(X\u22a4 t \u03b8\u2217 a\u2217 t ) \u2212\u03b6(X\u22a4 t \u03b8\u2217 at),\nwhere a\u2217 t = arg maxi\u2208[K] X\u22a4 t \u03b8\u2217 i . It is non-trivial to introduce the privacy guarantee in the design of the bandit algorithms. The standard notion of DP under continual observation would enforce to select almost the same action for different contexts and incur \u2126(T) regret [Shariff and Sheffet, 2018]. Here we utilize the more relaxed notion of Joint Differential Privacy under continuous observation [Shariff and Sheffet, 2018].\nDefinition 4.1 ((\u03b5, \u03b4)-Jointly Differential Privacy (JDP)). A randomized action policy A = (At)T t=1 is said to be (\u03b5, \u03b4)-jointly differentially private under continual observations if for any t, any pair of sequences D and D\u2032 differing in the t entry and any sequences of action ranges from time t + 1 to the end E>t, it holds for A>t(D) := (As(D))s>t) that P[A>t(D) \u2208E>t] \u2264 e\u03b5P[A>t(D\u2032) \u2208E>t] + \u03b4.\nWe present some standard assumptions in contextual bandits, and similar assumptions can be found in Goldenshluger and Zeevi [2013], Bastani et al. [2020], Bastani and Bayati [2020]. Assumption 4.1 (Optimal Arm Set). We have a partition [K] = Ksup \u222aKopt, so that for every\nP(X\u22a4\u03b8it \u2212 max j\u2208Kopt,j\u0338=it X\u22a4\u03b8j \u2264h) \u2264\u03bdh,\nwhere it := arg maxi\u2208Kopt X\u22a4 t \u03b8i for some \u03bd > 0.\nNext we impose the standard regularity assumption on the reverse link function [Li et al., 2017, Ren et al., 2020, Chen et al., 2020] which includes widely-used linear model and logistic regression.\n# 4.2 Private High Dimensional Bandit Algori\nBased on the previous assumptions, we design differentially private high-dimensional GLM bandits (Algorithm 3). Our algorithm follows the similar procedure of Bastani and Bayati [2020] to use two sets of estimators: the forced-sampling estimators {\u03b8t0,j}j\u2208[K] constructed using i.i.d. samples to select a pre-selected set of arms; and the all-sample estimators {\u03b8t,j}t>t0,j\u2208[K] to greedily choose the \u201dbest\u201d arm in the pre-selected set. Another ingredient of our algorithm is the so-called synthetic update, i.e., adding the noisy all-zero contexts and zero rewards to the collected samples for the unselected arm. This ingredient is similar to Han et al. [2021] while they focus on local differential privacy. For our synthetic update, we have the following privacy guarantee and the proof is deferred to Appendix B.1.\n# Theorem 4.1 (Privacy Guarantee). Algorithm 3 is (\u03b5, \u03b4)-JDP.\nAlthough it is natural to run Algorithm 2 for estimators for for arm i \u2208[K], we are in fact acing various loss functions, say Ft(\u03b8t) := E[\u2207ft(\u03b8t,i; xt,at, yt)|Ft\u22121], at each time t. While all of he loss functions share the same minimizers \u03b8\u2217 i , \u2206t = dt \u2212\u2207Ft(\u03b8t,i) in Algorithm 2 is not mean zero and thus the recursive gradient is not an unbiased estimator for the population gradient.\nAs in the SCO setting, to show that the norm of the gradient estimation error \u2206t converges to zero sufficiently fast, we reformulate \u2206t as the sum of a sequence {\u03b6t,\u03c4}t \u03c4=1. Our SCO results enjoy the i.i.d. nature of the data and thus {\u03b6t,\u03c4}t \u03c4=1 is a martingale difference sequence which can be controlled by an Azuma-Hoeffding-type concentration inequality. In the bandits setting, after the forced-sampling period, the sample distribution for each arm evolves by time, and thus the sequence is no longer conditional mean zero. To overcome the difficulty, we develop a novel lemma on bridging the gradient error to the total variance difference of distributions between each time step, which is the key to our success in deriving the nontrivial regret bound in this setting. Lemma 4.1. For each arm i \u2208Kopt, suppose that the greedy action begins to be picked at t0, then for any t > t0 we have with probability at least 1 \u2212\u03b1,\n\u2225\u2206t\u2225\u221e\u2272 \ufffd log((d + T)/\u03b1) \ufffd(MD + \u03b2) \u221a t + \u03b2DM t \ufffd Csc( \u03b1 d + t0 )\u221at0 + \u03bd t \ufffd \u03c4=t0+1 \u2225\u03b8\u03c4\u22121,i \u2212\u03b8\u2217 i \u22251 \ufffd\ufffd ,\nwhere Csc(\u03b1) = O(log(dT/\u03b1)) is specified in the complete version (Lemma B.3).\nSuch lemma provides a guideline on tuning the warm-up stage length of the algorithm. In particular, it implies that polylog(T) length of warm-up is sufficient to get a \u02dcO( 1 \u221a t)-decayed gradient estimation error for each arm i \u2208Kopt if the previous estimators converge to the\nunderlying one at sufficiently fast rates. Such a low gradient estimation error is sufficient for the fast parameter convergence in the consequent time steps. As far as we know, this is the first attempt to directly apply variance reduction in a nonstationary environment, which is sharply contrast to the previous solutions. In reinforcement learning (RL), as pointed out by [Papini et al., 2018], variance reduction can potentially improve much the sample efficiency since the collection of the samples requires the agent to interact with the environment, which could be costly. However, the sampling trajectories is generated by an RL algorithm. Thus the direct usage of the variance reduction also suffer from the changing distribution of the collected sample once their RL algorithm improves based on previous experience. This also applies to the bandits setting which shares the similar spirit in the data collection process. In overcome this, previous work [Sutton et al., 2016, Papini et al., 2018, Xu et al., 2020], mainly employ importance sampling to correct the distribution shift and construct an unbiased estimator for the policy gradient with respect to the snapshot policy. However, importance sampling is prone to high variance, e.g., [Thomas et al., 2015]. We prove the desired convergence rate of the estimation error by induction in Section B.2, and here we present the corresponding theorem. Theorem 4.2 (Estimation Error). For the full-sample estimator \u03b8t,i, when t > t0 = O( log(dT/\u03b1) log(T \u03b52\nfor some constant t0 and Cin(\u03b1) = O( log2(dT/\u03b1) log(T) \u03b52 ) specified in Section B.2.\nNow we are ready to present our regret bound by converting the estimation error to regret whose formal proof is given in Section B.3.\nRemark 4.1. This regret has a sublinear growth rate, and it is the first regret bound for DP high-dimensional generalized linear bandits. In particular, the upper bound above has only a polylogarithmic growth concerning dimension d, as desired in high dimensional scenarios. Compared with the regret bound O(log2(dT)) without DP in Bastani and Bayati [2020], our upper bound contains an extra O(log2 T) factor, which is due to our simplified proof to shed light on the main idea. We leave the refinement as future directions.\n# 5 Experiments\nIn this section, we present experiment results to demonstrate the efficacy a algorithm.\nFirstly we provide an algorithm to generate the generalized Gaussian noise in Lemma 2.1, which will be used by DP-TOFW, DP-POFW and NoisySFW (Algorithm 3 in [Bassily et al., 2021b]) in the following experiment. When 2 < p \u2264\u221e(i.e. 1 \u2264q < 2), the corresponding Generalized Gaussian Noise is a re-scaled standard Gaussian noise under \u21132 norm. We focus on the case 1 < p < 2, in which the Generalized Gaussian Noise follows the p.d.f. defined in Lemma 2.1 with \u2225\u00b7\u2225+ = \u2225\u00b7\u2225q+, q+ = min{q \u22121, log d \u22121}.\nAlgorithm 4 Generation of the \u2113q+ Gaussian Noise\n1: Input: dimension d, q+, noise level \u03c3+\n2: Generate r2 \u223cGamma(d/2, 2\u03c3+)\n3: Generate d independent random real scalars \u03f5i \u223cG(1/q+, q+) where G(1/q+, q+) is the\ngeneralized normal distribution\n4: Construct the vector x of component xi = si \u00b7 \u03f5i and {si}i\u2208[d] are independent random signs\n5: Output: y = r\nx\n\u2225x\u2225\u2113q+\nProof. By Lemma 3.1, we know that if Z \u223cG\u2225\u00b7\u2225+(0, \u03c32 +) in d-dimensional space, then \u2225Z\u22252 + follows Gamma distribution \u0393(d/2, 2\u03c3+). Since the generalized Gaussian distribution is \u2113\u03ba+ radially symmetric, Lemma 3.2 in [Calafiore et al., 1998] proves that conditional on \u2225Z\u22252 + = r2 for any r > 0, Z is uniformly distributed on \u2113\u03ba+ spherical with radius r. Thus the remaining part is to sample uniformly from the sphere with radius \u2225Z\u2225+ in \u2113\u03ba+ space, which is achieved by modifying Algorithm 4.1 in [Calafiore et al., 1998] (step 3-5 in Algorithm 4).\n# 5.2 Experimental Setting\nIn this section, we consider the linear regression setting,\nwhere the design matrix X \u2208Rd\u00d7n, true parameter \u03b8 \u2208Rd, output y \u2208Rn, and \u03f5 \u223cN(0, \u03bd2In is a noise vector. We define the loss function as L(\u02c6\u03b8, X) = 1 n \ufffdn i=1(yi \u2212\u27e8xi, \u02c6\u03b8\u27e9)2 for any given estimation \u02c6\u03b8, where yi is the i-th entry of y and xi is the i-th column of X. Therefore, the excess risk will be F(\u02c6\u03b8) = E[L(\u02c6\u03b8)] where the expectation is taken with respect to the randomnes in X and \u03f5. Here we will use the loss function over a separate testing set as an empirica estimation of the excess population risk, which we denote as L(\u02c6\u03b8, Xtest) . And we furthe introduce suboptimality as SubOpt = L(\u02c6\u03b8,Xtest)\u2212L(\u03b8,Xtest) L(\u03b80,Xtest)\u2212L(\u03b8,Xtest). Here \u03b80 is zero vector, serving as the initialization of all algorithms. All experiments are finished on a server with 256 AMD EPYC 7H12 64-Core Processor CPUs. The code to reproduce our experimental results is shared in ou Github Repo.\n\nTo demonstrate the efficacy and efficiency of our algorithm in 1 \u2264p \u2264\u221eregimes, we choose p = 1.5 and p = \u221eas our geometries. We compare our DP-TOFW with NoisySFW (Algorithm 3 in Bassily et al. [2021b]), LocalMD (Algorithm 6 in Asi et al. [2021]) when p = 1.5 and with NoisySGD (Algorithm 2 in Bassily et al. [2020]) when p = \u221e. We generate T samples i.i.d. from a normal distribution with mean zero and standard deviation 0.05, and then normalize them by their q-norm to ensure each sample maintain unit q-norm. We also generate the true underlying parameter \u03b8 by setting all its entries to be sampled from a normal distribution with mean zero and standard deviation 0.05 and then normalized it by its p-norm. The size of the testing set is 10000. For all the experiment, we set the radius of constrain set C as 2 and guarantee (1, 1/T)-DP. To comprehensively demonstrate the performance of our algorithm, we conduct our experiment with T = 1000, 2000, 5000 and 10000 with dimension d = 5, 10 and 20. To achieve the best performance for each algorithm, we will scale their default learning rate by a grid of scaling factors. In Figure 5.2, we show the SubOpt of several algorithms under different learning rate scalings. As we can see, comparing with NoisySFW in p = 1.5 and NoisySGD in p = \u221e, DP-TOFW is robust against learning rate scaling. In Table 5.1, we show the risk, SubOpt and wall-clock time for all algorithms with their best learning rate scaling under different T and d combinations. All the results are based on 10 independent runs with different random seeds. As we can see, our proposed significantly outperforms NoisySFW in terms of risk while our DP-TOFW achieves comparable risk with NoisySGD but with much less computational cost. One thing we need to mention here is that LocalMD does not converge regardless of the learning rate scaling. We suspect that this is due to the large constants before their Bregman divergence, and the standard deviation of their Gaussian noise. In Figure 5.3, we visualize the SubOpt against wall-clock time of NoisySFW and DP-TOFW with their best learning scaling under p = 1.5. We notice that NoisySFW converges faster than DP-TOFW because it has a smaller number of total iteration (O(\u221an)) in centralized setting, while DP-TOFW needs to receive the data one by one and triggers the tree mechanism upon each data arrival (O(n) times).\n# 5.4 Comparison with DP-Bandit Algorithms\nMoreover, we conduct the experiment on our bandits applications. We compare our DP-HDB with the Linear UCB via Additive Gaussian Noise algorithm (DPUCB) in [Shariff and Sheffet, 2018]. We choose the time horizon T = 10000, the dimension d = 50, number of arms K = 2 and privacy epsilon \u03b5 = 1. The other parameters are set as recommended. The comparison between two bandit algorithms\u2019 cumulative regret is demonstrated in Figure 5.1. Our proposed algorithm significantly outperforms DPUCB in the cumulative regret.\n# 6 Conclusions\nIn this paper, we present a new framework for the online convex optimization in \u2113p geometry and high dimensional decision making with differential privacy guarantee. Our framework can\nTable 5.1: Experiment Results. We do not distinguish our DP-TOFW (Algorithm 1) and DP-POFW (Algorithm 2) and denote them as OFW in this table as they belong to our unif online Frank-Wolfe framework\nRisk\nSubOpt\nTime\nT\nd\np\nalgo\n1000\n5\n1.5\nNoisySFW\n0.0885\u00b10.00907\n0.522\u00b10.0565\n0.0189\u00b10.00216\nOFW\n0.00536\u00b10.00155\n0.0172\u00b10.00987\n0.0929\u00b10.00157\ninf\nNoisySGD\n0.0259\u00b10.0176\n0.0802\u00b10.0613\n30.2\u00b10.805\nOFW\n0.0357\u00b10.0216\n0.112\u00b10.0727\n0.0765\u00b10.00345\n10\n1.5\nNoisySFW\n0.0771\u00b10.0107\n0.953\u00b10.0947\n0.0216\u00b10.000514\nOFW\n0.0183\u00b10.00332\n0.201\u00b10.0483\n0.105\u00b10.0124\ninf\nNoisySGD\n0.0525\u00b10.0149\n0.636\u00b10.186\n31.2\u00b10.516\nOFW\n0.0915\u00b10.0209\n0.582\u00b10.157\n0.0852\u00b10.0109\n20\n1.5\nNoisySFW\n0.0414\u00b10.000302\n1.05\u00b10.0108\n0.0217\u00b10.0027\nOFW\n0.0307\u00b10.00344\n0.775\u00b10.128\n0.108\u00b10.00901\ninf\nNoisySGD\n0.0202\u00b10.00262\n0.955\u00b10.111\n31.4\u00b10.945\nOFW\n0.0766\u00b10.00464\n0.982\u00b10.0768\n0.0825\u00b10.0114\n2000\n5\n1.5\nNoisySFW\n0.0746\u00b10.00644\n0.44\u00b10.0287\n0.0376\u00b10.000492\nOFW\n0.00285\u00b10.000197\n0.00235\u00b10.00106\n0.222\u00b10.0184\ninf\nNoisySGD\n0.0127\u00b10.00371\n0.0344\u00b10.0133\n119\u00b10.962\nOFW\n0.0152\u00b10.00585\n0.0432\u00b10.02\n0.156\u00b10.00851\n10\n1.5\nNoisySFW\n0.0701\u00b10.00493\n0.887\u00b10.0691\n0.0366\u00b10.00502\nOFW\n0.00704\u00b10.00247\n0.0595\u00b10.0321\n0.189\u00b10.00384\ninf\nNoisySGD\n0.0382\u00b10.0133\n0.484\u00b10.178\n124\u00b12.36\nOFW\n0.0582\u00b10.0113\n0.364\u00b10.0795\n0.159\u00b10.00928\n20\n1.5\nNoisySFW\n0.0376\u00b10.00262\n0.957\u00b10.0673\n0.0397\u00b10.00427\nOFW\n0.018\u00b10.00374\n0.406\u00b10.106\n0.2\u00b10.00776\ninf\nNoisySGD\n0.0209\u00b10.00148\n0.947\u00b10.0672\n125\u00b10.345\nOFW\n0.067\u00b10.00799\n0.82\u00b10.114\n0.164\u00b10.0098\n5000\n5\n1.5\nNoisySFW\n0.0587\u00b10.0212\n0.35\u00b10.135\n0.0979\u00b10.0143\nOFW\n0.00258\u00b11.58e-05\n0.000702\u00b10.00051\n0.469\u00b10.00995\ninf\nNoisySGD\n0.00538\u00b10.000982\n0.00989\u00b10.00378\n742\u00b13.35\nOFW\n0.00667\u00b10.000392\n0.0145\u00b10.00153\n0.397\u00b10.0291\n10\n1.5\nNoisySFW\n0.0659\u00b10.00926\n0.808\u00b10.115\n0.0867\u00b10.00326\nOFW\n0.00376\u00b10.000479\n0.0163\u00b10.0053\n0.477\u00b10.0189\ninf\nNoisySGD\n0.0201\u00b10.00175\n0.115\u00b10.0118\n747\u00b12.4\nOFW\n0.022\u00b10.00347\n0.125\u00b10.0204\n0.38\u00b10.0117\n20\n1.5\nNoisySFW\n0.0401\u00b10.000848\n1\u00b10.0264\n0.0834\u00b10.0121\nOFW\n0.00962\u00b10.00209\n0.185\u00b10.0558\n0.527\u00b10.0121\ninf\nNoisySGD\n0.049\u00b10.00585\n0.602\u00b10.0571\n755\u00b12.89\nOFW\n0.0535\u00b10.00736\n0.637\u00b10.105\n0.413\u00b10.004\n10000\n5\n1.5\nNoisySFW\n0.0607\u00b10.027\n0.351\u00b10.161\n0.163\u00b10.00869\nOFW\n0.00255\u00b14.72e-05\n0.000318\u00b10.000179\n1.04\u00b10.0295\ninf\nOFW\n0.00337\u00b10.000336\n0.00293\u00b10.00106\n0.76\u00b10.0157\n10\n1.5\nNoisySFW\n0.0646\u00b10.00522\n0.789\u00b10.0744\n0.175\u00b10.0253\nOFW\n0.00282\u00b10.00025\n0.00465\u00b10.00184\n1.07\u00b10.0987\ninf\nNoisySGD\n0.0103\u00b10.000748\n0.0505\u00b10.00514\n3.03e+03\u00b18.8\nOFW\n0.00976\u00b10.00259\n0.0467\u00b10.0159\n0.803\u00b10.013\n20\n1.5\nNoisySFW\n0.0391\u00b10.00178\n0.937\u00b10.0303\n0.153\u00b10.0194\nOFW\n0.00487\u00b10.000584\n0.0592\u00b10.0155\n1.04\u00b10.102\ninf\nNoisySGD\n0.0409\u00b10.00362\n0.482\u00b10.0453\n3.12e+03\u00b127.6\nOFW\n0.0316\u00b10.00192\n0.363\u00b10.0283\n0.844\u00b10.0464\n23\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8045/8045d6c9-3b43-4f03-a15c-f158beb0acc3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5.1: Comparison between cumulative regret of DP-UCB and our DP-HDB algorithm</div>\ncontinually release the solutions in a fully-online update manner while still maintain privacy protection for the whole time horizon. Besides the privacy guarantee, our algorithm achieves in linear time the optimal rates when 1 < p \u22642 and the state-of-the-art rates that matches the non-private lower bound when 2 < p \u2264\u221e. The flexibility to extend to p = 1 case and the novel exploitation of the recursive gradient estimator in our algorithm also allow us to design the first high dimensional bandits algorithm satisfying DP requirements with sub-linear regret. The efficacy of the proposed algorithms are demonstrated by comparative experiments with various popular DP-SCO and DP-Bandit algorithms.\n# A Proofs of Section 3\n# A.1 Proof of Theorem 3.1\nProof of Theorem 3.1. We expend dt as follow\nwhere the last inequality is due to the fact that \u03c1t = 1 t+1. If we consider the tree based mechanism in Algorithm 5, each sample xi is involved in at most \u2308log2 n\u2309+ 1 nodes in the tree. And all partial summations can also be determined by at most \u2308log2 n\u2309nodes. The privacy analysis of the partial sum now reduces to the privacy analysis of the tree. Suppose adjacent datasets D and D\u2032 differ by sample xi and x\u2032 i, then for any sets B = (B1, B2, ..., B2\u2308log2 n\u2309+1\u22121) corresponding to the post-order traversal of the binary tree, it suffices\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/141e/141e8e60-ddbc-4c9b-8f9c-49510a4b0dc9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7c4/a7c4ac4d-efbc-485a-ad0d-d3f373f5db01.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c) NoisySGD under p = \u221e</div>\n<div style=\"text-align: center;\">Figure 5.2: Training curves of four algorithms with different learning rate scalings. Here we se T = 2000, d = 10. And they are all under (1, 1/T)-DP.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/82ed/82eda62b-72b8-4319-9da8-bb0a4e1a5958.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ba69/ba696d34-4797-47ed-bb10-efcbda5b24d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) T = 1000 and d = 5</div>\n<div style=\"text-align: center;\">Figure 5.3: Comparison of SubOpt and wall-clock time between DP-TOFW and NoisySFW. The shadows indicate \u00b11\u00d7std across different random seeds. Here we select the best learning rate scaling for both algorithms, according to the results in Figure 5.2.</div>\n<div style=\"text-align: center;\">(d) DP-TOFW under p = \u221e</div>\n<div style=\"text-align: center;\">(b) T = 2000 and d = 10</div>\nto prove that\nP(A1(D) \u2208B1, ..., A2\u2308log2 n\u2309+1\u22121(D) \u2208B2\u2308log2 n\u2309+1\u22121) \u2264e\u03b5P(A1(D\u2032) \u2208B1, ..., A2\u2308log2 n\u2309+1\u22121(D\u2032) \u2208B2\u2308log2 n\u2309+1\u22121) + \u03b4\nHere 2\u2308log2 n\u2309+1 \u22121 is the maximum number of nodes (including root and leaves) in a tree with \u2308log2 n\u2309+ 1 levels. For node Am including xi, suppose that it stores the summation \ufffdl j=k \ufffd (j + 1)\u2207f(\u03b8j, xj) \u2212j\u2207f(\u03b8j\u22121, xj) \ufffd , we have then conditioned on A1(D) = A1(D\u2032), ..., Am\u22121(D) = Am\u22121(D\u2032), \u03b8j(D) = \u03b8j(D\u2032) = \u03b8j, \u2200j \u2264l. Thus the difference between xi and x\u2032 i will cause the difference between (i + 1)\u2207f(\u03b8i, xi) \u2212i\u2207f(\u03b8i\u22121, xi) and (i + 1)\u2207f(\u03b8i, x\u2032 ) \u2212i\u2207f(\u03b8i\u22121, x\u2032 ).\nHere 2\u2308log2 n\u2309+1 \u22121 is the maximum number of nodes (including root and leaves) in a tree with \u2308log2 n\u2309+ 1 levels. For node Am including xi, suppose that it stores the summation \ufffdl j=k \ufffd (j + 1)\u2207f(\u03b8j, xj) \u2212j\u2207f(\u03b8j\u22121, xj) \ufffd , we have then conditioned on A1(D) = A1(D\u2032), ..., Am\u22121(D) = Am\u22121(D\u2032), \u03b8j(D) = \u03b8j(D\u2032) = \u03b8j, \u2200j \u2264l. Thus the difference between xi and x\u2032 i will cause the difference between (i + 1)\u2207f(\u03b8i, xi) \u2212i\u2207f(\u03b8i\u22121, xi) and (i + 1)\u2207f(\u03b8i, x\u2032 i) \u2212i\u2207f(\u03b8i\u22121, x\u2032 i). which has \u2113q sensitivity 2(\u03b2D + L) because \u2225 \ufffd (i + 1)\u2207f(\u03b8i, xi) \u2212i\u2207f(\u03b8i\u22121, xi) \ufffd \u2212 \ufffd (i + 1)\u2207f(\u03b8i, x\u2032 i) \u2212i\u2207f(\u03b8i\u22121, x\u2032 i) \ufffd \u2225q \u22642i\u03b2\u2225\u03b8i \u2212\u03b8i\u22121\u2225p + \u2225\u2207f(\u03b8i, xi) \u2212\u2207f(\u03b8i, x\u2032 i)\u2225q\n\u2225 \ufffd (i + 1)\u2207f(\u03b8i, xi) \u2212i\u2207f(\u03b8i\u22121, xi) \ufffd \u2212 \ufffd (i + 1)\u2207f(\u03b8i, x\u2032 i) \u2212i\u2207f(\u03b8i\u22121, x\u2032 i) \ufffd \u2225q \u22642i\u03b2\u2225\u03b8i \u2212\u03b8i\u22121\u2225p + \u2225\u2207f(\u03b8i, xi) \u2212\u2207f(\u03b8i, x\u2032 i)\u2225q \u22642(\u03b2D + L).\nAccording to the above sensitivity, and using the fact that \u2225\u00b7\u2225q,+ is \u03baq,+-smooth, we can now apply the generalized Gaussian in Lemma 2.1. We add noise G\u2225\u00b7\u2225+(0, 8(\u2308log2 n\u2309+ 1)2\u03baq log((\u2308log2 n\u2309+\n1)/\u03b4)(\u03b2D + L)2/\u03b52) independently to each node to ensure that each node is (\u03b5/(\u2308log2 n\u2309+ 1), \u03b4/(\u2308log2 n\u2309+ 1))-differentially private. We recall that each sample xi is involved in at most \u2308log2 n\u2309+ 1 nodes in the tree. We denote the path from xi to the root of the tree as pathi, where |pathi| \u2264\u2308log2 n\u2309+ 1. And here we use p to denote the density of (A1(D), ..., A2\u2308log2 n\u2309+1\u22121(D)) and p\u2032 for its counterpart regarding dataset D\u2032. Then for any B = (B1, B2, ..., B2\u2308log2 n\u2309+1\u22121), we have\n# A.2 Proof of Lemma 3.1\nBy\n \u00b7 we know that the tail of \u2225Zj\u22252 + is exactly the tail of \u0393(d/2, 2\u03c32 +) at \u03bb, which means \u2225Zj\u22252 + follows \u0393(d/2, 2\u03c32 +). Thus \u2225Zj\u22252 + \u2212E[\u2225Zj\u22252 +] is subGamma(2\u03c34 +d, 2\u03c32 +) , then the standard tail bound of sub-Gamma distribution implies P(\u2225Zj\u22252 + > E[\u2225Zj\u22252 +] + 2 \ufffd \u03c34 +d\u03bb + 2\u03c32 +\u03bb) \u2264exp(\u2212\u03bb) (A.2)\n(A.2)\nProposition A.1 (Azuma-Hoeffding inequality in regular space). Given the \u03ba-smooth norm \u2225\u00b7 and a vector-valued martingale difference sequence dt with respect to {Ft}t, we have if\nthen\n\ufffd \ufffd We provide the a detailed version of Proposition 3.1 in the following proposition.\nProposition A.2. We denote \u2206t = dt \u2212\u2207F(\u03b8t). Assume Assumption 2.2 and 2.3, for t \u2208[n], we have that with probability at least 1 \u2212\u03b1, Algorithm 1 will satisfies\n+ \ufffd (1 \u2212\u03c1k) \ufffd \u2207f(\u03b8\u03c4, x\u03c4) \u2212\u2207f(\u03b8\u03c4\u22121, x\u03c4) \u2212(\u2207F(\u03b8\u03c4) \u2212\u2207F(\u03b8\u03c4\u22121) \ufffd\ufffd\nRecall that \u22061 = \u2207f(\u03b81, x1) \u2212\u2207F(\u03b81). And we observe that E[\u03b6t,\u03c4|F\u03c4\u22121] = 0 where F\u03c4 is the \u03c3-field generated by {x1, x2, ..., x\u03c4\u22121}. Therefore, {\u03b6t,\u03c4}t \u03c4=1 is a martingale difference sequence. In what follows, we derive upper bounds of \u2225\u03b6t,\u03c4\u2225q. We start by observing that for any \u03c4 = 1, 2, ..., t,\n\ufffd We can bound \u2225\u03b6t,1\u2225q:\n\u2225\u03b6t,1\u2225q \u2264 1 t + 1\u2225\u2207f(\u03b81, x1) \u2212\u2207F(\u03b81)\u2225q \u2264 G t + 1 \u225cct,1,\n(A.3)\n(A.4)\n(A.5)\n\u2225\u03b6t,\u03c4\u2225q \u2264 \ufffd k=\u03c4 (1 \u2212\u03c1k) \ufffd \u2225\u2207f(\u03b8\u03c4, x\u03c4) \u2212\u2207f(\u03b8\u03c4\u22121, x\u03c4)\u2225q + \u2225\u2207F(\u03b8\u03c4) \u2212\u2207F(\u03b8\u03c4\u22121)\u2225q \ufffd + . . . + \u03c1\u03c4 t\ufffd (1 \u2212\u03c1k)\u2225\u2207f(\u03b8\u03c4, x\u03c4) \u2212\u2207F(\u03b8\u03c4)\u2225q\nwhere the second inequality follows from Assumption 2.2 and 2.3, and the last inequality is du to \u03b7\u03c4 = \u03c1\u03c4 and the definition of D. Now according to Proposition A.1, we have\nwe have with probability at least 1 \u2212\u03b11,\n\ufffd \ufffd According to Lemma 3.1, we know that \u2225Zj\u22252 q,+ follows Gamma distribution \u0393(d/2, 2\u03c3+). Selecting \u03bb = log(\u2308log2 n\u2309/\u03b12), by E[\u2225Zj\u22252 q,+] = \u03c32 +d and Eq. (A.2), we get with probability at least\n\ufffd \ufffd According to Lemma 3.1, we know that \u2225Zj\u22252 q,+ follows Gamma distribution \u0393(d/2, 2\u03c3+). Selecting \u03bb = log(\u2308log2 n\u2309/\u03b12), by E[\u2225Zj\u22252 q,+] = \u03c32 +d and Eq. (A.2), we get with probability at least 1 \u2212\u03b12/\u2308log2 n\u2309,\n\u2225Zj\u22252 q,+ \u2264\u03c32 +d + 2\u03c32 + \ufffd d log(\u2308log2 n\u2309/\u03b12) + 2\u03c32 +d log(\u2308log2 n\u2309/\u03b12). Thus with probability at least 1 \u2212\u03b12, we have max j\u2208Mt\u2225Zj\u22252 q,+ \u2264\u03c32 +d + 2\u03c32 + \ufffd d log(\u2308log2 n\u2309/\u03b12) + 2\u03c32 +d log(\u2308log2 n\u2309/\u03b12), here we use the fact that |Mt| \u2264\u2308log2 n\u2309. Thus with probability at least 1 \u2212\u03b12, \ufffd\ufffd\ufffd\ufffd \ufffd j\u2208Mt Zj \ufffd\ufffd\ufffd\ufffd q,+ \u2264\u2308log2 n\u2309max j\u2208Mt\u2225Zj\u2225q,+ \u2264\u2308log2 n\u2309\u03c3+ \ufffd d + 2 \ufffd d log(\u2308log2 n\u2309/\u03b12) + 2d log(\u2308log2 n\u2309/\u03b12) \ufffd1/2.\n(A.7)\n(A.8)\n# A.4 Proof of Theorem 3.2\nwhere the second inequality is due to definition of vt. According to Proposition A.2, with\n\ufffd (A.9)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e6ff/e6ff6399-c0ba-4b9e-94c5-6f6a2461ab6b.png\" style=\"width: 50%;\"></div>\nThen we have\nNow setting \u03b1\u2032 = \u03b1 n, and recalling that h1 \u2264\u03b2D2",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of differentially private stochastic convex optimization (DP-SCO) in the context of streaming data, where samples arrive sequentially and cannot be stored for long. Previous methods have not adapted well to this setting, necessitating a new approach.",
        "problem": {
            "definition": "The problem involves optimizing a population loss function under differential privacy constraints while handling streaming data, which presents unique challenges compared to traditional offline optimization.",
            "key obstacle": "Existing algorithms for DP-SCO require large batch sizes and fail to operate effectively in the streaming data context, which limits their applicability."
        },
        "idea": {
            "intuition": "The proposed idea stems from the observation that existing variance reduction techniques in non-private settings can be adapted to work under differential privacy constraints in an online context.",
            "opinion": "The proposed method is a private variant of the online Frank-Wolfe algorithm that employs recursive gradients for variance reduction, allowing for effective parameter updates with each incoming data point.",
            "innovation": "This method introduces a novel mechanism for variance reduction that operates in a non-stationary setting, achieving optimal excess risk rates in various \u2113p geometries."
        },
        "method": {
            "method name": "Private Online Frank-Wolfe Algorithm",
            "method abbreviation": "DP-OFW",
            "method definition": "A differentially private online algorithm that updates parameters using recursive gradients while maintaining privacy guarantees.",
            "method description": "The algorithm processes incoming data sequentially, adjusting its output while ensuring differential privacy through a tree-based mechanism.",
            "method steps": [
                "Initialize parameters and privacy settings.",
                "For each incoming data point, compute the gradient.",
                "Apply the tree-based mechanism to maintain privacy while aggregating gradients.",
                "Update parameters based on the computed gradients."
            ],
            "principle": "The method is effective due to its ability to reduce the accumulated noise in gradient estimates, leveraging the structure of the streaming data to maintain performance."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted to compare the proposed DP-OFW algorithm against existing DP-SCO and DP-Bandit algorithms, using various datasets and parameter settings.",
            "evaluation method": "The performance was assessed through metrics such as excess risk, regret, and computational time, with results averaged over multiple runs to ensure reliability."
        },
        "conclusion": "The proposed algorithm demonstrates significant improvements in both efficiency and effectiveness in handling high-dimensional data under differential privacy constraints, achieving state-of-the-art performance compared to existing methods.",
        "discussion": {
            "advantage": "The main advantages of the proposed approach include its adaptability to streaming data, optimal performance in various \u2113p settings, and the ability to maintain privacy without sacrificing accuracy.",
            "limitation": "One limitation is that while the algorithm performs well in many scenarios, its performance may degrade in extremely high-dimensional spaces or under certain non-stationary conditions.",
            "future work": "Future research could focus on refining the algorithm to further enhance its robustness in high-dimensional settings and exploring other applications of the proposed method in different domains."
        },
        "other info": {
            "additional details": {
                "key words": "Differential privacy, Online Convex Optimization, Stochastic Convex Optimization, High Dimensional Contextual Bandits",
                "authors": [
                    "Yuxuan Han",
                    "Zhicong Liang",
                    "Zhipeng Liang",
                    "Yang Wang",
                    "Yuan Yao",
                    "Jiheng Zhang"
                ],
                "affiliations": "Department of Mathematics, Department of Industrial Engineering and Decision Analytics, The Hong Kong University of Science and Technology"
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of differentially private stochastic convex optimization (DP-SCO) in the context of streaming data, where samples arrive sequentially and cannot be stored for long."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind developing the proposed method is the inadequacy of existing algorithms for DP-SCO in handling streaming data, which presents unique challenges compared to traditional offline optimization."
        },
        {
            "section number": "1.3",
            "key information": "The main objective of the paper is to introduce a private variant of the online Frank-Wolfe algorithm that employs recursive gradients for variance reduction, allowing for effective parameter updates with each incoming data point."
        },
        {
            "section number": "3.5",
            "key information": "The proposed method is a differentially private online algorithm that updates parameters using recursive gradients while maintaining privacy guarantees, demonstrating a novel mechanism for variance reduction in a non-stationary setting."
        },
        {
            "section number": "4.1",
            "key information": "The proposed algorithm shows significant improvements in efficiency and effectiveness in handling high-dimensional data under differential privacy constraints."
        },
        {
            "section number": "7.1",
            "key information": "One limitation of the proposed algorithm is its performance degradation in extremely high-dimensional spaces or under certain non-stationary conditions."
        },
        {
            "section number": "7.4",
            "key information": "Future research could focus on refining the algorithm to enhance its robustness in high-dimensional settings and exploring other applications of the proposed method in different domains."
        }
    ],
    "similarity_score": 0.5658653183894785,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/On Private Online Convex Optimization_ Optimal Algorithms in $_ell_p$-Geometry and High Dimensional Contextual Bandits.json"
}