{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2405.10098",
    "title": "When Large Language Model Meets Optimization",
    "abstract": "Optimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality. This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios. This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions.",
    "bib_name": "huang2024largelanguagemodelmeets",
    "md_text": "# When Large Language Model Meets Optimization\nSen Huanga, Kaixiang Yangb, Sheng Qic and Rui Wangc,\u2217\naSchool of Electronic and Information Engineering, South China University of Technology bSchool of Computer Science and Engineering, South China University of Technology cCollege of Systems Engineering, National University of Defense Technology\nA R T I C L E I N F O\nA B S T R A C T\nKeywords: Large Language Model Optimization Algorithm Evolutionary Computation\nOptimization algorithms and large language models (LLMs) enhance decision-making in dynamic environments by integrating artificial intelligence with traditional techniques. LLMs, with extensive domain knowledge, facilitate intelligent modeling and strategic decision-making in optimization, while optimization algorithms refine LLM architectures and output quality. This synergy offers novel approaches for advancing general AI, addressing both the computational challenges of complex problems and the application of LLMs in practical scenarios. This review outlines the progress and potential of combining LLMs with optimization algorithms, providing insights for future research directions.\n# 1. Introduction\nOptimization algorithms (OA) is becoming increasingly important as a class of heuristic search algorithms in the broad field of artificial intelligence and machine learning [89, 1, 114]. OA draws on the natural mechanisms of biological evolution, including processes such as natural selection, heredity, mutation, and hybridization, for solving complex optimization problems. These algorithms are widely used in many fields due to their global search capability, low dependence on problem structure, and ease of parallelization. Optimization algorithms pivotal in diverse fields such as logistics, finance [84], healthcare [88], and artificial intelligence [25], aim to identify the best solution from available alternatives. They are essential for making decisions efficiently and effectively in an era of rapidly increasing data complexity and volume. The continuous advancement in optimization techniques has resulted in significant enhancements to algorithmic strategies, each customized to address specific types of problems and operational constraints. From deterministic methods addressing linear problems to stochastic approaches for global optimization under uncertainty, optimization algorithms hold promise across a broad spectrum of research and practical applications. With the development of technology, especially when dealing with large-scale, high-dimensional and dynamically changing optimization problems, traditional algorithms often face performance bottlenecks. Evolutionary computing provides effective solutions to these problems with its unique search strategy. In addition, the flexibility and adaptability of evolutionary computation enable it to be combined with a variety of other computational techniques to form hybrid algorithms for further performance enhancement. In the rapidly evolving field of artificial intelligence, Large Language Models (LLMs) such as GPT (Generative\n\u2217Corresponding author. huangsen@scut.edu.cn ( Sen Huang); yangkx@scut.edu.cn ( Kaixiang Yang); qisheng@nudt.edu.cn ( Sheng Qi); ruiwangnudt@gmail.com ( Rui Wang)\nHuang et al.: Preprint submitted to Elsevier\nPre-trained Transformer) [14] provide a significant breakthrough with their advanced natural language understanding and generation capabilities. These models have revolutionized applications ranging from automated writing assistants to sophisticated conversational agents. LLMs have become pivotal in advancing fields like natural language processing, image recognition with their extensive parameters and deep learning capabilities, and machine learning, offering robust solutions for complex data-driven challenges. LLMs have achieved breakthroughs in traditional NLP tasks like text generation and language translation through extensive data training, and they also show promising potential in the emerging fields of algorithm design and optimization. Traditional optimization algorithm design, dependent on human expertise [92], is both time-consuming and potentially limited by the experts\u2019 knowledge. The advent of large-scale language models, however, has transformed this arena. These models learn extensive algorithmic patterns and strategies, enabling them to devise new algorithms and tailor solutions to specific challenges. Furthermore, constructing and training large language models require significant computational resources and large datasets [156], which escalate research and development costs and constrain the applicability and generalization of LLMs. Optimization algorithms are crucial in developing LLMs, enabling researchers to efficiently tailor and refine model structures for specific applications. By enhancing the training process, boosting computational efficiency, and lowering resource consumption, these algorithms facilitate the construction and application of large-scale language models. These algorithms enhance the models\u2019 generalization capabilities and robustness, enabling improved performance amidst real-world complexity and uncertainty. The goal in designing optimization algorithms for LLMs is to enhance their operational efficiency and reduce resource consumption, without compromising, and possibly improving, model performance.\nThis review aims to systematically analyze research on developing optimization algorithms with LLMs, and optimizing LLMs with optimization algorithms. It summarizes related research and application scenarios, and explores the diverse aspects of these applications. Section 2 provides a comprehensive review of large language model development, also known as macromodels, tracing their progression from basic predictive models to sophisticated systems that can comprehend and generate human-like text. Section 3 focus to optimization algorithms, concisely tracing their evolution from basic iterative methods to advanced algorithms essential for efficiently scaling AI models. Section 4 examines research that approaches large language models as optimization problems, highlighting innovative methods employed as search operators and in designing optimization algorithms. Section 5 discusses recent advances in optimization algorithms tailored for refining large-scale models. It highlights how these algorithms can enhance design, boost efficiency, and improve the performance of LLMs. Section 6 examines the practical applications of integrating optimization algorithms with LLMs, highlighting real-world implementations and their benefits. Section 7 is future outlook and research trends, in which summarizes the insights gained from this exploration and provides an outlook on the potential future developments in this exciting intersection of AI research. In summary, we conducted a comprehensive study on the development and application of optimization algorithms for large models, aiming to provide valuable references and insights for future research.\n# 2. Large language models\nLanguage has a crucial role in human cognition, enabling communication and expression from early childhood to adulthood [83]. Teaching robots to imitate human-like language skills is a difficult task due to their intrinsic lack of cognitive capacity for understanding and expressing language. Computational linguistics aims to close this divide by using advanced Artificial Intelligence (AI) algorithms to enable machines to possess reading, writing, and communication skills that like those of humans [126]. The rise of Large Language Models (LLMs) undoubtedly represents an important milestone in the evolution of Natural Language Processing (NLP). These models, such as GPT-3 and GPT-4 of the GPT family [14], are built on the Transformer architecture and have up to billions of parameters. They achieve a deep understanding of natural language and generative capabilities by pre-training on massive text datasets. The evolution of LLMs has gone through several notable stages: from the early days of Statistical Language Models (SLMs) and Neuro-Linguistic Models (NLMs), to Pre-Trained Language Models (PLMs), and ultimately to today\u2019s large-scale language models. While PLMs like BERT and GPT-2 have achieved remarkable success in NLP tasks, the emergence of LLMs has revolutionized the game in this field [23]. Not only can they be adapted to a wide\nHuang et al.: Preprint submitted to Elsevier\nrange of tasks through large-scale pre-training, but they are also further optimised through fine-tuning, demonstrating a wide range of potential in application scenarios such as chatbots, search engine optimization and office automation The rise of Large Language Models (LLMs) undoubtedly represents an important milestone in the evolution of Natural Language Processing (NLP). These models, such as GPT3 and GPT-4 of the GPT family [14], are built on the Transformer architecture and have up to billions of parameters. They achieve a deep understanding of natural language and generative capabilities by pre-training on massive text datasets. The evolution of LLMs has gone through several notable stages: from the early days of Statistical Language Models (SLMs) and NLMs, PLMs, and ultimately to today\u2019s large-scale language models. While PLMs like BERT and GPT-2 have achieved remarkable success in NLP tasks, the emergence of LLMs has revolutionized the game in this field. Not only can they be adapted to a wide range of tasks through large-scale pre-training, but they are also further optimized through fine-tuning, demonstrating a wide range of potential in application scenarios such as chatbots, search engine optimization, and office automation [156]. The excellence of Large Language Models (LLMs) in the field of Natural Language Processing (NLP) is due to several key components of their design, which together give LLMs powerful language understanding and generation capabilities [156]. First, \u201cpre-training\u201d is one of the core processes of LLMs. By pre-training on large-scale textual datasets, LLMs are able to learn the basic structures and patterns of language. These datasets typically contain billions of words covering a wide range of topics and language styles, allowing the models to capture the diversity and complexity of the language. Second,\u201cadaptability\u201d is another key characteristic of LLMs. After pre-training, LLMs can be further fine-tuned to adapt to specific downstream tasks, such as text classification, sentiment analysis, or machine translation. This adaptability allows LLMs to optimise their performance for specific tasks, leading to better results in various NLP challenges [91]. In terms of \u201capplications\u201d, the broad applicability of LLMs is another reason for their popularity. Not only do they perform well in traditional NLP tasks, but they can also be applied to a wider range of domains, such as the development of chatbots, the optimization of search engines, the construction of content recommendation systems, and the development of automated office tools. Finally, \u201cperformance evaluation\u201d is critical to ensure the reliability and effectiveness of LLMs. Through a series of standardised testing and evaluation protocols, researchers are able to quantify the performance of LLMs and ensure that they work consistently across a range of tasks and conditions. Performance evaluation also includes studies of model bias, fairness, and interpretability, which are key factors in improving model quality and trust. LLMs are becoming a key driver in the field of AI, and their development and application are attracting widespread attention from industry and academia. LLMs represented by ChatGPT and GPT-4 have not only made significant\nPage 2 of 23\nprogress in technology but also promoted in-depth discussions on artificial general intelligence (AGI) conceptually [156]. OpenAI\u2019s technical article proposes that GPT-4 [2] may be an early attempt to move towards AGI, which all indicates the critical position of LLMs in the development of AI [16]. In the field of Natural Language Processing (NLP), LLMs are becoming a common tool for solving various linguistic tasks, changing the previous research and application paradigm. The Information Retrieval (IR) field is also feeling the winds of change, with traditional search engines facing the challenge of emerging information access methods such as AI chatbots, such as New Bing3, which is an attempt to enhance search results based on LLMs [17]. In addition, the field of computer vision (CV) is exploring multimodal models that combine vision and language, and the multimodal input support of GPT-4 is a manifestation of this trend. The rise of LLMs heralds the birth of a whole new ecosystem of apps based on these advanced models. Microsoft 365 leverages LLMs (e.g., Copilot) to automate the office, while OpenAI introduces plug-in functionality in ChatGPT, all of which demonstrate the potential of LLMs to enhance productivity and extend application scenarios [136]. While LLMs have brought about many positive changes, they also present a number of challenges, particularly in terms of the security and accuracy of the generated content. In addition, the training of LLMs requires substantial computational resources, which is a challenge for research institutes as it limits the ability to perform extensive experiments and optimization of the models [22].We have summarised the relevant restrictions below: 1) Computational resources: LLMs demand substantial computational resources for training and inference, which can pose challenges for implementing optimization algorithms at scale. Ensuring access to adequate computational infrastructure remains a significant hurdle. 2) Data efficiency: While LLMs have demonstrated impressive performance, they often require large amounts of data for effective training. This reliance on extensive datasets can be a bottleneck for optimization algorithms, especially in scenarios where data availability is limited or costly to obtain. 3) Interpretability and explainability: The inherent complexity of LLMs poses challenges for the interpretability and explainability of optimization algorithms. Understanding the decision-making process of these models and interpreting their outputs can be challenging, particularly in critical applications where transparency is essential. 4) Generalization and Robustness: Ensuring the generalization and robustness of optimization algorithms trained using LLMs is another key challenge. Over-reliance on specific patterns in the training data may lead to poor generalization performance on unseen data or vulnerability to adversarial attacks.\nHuang et al.: Preprint submitted to Elsevier\n# 3. Classic Optimization Algorithm 3.1. Traditional optimization algorithms for optimization problems Optimization algorithms have wide applications \noptimization problems Optimization algorithms have wide applications in fields such as industry, economics, and management. With the rapid development of artificial intelligence, optimization algorithms play a crucial role in achieving intelligence and automation. Traditional optimization algorithms include deterministic optimization algorithms, approximation algorithms, and heuristic algorithms. Deterministic optimization algorithms include Linear Programming (LP)[98], Integer Programming (IP)[107], Mixed Integer Programming (MIP)[45], Convex Optimization[130], Adaptive Dynamic Pro gramming (ADP)[74], etc. Deterministic optimization algorithms can guarantee finding the global optimal solution, but they are generally not suitable for large-scale problems. Polynomial-time approximation algorithms can find a good solution within a reasonable time frame, but they do not guarantee optimality. In other words, approximation algorithms do not guarantee finding the global optimal solution. For some specific problems, optimality guarantees may not exist at all. Heuristic algorithms employ specially designed functions to intelligently explore the solution space [35]. Heuristic Algorithm rely on intuitive rules, trial-and-error strategies, and practical insights to approximate solutions within acceptable bounds. Heuristic algorithms are particularly effective for problems with highdimensional search spaces or combinatorial complexities where exact solutions are impractical. Heuristic algorithms include greedy algorithm[20], Tabu Search[4, 5, 49], genetic algorithm[131, 63], differential evolution algorithm[97, 33], Cooperative co-evolution algorithm[111, 18, 125]. Heuristic algorithms have been widely used due to their excellent computational performance, but they typically require customization and domain expertise for specific problems. Additionally, heuristic algorithms may converge to local optimal solutions and have high time complexity. Recently, the superiority of using Estimation of Distribution Algorithm (EDA) in solving optimization problems has been demonstrated. EDA is a prominent optimization technique that employs probabilistic models to guide the search process. Unlike traditional evolutionary algorithms, which rely on mutation and recombination operators, EDA focuses on building and updating a probabilistic model of promising solutions. This model is then sampled to generate new candidate solutions, effectively balancing exploration and exploitation. Yang et al. [145] proposed ACSEDA based on the Gaussian distribution model, and calculates the covariance according to an enlarged number of promising individuals. In contrast to solely relying on solutions from the current generation to estimate the Gaussian model, \ud835\udc38\ud835\udc37\ud835\udc342 [69] incorporates a strategy where a set number of highquality solutions from previous generations are retained in an archive. These historical solutions are then utilized to aid in estimating the covariance matrix of the Gaussian model.\nPage 3 of 23\nDong et al. [38] introduced a latent space-based EDA (LSEDA), which converts the multivariate probabilistic model of Gaussian-based EDA into a principal component latent subspace with reduced dimensionality. This transformation effectively decreases the complexity of EDA while preserving its probability model, ensuring that crucial information is retained. Consequently, LS-EDA enhances performance scalability for large-scale global optimization problems. In recent times, hybrid EDAs have emerged as a prominent research focus. Li et al. [67] proposed IDE-EDA, an enhanced version of DE achieved by integrating the EDA. Zhang et al. [151] introduced a new hybrid evolutionary algorithm for continuous global optimization problems, Zhou et al. [160] proposed a fusion of an Estimation of Distribution Algorithm (EDA) with both economical and costly local search (LS) methods. This integration aims to leverage both global statistical insights and individual location-specific information for enhanced optimization performance.\n# 3.2. Reinforcement learning methods for optimization problems Another famous learning paradigm is rei\noptimization problems Another famous learning paradigm is reinforcement learning. Reinforcement learning learns and improves its strategy by interacting with the environment, aiming to maximize long-term cumulative rewards. The aforementioned heuristic algorithms typically find optimal or approximate solutions by searching the solution space, guided by heuristic information. However, they do not consider interaction with the environment during the search process. Unlike supervised and unsupervised learning, in reinforcement learning, agents can only learn through trial and error, rather than relying on labeled data or searching for the inherent structure of the data. Furthermore, reinforcement learning can be categorized into classical reinforcement learning methods and deep reinforcement learning methods, which will be introduced below. According to [65], classic RL methods can be divided into model-based and model-free approaches. Model-free methods can be further categorized into value-based, policybased, and actor-critic methods. Value-based methods seek the optimal policy by estimating the value function. This approach is suitable for smaller state spaces and discrete action spaces but faces challenges to extend to continuous action spaces and has the \"high bias\" problem. The error between the estimated value function and the actual value function is difficult to eliminate. Policy-based methods, on the other hand, do not require estimating the value function. Instead, they directly fit the policy function using neural networks. By training and updating the policy parameters, the optimal policy is generated. This method is suitable for continuous action spaces but requires sampling a large number of trajectories, and there is a huge difference between each trajectory, leading to the \"high variance\" problem. To address the contradiction between high bias and high variance, the actor-critic method emerges. The actor-critic method constructs an agent that can both output policies and evaluate their quality in real-time using the value function.\nHuang et al.: Preprint submitted to Elsevier\nGenerally, an actor-critic network consists of two parts: the actor network and the critic network. The actor network is used to generate policies to approximate the policy function, while the critic network is used to evaluate policies to approximate the value function. Representative works of these methods will be introduced below. Q-learning[132] is a classic value-based RL algorithm and is currently the most widely used model-free RL algorithm. Q-learning first initializes a Q-function, typically represented as a Q-table. It selects an action based on the current state using the \ud835\udf16-greedy strategy, performs the selected action, and observes the reward obtained and the next state transitioned to. The Q-function is updated using the Bellman equation, repeat the above steps and update the Q value until the stop condition is reached. Finally, the optimal policy is extracted based on the learned Q-function. Double Q-learning[52] is an improved version of the Q-learning algorithm which alleviates the overestimation problem by using two Q-functions. In each round of interaction with the environment, one of the value function estimators is alternately selected to choose actions, while the other is used for action value estimation. Existing research has demonstrated that the Double Q-learning method achieves higher stability and greater long-term returns. The REINFORCE algorithm[135], also known as the Monte Carlo Policy Gradient Reinforcement Learning algorithm, is a classical policy gradient method. The goal of the REINFORCE is to update parameters along the gradient direction to maximize the objective function. On the basis of the classical policy gradient algorithms, Trust Region Policy Optimization (TRPO)[108] ensures that the KullbackLeibler Divergence between the new and the old policy does not exceed a predefined threshold during each policy update. This threshold represents the \"trust region\" of policy updates, indicating the similarity between the new and old policies, thereby ensuring the stability of policy optimization. Proximal Policy Optimization (PPO)[109] is an improvement over TRPO, which is simpler to implement and requires less computation in practical use. Actor-Critic (AC)[60] algorithm combines the advantages of both policy-based and value-based methods. It learns both the policy and the value function simultaneously. The actor trains the strategy based on the value function feedback from the critic, while the critic trains the value function using the Time Difference Method (TD) for single step updates. The aforementioned REINFORCE algorithm uses a stochastic policy function, which outputs the probability distribution of actions for a given state, and then selects an action based on the probability distribution. In contrast to the REINFORCE algorithm, Deterministic Policy Gradient (DPG)[112] algorithm employs deterministic policy function, which directly outputs a deterministic action for given states and update policy parameters by maximizing expected returns. DPG integrates deterministic policy gradients only in the state space, greatly reducing the need for sampling and enabling the handling of larger action spaces. However, the coupling between policy updates and value estimation\nPage 4 of 23\nin DPG leads to insufficient stability, particularly being highly sensitive to hyperparameters. The difficulty of tuning hyperparameters in actor-critic algorithms and challenges in reproducibility make them hard to apply in practical scenarios. When extended to application fields, the robustness of the algorithm is also one of the most concerned core issues. The above methods are relatively simple and intuitive, easy to understand and implement, and suitable for smallerscale problems. In relatively fewer samples, classical RL methods typically achieve good performance, especially in stable environments and reward settings. More importantly, the above method exhibits strong interpretability, allowing for a clear understanding of the agent\u2019s behavior and decision-making process within the environment. Although RL has achieved remarkable achievements, previous methods have often struggled to handle highdimensional data such as images, text, etc. This limitation has constrained its ability to deal with complex tasks and environments. Classical RL methods often find it challenging to strike a good balance between exploration and exploitation, leading to susceptibility to local optima, especially in high-dimensional and complex environments where issues of insufficient exploration are more pronounced. The reason for the aforementioned situation is that RL algorithms, like other algorithms, face challenges such as memory complexity, computational complexity, sample complexity, etc[6]. However, the powerful representation learning capability and function approximation capability of deep learning bring a completely new solution for RL. Deep learning is a branch of machine learning aimed at using multi-layer neural network models to learn representations and features of data, and solving various tasks through these representations and features. Deep learning models have strong nonlinear function approximation capabilities, allowing them to learn more complex and accurate data representations and features. Deep learning models have strong generalization and representation learning capabilities, enabling them to learn more accurate and effective policies or value functions, thereby allowing RL agents to tackle more complex and high-dimensional tasks and environments, achieving higher performance and accuracy. Deep reinforcement learning (DRL) is the product of integrating both RL and DL. Similarly, DRL methods are also divided into three types: value-based, policy-based, and actor-critic methods. Among them, value-based DRL employs DL to approximate value functions, while policy-based DRL uses DL to approximate policies and solve decision-making policies based on policy gradient rules. The following will introduce representative works in DRL. In 2013, Mnih et al. from DeepMind combined DL with Q-learning, proposing the groundbreaking Deep Q-network (DQN)[94]. DQN is a DRL algorithm based on Q-learning. On one hand, it utilizes deep neural networks as value function estimators, and on the other hand, it introduces experience replay and target networks. The experience replay mechanism breaks the high dependency between sampled samples, while the target network alleviates the instability\nHuang et al.: Preprint submitted to Elsevier\nof neural networks during training. These two mechanisms work together to enable the DQN algorithm to achieve performance close to or even surpassing human levels in most Atari games. Double DQN[127], based on Double Qlearning, is an improvement over DQN. Similar to how DQN extends Q-learning, DDQN addresses the overestimation issue by using two Q-networks. DDQN achieves better stability and algorithm performance compared to DQN. In 2017, Dai et al. combined RL with graph embedding and proposed S2V-DQN[58]. They utilized a graph embedding network called structure2vec (S2V) to represent the policy in greedy algorithms and employed multi-step DQN to learn greedy policies parameterized by the graph embedding network. The S2V-DQN algorithm generates high-quality solutions faster, sometimes finding better solutions than commercial solvers within a longer timeframe. In 2015, inspired by the ideas of DQN, Lillicrap et al. combined neural networks with the DPG algorithm to propose DDPG[71]. DDPG employs two different parameterized deep neural networks to represent the value network and the deterministic policy network. The policy network is responsible for updating the policy, while the value network outputs the Q-values for state-action pairs. Similar to DQN, DDPG also utilizes target networks to overcome the instability issues during network updates. Zhang et al. [152] considered the feasibility constraints of NP-hard problems, embedding heuristic functions into the transformer architecture, and applying DRL to combinatorial optimization problems with feasibility constraints. Ma et al.[85] introduced a Graph Pointer Network (GPN) to solve the classical TSP problem and combined it with a hierarchical RL framework to address the TSP problem with time window constraints. Multiple Traveling Salesman Problems (MTSP) are more complex, and Hu et al.[54] designed a network consisting of a shared graph neural network and distributed policies to learn a common policy expression suitable for MTSP. Experimental results demonstrated the effectiveness of this approach on large-scale problems. In 2016, Mnih et al.[93] developed an improved ActorCritic algorithm called A3C (Asynchronous Advantage Actor-Critic). By utilizing asynchronous gradient descent to optimize the parameters of deep neural networks (DNNs), A3C significantly improved the efficiency of policy optimization. Vinyals et al. [129] proposed the Pointer Network model for solving combinatorial optimization problems, which initiated a series of research studies on utilizing DNN for solving combinatorial optimization problems. This model was inspired by the Seq2Seq model in machine translation. It employs a deep neural network-based encoder to encode the input sequence of the combinatorial optimization problem (such as city coordinates), then utilizes a decoder and attention mechanism to compute the selection probabilities of each node. Finally, it selects nodes in an autoregressive manner until obtaining a complete solution. Due to the supervised nature of the training method proposed by Vinyals et al.[129], the quality of the solutions it obtains will\nPage 5 of 23\nnever exceed the quality of the sample solutions. Recognizing this limitation, Bello et al.[9] employed a RL approach to train the Pointer Network model. They treated each problem instance as a training sample, using the objective function of the problem as feedback signals, and trained the model based on REINFORCE. They also introduced a critic network as a baseline to reduce training variance. Furthermore, Nazari et al.[96] extended the Pointer Network to handle dynamic VRP problems. They replaced the LSTM in the Encoder input layer with a simple one-dimensional convolutional layer, effectively reducing computational costs. While maintaining optimization effectiveness, the training time was reduced by 60%. In recent years, the Transformer[128] has achieved tremendous success in the field of natural language processing. Its multi-head attention mechanism enables better extraction of deep features from problems. In view of this, several recent studies have drawn inspiration from the Transformer for solving combinatorial optimization problems. Deudon et al. [36] improved traditional pointer network models by incorporating ideas from the Transformer. They utilized a similar structure to the Transformer in the encoder, while the decoder employed linear mapping of the decisions from the last three steps to obtain a reference vector, thereby reducing model complexity. The attention calculation method remained the same as in traditional Pointer Network models, and the classic REINFORCE method is still used to train the model. Kool et al.[61] proposed a new method capable of solving multiple combinatorial optimization problems using attention mechanisms. The attention calculation method in this model adopted the selfattention computation method from the transformer, with additional computational layers to enhance performance. They further designed a greedy rollout baseline to replace the Critic network, leading to significant improvements in optimization performance. Deep reinforcement learning, as one of the most popular research directions in the field of artificial intelligence, has shown great potential in solving complex tasks and addressing various real-world problems. However, DRL also has its limitations, such as data requirements, sample efficiency, computational resources, interpretability, etc. Despite achieving some success in both research and application domains, DRL fundamentally remains constrained to simulated environments with ideal, highly structured experimental data design. They heavily rely on the design and training of specific models. Therefore, there is a growing interest in the design and optimization of automatic algorithms.\n# 4. LLMs as Optimization 4.1. LLMs as the Black-box Optimization Search Model There is a strong alignment between Large Languag\nModel There is a strong alignment between Large Language Models (LLMs), which are powerful in generating creative texts, and Evolutionary Algorithms (EAs), capable of\nHuang et al.: Preprint submitted to Elsevier\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8e7/c8e78a18-b67e-4f19-80b2-54d358a8ffbb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The split of LLMs assist optimization algorithms.</div>\ndiscovering diverse solutions to complex real-world problems [24]. With their powerful knowledge storage and generation capabilities, LLMs can support optimization algorithms in problem decomposition, parameter search, and solution generation. As show in Fig. 1, LLMs can be classified into two categories for enhancing optimization algorithms: 1) One category is to use the large model as the search operator of the black-box optimization model, which makes full use of the knowledge storage capacity and experience of LLMs and thus can effectively reduce the input of workforce. 2) The other category of approach is to give full play to the generative capacity of the large model and to make full use of the understanding of the optimization problem by the large model as the input of model and then generate suitable optimization algorithm configurations or generate optimization algorithms for solving specific problems. The use of large models to assist the design of optimization algorithms has achieved preliminary research and widespread attention. How to give full play to the advantages of large models in the design of algorithms and integrate them into the optimization algorithm framework has become the key to the research in this field. A detailed classification of what is the goal of the auxiliary optimization model with LLM is shown in Table 1. Yang et al. [143] proposed the Optimization by PROmpting (OPRO), which utilizes natural language descriptions to guide LLMs in searching for solutions for optimization problems. This method is particularly effective for derivativefree optimization scenarios that are common in real-world applications. Technically, OPRO includes previously generated solutions and their values, allowing the LLM to iteratively improve upon solutions. The framework also intelligently balances benefits and costs, crucial for effective optimization, by adjusting the sampling temperature of the LLM to encourage both refinement of existing solutions and exploration of new ones. Chen et al. [27] explored cue optimization methods in multi-step tasks to improve task execution efficiency. By constructing a discrete LLM-based prompt optimization framework, the framework automatically provides suggestions for improvement by integrating human-designed feedback rules and preference alignment. It is noted that while LLM performs well in single-step tasks, real-world multi-step tasks pose new challenges, such as more complex cueing content, difficulty in evaluating individual step impacts, and the fact that different people may have different task execution preferences. To address\nPage 6 of 23\nResearch Field\nCategory\nLLM\u2019s Goal\nRelated Works\nLLMs as the Black-box\nOptimization Search Model\nGuided Search Operator\nLLMs combined with natural language\ndescriptions or manual a priori knowl-\nedge search for suitable optimization al-\ngorithms\n[143, 27, 153, 3]\nHeuristic Search Operators\nLLMs rely on knowledge storage and\nproblem analysis capabilities to search for\nsuitable optimization algorithms\n[146,\n159,\n90,\n79]\nMulti-Objective\nOptimization\nLLMs search for multiple objectives and\ntrade-offs between the importances of the\ninter-objectives\n[13, 75, 11]\nLLMs as the Generator of\nOptimization Algorithms\nOption Generate with the\nCognitive of LLMs\nLLMs analyze the problem and generate\nsuitable optimization algorithm options\nbased on a priori optimization algorithm\nknowledge\n[150, 80, 86]\nAlgorithm Generate with\nwith Chain-of-Thought\nGenerate optimization algorithms, opti-\nmization steps, or code with the help of\nthe chain of thought ability of LLM to\nsolve complex problems\n[77, 76, 41, 101,\n12, 53]\nthese issues, the researchers introduced human feedback, leveraging human expertise in providing input and combining it with a genetic algorithm-style framework to optimize cues. For the current emerging optimization problem of neural architecture search, Zhang et al. [153] designed an automated machine learning (AutoML) system based on largescale language models (LLMs), AutoML-GPT. AutoMLGPT utilizes a GPT model as a bridge to connect multiple AI models and dynamically uses optimized hyperparameters to train the models. The system automatically generates corresponding prompt paragraphs to search for the optimal model architecture and parameters by dynamically taking inputs from user requests and data cards. It then automatically executes the entire experimental process, from data processing to model architecture, hyperparameter tuning, and prediction of training logs. AhmadiTeshnizi et al. [3] proposed a large language model (LLM)-based agent called OptiMUS. OptiMUS is designed to formulate and solve mixed-integer linear programming (MILP) problems from natural language descriptions. It is capable of developing mathematical models, writing and debugging solver code, developing tests, and checking the validity of the generated solutions.\n# 4.1.1. Heuristic Search Operators To tackle NP-hard combinatorial\nTo tackle NP-hard combinatorial optimization problems (COPs) using large language models (LLMs) as HyperHeuristics (LHHs), Ye et al. [146] proposed ReEvo, a framework that emulates the reflective design approach of human experts. It leverages the scalable inference capabilities of LLMs, Internet-scale domain knowledge, and powerful evolutionary search strategies. ReEvo operates by generating heuristics through LLMs with minimal human intervention, offering an open-ended heuristic space and the potential for Knowledge and competence beyond that of human experts.\nHuang et al.: Preprint submitted to Elsevier\nThe research aims to address the complexity and heterogeneity of COPs by automating the design process of heuristics, which traditionally requires extensive trial and error from domain experts. ReEvo incorporates a dual-level reflection mechanism, where short-term reflections are used to analyze heuristics\u2019 relative performance, and long-term reflections are accumulated to guide their evolution. This reflective process allows ReEvo to adapt and improve heuristics over time, leading to smoother fitness landscapes and more effective search results. Zhong et al. [159] introduced a groundbreaking approach to the design of metaheuristic algorithms by utilizing the capabilities of the large language model (LLM) ChatGPT-3.5. They proposed a animal-inspired metaheuristic algorithm named Zoological Search Optimization (ZSO), which is developed to tackle continuous optimization problems. The ZSO algorithm is designed to mimic the collective behaviors of animals, incorporating two key search operators: the prey-predator interaction operator and the social flocking operator, which together balance exploration and exploitation effectively. A similar approach called Language Model Crossover (LMX) utilizes large pre-trained Language Models (LLMs) to generate new candidate solutions. [90] LMX does this by combining the parent solutions into a prompt and then feeding that prompt into the LLM to collect offspring from the output. This approach is simple to implement and generates high-quality progeny across various domains, including binary strings, mathematical expressions, English sentences, image generation prompts, and Python code. Liu et al. [79] explore the potential of using Large Language Models (LLMs), such as GPT-4, to generate novel hybrid population intelligence optimization algorithms. The research focuses on using GPT-4 to identify and decompose six population algorithms that perform well in sequential optimization: particle swarm optimization (PSO), cuckoo search (CS), artificial bee colony algorithm\nPage 7 of 23\n(ABC), grey wolf optimizer (GWO), self-organizing migration algorithm (SOMA), and whale optimization algorithm (WOA) by constructing hints to guide the LLMs to search for the optimal from the current population\u2019s parent solution. INSTINCT (INSTruction optimization using Neural bandits Coupled with Transformers) [72] is a black-box LLMs cue optimization method. The method employs a novel neural band algorithm, Neural Upper Confidence Bound (NeuralUCB), in place of the Gaussian Process (GP) model in BO.NeuralUCB uses a neural network (NN) as a proxy while retaining the theoretical basis of the trade-off between exploration and exploitation in BO. Theoretical basis for the trade-off between exploration and exploitation. More importantly, NeuralUCB allows for the natural coupling of NN agents with hidden representations learned from pretrained transformers (i.e., open-source LLMs), significantly improving algorithmic performance.\n# 4.1.2. Multi-Objective Optimization Brahmachary et al. [13] introduce\nBrahmachary et al. [13] introduces an approach to numerical optimization using Large Language Models (LLMs) called Language-Model-Based Evolutionary Optimizer (LEO which leverages the reasoning capabilities of LLMs to perform zero-shot optimization across a variety of scenarios, including multi-objective and high-dimensional problems. LEO lies in its population-based strategy, which incorporates an elitist framework consisting of separate explore and exploit pools of solutions. This strategy not only harnesses the optimization capabilities of LLMs but also mitigates the risk of getting stuck in local optima. The method is distinct from other auto-regressive, evolutionary, or populationbased methods as it uses LLMs to generate new candidate solutions, providing a unique balance between exploration and exploitation. It is shown through a series of test cases that LEO is not only capable of handling single-objective but also of solving multi-objective optimization problems well. Liu et al. [75] leverage the capabilities of large language models (LLMs) to design operators for multi-objective evolutionary algorithms (MOEAs). The research addresses the challenges associated with the manual design of search operators in MOEAs, which often require extensive domain knowledge and can be time-consuming. The authors propose a method for decomposing the multi-objective optimization problem (MOP) into several single-objective subproblems (SOPs). LLMs are employed as search operators for each subproblem through prompt engineering. This allows the LLM to serve as a black-box search operator in a zero-shot manner without problem-specific training. Bradley et al. [11] introduce a new approach called Quality-Diversity through AI Feedback (QDAIF) that combines evolutionary algorithms and largescale language models (LLMs) to generate high-quality and diverse candidates for optimization algorithms. The core idea of QDAIF is to use language models to create variants and evaluate the quality and diversity of the candidates. The EA is responsible for maintaining the library of optimization algorithms and replacing the newly generated higher quality and more diverse solutions to the relevant positions in the\nHuang et al.: Preprint submitted to Elsevier\nlibrary based on the evaluation of the LLMs to achieve an iterative optimization search process.QDAIF can find a set of higher quality and more diverse solutions within the search space, which is a successful application of the LLMs in QD problems.\n# 4.2. LLMs as the Generator of Optimization Algorithms\n# 4.2. LLMs as the Generator of Optimization Algorithms Optimization algorithms usually require the de\nAlgorithms Optimization algorithms usually require the design of suitable optimization schemes based on the specified tasks. Due to the problem-understanding and algorithmic analysis capabilities of LLMs, the design of optimization methods based on LLMs can generate suitable optimization method selection and combination schemes compared to the optimization methods in the pre-LLM era [124, 123].\n# 4.2.1. Option Generate with the Cognitive of LLMs Zhang et al. [150] explore the use of large language m\nZhang et al. [150] explore the use of large language models (LLMs) to generate optimal configurations during Hyperparametric Optimization (HPO). The generation method does not rely on a predefined search space and consists of selecting parameters that can be optimized and specifying bounds for these parameters. Furthermore, the process treats the code of the specified model as hyperparameters to be output by the LLM, which goes beyond the capabilities of existing HPO methods. Liu et al. [80] proposes a system named AgentHPO, which leverages the advanced capabilities of LLMs to streamline the HPO process, traditionally a labor-intensive and complex task that requires significant computational resources and expert knowledge. AgentHPO lies in its unique architecture that incorporates two specialized agents: the Creator and the Executor. The Creator agent interprets task-specific details provided in natural language and generates initial hyperparameters (HPs), emulating the role of a human expert. It utilizes extensive domain knowledge and sophisticated reasoning to propose HP configurations that are expected to yield optimal model performance. Ma et al. [86] explored the effectiveness of Large Language Models (LLMs) as cueing optimizers. They find that LLM optimizers struggle to accurately identify the root cause of errors during reflection and often fail to generate appropriate cues for the target model through a single cueing optimization step, even when semantically valid. Further, they proposed a new paradigm of \"automated behavioral optimization\" designed to more controllably and directly optimize the behavior of the target model.\n# 4.2.2. Algorithm Generate with with Chain-of-Thought Liu et al. [77] proposed a approach\nLiu et al. [77] proposed a approach called Algorithmic Evolution using Large Language Models (AEL), which aims to automate the generation of efficient algorithms for specific optimization problems.AEL creates and improves algorithms by interacting with Large Language Models (LLMs) within an evolutionary framework, eliminating the need for model training and significantly reducing the need for\nPage 8 of 23\ndomain knowledge and expert skills. The constructive algorithms generated by AEL outperform hand-crafted heuristics and algorithms generated directly from LLMs based on the Traveling Provider Problem (TSP) example. Further, they introduce a improved framework for automated algorithm design that combines evolutionary computation and LLMs [76]. By automating algorithm design, combination, and modification, the AEL framework significantly reduces manual effort and eliminates the need for model training. The researchers used the AEL framework to design a Guided Local Search (GLS) algorithm for solving the TSP. The PromptBreeder (PB) system [41] utilizes the Chain-of-Thought Prompting strategy, which can significantly improve the reasoning ability of Large Language Models (LLMs) in different domains. It is a generalized mechanism for self-referential self-improvement of large language models (LLMs). At the heart of the system is self-reference: not only do the task prompts evolve, but the mutation prompts used to generate them also improve over time.PB outperforms existing state-of-the-art prompting strategies, such as Chain-of-Thought and Plan-and-Solve prompts, in several commonly used benchmark tests\u2014andSolve prompts. Pluhacek et al. [101] identified and decomposed six population algorithms that perform well on sequential optimization problems through LLM by augmenting the population. Enhanced Swarm Exploration and Exploitation Optimizer(ESEEO) aims to maintain population diversity and effectively balance exploration and exploitation by combining elements of Particle Swarm Optimization (PSO), Cuckoo Search (CS), and Artificial Bee Colony (ABC). Combining Limited Evaluation Population Optimizer (LESO) Designed to solve expensive optimization problems with a limited number of objective function evaluations, LESO combines the features of PSO, Grey Wolf Optimizer (GWO), and ABC to achieve effective exploration and exploitation within a limited number of assessments. LLMs are a challenge in Genetic Programming (GP) approaches. Bradley et al. [12] present an optimized algorithm generation tool for implementing LLMs that converts natural language descriptions into implementation code and automatically repairs program errors. In addition, this paper presents two uses of LLMs as evolutionary operators: difference modeling and LMX crossover. The former is a language model specialized for predicting code discrepancies, and the latter is a technique for generating candidate solutions using multiple parents. Similarly, Hemberg et al. [53] proposed a LLM-based GP algorithm, called LLM-GP, for generating optimization algorithm code. Unlike conventional GP algorithms, LLM-GP harnesses the power of pre-trained pattern matching and sequence complementation capabilities of the LLM. This unique feature allows for the design and implementation of genetic operators, paving the way for more efficient and effective algorithm generation.\nHuang et al.: Preprint submitted to Elsevier\n# 5. Optimization algorithms optimize large language models Large Language Models (LLMs) have exhibited e\nLarge Language Models (LLMs) have exhibited exceptional proficiency in many natural language processing tasks, encompassing text generation and sentiment analysis. Nevertheless, the task of optimizing their performance and efficiency continues to be a crucial obstacle, especially in intricate and unpredictable circumstances. Optimization Algorithms (OA) have emerged as a promising method to improve LLMs. OA utilizes the concepts of natural selection to perform repeated searches inside the parameter space of LLMs [24, 139]. It focuses on tasks like prompt engineering, model architecture optimization, hyperparameter setting, and multi-task learning. This holistic strategy seeks to discover optimal solutions for problems that have extensive search spaces, thereby enhancing the effectiveness of LLMs in several fields, such as natural language processing, software engineering, and neural architecture search. In this section, we will delve into the complexities of OA optimization for LLMs, exploring various strategies such as model tuning, prompt tuning and network architecture search to unlock the full potential of these transformative language models. The framework of the optimization algorithm to optimize LLMs is shown in Fig. 2.\n# 5.1. Optimize model tuning 5.1.1. Multi-task learning optimization Multi-task learning (ML) optimizatio\nMulti-task learning (ML) optimization of large models is an approach that uses optimization methods such as evolutionary algorithms to optimize model architectures for multiple tasks simultaneously. Through multi-task learning, researchers identify optimal architectures for multiple target tasks simultaneously in large pre-trained models [24, 78]. The advantage of this approach is the ability to share and interactively learn relevant information between different tasks, thus improving the generalization ability and efficiency of the model. Table 2 summarises the main optimization model tuning class methods The optimization strategy of multi-task learning reduces training costs, enhances model performance and improves adaptability across diverse domains and tasks by concurrently identifying optimal model architectures for multiple objectives using evolutionary algorithms [133]. For example, Choong et al. [30] proposed the concept of a diverse set of compact machine learning model sets designed to efficiently address multiple target architectures in large pre-trained models through an evolved multi-task learning paradigm. Baumann et al. [8] present an evolutionary multiobjective approach designed to optimize prompts in large language models (e.g., ChatGPT), demonstrating its effectiveness in crafting prompts in a sentiment analysis task that effectively captures conflicting emotions. Yang et al. [144] treated instruction generation as an evolutionary multiobjective optimization problem, using a large-scale language model to simulate instruction operators in order to improve the quality and diversity of generated instructions.\nPage 9 of 23\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ee4/4ee46b76-47db-48db-ace0-5b7eaafa74c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: OAs methodological framework for optimizing LLMs</div>\n<div style=\"text-align: center;\">Table 2 Summary of model tuning based on OAs</div>\n<div style=\"text-align: center;\">Summary of model tuning based on OAs</div>\nAlgorithms\nMain tasks\n0bjectives\nUsing OAs\nMulti-task\nlearning\n[30]\nJATs Implementation of One-off Diverse Com-\npact Set of Sets Machine Learning Models\nMulti-tasking\nand\nmulti-objective\noptimization\nNeuroevolutionary\nmultitasking\nEMO-Prompts [8]\nGenerate prompts that lead LLM to produce\ntext with two conflicting emotions\nEvolutionary\nMulti-\nObjective\nNSGA-II\nStructured\nPruning\n[59]\nSubparts of the network with optimal perfor-\nmance after fine-tuning for model compression\nMulti-objective NAS\nWeight Shared NAS\nLLM-Pruner [87]\nModel compression in a task agnostic manner\nSingle-objective\nLow-Rank Adaptation\nMeanwhile, Gupta and Bali et al. [7, 48] have investigated the use of multi-objective multi-task evolutionary algorithms, such as the MO-MFEA algorithm, in creating task-specific, small-scale models derived from Large Language Models (LLMs) in the field of online search architecture study. These specialized models are created from the LLM as a general base model and exhibit enhanced performance or more compression in different application fields and neural network designs.\n# 5.1.2. Based on structural pruning Structural pruning optimizes lar\nStructural pruning optimizes large language models (LLMs) by selectively removing non-critical coupled structures based on gradient information, effectively reducing model size while preserving functionality and ensuring taskagnosticism. Structural pruning is an essential optimization technique used to enhance pre-trained LLMs for subsequent tasks, such as text categorization and sentiment analysis. Structural pruning, as suggested by Klein [59], seeks to uncover numerous subnetworks of LLMs that achieve a compromise between performance and size, making them easier to use in different real-world applications. This approach employs a multi-objective local search algorithm to identify numerous Pareto-optimal subnetworks effectively. It does\nHuang et al.: Preprint submitted to Elsevier\nthis by minimizing evaluation costs via weight sharing. Ma et al. [87] propose the LLM-Pruner approach to optimize large language models by selectively removing non-critical coupling structures based on gradient information to achieve efficient compression while preserving functionality and task agnosticism. Gholami et al. [44] demonstrate that weight pruning can be used as an optimisation strategy for the Transfer architecture, proving that judicious pruning can significantly reduce model size without sacrificing performance, thus contributing to bridging the gap between model efficiency and performance.\n# 5.2. Prompt optimization Prompt tuning, often ref\n5.2. Prompt optimization Prompt tuning, often referred to as optimization for prompt tuning, is a method used to fine-tune large language model (LLMs) prompts. This methodology does not need access to the underlying model parameters and gradients. This strategy is especially beneficial for closed-source models that have limited access. Prompt optimisation enhances the efficacy of model creation in fewer or zero-shot scenarios by fine-tuning the input prompts. Optimization algorithms (OAs), renowned for their adaptability and efficiency in situations where the internal workings are unknown, are used to discover prompts that improve job performance just by using\nPage 10 of 23\n<div style=\"text-align: center;\">Table 3 Summary of optimization prompts</div>\n<div style=\"text-align: center;\">Summary of optimization prompts</div>\nAlgorithms\nMain tasks\nTypes\nUsing OAs\nBBT [119]\nOptimising large pre-trained language models\npublished as a service\nBlackbox optimization\nDerivative-Free\nOptimization, CMAES\nBBTv2 [118]\nAdjustment of cues by gradient-independent\nmethods\nBlackbox optimization\nCMAES\nBBT-VLMS [148]\nBlack-box optimization of visual language mod-\nels\nBlackbox optimization\nDerivative-Free\nOptimization,CMAES\nTextual\ninversion\n[40]\nOptimizing the embedded representation of spe-\ncific concepts in text\nContinuous\nIterative\nEvolutionary\nStrategy,CMAES\nClip-tuning [21]\nEnabling\noptimisation\nof\nprompts\nwithout\nmodel weight access rights\nContinuous\nDerivative-free optimiza-\ntion, CMAES\nRGLF [110]\nEffective hint tuning for LLMs in resource-\nconstrained black-box API settings\nContinuous\nCMAES\nGrIPS [102]\nPrompts for Improving Large Language Models\nDiscrete\nEdit-based\nSearch,\nGe-\nnetic Algorithms\nGPS [155]\nAutomatically\nsearch\nfor\nhigh-performance\nprompts to optimize model performance for\nspecific tasks\nDiscrete\nGenetic Algorithm\nPlum [99]\nOptimizing and customising large pre-trained\nlanguage models\nDiscrete\nMetaheuristics\nthe outcomes of LLMs reasoning. Prompt tuning may be categorized into two distinct types: continuous and discrete. Continuous prompt tuning employs continuous optimisation algorithms, such as CMAES [51], to improve the quality of embedding prompts [24]. This process involves techniques like partitioning and subspace decomposition to enhance the embedding space. Conversely, discrete prompt tuning uses discrete optimization algorithms to explore the prompt space directly. It utilizes specialized genetic operators to adjust prompts in order to address the problem of combinatorial explosions in the search space. Table 3 summarises the main methods of optimizing prompts\n# 5.2.1. Continuous prompt optimization Continuous prompt tuning is used to op\nContinuous prompt tuning is used to optimize the performance of LLMs and is often employed to tune the embedding of prompts. The embedding vectors of the prompts are iteratively tuned to maximize the performance of the model on a particular task, thus improving the quality and performance of the model generation [24]. Continuous prompt tuning typically explores different strategies and techniques, such as stochastic embedding, subspace decomposition, and knowledge distillation, in order to improve the embedding quality and the search performance for the optimization of large models. For example, Sun et al. [118] presented BBTv2, an improved version of Black-Box Tuning, which uses a divide-and-conquer gradient-free algorithm to optimize prompts at different layers of pre-trained models, achieving comparable performance under few-shot settings. Fei et al. [40] introduced a gradient-free framework for optimizing continuous textual inversion in an iterative evolutionary strategy. It accelerates the optimization process with minimal performance loss and compares performance with gradient-based models with variant GPU/CPU platforms.\nHuang et al.: Preprint submitted to Elsevier\nPryzant et al. [103] proposed Automatic Prompt Optimisation (APO) using numerical gradient descent techniques to automatically improve the prompts of Large Language Models (LLMs), obtaining significant performance gains in various NLP tasks and jailbreak detection. Zheng et al. [158] Black-box prompt optimization using subspace learning (BSL) enhances the versatility of prompt optimization across tasks and LLMs by identifying common subspaces through meta-learning, ensuring competitiveness across a variety of downstream tasks. Recently, some researchers have used techniques such as knowledge distillation, variational reasoning, and federated learning to improve search efficiency, generalization, and security. For example, Shen et al. [110] presented techniques for adapting large pre-trained language models (PLMs) to downstream tasks using only black-box API access, achieving competitive performance with gradient-based methods while also considering predictive uncertainty in prompts. Sun et al. [117] present a set of techniques for improving the efficiency and performance of black-box optimization (BBTRGB) for tuning large language models without access to gradient and hidden representations, demonstrating its effectiveness in a variety of natural language understanding tasks. Han et al. [50] proposed GDFO, which ensembles gradient descent and derivative-free optimization for optimising task-specific successive prompts of large pre-trained language models in black-box tuning scenarios, obtaining significant performance gains over previous state-of-theart approaches. Sun et al. [116] proposed FedBPT for federated black-box prompt tuning, a framework for efficient and privacy-preserving fine-tuning of pre-trained language models through collaborative prompt optimization without access to model parameters, reducing communication and memory costs while maintaining competitive performance.\nPage 11 of 23\nChai et al. [21] introduced the Clip-Tuning technique to enhance search efficiency and offer more detailed and diverse evaluation feedback throughout the black-box tuning procedure. Clip-tuning differs from utilizing a single random projection matrix to reduce dimensionality in BBT. Instead, it utilizes pre-trained sampling on dropout models during inference. This process generates many subnetworks that act as predictive projections of samples in the original highdimensional space. The search technique achieves faster convergence to the ideal solution by aggregating rewards from predictions made by several subnetworks. Continuous prompt tuning is a method for optimising the performance of large language models where prompts are represented as continuous vectors that exist in a continuous embedding space. These vectors are optimized in a continuous embedding space. The search space is continuous and can be differentiated, allowing the use of gradientbased optimization techniques. Continuous prompt tuning is suitable for black-box optimization scenarios where the internal structure of the model and gradient information are not accessible. However, this approach may require more computational resources due to the complex mathematical operations and gradient calculations involved.\n# 5.2.2. Discrete prompt optimization Discrete prompt optimization is a\nDiscrete prompt optimization is a method for finding optimal prompts in a pre-trained language model, where the prompts are represented as discrete text sequences. These methods typically use genetic algorithms, particle swarm optimization, or other heuristic-based search methods to search in a discrete prompt space to find the optimal prompt sequence [139, 24]. Unlike continuous prompt tuning, discrete prompt optimization focuses more on tuning prompts at the level of text sequences and is suitable for tasks based on text sequences, such as text generation or classification. Before the emergence of large language models, researchers have been inclined to investigate the application of optimization methods to enhance the performance of pre-trained language models. For instance, Greedy Teaching Prompt Search (GrlPS) [102] uses a stepwise search approach without gradients, whereas Genetic Prompt Search (GPS) [142] is grounded in the ideas of genetic algorithms. The majority of these research employ evolutionary algorithms (EAs) as the primary search engine, whereas the language model is tasked with generating and assessing potential prompts [155]. Within the discrete prompt space, specialized genetic operators are employed to fine-tune heuristics and directly identify the most optimal prompts. This, in turn, enhances the quality of the model\u2019s response to a given task. Typically, these studies employ optimization algorithms as a search framework, where Large Language Models (LLMs) are used to generate and evaluate prompts. Nevertheless, it is important to acknowledge that this research mainly concentrates on particular rapid engineering situations and has restricted breadth. To fully harness the potential of discrete optimization in black-box prompt optimization, it is necessary to tackle the issue of combinatorial\nHuang et al.: Preprint submitted to Elsevier\nexplosion in discrete search spaces. For example, Zhou et al. [161] proposed a simple black-box search method called ClaPS, which achieves state-of-the-art performance on a variety of tasks and LLMs while significantly reducing the search cost by clustering and pruning the search space to focus on key prompting tokens that affect LLM prediction. Yu et al. [148] proposed a black-box prompt tuning framework for visual-verbal models, which optimizes visual and verbal prompts in an intrinsic parameter subspace through an evolutionary strategy, enabling task-relevant prompt learning without back-propagation. Pan et al. [99] introduced meta-heuristic algorithms as a generic prompt learning method, and demonstrated their effectiveness in black-box prompt learning and Chain-of-Thought prompt tuning by testing six typical methods and were able to discover more understandable prompts, opening up more possibilities for prompt optimisation. Lapid et al. [62] proposed a method for attacking large language models using genetic algorithms to reveal the vulnerability of the models to malicious manipulation and to provide a diagnostic tool for assessing and enhancing the consistency of language models with human intentions. Guo et al. [46] presented EvoPrompt, a discrete prompt optimization framework for the automatic optimization of LLMs prompts using evolutionary algorithms. By linking LLMs and EAs, the method achieved significant performance improvements over manually designed prompts and existing automatic prompt generation methods on 31 datasets, demonstrating the potential of combining LLMs and EAs. Pinna et al. [100] present a method for improving the generation of code for large language models using genetic improvement techniques, which significantly improves the quality of the generated code through user-supplied test cases, demonstrating the potential of combining LLM with evolutionary techniques. Generally, discrete prompt optimisation is a method to optimise the performance of large language models in a black-box environment, which searches for optimal prompt words or phrases in a discrete prompt space through techniques such as genetic algorithms, heuristic search, and clustering pruning without the need for internal gradient information of the model. The advantages include effective performance improvement without relying on the internal information of the model in a black-box environment, and adaptability to tasks with a small number of samples or zero samples, while the disadvantages include the possibility of facing a huge search space, the tendency to fall into local optimums, the sensitivity of hyper-parameters, the limited ability of generalisation, the difficulty of interpreting the results, and the high dependence on the choice of evaluation metrics.\n# 5.2.3. Blackbox optimization prompt tuning Black-box optimization of large models refer\nBlack-box optimization of large models refers to the process of optimizing and tuning large pre-trained models (e.g., large language models) within a black-box optimization framework. Compared to traditional black-box optimization, black-box optimization of large models is more challenging\nPage 12 of 23\nbecause the complexity of large pre-trained models and a large number of parameters makes the optimization process more complex and time-consuming. In black-box optimization of large models, optimization algorithms typically optimize the performance of pre-trained models by interacting with them and adjusting their inputs or parameters step-bystep without having direct access to the internal structure or parameters of the model. In recent years, a number of researchers have focused on the application of black-box optimization to large-scale language models (LLMs) and visual-linguistic models, proposing a variety of methods to optimize model performance without accessing the model\u2019s internal parameters or gradients. Yu et al. [149] optimize a visual language model using a dialogue-feedback-based approach. Guo et al. [47] introduced the collaborative black-box tuning (CBBT) technique. Sun et al. [119] develop the a black-box tuning framework for Language Models as a Service (LMaaS). Diao et al. [37] proposed a black-box discrete prompt learning (BDPL) algorithm. And the work of Yu et al. [148] introduces a blackbox prompt tuning framework for visual language models. These studies demonstrate that in black-box scenarios where model weights cannot be directly modified, external prompt learning and optimization are used to effectively improve model performance in image classification, text-to-image generation, and adaptation to different downstream tasks.\n# 5.3. Self-Tuning optimization Compared to the initialization\n5.3. Self-Tuning optimization Compared to the initialization method under the EvoPrompt framework, which relies on hand-prompted optimization. In recent years some researchers have proposed automated prompting methods. Use as a gene operator in EAs to automatically create high-quality prompts for yourself and others. Table 4 summarises the main Self-Tuning optimization methods. For example, Singh et al. [113] applied an interpretable auto-prompt (iPrompt) to generate a natural language string that explains the data. Fernando et al. [41] propose a self-improving mechanism for PromptBreder that evolves and adapts cues for different domains, outperforming existing strategies on arithmetic, common-sense reasoning, and hate speech classification tasks. Pryzant et al. [104] proposed a simple and non-parametric solution, Automated Prompt Optimisation (APO), which automatically performs fast improvement prompts by using techniques inspired by numerical gradient descent. Li et al. [68] proposed SPELL, a black-box evolutionary algorithm that uses a large language model to automatically optimize text style cues, demonstrating rapid improvements for a variety of text tasks. Furthermore, LLMs can serve as a flexible prompt selector for jobs that are not inside the domain it was trained on. Self-tuning can operate inside a versatile language domain without being dependent on parameter updates [24]. For example, Zhang et al. [154] proposed Auto-Instruct, an approach that utilizes the generative power of LLMs to automatically improve the quality of instructions for a variety of tasks, going beyond manually written instructions\nHuang et al.: Preprint submitted to Elsevier\nand existing baselines in a variety of out-of-domain tasks, with significant generalisability to other LLMs.\n# 5.4. Optimize network architecture search Prompt-based optimization tools improve the \n5.4. Optimize network architecture search Prompt-based optimization tools improve the quality of model output by optimizing the input format. Another approach known as LLM Network Architecture Search (NAS) focuses on directly optimizing the architecture of LLM models, and in the context of Large Language Models (LLMs), NAS can take a different form by optimizing the architecture of the model directly rather than by tuning the parameters of the model. Table 5 summarises the main optimized network architecture search search methods As the complexity of neural network models increases, manually designing efficient network architectures becomes time-consuming and challenging. NAS eases the burden on researchers by automating the design process, allowing efficient exploration of the vast search space to discover more efficient, generalized and less resource-intensive model architectures [81, 139, 163]. Previously NAS was optimized by simulating the process of natural selection. It involves the steps of randomly generating an initial population, selection, crossover (or recombination, as it is called), and mutation until termination conditions are met [39]. With the development of deep learning techniques and the increase of arithmetic power, the NAS field is also exploring new optimization strategies to optimize large models. With the increase of computational resources and the proposal of new algorithms, the efficiency and effectiveness of NAS have been significantly improved. Nasir et al. [95] proposed a new NAS algorithm that effectively combines the advantages of LLMs and Quality Diversity (QD) algorithms to automate the search and discovery of high-performance neural network architectures. So et al. [115] proposed an evolutionary Transformer discovered through evolutionary architectural search in multilingual tasks superior Transformer that achieves better performance with fewer parameters and maintains high quality even at smaller sizes. More sophisticated and effective search strategies have been proposed by researchers in recent years to improve the performance of large models. For example, Gao et al. [43] proposed an automatic method (AutoBERT-Zero) for discovering the backbone structure of a general-purpose language model (LLM) using a well-designed search space and an operation-first evolutionary strategy, as well as a twobranch weight-sharing training strategy, to improve search efficiency and performance. Ganesan et al. [42] perform task-independent pre-training of BERT models while generating differently shaped sub-networks by varying the hidden dimensions in the Transformer layer. Rather than optimizing for a specific task, it generates a series of different-sized models by varying the hidden dimensions of the network, which can be fine-tuned for various downstream tasks. Yin et al. [147] proposed the use of one-shot Neural Architecture Search (one-shot NAS) to automatically search for architectural hyperparameters. A large SuperPLM is obtained through one-shot learning, which can be used as a proxy for\nPage 13 of 23\n<div style=\"text-align: center;\">Table 4 Summary of self-Tuning optimization</div>\n<div style=\"text-align: center;\">Summary of self-Tuning optimization</div>\nAlgorithms\nMain tasks\nObjectives\nUsing OAs\nSPELL [68]\nCombining Evolutionary Algorithms and Text\nGeneration Capabilities of LLMs to Optimise\nPrompts for LLMs in a Black Box Environment\nClassification accuracy\nVariants based on evolu-\ntionary algorithms\nPromptbreeder [41]\nImproves LLM performance on specific tasks\nby automating the cue search and optimisation\nprocess without the need to manually engineer\nprompts\nPerformance score\nGenetic Algorithm\nInterpretable Auto-\nprompting [113]\nIteratively use LLM to generate explanations\nand reorder them based on their performance\nwhen used as prompts\nAccuracy\nand\ninterpretability\nIterative local search al-\ngorithm\nAPO [104]\nAutomatically improves prompts to reduce the\nheavy trial and error required to write prompts\nmanually\nPerformance of Initial\nPrompts\nTextual gradient descent\nAuto-Instruct [154]\nAutomatic generation and optimisation of in-\nstructions for LLMs\nClassification accuracy\nMetaheuristic algorithms\nall potential sub-architectures. An evolutionary algorithm is also used to search for the best architectures on the SuperPLM, and then the corresponding sub-models are extracted based on these architectures and further trained. Javaheripi et al. [55] proposed a no-training Neural Architecture Search (NAS) algorithm for finding Transformer architectures that have an optimal balance between task performance (perplexity) and hardware constraints (e.g., peak memory usage and latency). Zhou et al. [162] proposed a Transformer architecture search method called T-Razor, which uses zerocost agent-guided evolution to improve the search efficiency and evaluates and ranks Transformers by introducing metrics such as synaptic diversity and synaptic saliency to efficiently find optimized architectures in the Transformer search space. Klein et al. cites klein2023structural proposed Neural Architecture Search (NAS) based on weight sharing as a structural pruning method for finding the optimal balance between optimization efficiency and generalization performance to achieve compression of large language models (LLMs) in order to reduce the model size and inference latency. Overall, the main advantage of NAS for optimising large models is its ability to automate the exploration and discovery of efficient network architectures for specific tasks, significantly improving model performance while reducing manual design and tuning efforts. With intelligent search strategies, NAS helps save computational resources and time. However, this approach also faces challenges, including the large search space, the possibility of falling into local optimal solutions, and the large amount of computational resources required in the initial search and training phases. In addition, the selection and tuning of optimization algorithms require expertise, and the generalization ability of the network architecture obtained from the search still needs further validation. Future research may focus on improving the search efficiency, reducing the computational cost, and enhancing the generalisability and adaptability of the model.\nHuang et al.: Preprint submitted to Elsevier\n# 6. Application of LLMs-based Optimization Algorithms\nAs show in Fig. 3, optimization algorithms are pivotal in various applications, broadly categorized into software programming, neural architecture search and content generation. LLM-based optimization algorithms are becoming increasingly important in artificial intelligence, especially in machine learning. They are used for software programming and neural architecture search to help design efficient network architectures. Furthermore, these algorithms are employed as innovative tools in content generation, optimizing the creation process to produce relevant and engaging content. This bifurcation in application highlights the versatility and evolving role of optimization algorithms in addressing both conventional challenges and pioneering technological advancements.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/70da/70dafc44-c551-4c84-b71e-0452f84f73e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: The split of LLMs assist optimization algorithms.</div>\n# 6.1. Assisted Optimization Programming: Software Programming In the wave of artificial intelligence, optimiza\nSoftware Programming In the wave of artificial intelligence, optimization algorithms based on LLMs have gradually become an important research area to promote code generation and software development. With many parameters and deep learning capabilities, LLMs have shown powerful capabilities in many fields, such as natural language processing, image recognition, etc. Especially in the process of software development, the application of large models can improve the efficiency\nPage 14 of 23\n<div style=\"text-align: center;\">Summary of OAs-based network architecture for LLMs</div>\nAlgorithms\nMain tasks\nObjectives\nUsing OAs\nAutobert-zero [43]\nAutomatic exploration of new self-attentive\nstructures and overall efficient pre-trained lan-\nguage model backbone architecture\nClassification\nperformance\nOperation-Priority NAS\nAutoTinyBERT\n[147]\nAutomatic hyperparameter optimization for effi-\ncient compression of pre-trained language mod-\nels\nClassification\nperformance\nOne-shot NAS\nLlmatic [95]\nDiscover diverse and powerful neural network\narchitectures\nNetwork architecture\nQuality-Diversity\noptimization\nSuperShaper [42]\nDiscovering networks with effective trade-offs\nbetween accuracy and model size\nClassification accuracy\nEvolutionary algorithm\nEvoPrompting [25]\nExploring the use of LLMs as generalised adap-\ntive variation and crossover operators for NAS\nalgorithms\nClassification accuracy\nEvolutionary algorithm\nof code generation and further enhance the model performance through optimization algorithms. By automatically generating code for training models, non-professionals can also easily train efficient machine learning models, greatly reducing the technical threshold and expanding the audience for machine learning technology. Meanwhile, code integration practices in model development, such as static code integration and dynamic code integration, also play a key role in improving the efficiency and quality of software development. Weyssow et al. [134] explore using Large Language Models (LLMs) for code generation tasks, focusing on Parameter-Efficient Fine-Tuning (PEFT) optimization techniques. The methodology aims to optimize the fine-tuning process of LLMs by updating a small portion of the model\u2019s parameters instead of all of them using the PEFT technique to achieve efficient fine-tuning in resource-constrained environments. Cassano et al. [19] present a system called MultiPL-E, which is a system for translating code generation benchmark tests from Python to other programming languages. Further, Pinna et al. [100] point out the application of automatic code generation based on problem descriptions and that even the most efficient LLMs often fail to generate correct code. Therefore, to address the question of how to enhance code generation based on Large Language Models (LLMs) through a Genetic Improvement (GI) approach, an evolutionary algorithm-based approach is proposed that uses Genetic Improvement (GI) to improve LLM-generated code using a collection of user-supplied test cases. Program synthesis (PS), a form of automation, aims to reduce the time and effort required for software development while improving code quality. Although Genetic Programming (GP) is a competing approach to solving the program synthesis problem, it has limitations in evolving syntactically correct and semantically meaningful programs. Tao et al. [122] used a combination of Generative Pre-trained Transformers (GPTs) and Grammar-Guided Genetic Programming (G3P) to solve the program synthesis problem. GPTs) and Grammar-Guided Genetic Programming (G3P) to address\nHuang et al.: Preprint submitted to Elsevier\nthe program synthesis problem. The OpenAI team has developed a language model called CodeX [26], which has been fine-tuned using publicly available code on GitHub and investigated for its ability to write Python code. A special production version of Codex is GitHub Copilot, a programming aid. Brownlee et al. [15] explored how large language models (LLMs) can be applied to variation operations in Genetic Improvement (GI) to improve the efficiency of the search process.GI is a search-based technique used to improve the non-functional attributes, such as execution time, and functional attributes, such as fixing defects, of existing software. Ji et al. [57] specifically presents a research overview of the assessment and interpretation of code generation capabilities based on large language models (LLMs), including two main phases: data collection and analysis. In the data collection phase, the prompts\u2019 features are quantified by extracting their linguistic features and performance metrics from the generated code. In the analysis phase, causal diagrams are constructed using causal discovery algorithms and further analyzed to identify principles of hint design. Codex [26] was fine-tuned using publicly available GitHub code to enhance its Python coding capabilities. The method evaluated Codex using a new test set, HumanEval, which assesses the functional correctness of programs synthesized from documentation strings.In this dataset, Codex successfully solved 28.8% of the problems, compared to GPT-3, which solved none, and GPT-J, which solved 11.4%. CODAMOSA [66] integrates a pre-trained Codex with Search-Based Software Testing (SBST) to enhance test case code coverage. SBST generates high-coverage test cases by combining test case generation with mutation for programs under test. However, SBST may face stagnant coverage, meaning it struggles to produce new test cases that increase coverage. When SBST\u2019s coverage improvement stagnates, the CODAMOSA algorithm aids in relocating to more advantageous search space areas by using Codex to generate example test cases for functions with lower coverage. Wu et al. [137] highlight the advances in code generation by\nPage 15 of 23\nlarge-scale language models (LLMs) and the associated security risks, particularly the critical vulnerabilities in the generated code. Although some LLM providers have sought to mitigate these issues through human guidance, their efforts have yet to yield robust and reliable code LLMs in practical applications. They introduce the DeceptPrompt algorithm, designed to generate adversarial natural language instructions that prompt code LLMs to produce functionally correct yet vulnerable code. DeceptPrompt employs a systematic, evolution-based algorithm with a fine-grained lossy approach. The algorithm uniquely excels at identifying natural prefixes/suffixes with benign, non-directional semantics and effectively induces code LLMs to generate vulnerable code. This feature enables researchers to conduct near-worstcase red-team tests on these LLMs in real-world scenarios through natural language. In summary, in software programming, large language models (LLMs) enhance code generation efficiency via optimization algorithms and reduce the complexity of machine learning technology. This simplification allows non-experts to train efficient models, thereby broadening the reach of machine learning.\n# 6.2. Assisted Optimization Framework: Neural Architecture Search Neural Architecture Search (NAS), an important t\nArchitecture Search Neural Architecture Search (NAS), an important technique for the automatic design of neural networks, is undergoing a transformation driven by combining LLMs and optimization algorithms. With their massive parameters and deep learning capabilities, LLMs show unprecedented potential in handling complex tasks. At the same time, the combination of well-designed optimization algorithms can further accelerate the Neural Architecture Search process, improving the search efficiency and the performance of the resulting models. With the continuous application of big models in NAS, we see their great potential in the automatic search and optimization of neural network structures, which not only greatly saves labor costs but also improves the innovation and diversity of model design. Nasir et al. [95] present LLMatic, a large model-based Neural Architecture Search (NAS) algorithm that uses two QD archives to search for competitive networks, which combines the code generation capabilities of Large Language Models (LLMs) with the diversity and robustness of Quality Diversity (QD) algorithms.LLMatic utilizes the LLMs to generate new architectural variants and combines the QD algorithms (especially the MAP-Elites algorithm) to discover diverse and robust solutions. Chen et al. [25] found that an approach combining evolutionary prompt engineering and soft prompt tuning, EvoPrompting, consistently discovers diverse and high-performance models. A method for creating and curating data using evolutionary search to improve in-context prompting examples for LM is presented. While focused on neural architecture design tasks, this approach is equally applicable to LM tasks that rely on in-context learning (ICL) or cue tuning. Jawahar et al. [56] build new uses for Performance Predictors (PP) by using Large Language\nHuang et al.: Preprint submitted to Elsevier\nModels (LLMs) that predict the performance of specific Deep Neural Network (DNN) architectures on downstream tasks. A hybrid search algorithm (HS-NAS) is proposed, which uses LLM-Distill-PP in the initial phase of the search and a baseline predictor for the remainder of the search.HSNAS reduces the search time by about 50% with performance comparable to that of the SOTA NAS and sometimes improves latency, GFLOPs, and model size. Jawahar et al. [56] introduced LLM-PP, a precise performance predictor developed using LLM for few-shot prompting. It achieves a mean absolute error (MAE) comparable to the state of the art (SOTA). LLMDistill-PP, developed as a more costeffective predictor, caters to applications like Neural Architecture Search (NAS) that require numerous predictions. Additionally, the new HS-NAS algorithm is introduced. It leverages the strengths of LLMDistill-PP and the state-ofthe-art performance estimator, reducing NAS search times by half and identifying more efficient architectures. Zheng et al. [157] explored the potential of GPT-4 models for the Neural Architecture Search (NAS) task of designing effective neural network architectures. At the same time, they propose an approach called GPT-4 Enhanced Neural archItectUre Search (GENIUS), which leverages the generative power of GPT-4 as a black-box optimizer to navigate the architectural search space quickly, identify promising candidate architectures, and iteratively refine these candidate architectures to improve performance. EvoPrompting [25] employs advanced Language Models (LMs) for code-level Neural Architecture Search (NAS). This approach integrates evolutionary prompt engineering with soft-prompt tuning. It aims to iteratively refine contextual prompts and enhance prompt tuning on LMs, thereby boosting their capacity to generate innovative and diverse solutions for complex reasoning tasks. Radford et al. [105] describe a method to enhance Natural Language Understanding (NLU) using Generative Pre-Training. They show that this approach significantly boosts performance across various NLU tasks by initially pre-training a language model on a vast corpus of unlabeled text, then applying supervised fine-tuning for particular tasks. Chowdhery et al. [31] proposed PaLM (Pathways Language Model), a large-scale language model, PaLM has demonstrated excellent performance on a variety of Natural Language Processing (NLP) tasks, and PaLM also has very good performance on network structure design, structure search. In summary, integrating LLMs with optimization algorithms in neural network architecture search (NAS) enhances search efficiency, fosters innovation, diversifies model designs, and opens new avenues for the automated design of complex neural architectures.\n# 6.3. Assisted Optimization Generation: Content Innovation Generation Innovative content generation has become a key driv\nInnovation Generation Innovative content generation has become a key driver for developing media, entertainment, arts, and scientific discovery. Applying big artificial intelligence models combined with optimization algorithms is increasingly important in\nPage 16 of 23\nthis process. In summary, using optimization algorithms based on large models in innovative content generation is not only about the innovation and diversity of content but also promotes and facilitates scientific and technological innovation development. Xiao et al. [141] proposed a pattern-centric text generation framework, PatternGPT, to address the error-prone nature of Large Language Models (LLMs) and the inability to use external knowledge in text generation tasks directly. The framework uses algorithms to search for or generate high-quality patterns based on judgmental criteria. It leverages the pattern extraction capabilities of LLMs to develop a diverse set of structured and formalized patterns, which can help to bring in external knowledge for computation. Chen et al. [28] enhance the performance of Large Language Models (LLMs) in language generation tasks through Model-Adaptive Prompt Optimization (MAPO), a prompt optimization method that can be widely applied to various downstream generation tasks. Similarly, they propose a new paradigm for news summary generation that uses Large Language Models (LLMs) to improve the quality of news summary generation through evolutionary fine-tuning [140]. The method uses LLM to extract multiple structured event patterns from news passages, evolves a population of event patterns via a genetic algorithm, and selects the most adapted event patterns to input into LLM to generate news summaries. PanGu Drug Model [73] is a graph-to-sequence asymmetric conditional variational autoencoder designed to improve molecular property representation and performance in drug discovery tasks. The model is inspired by conversions between molecular formulas and structural formulae in the chemistry classroom and can appropriately characterize molecules from both representations. Liang et al. [70] presented a prototype of a DrugChat system designed to provide ChatGPT-like capabilities for drug compound analysis.DrugChat, by combining graph neural networks (GNNs), large language models (LLMs), and adapters, enables users to upload molecular maps of compounds and ask various questions during multiple rounds of interaction. Diagrams and ask different questions in numerous rounds of interaction, which the system then answers. To break the bottleneck of literate graph technology, Berger et al. [10] proposes the framework of StableYolo, which aims to optimize the image generation quality of large language models (LLMs) by applying evolutionary computation to the Stable Diffusion model while adjusting the prompts and model parameters. The core idea of StableYolo is to improve the image generation quality of photo-realistic styles by combining visual evaluation with multi-objective search. The core concept of StableYolo is to enhance the quality of image generation in photo-realistic style by combining visual evaluation with multi-objective search. The system uses the confidence estimate of the Yolo model as a fitness function and searches for the optimal combination of cue words and model parameters using a Genetic Algorithm (GA). To explore additional research related to LLMs, including cognitive functions of LLMs, behavior and learning\nHuang et al.: Preprint submitted to Elsevier\nin game-theoretic environments, and Big Five personality traits, Suzuki et al. [120] propose a model for the evolution of personality traits based on Large Language Models (LLMs), specifically those related to cooperative behavior. The approach demonstrates how LLMs can enhance the study of human behavioral evolution and is based on evolutionary game theory by using an evolutionary model that assumes that human behavioral choices in game-theoretic situations can be simulated by providing LLMs with high-level psychological and cognitive trait descriptions. De et al. [34] explored the phenomenon of the self-organized formation of scale-free networks in social interactions between large language models (LLMs). Scale-free networks are a typical emergent behavior in complex systems, especially in online social media, where users can follow each other and form social networks with specific structural features. Lu et al. [82] propose a novel learning framework, SELF (Self-Evolution with Language Feedback), which aims to continuously enable large-scale language models (LLMs) to improve themselves through self-feedback and self-improvement. The SELF framework is inspired by the human self-driven learning process, which consists of an initial attempt, reflective feedback, and The SELF framework is inspired by the human self-driven learning process, which involves a cycle of initial attempts, reflective feedback, and behavioral improvement to improve the model\u2019s capabilities. The ELF framework also enables smaller LLMs to improve themselves, which can be reversed to facilitate the development of larger predictive models. In summary, the proposed systems and frameworks, including DrugChat and SELF, illustrate the development of personalized, intelligent tools for analyzing drug compounds, generating news summaries, and mimicking human behaviors. These tools continuously improve their performance through self-learning and feedback mechanisms, enhancing efficiency and accuracy in related fields.\n# 7. Future Outlook and Research Trends\nIn the previous sections, we have examined recent advances in the fields of long-term memory models (LLMs) and optimization algorithms (OAs). Nonetheless, there are still many challenges and unresolved issues between these two fields. Therefore, the aim of this section is to explore directions for future research in order to provide scholars with the opportunity to explore new areas beyond the boundaries of current knowledge, to ask new research questions, and to reinvigorate the field. Theoretical Foundations and Methodologies. Experimental studies have confirmed the effectiveness of combining large-scale language models (LLMs) with OAs in solving small-scale problems [79, 90]. However, the motivation for their interaction has not yet been clarified. To further promote the performance of algorithms, we need to deeply explore the mechanism of mutual reinforcement between LLMs and OAs in theoretical studies and analyze their complementary advantages and potential problems in practical\nPage 17 of 23\napplications in detail through large-scale empirical studies. In addition, it is crucial to conduct in-depth theoretical analyses of algorithms combining LLMs and OAs, which includes evaluating their convergence, time complexity and space complexity. Also, investigating the impact of algorithmic parameter settings on performance, as well as performance guarantees or theoretical limitations of the algorithms on different problem types, are key steps in advancing the algorithms. Further, exploring optimization theory [78], such as clarifying the definition and characterization of the objective function, dealing with constraints, and analyzing the feasible solution space of a problem, will provide a solid theoretical foundation for the design and application of algorithms to achieve better algorithmic performance in solving more complex problems. Automated Intelligent Optimization. In the optimization context, large language models (LLMs) show significant potential, especially in enhancing the automation and intelligence of optimization algorithms (OAs). Learning from multimodal data during the pre-training phase allows LLMs to understand and generate cross-modal content [138]. This provides a new search and mutation strategy for OAs when performing cross-modal operations. This capability of LLMs can facilitate OAs in achieving a more efficient global search in multimodal optimization problems. At the same time, as the technology of LLMs continues to advance, it is expected that they will drive the performance of OAs in modeling complex evolutionary mechanisms, especially when dealing with optimization problems with large-scale search spaces [32]. However, current research has yet to explore the potential of LLMs in evolutionary optimization, and there remain challenges, such as how to combine LLMs and OAs better and how to handle complex search spaces. In addition, the pre-training of LLMs on large amounts of textual data embeds them with rich domain knowledge, which provides a robust knowledge base for OAs.LLMs can assist OAs in better integrating domain-specific knowledge in the optimization process, thus improving the efficiency of optimization and the quality of solutions. For example, LLMs can generate high-quality initial solutions, improve problem formulation, and provide solution coding and definition of solution spaces. In addition, LLMs can provide guiding principles for algorithm design, enabling EAs to handle complex optimization problems such as multiobjective, discrete and dynamic more effectively [121]. With the rapid development of LLMs technology, they are expected to play an even more critical role in the future evolutionary optimization field, driving the field toward higher levels of automation and intelligence. Robustness and Prompt Engineering. Utilizing optimization techniques is a crucial method for improving the capabilities of LLMs in engineering applications. Common approaches involve utilizing LLMs as optimization operators within EIA frameworks to consistently produce fresh prompt. This technique has consistently shown efficacy and superiority in numerous investigations. Nevertheless, certain\nHuang et al.: Preprint submitted to Elsevier\nobstacles persist. Firstly, it is crucial to pay close attention to the initialization of the optimization process as it will have a substantial impact on the outcomes [25, 68]. It is crucial to have cue templates that are generic and customizable in order to provide accurate and valid prompts. Random initialization may not be capable of using existing information, and manual seeding may add bias. In addition, when confronted with issues that contain a significant amount of previous knowledge, the range of possible prompts to consider increases exponentially as the length of the cue and the size of the vocabulary grow. This can result in over fitting or becoming trapped in local optimal solutions. Furthermore, these approaches lack stability and strongly depend on the capabilities of the LLM, rendering them susceptible to stochasticity [68]. If the LLM lacks the ability to comprehend and efficiently employ the cues, it may undermine the effectiveness of the approach. Further research should strive to tackle these obstacles by creating more resilient techniques. For instance, in the context of initialization, the technique of multisource seeding can be investigated to automatically improve the size and quality of the initial population utilising LLM. When dealing with intricate search spaces, it is essential to develop efficient optimisation algorithms. This may involve combining more comprehensive sets of optimisation operators, using the advantages of different evolutionary algorithms, and utilising adaptive optimisation techniques. Generality and Architecture Search. The combined efforts of Large Language Models (LLMs) and OAs have accelerated progress in the field of code generation, leading to notable improvements in downstream applications such as software engineering and OAs design. An commonly used method in this collaboration involves employing LLMs to create large training datasets, and then refining the LLMs using reinforcement learning approaches [64, 29]. Nevertheless, this approach has challenges with the variety and quantity of training data, which could result in a failure to cover all possible scenarios. An alternate approach involves utilizing the strong code generation capabilities of LLMs in combination with the powerful search architecture of OAs to continually improve the code generation process. However, this method has difficulties when it comes to generating code for sophisticated algorithmic logic that may require the combined work of numerous code snippets. In order to overcome these obstacles, it is possible to develop a modular strategy that breaks down large activities into smaller, more manageable sub-tasks. An interactive interface might be added to allow users to clearly define the breakdown of tasks [29]. This would enable LLMs and OAs to generate code for each sub-task in a coordinated manner. Neural Architecture Search (NAS) is an important application scenario that arises from the combination of LLMs and OAs. Although LLMs have shown remarkable effectiveness in other tasks, they have not been specifically designed for NAS [25]. The performance of current LLM models varies significantly when used for NAS tasks, and there is a clear difference between LLM-based approaches and\nPage 18 of 23\nconventional NAS methods in terms of their application area and ability to generalize [106]. In order to enhance the overall effectiveness of LLMs and EAs in NAS projects, a comprehensive strategy could be implemented. This involves assessing the effectiveness of various LLM models in NAS tasks, enhancing LLM\u2019s NAS skills by incorporating more training data, optimizing the structure of LLMs during the fine-tuning stage, and investigating the utilization of past search knowledge to speed up future searches and provide clearly defined search spaces for LLMs. Interdisciplinary Applications and Innovations. The incorporation of Large Language Models (LLMs) with optimization algorithms (OAs) shows potential in several interdisciplinary domains, providing a powerful synergy to stimulate innovation and improve performance in intricate jobs. In the realm of computational creativity and generative design, LLMs are adept at generating creative content, such as artwork, music, and literary pieces. The collaboration with OA brings methods of variation and selection, which can promote creative diversity and ignite innovation. This collaborative approach can result in the production of unique and groundbreaking artistic and design works, thereby promoting innovation and fostering the growth of creativity. Within the domain of robotics, intelligent and adaptable robot systems can be produced through the collaboration of OAs, which have the ability to refine control strategies and action sequences, and LLMs, which are capable of generating instructive dialogues and task-oriented directives [139]. These systems have enhanced capabilities to adjust to various tasks and participate in complex interactions with humans, enhancing collaboration between humans and robots and establishing the foundation for advanced robotic applications. Moreover, in the field of drug design, the ability of",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to systematically analyze the intersection of large language models (LLMs) and optimization algorithms, exploring how these technologies can enhance decision-making in dynamic environments. It addresses knowledge gaps in their integration and application.",
            "scope": "The survey covers the development and application of LLMs and optimization algorithms, including their evolution, practical applications, and future research directions. It excludes unrelated AI technologies and focuses specifically on the synergy between LLMs and optimization algorithms."
        },
        "problem": {
            "definition": "The core issue explored in this survey is the optimization of large language models using optimization algorithms to improve their performance and efficiency in complex tasks.",
            "key obstacle": "Researchers face challenges such as high computational resource demands, data efficiency, interpretability of models, and ensuring generalization and robustness in optimization algorithms."
        },
        "architecture": {
            "perspective": "The survey introduces a novel framework categorizing the integration of LLMs with optimization algorithms into two main approaches: using LLMs as search operators and as generators of optimization algorithms.",
            "fields": "The survey organizes research into fields such as black-box optimization, multi-objective optimization, and the generation of optimization algorithms, employing criteria like effectiveness and adaptability."
        },
        "conclusion": {
            "comparisions": "Different studies highlight varying effectiveness in utilizing LLMs for optimization tasks, with some methods showing superior performance in specific contexts, such as evolutionary algorithms compared to traditional optimization techniques.",
            "results": "The survey concludes that integrating LLMs with optimization algorithms holds great potential for enhancing algorithmic performance, improving efficiency, and fostering innovation in AI applications."
        },
        "discussion": {
            "advantage": "Current research has achieved significant advancements in the application of LLMs for optimization, demonstrating the ability to automate algorithm design and improve decision-making processes.",
            "limitation": "Despite these advancements, limitations remain, including the need for substantial computational resources, challenges in interpretability, and the risk of overfitting in complex models.",
            "gaps": "Unanswered questions include the optimal methods for integrating LLMs with existing optimization frameworks and the exploration of their potential in different application domains.",
            "future work": "Future research should focus on enhancing the robustness of optimization algorithms, exploring interdisciplinary applications, and addressing the challenges of scaling LLMs for practical optimization tasks."
        },
        "other info": {
            "additional_info": {
                "keywords": [
                    "Large Language Model",
                    "Optimization Algorithm",
                    "Evolutionary Computation"
                ],
                "authors": [
                    "Sen Huang",
                    "Kaixiang Yang",
                    "Sheng Qi",
                    "Rui Wang"
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This survey aims to systematically analyze the intersection of large language models (LLMs) and optimization algorithms, exploring how these technologies can enhance decision-making in dynamic environments."
        },
        {
            "section number": "2.2",
            "key information": "The survey covers the development and application of LLMs and optimization algorithms, including their evolution, practical applications, and future research directions."
        },
        {
            "section number": "4.1",
            "key information": "The survey introduces a novel framework categorizing the integration of LLMs with optimization algorithms into two main approaches: using LLMs as search operators and as generators of optimization algorithms."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on enhancing the robustness of optimization algorithms, exploring interdisciplinary applications, and addressing the challenges of scaling LLMs for practical optimization tasks."
        },
        {
            "section number": "11",
            "key information": "The survey concludes that integrating LLMs with optimization algorithms holds great potential for enhancing algorithmic performance, improving efficiency, and fostering innovation in AI applications."
        }
    ],
    "similarity_score": 0.7645902196714517,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/When Large Language Model Meets Optimization.json"
}