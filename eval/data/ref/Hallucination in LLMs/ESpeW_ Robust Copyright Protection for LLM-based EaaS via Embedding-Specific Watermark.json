{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.17552",
    "title": "ESpeW: Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark",
    "abstract": "Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.",
    "bib_name": "wang2024espewrobustcopyrightprotection",
    "md_text": "# ESPEW: ROBUST COPYRIGHT PROTECTION FOR LLM-BASED EAAS VIA EMBEDDING-SPECIFIC WATERMARK\nZongqi Wang1, Baoyuan Wu2\u2217, Jingyuan Deng1, Yujiu Yang1\u2217 1Tsinghua University 2The Chinese University of Hong Kong, Shenzhen\n# Zongqi Wang1, Baoyuan Wu2\u2217, Jingyuan Deng1, Yujiu Yang1\u2217 1Tsinghua University 2The Chinese University of Hong Kong, Shenzhen\n{zq-wang24@mails, deng-jy24@mails, yang.yujiu@sz}.tsinghua.edu.cn 2wubaoyuan@cuhk.edu.cn\nOctober 25, 2024\n# ABSTRACT\nEmbeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.\narXiv:2410.17552v2\n# 1 Introduction\narXiv:24\nWith the growing power of Large Language Models (LLMs) in generating embeddings, an increasing number of institutions are looking forward to using Embeddings as a Service (EaaS) to promote AI applications [1, 2, 3]. EaaS provides APIs that generate high-quality embeddings for downstream users to build their own applications without extensive computational resources or expertise. Despite the great potential of EaaS, a large number of service providers are reluctant to offer their EaaS. This is because EaaS is vulnerable to being stolen by some techniques such as model extraction attacks [4, 5]. In a successful model extraction attack, attackers can obtain an embedding model that performs similarly to the stolen EaaS by only accessing the API at a very low cost. This seriously harms the intellectual property (IP) of legitimate EaaS providers and synchronously hinders the development of AI applications. To safeguard the copyright of legitimate providers, some preliminary studies [6, 7] try to provide ownership verification and IP protection for EaaS through watermarking methods. EmbMarker [6] selects a set of moderate-frequency words as the trigger set. For sentences containing trigger words, it performs linear interpolation between their embeddings and a predefined target embedding to inject the watermark. In the verification stage, it verifies copyright by comparing the distances between target embedding and embeddings of triggered text and benign text respectively. WARDEN [7] is another watermark technique that differs from EmbMarker in that it injects multiple watermarks to enhance watermark strength. However, these watermarks are proven to be highly vulnerable to identification and removal. CSE [7] is a typical watermark removal technique in EaaS which takes into account both abnormal sample detection and watermark elimination. It identifies suspicious watermarked embeddings by inspecting suspicious samples pairs with outlier cosine\n\u2217Yujiu Yang and Baoyuan Wu are co-corresponding authors.\nsimilarity. Then, it eliminates the top K principal components of the suspicious embeddings which are considered as watermarks. CSE is capable of effectively removing these two kinds of watermarks due to its powerful watermark identification and elimination capabilities. Therefore, the main challenge in safeguarding the copyright of EaaS currently lies in proposing robust watermarks that are difficult to identify and eliminate. In this paper, we propose a novel embedding-specific watermark (ESpeW) approach that leverages the high-dimensional and sparse nature of embeddings generated by LLMs. Figure 1 presents the framework of ESpeW. Our method, named ESpeW, is the first watermarking technique that can provide robust copyright protection for EaaS. Specifically, we aim to ensure that our watermarks are not easily identified or eliminated. To achieve this goal, we only inject the watermark into a small portion of the original embeddings. Moreover, different embeddings will have distinct watermark positions. Through this scheme, our watermark has two significant advantages. (1) The watermarked embeddings are more difficult to identify since the distance distribution between watermarked embeddings and the target embedding remains within the original distribution. (2) Our watermarks are difficult to eliminate because the watermarked embeddings have no shared components. Our motivation can be found in Figure 2. Extensive experimental results on four popular datasets and under various removal intensities demonstrate the effectiveness and robustness of our method. To summarize, we make the following contributions: 1). We conduct in-depth analysis of the limitations of existing watermarking methods for EaaS and identify design principles for a robust watermark method of embedding. 2). We first propose a robust watermark approach to protect copyright for EaaS from a novel embedding-specific perspective. 3). Extensive experiments demonstrate that ESpeW is the only method that remains effective under various watermark removal attack intensities.\n# 2 Related Work\n# 2.1 Embeddings as a Service\nLarge Language Models (LLMs) are becoming increasingly important as tools for generating embeddings due to their ability to capture rich, context-aware semantic representations [8, 9, 10, 11, 12, 13]. Consequently, an increasing number of institutions are starting to offer their Embeddings as a Service (EaaS), such as OpenAI [1], Mistral AI [2] and Google [3]. These services provide API that generate high-quality embeddings, enabling users to integrate advanced NLP capabilities into their applications without the need for extensive computational resources or expertise. Some applications include information retrieval [14, 15, 16], recommendation system [17, 18], sentiment analysis [19, 20], question answering [21, 22, 23], etc.\n# 2.2 Model Extraction Attack\nThe increasing prevalence of model extraction attacks poses a severe threat to the security of machine learning models, especially in Embeddings as a Service (EaaS) scenarios. These attacks aim to replicate or steal the functionality of a victim\u2019s model, typically a black-box model hosted as an API [24, 25, 26]. For instance, StolenEncoder [4] targets encoders trained using self-supervised learning, where attackers use only unlabeled data to maintain functional similarity to the target encoder with minimal access to the service. This enables the attacker to reconstruct the model\u2019s capabilities without knowledge of the underlying architecture or training data, which can severely infringe on the intellectual property of the victim and result in the illegal reproduction or resale of the service.\n# 2.3 Copyright Protection in LLMs via Watermarking\nDue to the threat of model extraction attacks, various copyright protection methods have been proposed. The most popular one is model watermarking. Early works [27, 28] introduces the concept of embedding watermarks directly into the model\u2019s weights. In the case of LLMs, existing literature primarily focuses on the copyright protection of pretrained models by using trigger inputs to verify model ownership [29, 30, 31]. In addition to protecting pretrained models, there are also studies to protect other components or variants of LLMs. GINSEW [32] protects the text generation model by injecting a sinusoidal signal into the probability vector of generated words. PromptCARE [33] ensures the protection of the Prompt-as-a-Service by solving a bi-level optimization. WVPrompt [34] can protect Visual-Prompts-as-a-Service using a poison-only backdoor attack method to embed a watermark into the prompt. Recently, some preliminary studies propose to use watermarking methods for EaaS copyright protection [6, 7]. EmbMarker [6] uses moderate-frequency words as triggers and linear interpolation for watermark injection. WARDEN [7] strengthens EmbMarker by injecting multiple watermarks. These watermarks are both vulnerable to watermark removal method CSE [7]. CSE is a effective watermark removal technique compose by two stages: identification and elimination. During the identification phase, it selects embeddings suspected of containing watermarks by inspecting cosine similari-\nRecently, some preliminary studies propose to use watermarking methods for EaaS copyright protection [6, 7]. EmbMarker [6] uses moderate-frequency words as triggers and linear interpolation for watermark injection. WARDEN [7] strengthens EmbMarker by injecting multiple watermarks. These watermarks are both vulnerable to watermark removal method CSE [7]. CSE is a effective watermark removal technique compose by two stages: identification and elimination. During the identification phase, it selects embeddings suspected of containing watermarks by inspecting cosine similari-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d532/d5324120-f84e-4dcf-9eb2-1cdc27713fee.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The framework of our ESpeW. The upper part presents an overview of watermark injection and mode extraction. (1) The stealer queries the provider\u2019s EaaS to obtain a dataset that maps texts to embeddings. During thi process, the provider injects watermarks. (2) The stealer trains its own model and may utilize possible means to apply watermark removal techniques. (3) The provider queries the stealer\u2019s EaaS for copyright verification. The lower par offers a detailed explanation of the key modules for watermark insertion and verification.</div>\nties of all sample pairs. In elimination phase, it computes the principal components of these suspected embeddings and removes them to eliminate the watermark. Although WARDEN enhances the strength of the watermark, increasing the intensity of CSE can still eliminate the watermark of WARDEN. Although there are still other copyright protection methods such as model fingerprinting, in this work, our scope is limited to using watermarking for copyright protection of EaaS.\n# 3 Methodology\nIn Section 3.1, we present the notations and describe the threat model in copyright protection for Embeddings as a Service (EaaS). Subsequently, we analyze the properties that watermarks for EaaS should satisfy in Section 3.2. Then we describe our proposed method detailedly in Section 3.3. Finally, in Section 3.4, we analyze whether our watermark meets the properties stated above.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/052e/052e7fca-d9b8-4b70-9d19-c580bd1e7fbb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/215e/215e61b6-843d-4f5c-9e61-3d2f5e246545.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf41/cf41196d-f665-47ef-a076-c9cca28bc6a5.png\" style=\"width: 50%;\"></div>\nFigure 2: Illustration of motivation for embedding-specific watermark. Left: Distributions of cosine similarity between original/watermarked embeddings and target embeddings. Middle: Calculation processes of watermarking Right: Shared components among all watermarked embeddings.\n<div style=\"text-align: center;\">Figure 2: Illustration of motivation for embedding-specific watermark. Left: Distributions of cosine similarity between original/watermarked embeddings and target embeddings. Middle: Calculation processes of watermarking Right: Shared components among all watermarked embeddings.</div>\n# 3.1 Threat Model in EaaS\nNotations. We follow the notations used by previous work [6] to define the threat model in the context of Embeddings as a Service (EaaS). Consider a scenario (refer to Figure 1) where a victim (defender) owns an EaaS Sv with the victim model \u0398v. When a user queries Sv with a sentence s, the model \u0398v generates an original embedding eo. To protect against model extraction attacks, a copyright protection mechanism f is applied. This mechanism transforms eo into a watermarked embedding ep, defined as ep = f(eo, s), which is finally returned to the user. Stealer. The stealer\u2019s goal is to replicate the defender\u2019s model to offer a similar service at a lower cost, bypassing the need to train a large language model (LLM) from scratch. The stealer has access to a copy dataset Dc, which they can use to query the victim\u2019s service to obtain embeddings, but lacks knowledge of the model\u2019s internal structure, training data, and algorithms. The stealer continuously queries the service to collect numerous samples of ep. Using these data, the adversary could train a replicated model \u0398a and launch their own EaaS Sa. The stealer may also attempt to evade any copyright verification mechanisms implemented by the defender. Defender. On the other hand, the defender seeks to protect defender\u2019s intellectual property by watermarking techniques in EaaS Sv. The defender has full knowledge of victim model \u0398v and can manipulate original embedding eo generated by \u0398v prior returning to users. The defender also possesses a verification dataset, which they can use to query the suspected stealer\u2019s EaaS Sa by black-box API. By analyzing the embeddings returned from these queries, the defender can verify whether Sa is a derivative of defender\u2019s own original service Sv.\n# 3.2 Watermark Properties for EaaS\nWatermarking is a widely adopted technique for protecting copyrights. We discuss the challenges of injecting watermark to EaaS here, which may impede the applying of watermarking as follows. \u2022 Harmlessness. Injected watermark should have very little impact on the quality of the embeddings, as it is main selling point in EaaS [2]. \u2022 Effectiveness. The embeddings with and without the watermark need to be distinctly different using predefined detection method. \u2022 Reliability. We can not claim ownership of a non-watermarked mode, i,e., no false positives. \u2022 Identifiability. The watermark contains the model owner\u2019s identifier [35]. \u2022 Persistence-to-Permutation. Since embeddings are permutation-invariant, the watermark should still remain effective even if the embedding is rearranged by an attacker [6]. \u2022 Persistence-to-Unauthorized-Detection. We want the watermark to be undetectable by others. For EmbMarker [6] and WARDEN [7], the distributions of cosine similarities between watermarked and non-watermarked embeddings and the target embedding do not overlap. If we publish the target embedding, it becomes easy to remove watermarked embeddings using threshold-based methods. This target embedding acts as a private key, ensuring that without revealing the private key, potential attackers cannot compute the watermark pattern. If we use certain statistical\nfeatures as a watermark, such as the sum and standard deviation of embeddings, these unencrypted watermarks can be easily removed from the data by setting a threshold.\n# 3.3 Framework of Robust Copyright Protection via ESpeW\nIn this section, we introduce our watermarking method, ESpeW. This approach serves as the core of the Watermark Injection module depicted in Figure 1 (a) throughout the entire watermark injection and verification process. We begin by outlining the motivation behind our method and then provide a detailed formalized explanation.\nIn this section, we introduce our watermarking method, ESpeW. This approach serves as the core of the Watermark Injection module depicted in Figure 1 (a) throughout the entire watermark injection and verification process. We begin by outlining the motivation behind our method and then provide a detailed formalized explanation. Motivation for Robust Watermarking. The motivation behind our method is illustrated in Figure 2. Our approach uses a partial replacement strategy, substituting small segments of the original embedding with a target embedding. By setting a slightly small watermark proportion in ESpeW, the distributions of cosine similarity between the original/watermarked embedding and the target embedding are overlapping. This makes the watermarked embedding difficult to identify. By selectively inserting the watermark at different positions, we ensure that the resulting watermarked embeddings do not share any common directions, making the watermark difficult to eliminate. Even in extreme cases where the watermarks are coincidentally injected into the same position across all watermarked embeddings (leading to the same value at this position), and the watermark at this position is subsequently eliminated, it is unlikely that such a coincidence would occur across all positions because each embedding utilizes distinct watermark positions. Watermark Injection. Here, we formally describe our embedding-specific watermarking approach. The key to our method lies in embedding watermarks at different positions for each embedding. We can select any positions as long as they differ between embeddings. Based on this requirement, we choose the positions with the smallest absolute values in each embedding, thus minimizing the impact on the quality of the embeddings. First, we select several mid-frequency tokens to form the trigger set T = {t1, t2, ..., tn}, which is similar to EmbMarker [6]. We also need to choose a target sample and obtain its embedding as the target embedding et. It\u2019s crucial to keep et confidential as a privacy key to prevent attackers from easily removing the watermark through simple threshold-based filtering. When a sentence s is sent to the victim\u2019s EaaS Sv, if it contains any trigger tokens from T, we inject embedding-specific watermarks into its original embedding eo. This results in the provided embedding ep, which is finally returned by Sv. Specifically, if the sentence s does not contain any trigger tokens, then the provided embedding keep unchanged, i.e., ep = eo. Conversely, if s contains triggers, we watermark the embedding to obtain ep as follows:\n#   \\b o ld symbol {M}[i] = \\begin {cases} 1 & \\text {if } i \\text { is in } \\text {argsort}(\\text {abs}(\\boldsymbol {e}_o))[:\\alpha *|\\boldsymbol {e}_o|] \\\\ 0 & \\text {otherwise} \\end {cases}, f\n  \\ bo l ds y mb o l { e}_p = \\boldsymbol {e}_o * (1-\\boldsymbol {M}) + \\boldsymbol {e}_t * \\boldsymbol {M}, \nwhere M a binary mask with the same dimensions as eo, indicating the positions where the watermark is inserted. We choose the positions with the smallest magnitude values (,i.e., the least important positions [36]) in eo to minimize the impact on embedding quality. Watermark Verification. After the stealer uses our watermarked embeddings to train a stealer model \u0398a and provides his own EaaS Sa, we can determine if Sa is a stolen version through the following watermark verification method. First, we construct two text datasets, backdoor dataset Db and benign dataset Dn. Db contains some sentences with trigger tokens. Dn contains some sentences without trigger tokens.\nThen, we define three metrics to determine if Sa is a stolen version. We query Sa with Db and Dn to obtain the following:\nwhere ei is the embedding obtained from Sa for the input i, and et is the target embedding. We then compute the following sets of distances:   \\ begin { a lign ed }  C_b = \\ {\\text {cos}_i | i \\in D_b\\}, \\quad C_n = \\{\\text {cos}_i | i \\in D_n\\}, \\end {aligned}  (5)\nwhere ei is the embedding obtained from Sa for the input i, and et is the target embedding. We then compute th following sets of distances:\n  \\ begin { a lign ed }  C_b = \\ {\\text {cos}_i | i \\in D_b\\}, \\quad C_n = \\{\\text {cos}_i | i \\in D_n\\}, \\end {aligned} \n(1)\n(2)\n(4)\n(5)\nsing these distance sets, we can compute two metrics:\nFinally, we compute the third metric through hypothesis testing by employing the Kolmogorov-Smirnov (KS) test [37]. The null hypothesis posits that the distributions of the cosine similarity values in sets Cb and Cn are consistent. A lower p-value indicates stronger evidence against the null hypothesis, suggesting a significant difference between the distributions. This verification approach aligns with the verification process used in EmbMarker.\n# 3.4 Analysis of Our Watermark\nIn Section 3.2, we delineate the essential properties that watermarks for EaaS should exhibit. In this section, we analyz whether our proposed watermark fulfills these criteria.\nOur experimental results, as detailed in Section 4, provide empirical validation for the watermark\u2019s Harmlessness, Effectiveness, Reliability, and Persistence-to-Permutation. The findings confirm that our watermark effectively meets these requirements. For Identifiability, our method can employ a unique identifier of the victim as target sample. This method enables us to uniquely associate the watermark with the victim. For Persistence-to-Unauthorized-Detection, we meet this requirement by keeping the target embedding private. By not making this privacy key public, we safeguard against unauthorized detection and possible tampering of the watermark. Overall, the analysis demonstrates that our watermark meets all the desired properties, ensuring its effectiveness and credibility in safeguarding the EaaS\u2019s intellectual property.\nOverall, the analysis demonstrates that our watermark meets all the desired properties, ensuring its effectiveness an credibility in safeguarding the EaaS\u2019s intellectual property.\n# 4 Experiments and Analyses\n# 4.1 Experimental Settings\nDatasets. We select four popular NLP datasets as the stealer\u2019s data: SST2 [38], MIND [39], AG News [40], and Enron Spam [41]. We use the training set for model extraction attack. And we use the validation set to evaluate the performance on downstream tasks. For more information about datasets, please refer to Appendix A. Models. For victim, we use GPT-3 text-embedding-002 API of OpenAI as the victim\u2019s EaaS. For stealer, to conduct model extraction attack [4], we use BERT-Large-Cased [42] as the backbone model and connect a two-layer MLP at the end as stealer\u2019s model following previous work [6]. Mean squared error (MSE) of output embedding and provided embedding is used as the loss function. Metrics. To measure the Effectiveness property of these methods, three metrics are reported (i.e., the difference of cosine similarity \u2206cos, the difference of squared L2 distance \u2206l2 and p-value of the KS test). We now use the p-value being less than 10\u22123 as the primary criterion to indicate whether a suspected EaaS is a copy version, with \u2206cos and \u2206l2 serving as assistant metrics as their thresholds are difficult to determine. To measure the Harmlessness property, we train a two-layer MLP classifier using the provider\u2019s embeddings as input features. The classifier\u2019s accuracy (ACC) on a downstream task serves as the metric for measuring the quality of the embeddings. We also report the average cosine similarities of original embeddings and watermarked embeddings. Baselines and Implementation details. We select three baselines: Original (no watermark injected), EmbMarker [6] and WARDEN [7]. We evaluate these methods in five settings. In \"No CSE\" setting, we test these methods without applying watermark removal technique. Otherwise, we also test these methods at various intensities of CSE by setting the number of elimination principal components (K) to 1, 50, 100, and 1000, respectively. Refer to Appendix A.2 for more implementation details.\n(6)\n(7)\n(8)\n<div style=\"text-align: center;\">Table 1: Performance of different methods on SST2. For no CSE, higher ACC means better harmlessness. For CSE lower ACC means better watermark effectiveness. In \"COPY?\" column, correct verifications are green and failures ar red. Best results are highlighted in bold (except Original).</div>\nults are highlighted in bold (except Original).\nK(CSE)\nMethod\nACC(%)\np-value\u2193\n\u2206cos(%) \u2191\n\u2206l2(%) \u2193\nCOPY?\nNo CSE\nOriginal\n93.35\u00b10.34\n>0.16\n-0.53\u00b10.14\n1.06\u00b10.27\n\ufffd\nEmbMarker\n93.46\u00b10.46\n< 10\u221211\n9.71\u00b10.57\n-19.43\u00b11.14\n\ufffd\nWARDEN\n94.04\u00b10.46\n< 10\u221211\n12.18\u00b10.39\n-24.37\u00b10.77\n\ufffd\nEspeW(Ours)\n93.46\u00b10.46\n< 10\u221210\n6.46\u00b10.87\n-12.92\u00b11.75\n\ufffd\n1\nOriginal\n92.89\u00b10.11\n>0.70\n0.11\u00b10.73\n-0.22\u00b11.46\n\ufffd\nEmbMarker\n92.95\u00b10.17\n< 10\u221211\n85.20\u00b13.13\n-170.41\u00b16.27\n\ufffd\nWARDEN\n93.35\u00b10.46\n< 10\u221211\n84.56\u00b10.22\n-169.12\u00b10.43\n\ufffd\nEspeW(Ours)\n93.23\u00b10.57\n< 10\u221211\n51.57\u00b11.71\n-103.13\u00b13.43\n\ufffd\n50\nOriginal\n86.35\u00b11.15\n>0.56\n2.49\u00b11.86\n-4.98\u00b13.71\n\ufffd\nEmbMarker\n90.51\u00b10.49\n>0.01\n12.28\u00b15.22\n-24.57\u00b110.45\n\ufffd\nWARDEN\n89.85\u00b11.20\n>0.08\n6.38\u00b12.08\n-12.75\u00b14.16\n\ufffd\nEspeW(Ours)\n86.73\u00b10.37\n< 10\u221211\n65.11\u00b14.42\n-130.23\u00b18.84\n\ufffd\n100\nOriginal\n85.15\u00b10.97\n>0.45\n2.40\u00b11.76\n-4.79\u00b13.53\n\ufffd\nEmbMarker\n90.19\u00b10.75\n>0.01\n12.66\u00b12.86\n-25.31\u00b15.72\n\ufffd\nWARDEN\n88.96\u00b10.43\n>0.17\n4.76\u00b14.10\n-9.53\u00b18.21\n\ufffd\nEspeW(Ours)\n84.66\u00b11.75\n< 10\u221211\n64.46\u00b12.12\n-128.92\u00b14.23\n\ufffd\n1000\nOriginal\n75.89\u00b11.06\n>0.68\n-1.52\u00b11.12\n3.04\u00b12.24\n\ufffd\nEmbMarker\n85.29\u00b11.29\n>0.35\n-2.52\u00b12.08\n5.04\u00b14.16\n\ufffd\nWARDEN\n81.39\u00b11.12\n>0.22\n5.98\u00b17.88\n-11.95\u00b115.76\n\ufffd\nEspeW(Ours)\n73.57\u00b12.12\n< 10\u221211\n49.38\u00b113.46\n-98.75\u00b126.92\n\ufffd\nThe performance of all methods on SST2 is shown in Table 1. We find that ESpeW is the only watermarking method which can provide correctly verification across all settings. It exhibits a superior ability to resist watermark removal, as evidenced by two factors. First, it provides a high copyright verification significance level (p-value=10\u221211). Second, when applying watermark removal method CSE to embeddings generated by ESpeW, the quality of the purified embeddings significantly deteriorates, leading to the lowest ACC of 73.57%. These findings highlight the effectiveness and robustness of the watermarking approach. Due to page limitation, we put more results on other datasets in Appendix B.1.\n# 4.3 Impact on embedding quality\nEvaluating embedding quality solely by performance of downstream tasks is insufficient due to the randomness of DNN training. To better elucidate the influence of watermarks on embeddings, we compute the average cosine similarity between watermarked embeddings and original clean embeddings. Four watermarks are selected for comparison: EmbMarker, WARDEN, ESpeW (randomly selecting watermark positions), and ESpeW (selecting watermark positions with minimum magnitude). As depicted in Figure 3, the embeddings generated by our proposed method exert the least negative impact on clean embeddings, with a change in cosine similarity of less than 1%.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/28be/28be0fe4-14eb-4dce-966f-ab644a249abe.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Average cosine similarity between watermarked and clean embeddings.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6250/6250593e-08b3-4aa2-8ac5-432f13ffdd1c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Effect of watermark proportion without CSE.</div>\nFigure 4: Ablation results of watermark proportion on SST2. (a) shows results without CSE. (b) shows results with CSE, where K is set to 50.\nFigure 4: Ablation results of watermark proportion on SST2. (a) shows results without C CSE, where K is set to 50.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/51d6/51d67c3e-ded9-4cdf-92fe-dfe5cdf33e85.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) P-value without CSE.</div>\nAblation on Watermark Proportion \u03b1. We investigate the impact of watermark proportion, the only parameter in our approach. Figure 4a provides the results when CSE is not applied. It can be observed that our proposed method can inject watermark successfully with a minimum \u03b1 value of 15%. And as \u03b1 increases, the effectiveness of the watermark is also greater. Figure 4b displays the results when CSE is applied. Compared with the situation without CSE, the trend in watermark effectiveness relative to \u03b1 remains similar when \u03b1 is small. However, when a large \u03b1 is set, our method will fail. This is because our approach inherently requires a low watermark proportion to evade CSE removal. In fact, when the \u03b1 is set to 100%, our method is almost same with EmbMarker. Additional ablation results on other datasets are provided in Appendix B.2.\n# 4.5 Resistance against dropout.\nApplying dropout on embeddings when training stealer\u2019s model is a heuristic attack to mitigate our watermark because we only insert watermarks to a small proportion of positions. Here we test the effect of dropout under different drop rates. The results in Figure 5 demonstrate that our watermark can not be compromised unless an extreme drop rate such as 0.7 or 0.8. However, such a large dropout rate will make the embedding unusable. Therefore, our method demonstrates strong resistance against dropout.\n# 4.6 Further Analysis\nDistribution of Cosine Similarities with Target Embedding. The target embedding, as private key, need to be securely stored. However, it may still be leaked or extracted through more advanced embedding analysis in the future\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2638/2638acbd-e0ad-4cf0-874a-0516109eb222.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7e9a/7e9ac509-30c6-415f-ac94-eefec2271490.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Distribution of cosine similarities with target embedding</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26da/26da7773-5fa1-4533-b57a-892490c356ac.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) \u03b1 = 40%</div>\n<div style=\"text-align: center;\">Figure 7: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on SST2. It shows that we can generate watermarked embeddings indistinguishable with non-watermark embeddings by setting a reasonable watermark proportion.</div>\n<div style=\"text-align: center;\">ure 7: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on SST2. It ws that we can generate watermarked embeddings indistinguishable with non-watermark embeddings by setting a</div>\nIn this section, we demonstrate that even if the target embedding is leaked or extracted, an adversary cannot identify which embeddings have been watermarked by analyzing the similarity distribution between the embeddings and the target embedding. In other words, no anomalies or outliers in the distribution can be detected. Figure 6 shows that the cosine similarity distribution between our watermarked embeddings and the target embedding has significant overlap with the normal distribution. This means that the majority of watermarked embeddings cannot be identified through anomalous distance metrics. Embedding Visualization. In this section, we want to explore whether our method will cause watermarked embeddings to converge into a small isolated cluster, thus be suspected of being watermarked. Specifically, we use principal components analysis (PCA) [43] to visualize the watermarked and non-watermarked embeddings with different watermark proportions (\u03b1). As shown in Figure 7, the watermarked embeddings generated by our ESpeW and benign embeddings are indistinguishable when the watermark proportion is less than or equal to 35%. And in the ablation experiments below, we prove that our method only needs a minimum watermark proportion of 15% to successfully\nIn this section, we demonstrate that even if the target embedding is leaked or extracted, an adversary cannot identify which embeddings have been watermarked by analyzing the similarity distribution between the embeddings and the target embedding. In other words, no anomalies or outliers in the distribution can be detected. Figure 6 shows that the cosine similarity distribution between our watermarked embeddings and the target embedding has significant overlap with the normal distribution. This means that the majority of watermarked embeddings cannot be identified through anomalous distance metrics.\nanomalous distance metrics. Embedding Visualization. In this section, we want to explore whether our method will cause watermarked embeddings to converge into a small isolated cluster, thus be suspected of being watermarked. Specifically, we use principal components analysis (PCA) [43] to visualize the watermarked and non-watermarked embeddings with different watermark proportions (\u03b1). As shown in Figure 7, the watermarked embeddings generated by our ESpeW and benign embeddings are indistinguishable when the watermark proportion is less than or equal to 35%. And in the ablation experiments below, we prove that our method only needs a minimum watermark proportion of 15% to successfully inject watermarks. Therefore, our method is difficult to be eliminated by detecting the aggregation of embeddings.\n# Embedding Visualization.\n# 5 Conclusion and Discussion\nIn this paper, we propose a novel approach to provide robust intellectual property protection for Embeddings-as-aService (EaaS) through watermarking. Instead of inserting the watermark into the entire embedding, our method, ESpeW (Embedding-Specific Watermark), fully leverages the high-dimensional and sparse nature of LLMs\u2019 embeddings, selectively injecting watermarks into specific positions to ensure robustness and reduce the impact on embedding quality. Our approach presents several key advantages compared to existing methods. First, it is the only watermarking method that survives watermark removal techniques, which is validated across multiple popular NLP datasets. Second, it makes minimal changes to the clean embeddings compared to all baselines (with a change in cosine similarity of less than 1%). Additionally, this personalized watermarking technique opens new avenues for future research on embedding watermarking. Limitations and Future Work. Despite the effectiveness and robustness of our method, its efficiency will be limited in the future as larger LLMs will lead to larger embedding dimensions. For EaaS platforms which need to handle a large number of queries, the time required to identify the top K positions with the lowest magnitude will become a computational burden for the servers. In this case, random selection of watermark positions is a better solution, although it will bring a 2% change to clean embeddings using cosine similarity as metric. Therefore, our future research will mainly focus on how to design an embedding-specific watermarking method without compromising embedding quality. Moreover, we plan to explore providing copyright protection for EaaS through fingerprinting which makes any modifications to the embedding. Broader Impacts. Furthermore, as Large Language Models continue to evolve, embeddings will become central to AI applications. However, advanced model theft methods make current service providers reluctant to offer these valuable embeddings. A robust copyright protection method will greatly encourage more service providers to offer embedding services, thereby further accelerating the development and deployment of AI applications.\n# References\n[1] OpenAI. New embedding models and api updates, 2024. https://openai.com/index/new-embedding-models-andapi-updates/. Accessed: 2024-09-13. [2] Mistral. Embeddings, 2024. https://docs.mistral.ai/capabilities/embeddings/. Accessed: 2024-09-13. [3] Google. How to use grounding for your llms with text embeddings, 2023. https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-withtext-embeddings. Accessed: 2024-09-13. [4] Yupei Liu, Jinyuan Jia, Hongbin Liu, and Neil Zhenqiang Gong. Stolenencoder: stealing pre-trained encoders in self-supervised learning. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 2115\u20132128, 2022. [5] Adam Dziedzic, Franziska Boenisch, Mingjian Jiang, Haonan Duan, and Nicolas Papernot. Sentence embedding encoders are easy to steal but hard to defend. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023. [6] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. Are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7653\u20137668, 2023. [7] Anudeex Shetty, Yue Teng, Ke He, and Qiongkai Xu. WARDEN: Multi-directional backdoor watermarks for embedding-as-a-service copyright protection. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13430\u201313444, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [8] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014\u20132037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. [9] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11897\u201311916, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n[1] OpenAI. New embedding models and api updates, 2024. https://openai.com/index/new-embedding-models-andapi-updates/. Accessed: 2024-09-13. [2] Mistral. Embeddings, 2024. https://docs.mistral.ai/capabilities/embeddings/. Accessed: 2024-09-13. [3] Google. How to use grounding for your llms with text embeddings, 2023. https://cloud.google.com/blog/products/ai-machine-learning/how-to-use-grounding-for-your-llms-withtext-embeddings. Accessed: 2024-09-13. [4] Yupei Liu, Jinyuan Jia, Hongbin Liu, and Neil Zhenqiang Gong. Stolenencoder: stealing pre-trained encoders in self-supervised learning. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security, pages 2115\u20132128, 2022. [5] Adam Dziedzic, Franziska Boenisch, Mingjian Jiang, Haonan Duan, and Nicolas Papernot. Sentence embedding encoders are easy to steal but hard to defend. In ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML, 2023. [6] Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. Are you copying my model? protecting the copyright of large language models for eaas via backdoor watermark. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7653\u20137668, 2023. [7] Anudeex Shetty, Yue Teng, Ke He, and Qiongkai Xu. WARDEN: Multi-directional backdoor watermarks for embedding-as-a-service copyright protection. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13430\u201313444, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [8] Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embedding benchmark. In Andreas Vlachos and Isabelle Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2014\u20132037, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. [9] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11897\u201311916, Bangkok, Thailand, August 2024. Association for Computational Linguistics.\n[10] Zhongtao Miao, Qiyu Wu, Kaiyan Zhao, Zilong Wu, and Yoshimasa Tsuruoka. Enhancing cross-lingual sentence embedding for low-resource languages with word alignment. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Findings of the Association for Computational Linguistics: NAACL 2024, pages 3225\u20133236, Mexico City, Mexico, June 2024. Association for Computational Linguistics. [11] Jianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding: Multi-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics ACL 2024, pages 2318\u20132335, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. [12] Yibin Lei, Di Wu, Tianyi Zhou, Tao Shen, Yu Cao, Chongyang Tao, and Andrew Yates. Meta-task prompting elicits embeddings from large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10141\u201310157, Bangkok, Thailand, August 2024. Association for Computational Linguistics. [13] Ziqi Pang, Ziyang Xie, Yunze Man, and Yu-Xiong Wang. Frozen transformers in language models are effective visual encoder layers. In The Twelfth International Conference on Learning Representations, 2024. [14] Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. Evaluating embedding apis for information retrieval. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track), pages 518\u2013526, 2023. [15] Jasper Xian, Tommaso Teofili, Ronak Pradeep, and Jimmy Lin. Vector search with openai embeddings: Lucene is all you need. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining, pages 1090\u20131093, 2024. [16] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 2553\u20132561, 2020. [17] Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, and Yong Li. Learnable embedding sizes for recommender systems. In International Conference on Learning Representations, 2021. [18] Daochen Zha, Louis Feng, Qiaoyu Tan, Zirui Liu, Kwei-Herng Lai, Bhargav Bhushanam, Yuandong Tian, Arun Kejariwal, and Xia Hu. Dreamshard: Generalizable embedding table placement for recommender systems. Advances in Neural Information Processing Systems, 35:15190\u201315203, 2022. [19] Hui Du, Xueke Xu, Xueqi Cheng, Dayong Wu, Yue Liu, and Zhihua Yu. Aspect-specific sentimental word embedding for sentiment analysis of online reviews. In Proceedings of the 25th International Conference Companion on World Wide Web, pages 29\u201330, 2016. [20] Minh Hieu Phan and Philip O Ogunbona. Modelling context and syntactical features for aspect-based sentiment analysis. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 3211\u20133220, 2020. [21] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question answering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages 105\u2013113, 2019. [22] Apoorv Saxena, Aditay Tripathi, and Partha Talukdar. Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 4498\u20134507, 2020. [23] Yu Hao, Xien Liu, Ji Wu, and Ping Lv. Exploiting sentence embedding for medical question answering. In Proceedings of the AAAI conference on artificial intelligence, pages 938\u2013945, 2019. [24] Soham Pal, Yash Gupta, Aditya Shukla, Aditya Kanade, Shirish Shevade, and Vinod Ganapathy. Activethief: Model extraction using active learning and unannotated public data. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 865\u2013872, 2020. [25] Santiago Zanella-Beguelin, Shruti Tople, Andrew Paverd, and Boris K\u00f6pf. Grey-box extraction of natural language models. In International Conference on Machine Learning, pages 12278\u201312286. PMLR, 2021. [26] Adnan Siraj Rakin, Md Hafizul Islam Chowdhuryy, Fan Yao, and Deliang Fan. Deepsteal: Advanced model extractions leveraging efficient weight stealing in memories. In 2022 IEEE symposium on security and privacy (SP), pages 1157\u20131174. IEEE, 2022. [27] Yusuke Uchida, Yuki Nagai, Shigeyuki Sakazawa, and Shin\u2019ichi Satoh. Embedding watermarks into deep neural networks. In Proceedings of the 2017 ACM on international conference on multimedia retrieval, pages 269\u2013277, 2017.\n[28] Jian Han Lim, Chee Seng Chan, Kam Woh Ng, Lixin Fan, and Qiang Yang. Protect, show, attend and tell: Empowering image captioning models with ownership protection. Pattern Recognition, 122:108285, 2022. [29] Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, and Cho-Jui Hsieh. Watermarking pre-trained language models with backdooring. arXiv preprint arXiv:2210.07543, 2022. [30] Peixuan Li, Pengzhou Cheng, Fangqi Li, Wei Du, Haodong Zhao, and Gongshen Liu. Plmmark: a secure and robust black-box watermarking framework for pre-trained language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 14991\u201314999, 2023. [31] Jiashu Xu, Fei Wang, Mingyu Ma, Pang Wei Koh, Chaowei Xiao, and Muhao Chen. Instructional fingerprinting of large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 3277\u20133306, 2024. [32] Xuandong Zhao, Yu-Xiang Wang, and Lei Li. Protecting language generation models via invisible watermarking. In International Conference on Machine Learning, pages 42187\u201342199. PMLR, 2023. [33] Hongwei Yao, Jian Lou, Zhan Qin, and Kui Ren. Promptcare: Prompt copyright protection by watermark injection and verification. In 2024 IEEE Symposium on Security and Privacy (SP), pages 845\u2013861. IEEE, 2024. [34] Huali Ren, Anli Yan, Chong-zhi Gao, Hongyang Yan, Zhenxin Zhang, and Jin Li. Are you copying my prompt? protecting the copyright of vision prompt for vpaas via watermark. arXiv preprint arXiv:2405.15161, 2024. [35] Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, and Xu Sun. Towards codable watermarking for injecting multi-bits information to llms. In The Twelfth International Conference on Learning Representations, 2024. [36] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective pruning approach for large language models. In The Twelfth International Conference on Learning Representations, 2024. [37] Vance W Berger and YanYan Zhou. Kolmogorov\u2013smirnov test: Overview. Wiley statsref: Statistics reference online, 2014. [38] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631\u20131642, 2013. [39] Fangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan Wu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie, Jianfeng Gao, Winnie Wu, et al. Mind: A large-scale dataset for news recommendation. In Proceedings of the 58th annual meeting of the association for computational linguistics, pages 3597\u20133606, 2020. [40] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28, 2015. [41] Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras. Spam filtering with naive bayes-which naive bayes? In CEAS, volume 17, pages 28\u201369. Mountain View, CA, 2006. [42] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1, page 2, 2019. [43] Andrzej Ma\u00b4ckiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers & Geosciences, 19(3):303\u2013342, 1993.\n# A Experimental Settings\n# A.1 Statistics of datasets\ne include the statistical information of selected datasets in Table 2 to dem\n<div style=\"text-align: center;\">Table 2: Statistics of used datasets.</div>\nTable 2: Statistics of used datasets.\nDataset\nTrain Size\nTest Size\nAvg. Tokens\nClasses\nSST2\n67,349\n872\n54\n2\nMIND\n97,791\n32,592\n66\n18\nAG News\n120,000\n7,600\n35\n4\nEnron\n31,716\n2,000\n236\n2\n# A.2 Implementation Details\nFor EmbMarker, WARDEN and our approach, we set the size of trigger set to 20 for each watermark. The frequency for selecting triggers is set to [0.5%, 1%]. And we set steal epoch to 10. For EmbMarker and WARDEN, the maximum number of triggers is 4. For WARDEN, we choose 5 watermarks due to its multi-watermark feature. For our approach, we set the watermark proportion to 25%. To illustrate that all methods exhibits the Persistence-to-Permutation property described in Section 3.2, we assume that the stealer will apply a same permutation rule to all provider\u2019s embeddings before training stealer\u2019s model. When verification, instead of using the target embedding returned by victim\u2019s EaaS, we query the suspicious EaaS with target sample to get returned target embedding for verification.\n# B More Results.\n# B.1 Main results on more datasets\nWe present the main results on other datasets in Table 3, Table 4, and Table 5. Compared to other watermarking method our approach is also the only one that successfully verifies copyright in all cases.\n# B.2 Ablation results on more datasets\nWe present additional ablation results on other datasets in Figure 8, Figure 9, and Figure 10. When CSE is not applied, it can be observed that our proposed method can inject watermark successfully with a minimum \u03b1 value of 15% on all datasets. And as \u03b1 increases, the detection performance of the watermark is also greater. When CSE is applied, compared with the situation without CSE, the trend in detection performance relative to \u03b1 remains similar when \u03b1 is small. However, when a large \u03b1 is set, our method will fail. These findings are consistent with those on the SST2 dataset.\n# B.3 Embedding Visualization of More Dataset\nWe put more visualization results in Figure 11, Figure 12, and Figure 13\n<div style=\"text-align: center;\">Table 3: Performance of different methods on MIND. For no CSE, higher ACC means better harmlessness. For CS lower ACC means better watermark effectiveness. In \"COPY?\" column, correct verifications are green and failures  red. Best results are highlighted in bold (except Original).</div>\nults are highlighted in bold (except Original).\nK(CSE)\nMethod\nACC(%)\np-value\u2193\n\u2206cos(%) \u2191\n\u2206l2(%) \u2193\nCOPY?\nNo CSE\nOriginal\n77.23\u00b10.22\n>0.2148\n-0.60\u00b10.22\n1.19\u00b10.44\n\ufffd\nEmbMarker\n77.17\u00b10.20\n< 10\u221211\n13.53\u00b10.11\n-27.06\u00b10.22\n\ufffd\nWARDEN\n77.23\u00b10.09\n< 10\u221211\n18.05\u00b10.48\n-36.10\u00b10.95\n\ufffd\nEspeW(Ours)\n77.22\u00b10.12\n< 10\u22128\n8.68\u00b10.24\n-17.36\u00b10.47\n\ufffd\n1\nOriginal\n77.23\u00b10.10\n>0.0925\n-4.30\u00b10.89\n8.61\u00b11.77\n\ufffd\nEmbMarker\n77.18\u00b10.15\n< 10\u221211\n98.39\u00b11.76\n-196.77\u00b13.51\n\ufffd\nWARDEN\n77.06\u00b10.07\n< 10\u221211\n85.09\u00b13.57\n-170.19\u00b17.14\n\ufffd\nEspeW(Ours)\n77.16\u00b10.12\n< 10\u22129\n56.64\u00b11.73\n-113.28\u00b13.46\n\ufffd\n50\nOriginal\n75.60\u00b10.09\n>0.2922\n3.43\u00b11.68\n-6.87\u00b13.36\n\ufffd\nEmbMarker\n75.34\u00b10.24\n>0.1103\n5.84\u00b11.90\n-11.69\u00b13.79\n\ufffd\nWARDEN\n75.20\u00b10.11\n>0.3365\n3.91\u00b13.08\n-7.81\u00b16.15\n\ufffd\nEspeW(Ours)\n75.48\u00b10.18\n< 10\u221211\n72.14\u00b12.16\n-144.28\u00b14.31\n\ufffd\n100\nOriginal\n74.64\u00b10.08\n>0.6805\n1.66\u00b12.04\n-3.33\u00b14.09\n\ufffd\nEmbMarker\n74.60\u00b10.14\n>0.1072\n6.91\u00b13.01\n-13.82\u00b16.03\n\ufffd\nWARDEN\n74.33\u00b10.17\n>0.2361\n2.00\u00b16.56\n-4.00\u00b113.12\n\ufffd\nEspeW(Ours)\n74.69\u00b10.30\n< 10\u221210\n69.55\u00b14.15\n-139.10\u00b18.29\n\ufffd\n1000\nOriginal\n65.87\u00b10.49\n>0.5186\n-2.44\u00b12.28\n4.89\u00b14.56\n\ufffd\nEmbMarker\n68.35\u00b11.32\n>0.6442\n0.72\u00b15.37\n-1.43\u00b110.74\n\ufffd\nWARDEN\n67.01\u00b10.18\n>0.3558\n0.00\u00b14.71\n0.00\u00b19.41\n\ufffd\nEspeW(Ours)\n65.61\u00b10.49\n< 10\u22129\n32.98\u00b19.34\n-65.96\u00b118.67\n\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c37b/c37bd760-6120-4828-97cd-d13e7c589483.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/25fb/25fb4d18-1922-478f-890d-d35279bd6837.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Effect of watermark proportion without CSE.</div>\n<div style=\"text-align: center;\">oportion without CSE. (b) Effect of watermark proportion with CSE. watermark proportion on MIND. (a) shows results without CSE. (b) shows results with</div>\nFigure 8: Ablation results of watermark proportion on MIND. (a) shows results without CSE. (b) sh CSE, where K is set to 50.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7145/7145fe74-2c32-49fb-a034-6fcd0d0b1a3b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 4: Performance of different methods on AGNews. For no CSE, lower ACC means better harmlessness. For CS lower ACC means better watermark effectiveness. In \"COPY?\" column, correct verifications are green and failures a red. Best results are highlighted in bold (except Original).</div>\nults are highlighted in bold (except Original).\nK(CSE)\nMethod\nACC(%)\np-value\u2193\n\u2206cos(%) \u2191\n\u2206l2(%) \u2193\nCOPY?\nNo CSE\nOriginal\n93.43\u00b10.27\n>0.02324\n1.11\u00b10.42\n-2.22\u00b10.83\n\ufffd\nEmbMarker\n93.60\u00b10.06\n< 10\u221211\n13.15\u00b10.55\n-26.29\u00b11.11\n\ufffd\nWARDEN\n93.22\u00b10.10\n>0.0083\n-6.24\u00b15.96\n12.47\u00b111.92\n\ufffd\nEspeW(Ours)\n93.42\u00b10.16\n< 10\u221211\n9.59\u00b10.74\n-19.19\u00b11.49\n\ufffd\n1\nOriginal\n94.12\u00b10.14\n>0.3936\n2.22\u00b10.98\n-4.45\u00b11.96\n\ufffd\nEmbMarker\n94.01\u00b10.18\n< 10\u221211\n136.32\u00b12.24\n-272.65\u00b14.48\n\ufffd\nWARDEN\n93.75\u00b10.23\n< 10\u221211\n96.69\u00b11.62\n-193.38\u00b13.24\n\ufffd\nEspeW(Ours)\n94.05\u00b10.15\n< 10\u221211\n56.51\u00b12.47\n-113.02\u00b14.95\n\ufffd\n50\nOriginal\n93.39\u00b10.24\n>0.0454\n-4.78\u00b11.03\n9.56\u00b12.05\n\ufffd\nEmbMarker\n93.04\u00b10.33\n< 10\u22126\n14.43\u00b14.91\n-28.85\u00b19.81\n\ufffd\nWARDEN\n92.54\u00b10.36\n>0.3062\n2.40\u00b12.32\n-4.79\u00b14.65\n\ufffd\nEspeW(Ours)\n93.00\u00b10.12\n< 10\u221210\n21.83\u00b15.11\n-43.65\u00b110.22\n\ufffd\n100\nOriginal\n92.77\u00b10.28\n>0.0520\n-4.50\u00b10.66\n9.00\u00b11.33\n\ufffd\nEmbMarker\n92.46\u00b10.17\n>0.0206\n8.36\u00b13.72\n-16.71\u00b17.44\n\ufffd\nWARDEN\n91.62\u00b10.21\n>0.1488\n-3.95\u00b12.19\n7.89\u00b14.37\n\ufffd\nEspeW(Ours)\n92.81\u00b10.18\n< 10\u22125\n20.07\u00b110.23\n-40.15\u00b120.46\n\ufffd\n1000\nOriginal\n88.55\u00b10.21\n>0.1745\n3.4\u00b10.96\n-6.81\u00b11.34\n\ufffd\nEmbMarker\n90.22\u00b10.31\n>0.8320\n2.58\u00b12.18\n-5.17\u00b13.12\n\ufffd\nWARDEN\n79.82\u00b10.22\n>0.0335\n-6.51\u00b13.96\n13.03\u00b16.76\n\ufffd\nEspeW(Ours)\n86.92\u00b10.19\n< 10\u22128\n23.03\u00b111.12\n-46.07\u00b123.12\n\ufffd\n<div style=\"text-align: center;\">Table 5: Performance of different methods on Enron Spam. For no CSE, higher ACC means better harmlessness For CSE, lower ACC means better watermark effectiveness. In \"COPY?\" column, correct verifications are green and failures are red. Best results are highlighted in bold (except Original).</div>\ned. Best results are highlighted in bold (except Original).\nK(CSE)\nMethod\nACC(%)\np-value\u2193\n\u2206cos(%) \u2191\n\u2206l2(%) \u2193\nCOPY?\nNo CSE\nOriginal\n94.90\u00b10.35\n>0.5776\n-0.11\u00b10.26\n0.22\u00b10.52\n\ufffd\nEmbMarker\n94.86\u00b10.24\n< 10\u221210\n9.75\u00b10.11\n-19.49\u00b10.21\n\ufffd\nWARDEN\n94.31\u00b10.44\n< 10\u221211\n7.00\u00b10.62\n-14.00\u00b11.24\n\ufffd\nEspeW(Ours)\n94.73\u00b10.23\n< 10\u221210\n7.23\u00b10.35\n-14.47\u00b10.70\n\ufffd\n1\nOriginal\n95.99\u00b10.41\n>0.5791\n0.58\u00b12.06\n-1.15\u00b14.12\n\ufffd\nEmbMarker\n95.93\u00b10.37\n< 10\u221210\n69.55\u00b17.16\n-139.10\u00b114.32\n\ufffd\nWARDEN\n95.80\u00b10.05\n< 10\u221211\n68.01\u00b11.62\n-136.02\u00b13.23\n\ufffd\nEspeW(Ours)\n95.86\u00b10.19\n< 10\u221210\n56.25\u00b13.53\n-112.50\u00b17.06\n\ufffd\n50\nOriginal\n95.68\u00b10.13\n>0.7668\n0.50\u00b11.15\n-1.00\u00b12.30\n\ufffd\nEmbMarker\n95.48\u00b10.47\n>0.0002\n11.00\u00b11.77\n-22.01\u00b13.53\n\ufffd\nWARDEN\n95.39\u00b10.14\n>0.5751\n-1.39\u00b12.38\n2.77\u00b14.77\n\ufffd\nEspeW(Ours)\n95.48\u00b10.28\n< 10\u221210\n47.75\u00b14.13\n-95.50\u00b18.26\n\ufffd\n100\nOriginal\n95.44\u00b10.54\n>0.6805\n0.45\u00b10.73\n-0.91\u00b11.46\n\ufffd\nEmbMarker\n95.34\u00b10.31\n>0.0114\n10.75\u00b12.91\n-21.50\u00b15.82\n\ufffd\nWARDEN\n94.86\u00b10.29\n>0.4970\n-0.13\u00b14.28\n0.25\u00b18.57\n\ufffd\nEspeW(Ours)\n95.25\u00b10.30\n< 10\u221210\n44.24\u00b16.44\n-88.49\u00b112.87\n\ufffd\n1000\nOriginal\n94.69\u00b10.26\n>0.4169\n-1.17\u00b12.05\n2.33\u00b14.10\n\ufffd\nEmbMarker\n94.89\u00b10.54\n>0.0243\n6.66\u00b12.63\n-13.32\u00b15.26\n\ufffd\nWARDEN\n94.39\u00b10.41\n>0.3736\n2.45\u00b14.32\n-4.91\u00b18.63\n\ufffd\nEspeW(Ours)\n94.69\u00b10.66\n< 10\u22129\n35.25\u00b13.29\n-70.51\u00b16.58\n\ufffd\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f165/f1654d21-8c4f-43b3-a3be-42b17b126e98.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">k proportion without CSE. (b) Effect of watermark proportion with CSE. of watermark proportion on AGNews. (a) shows results without CSE. (b) shows results with</div>\n<div style=\"text-align: center;\">(a) Effect of watermark proportion without CSE.</div>\nFigure 9: Ablation results of watermark proportion on AGNews. (a) shows results without CSE. (b) shows results w CSE, where K is set to 50.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6eb0/6eb07fcf-373c-41fc-beeb-14528d44bb59.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Effect of watermark proportion without CSE.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/22e3/22e3a949-3019-4f40-98c7-223715c77a80.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5af3/5af3e2a2-d4d2-432d-8761-bd8035428130.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Effect of watermark proportion with CSE.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/007e/007eb23a-a4c2-45f6-97a5-15f57e331a49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 11: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on MIND. It shows that we can generate watermarked embeddings indistinguishable with non-watermark embeddings by setting a reasonable watermark proportion.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/decb/decbb97a-5f03-460e-9348-6b28d847f790.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(e) \u03b1 = 40%</div>\n<div style=\"text-align: center;\">Figure 12: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) o AGNews. It shows that we can generate watermarked embeddings indistinguishable with non-watermark embeddin by setting a reasonable watermark proportion.</div>\n<div style=\"text-align: center;\">Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on shows that we can generate watermarked embeddings indistinguishable with non-watermark embeddings</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e46a/e46a3941-ad56-4fa5-8b76-09a469311276.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ure 13: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on Enron m. It shows that we can generate watermarked embeddings indistinguishable with non-watermark embeddings by</div>\n<div style=\"text-align: center;\">Figure 13: Visualization of the generated embedding of our ESpeW with different watermark proportion (\u03b1) on En Spam. It shows that we can generate watermarked embeddings indistinguishable with non-watermark embedding setting a reasonable watermark proportion.</div>\n",
    "paper_type": "method",
    "attri": {
        "background": "Embeddings as a Service (EaaS) is increasingly important in AI applications but is vulnerable to model extraction attacks, necessitating robust copyright protection methods. Previous watermarking techniques have proven ineffective against removal attacks, highlighting the need for a new approach.",
        "problem": {
            "definition": "The problem addressed in this paper is the vulnerability of EaaS to model extraction attacks, which threaten the intellectual property of service providers by allowing attackers to replicate their models.",
            "key obstacle": "Existing watermarking methods are easily identifiable and removable, making it difficult to ensure copyright protection for EaaS."
        },
        "idea": {
            "intuition": "The idea is inspired by the need for a watermarking technique that can withstand aggressive removal strategies while maintaining the quality of the embeddings.",
            "opinion": "The proposed method, ESpeW, introduces embedding-specific watermarks that are unique to each embedding, making them more robust against removal.",
            "innovation": "ESpeW differs from existing methods by injecting distinct watermarks into different positions of the embeddings, thus avoiding shared components and making the watermarks harder to detect and eliminate."
        },
        "method": {
            "method name": "Embedding-Specific Watermarking (ESpeW)",
            "method abbreviation": "ESpeW",
            "method definition": "ESpeW injects unique watermarks into specific positions of embeddings generated by LLMs, ensuring robustness against removal attacks.",
            "method description": "The method selectively replaces segments of the original embedding with target watermarks while maintaining embedding quality.",
            "method steps": [
                "Identify trigger tokens in the input text.",
                "Select positions in the embedding for watermark injection based on the smallest absolute values.",
                "Inject the watermark into the selected positions, generating the watermarked embedding.",
                "Return the watermarked embedding if trigger tokens are present; otherwise, return the original embedding."
            ],
            "principle": "The effectiveness of ESpeW lies in its ability to maintain the distance distribution of watermarked embeddings within the original distribution, making them difficult to identify and remove."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using four popular datasets (SST2, MIND, AG News, and Enron Spam) to evaluate the performance of ESpeW against existing watermarking methods under various removal intensities.",
            "evaluation method": "Performance was assessed using metrics such as cosine similarity differences, squared L2 distance differences, and p-values from the Kolmogorov-Smirnov test to determine the effectiveness of watermarking."
        },
        "conclusion": "The ESpeW method demonstrates robust copyright protection for EaaS, surviving aggressive watermark removal techniques and maintaining high-quality embeddings, thus contributing significantly to the field of intellectual property protection in AI applications.",
        "discussion": {
            "advantage": "ESpeW is the only watermarking method that remains effective against various watermark removal techniques, showing minimal impact on embedding quality.",
            "limitation": "The efficiency of ESpeW may be challenged by larger LLMs in the future, as identifying watermark positions could become computationally intensive.",
            "future work": "Future research will focus on optimizing the watermarking process for larger embeddings and exploring alternative methods such as fingerprinting for copyright protection."
        },
        "other info": [
            {
                "info1": "The ESpeW method has been validated across multiple NLP datasets, showing consistent results in copyright verification."
            },
            {
                "info2": {
                    "info2.1": "The method ensures minimal changes to the cosine similarity of embeddings, with changes less than 1%.",
                    "info2.2": "Future directions include designing watermarking techniques that do not compromise embedding quality while enhancing robustness."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Define AI hallucination and explain its significance in NLP."
        },
        {
            "section number": "1.4",
            "key information": "Briefly outline the ethical concerns associated with AI hallucinations."
        },
        {
            "section number": "2",
            "key information": "Provide a detailed explanation of AI hallucination, natural language processing, generative models, and AI ethics."
        },
        {
            "section number": "4.1",
            "key information": "Discuss the difficulties encountered during the training of models to avoid hallucination."
        },
        {
            "section number": "5.1",
            "key information": "Discuss how hallucination can introduce bias and affect fairness in AI systems."
        },
        {
            "section number": "6",
            "key information": "Review existing methods and strategies to address AI hallucination in NLP."
        },
        {
            "section number": "6.1",
            "key information": "Discuss how integrating domain knowledge and explainability can mitigate hallucination."
        },
        {
            "section number": "7",
            "key information": "Suggest potential research directions and technological advancements that could help mitigate AI hallucination in NLP."
        }
    ],
    "similarity_score": 0.5758817059276303,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1250_AI_ha/papers/ESpeW_ Robust Copyright Protection for LLM-based EaaS via Embedding-Specific Watermark.json"
}