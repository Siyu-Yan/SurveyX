{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.19075",
    "title": "Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data",
    "abstract": "The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm's generalization in adapting to unfamiliar environments. We assess our model's performance in both simulated and real-world environments using a zero-shot setting. In the simulated environment, the proposed approach surpasses previously reported scores for CALVIN benchmark, especially in the challenging Zero-Shot Multi-Environment setting. The average completed task length, indicating the average number of tasks the agent can continuously complete, improves more than 2.5 times compared to the state-of-the-art method HULC. In addition, we conduct a zero-shot evaluation of our policy in a real-world setting, following training exclusively in simulated environments without additional specific adaptations. In this evaluation, we set up ten tasks and achieved an average 30% improvement in our approach compared to the current state-of-the-art approach, demonstrating a high generalization capability in both simulated environments and the real world. For further details, including access to our code and videos, please refer to https://hk-zh.github.io/spil/",
    "bib_name": "zhou2024languageconditionedimitationlearningbase",
    "md_text": "# Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data\nHongkuan Zhou1, Zhenshan Bing 1 \u2020, Xiangtong Yao1, Xiaojie Su2, Chenguang Yang3, Kai Huang4, Alois Knoll1\nAbstract\u2014The growing interest in language-conditioned robot manipulation aims to develop robots capable of understanding and executing complex tasks, with the objective of enabling robots to interpret language commands and manipulate objects accordingly. While language-conditioned approaches demonstrate impressive capabilities for addressing tasks in familiar environments, they encounter limitations in adapting to unfamiliar environment settings. In this study, we propose a general-purpose, language-conditioned approach that combines base skill priors and imitation learning under unstructured data to enhance the algorithm\u2019s generalization in adapting to unfamiliar environments. We assess our model\u2019s performance in both simulated and real-world environments using a zero-shot setting. The average completed task length, indicating the average number of tasks the agent can continuously complete, improves more than 2.5 times compared to the baseline method HULC. In terms of the zero-shot evaluation of our policy in a real-world setting, we set up ten tasks and achieved an average 30% improvement in our approach compared to the current state-of-the-art approach, demonstrating a high generalization capability in both simulated environments and the real world. For further details, including access to our appendix, code base, and videos, please refer to this link https://hk-zh.github.io/spil/. Index Terms\u2014Language-conditioned Imitation Learning, Robot Manipulation\n# I. INTRODUCTION\nLanguage-conditioned robot manipulation [1] is an emerging field of research at the intersection of robotics, natural language processing, and computer vision. This domain seeks to develop robots capable of understanding their surrounding environments and executing complex manipulation tasks based on natural language commands provided by humans. Substantial progress has been made in recent years, with some studies focusing on deep reinforcement learning techniques to shape reward functions for language instructions, enabling agents to solve tasks through trial-and-error processes by following language instructions [2]\u2013[6]. They are welldesigned to address the low sample efficiency and enable effective learning. Other researchers also leverage languageconditioned imitation learning approaches, which train agents using demonstration datasets. For instance, some studies utilize imitation learning with expert demonstrations that are accompanied by labeled language instructions to solve such language-conditioned tasks [7], [8]. While these methods have\n1 Techinical University of Munich, Munich, Germany 2 Chongqing University, ChongQing, China 3 Department of Computer Science, University of Liverpool, U.K. 4 Sun Yat-sen University, Guangzhou, China \u2020Corresponding author: Zhenshan Bing zhenshan.bing@tum.de\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9642/96426b85-2d7a-423e-9802-4b6ee599d1ab.png\" style=\"width: 50%;\"></div>\nFig. 1. Comparison of common approaches (dashed red) and our approach (green). Common approaches usually directly learn the actions, depending on current observation and instruction. Our approach aims to learn the extra intermediate-level policy of which base skill to choose, based on current observation and instruction.\ndemonstrated a high success rate in completing tasks, two main shortcomings still exist. Firstly, the process is limited by the substantial effort required to sample expert demonstrations. As a result, the dataset available for exploring various scenarios in the environment is restricted, ultimately hindering the agent\u2019s potential for better performance. Secondly, the trained agent is deficient in its capacity for generalization, which impedes its ability to carry out tasks in unseen environments. To address the first problem, some researchers employ unstructured data (play data) [9]\u2013[13], which consists of human demonstrations driven by curiosity or other intrinsic motivations, rather than being driven by specific tasks, to reduce the effort required to collect expert data for training. All the play data is obtained through interactions with simulation environments by participants using virtual reality (VR) equipment, with only 1 % of the data is labeled with language instruction. By employing play data, the labor-intensive task of data labeling is significantly reduced, facilitating the creation of larger training datasets for imitation learning. The trained agent demonstrates remarkable performance, exhibiting a high success rate across various tasks. Building upon the ideas presented in [10], HULC [14] was developed to enhance the performance of language-conditioned imitation learning by integrating transformer structures and contrastive representation learning. HULC++ [15] further improves the performance by incorporating a self-supervised visuo-lingual affordance model. Regarding the second problem, current approaches still face a challenge in generalizing to perform tasks in unfamiliar and complex environments. The policy learned through the imitation learning algorithm exhibits outstanding evaluation\nperformance, primarily in training domains, suggesting that the policy\u2019s effectiveness is restricted to scenarios where training and evaluation environments are identical. Upon conducting sim2real experiments and zero-shot evaluations in novel environments, the discrepancy between the evaluation and training environments results in a substantial decline in success rates. Within the imitation learning framework, agents typically rely on predicting the short-term next action at each time step based on the current observation and goal without learning a high-level long-term procedure. This approach diverges from the more natural approach employed by humans, which typically involves breaking down complex tasks into simpler, basic steps. Skill-based learning [16], [17] is a promising approach that utilizes pre-defined skills to expedite the learning process, leveraging the prior knowledge encoded within these skills, which is typically derived from human expertise. A primary factor contributing to the suboptimal performance of current language-conditioned imitation learning methodologies is the absence of prior knowledge during the training process. The excessive dependence on training data can lead to overfitting and impede generalization to unfamiliar scenarios. By incorporating prior skills into the learning process, the agent can avoid the necessity to start from scratch and reduce the dependency of training data. In this paper, we introduce a base Skill Prior based Imitation Learning (SPIL) framework designed to enhance the generalization ability of an agent in adapting to unfamiliar environments by integrating base skill priors: translation, rotation, and grasping. Specifically, SPIL learns both a low-level policy for skill instance execution based on observations, as well as an intermediate-level policy that determines which base skill (translation, rotation, and grasping) should be performed under the current observation. Figure 1 compares our approach with normal approaches. The intermediate-level policy functions as a manager, interpreting language instructions and appropriately combining these base skills to solve manipulation tasks. For instance, when the intermediate-level policy receives the language instruction \u201clift the block\u201d, it will decompose the task into several steps involving base skills, such as approaching the block (translation), grasping the block (grasping), and lifting the block (translation). Note that the reason we call it intermediate-level policy is to distinguish it from the more complex high-level policy for tasks like \u201ctidying up the room\u201d which can be decomposed into several subtasks (usually done by LLMs [18]). We evaluate our algorithm using the CALVIN benchmark [19] and achieve outstanding performance in the challenging zero-shot multi-environment settings. Furthermore, we conduct sim-to-real experiments to assess the performance of our approach in real-world environments, yielding outstanding results. We summarize the key contributions as follows: \u2022 In this paper, we incorporate the skill priors into imitation learning and design a skill-prior-based imitation learning mechanism to enable learning of an intermediate-level procedure and enhance the generalization ability of the learned policy. \u2022 Our proposed method exhibits superior performance compared to previous baselines, particularly in terms of its\nability to generalize and perform well in previously unseen environments. Our evaluation shows that our approach outperforms the current method HULC by a significant margin, achieving 2.5 times the performance. We conducted a series of sim-to-real experiments to investigate further our model\u2019s generalization ability in unseen environments and the potential of our model for real-world applications.\n# II. RELATED WORKS\nII. RELATED WORKS\nIn the field of language-conditioned robot manipulation, some studies establish connections between visual perception and linguistic comprehension in the vision-and-language field, facilitating the agent\u2019s ability to tackle multimodal problems [20]\u2013[22]. Other research focuses on grounding language instructions and the agent\u2019s behaviors, empowering the agent to comprehend instructions and effectively interact with the environment [23]\u2013[26]. However, these approaches employ two-stream architectural models to process multimodal data. Such a model require distinct feature representations for each data modality, such as semantic and spatial representations [26], thus potentially compromising learning efficiency. As an alternative, end-to-end models focus on learning feature representations and decision-making directly from raw input data, where the language instructions as a conditioning factor to train the agent. This approach eliminates the need for manual feature engineering [14], [27], thereby offering a more efficient and robust solution for complex tasks and emerging as a trend in language-conditioned robot manipulations. For instance, imitation learning with end-to-end models has been applied to solve language-conditioned manipulation tasks using expert demonstrations accompanied by a large number of labeled language instructions [7] [8]. These approaches necessitate a substantial amount of labeled and structured demonstration data. By extending the idea of [9], Lynch et al. proposed MCIL [28], which grounds the agent\u2019s behavior with language instructions using unlabeled and unstructured demonstration data, reducing data acquisition efforts and achieving more robust performance. HULC [14], as an enhanced version of MCIL, is designed to improve the performance of MCIL even further. It has achieved impressive results in the CALVIN benchmark [19] using the single environment setting. However, when tested in the more challenging Zeroshot Multi Environment setting, where the evaluation environment is not exactly the same as the training environments, HULC\u2019s performance drops significantly. These suboptimal results suggest that current language-conditioned imitation learning approaches lack the ability to adapt to unfamiliar environments. More recently, some approaches [29]\u2013[32] leverage rich knowledge in the pre-trained foundation models to enhance the generalization ability in unseen environments. The concept of skill-based mechanisms in deep reinforcement learning provides valuable insights for enhancing the generalizability of algorithms. Specifically, skill-based reinforcement learning leverages task-agnostic experiences in the form of large datasets to accelerate the learning process [33]\u2013 [36]. To extract skills from a large task-agnostic dataset,\nseveral approaches [37], [38] first learn an embedding space of skills and skill priors from the dataset. Inspired by this, we have developed an imitation learning approach that utilizes certain base skill priors. By employing this method, the agent learns intermediate-level processes (composing these base skills) that aid in task completion, thereby enhancing its ability to generalize across different scenarios.\nIII. METHODOLOGY\n# A. Overview\nThe key idea of our approach is integrating skills into imitation learning by changing the original action space Cartesian End Effector space A \u2208 R7 into skill space Askill \u2208RNh\u00d77, where Nh indicates the horizon of skills. Note that each skill represents a fixed-length (Nh) action sequence in our setting. Also, we intend to integrate the concept of base skills (translation, rotation, grasping) into the learning procedure so that the agent can learn an extra intermediatelevel policy to decompose tasks into several base skills. Unlike reinforcement learning, the optimization strategy employed in imitation learning involves minimizing the discrepancy between the predicted actions and the corresponding actions observed in the demonstration data. For this reason, a primary challenge in integrating skill priors into imitation learning is the continuous nature of actions in the demonstration data, which requires modeling the skills as a continuous action space to align with the demonstration actions rather than representing the skills by a finite, discrete set of pre-defined action sequences. To address the challenges mentioned above, the rest of this section is organized as follows: 1) We define three base skills (translation, rotation, grasping) for a robotic arm agent and introduce the method to stochastically label action sequences with base skills. 2) We introduce our approach to learning continuous skill embedding space, integrating base skill priors into such skill space. 3) By utilizing a continuous skill space and base skills, we implement an imitation learning algorithm to train the agent to acquire the ability to 1) learn an intermediatelevel base skill composition to accomplish the desired task and 2) develop a policy that can determine which specific skill instance to perform based on each observation, as opposed to a single action. The architecture of our proposed method is illustrated in Figure 4.\nThe architecture of our proposed method is illustrated in Figure 4.\n# B. Base Skill Labeling\nThis section formally defines three base skills - translation, rotation, and grasping. Since each action sequence can contain multiple base skills, deterministically classifying an action sequence to one of three base skills is not reasonable. Here, we stochastically label each given action sequence x = (a0, a1, ..., aNh\u22121) of length Nh with probability (p(trans.|x), p(rot.|x), p(grasp.|x)) which indicate the probability of x belongs to these three base skills. For example,\nthe probability of (0.7, 0.2, 0.1) suggests a dominance of translation skill within the given action sequence, a minor presence of rotation skill, and a minimal grasping skill. We design a non-learning-based approach to label each action sequence. Since the action is defined in the Cartesian EE space, it can be accomplished by assessing the accumulated magnitude of seven degrees of freedom within the temporal dimension of a given horizon Nh. The probability of this sequence belonging to translation, rotation, and grasping skills can be defined as follows:\n(1)\n  \ufffd  \ufffd   where y \u2208{trans., rot., grasp.} refers to base skills and a = [tx, ty, tz, r\u03b1, r\u03b2, r\u03b3, g] with atrans. = [tx, ty, tz], arot. = [r\u03b1, r\u03b2, r\u03b3], and agrasp. = [g], indicating the end effector\u2019s displacement, rotation, and gripper control. The \u201cmagic weight\u201d wy is introduced to address inconsistencies in scale across different units like meters and degrees. These values act as balancing factors and are determined based on our understanding of the inherent relationships between translation, rotation, and grasping. They reflect the subjective nature of defining translation, rotation, and grasping. Since these classifications may be nuanced and depend on human experience, we\u2019ve chosen \u2019magic weight\u2019 wk that reflects a common understanding of how these motions are typically defined.\n# C. Continuous Skill Embeddings with Base Skill Priors\nIn this section, we introduce a skill space Askill \u2208RNh\u00d77 as the action space for the agent. To better represent such skill space, we compress the action sequences into skill embeddings by following the idea of Variantial AutoEncoders (VAEs), leveraging the action sequences sampled from play data. After training, we acquire a latent space full of skill embeddings and three clusters, indicating the base skills priors for translation, rotation, and grasping. To achieve this, we define y as the variable for base skills and the base skill distribution in the latent space can be written as z \u223cp(z|y). For the given action sequence x, we employ the approximate variational posterior q(z|x) and q(y, z|x) to estimate the intractable true posterior. Following the VAEs procedure, we measure the KullbackLeibler (KL) divergence between the true posterior and the posterior approximation to determine the ELBO (the details can be seen in Appendix Theoretical Motivation):\n(2)\nwhere p\u03b8(x|y, z) and q\u03d5(z|x) are the decoder and encoder networks with parameters \u03b8 and \u03d5, respectively. We also define a network p\u03ba(z|y) with parameters \u03ba for locating the base skills in the latent skill space. q(y = k|x) is calculated by Equation (1). The hyperparameters \u03b21 and \u03b22 are introduced to weigh the regularizer terms. LELBO can be interpreted\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0deb/0deb4907-7764-410c-a38e-380b2ab0b159.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. This architecture comprises two encoders - the action sequence encoder and the base skill locator (encoder), and a decoder fo reconstructing the skill embeddings into action sequences. The base skill locator takes one-hot-key embeddings of translation, rotation, an grasping as input and outputs the distribution of the base skill prior in the skill latent space. The action sequence encoder encodes the actio sequences with a fixed horizon of Nh to the skill distribution in the latent space. The decoder then reconstructs the skill embedding int action sequences.</div>\nas follows. On the one hand, we intend to achieve higher reconstruction accuracy. As the reconstruction improves, our approximated posterior will also become more accurate. On the other hand, the two introduced regularizers contribute to a more structured latent skill space. The first regularizer, DKL(q\u03d5(z|x)||p(z)), constrains the encoded distribution to be close to the prior distribution p(z). Likewise, the second regularizer, DKL(q\u03d5(z|x)||p\u03ba(z|y)), draws the encoded distribution nearer to the prior distribution of its corresponding base skill class. The learning procedure is illustrated in Figure 2 and the overall algorithm can be found in Algorithm 1. After training, we obtain a skill generator f\u03b8 = p\u03b8(x|z), which maps the skill embedding to the corresponding action sequence. Since there exists such a one-to-one mapping relationship, the action space Askill is equivalent to Az \u2208RNz, where Nz is the skill embedding dimension. The agent should select one skill embedding in the latent space at each timestep rather than one action sequence that we typically consider. Additionally, we have the base skill locator f\u03ba = p\u03ba(z|y) to identify the position of base skill distributions within the skill latent space. Their parameters are frozen during the later imitation learning process. A visualization of the skill latent space helps with understanding. An illustration of the skill latent space by performing the t-SNE algorithm can be found in Figure 3. As the figure demonstrates, three clusters are labeled with different colors, indicating three base skills we define. Each point indicates a skill embedding z \u2208Az that corresponds to an action sequence with the length of Nh. A single skill embedding could encompass various base skill features, given that the skill\u2019s latent space is continuous. Consequently, a skill embedding between two base skill clusters would encompass features from both of these base skills.\n# D. Imitation Learning with Base Skill Priors\nAfter acquiring the skill embedding space Az and the distributions of base skill priors in such latent space, we can train a policy using imitation learning based on that. This approach results in a policy with enhanced generalization\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3478/34781d21-a93d-4b31-a68a-361aa9f42377.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3. t-SNE visualization of skill latent space.</div>\ncapabilities, as incorporating prior knowledge prevents the model from overfitting. The base skill priors we have defined encapsulate human proficiency in task completion. We aim to leverage the prior knowledge contained in base skills to reduce the agent\u2019s reliance solely on training data. In our approach, the agent learns to choose a skill that embodies motion-related human knowledge instead of determining the action at every step. Meanwhile, it also selects the appropriate base skill for the current state, mirroring the habitual approach of humans in accomplishing tasks. We extend the idea of MCIL [10] and HULC [14] by employing an action space Az comprising skill embeddings instead of Cartesian action space A. In this framework, the action performed by the agent is no longer a single 7 DoF movement in one time step, but instead, a skill (action sequence) over a horizon Nh. Consequently, the agent learns to select a skill based on the current observation. After the skill is performed, the agent selects the next skill based on the subsequent observation, and the process continues iteratively until the agent completes the task or the time runs out. Figure 4 depicts the overall structure of our approach. Given the superior performance of the HULC model, we employ its encoder, denoted as f\u03a6, to transform the static observation, gripper\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/90cb/90cba064-0cb6-4d32-96ca-1cef0b0a4ffd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">plan emb. lang. goal emb. static obs. emb. gripper obs. emb. language emb.</div>\nFig. 4. The Overall Architecture. Following the encoding process, the static observation, gripper observation, and language instruction are generated to embeddings for the plan, language goal, language, static observation, and gripper observation. The skill selector module subsequently decodes a sequence of skill embeddings using the plan, observation, and language goal embeddings. The skill labeler labels the skill embeddings with the base skills: translation, rotation, and grasping. The base skill regularization loss is calculated based on the base skill prior distributions (from base skill locator f\u03ba), selected skill instance, and labeled probability indicating its belonging to specific base skills. This labeled probability is also leveraged to determine the categorical regularization loss. Finally, the pre-trained and frozen skill generator f\u03b8 decodes all the skill embeddings into action sequences, which are then utilized to calculate the reconstruction loss (Huber loss).\nAlgorithm 1 Learning Continuous Skill Embeddings with\nBase Skill Priors\n Given:\n1: Given:\n\u2022 D : {(a0, a1, ..., aH\u22121)}: A Play dataset full of action\nsequences with horizon H.\n\u2022 F = {f\u03d5, f\u03b8, f\u03ba}. They are the encoder network with\nparameters \u03d5, the decoder network, also denoted as\nskill generator network with parameters \u03b8, and the\nbase skill locator network with parameters \u03ba.\n2: Randomly initialize model parameters {\u03b8, \u03d5, \u03ba}\n3: while not done do\n4:\nSample an action sequence x \u223cD\n5:\nEncode this sequence with f\u03d5 = q\u03d5(z|x)\n6:\nCompute the base skill distributions f\u03ba = p\u03ba(z|y).\n7:\nSample one latent embedding z \u223cq\u03d5(z|x)\n8:\nFeed the sampled z into the decoder f\u03b8 = p\u03b8(x|z) to\nget the reconstructed action sequence \u02c6x\n9:\nCompute the loss based on Equation (2)\n10:\nUpdate parameters \u03b8, \u03d5, \u03ba to minimize L\n11: end while\nobservation, and language instruction into their corresponding embeddings. All these embeddings align with the definition provided in the HULC model. Additionally, to extract the overall process information from the language instruction, we introduce extra language embedding. This process information is crucial for inferring the intermediate-level compositions of base skills required for successful task completion. We further analyze four key parts in our structure: \u2022 Skill Embedding Selector: The skill embedding selector, denoted as f\u03bb, selects skill embeddings in the pre-trained\n\u2022 Skill Embedding Selector: The skill embedding selector, denoted as f\u03bb, selects skill embeddings in the pre-trained\nlatent space. A bidirectional LSTM network is employed for this skill embedding selector. \u2022 Base Skill Selector: The base skill selector f\u03c9, also a bidirectional LSTM network, determines the base skill to which a given skill belongs. \u2022 Base Skill Locator: The base skill locator shares the same parameters with base skill locator f\u03ba in Figure 2. It has the task of locating the base skill locations in the latent space. The input to this network is a 3 \u00d7 3 identity matrix, signifying the one-hot representing of three base skills. These locations are used to calculate the regularization loss. \u2022 Skill Generator: The skill generator, denoted as f\u03b8 = p\u03b8(x|z) : Az \u2192Askill, shares the same parameters with the decoder component in Figure 2. Its parameters are frozen during the imitation learning process. Its function is to transform space from skill embedding space Az to skill space Askill. These skills (action sequences) are combined chronologically for a longer action sequence. The objective of our model is to learn a policy \u03c0(x|sc, sg) conditioned on the current state sc and the goal state sg and outputting x, a sequence of actions, namely a skill. Since we ntroduced the base skill concept into our model, the policy \u03c0(\u00b7) should also find the best base skill y for the current observation. We have \u03c0(x, y|sc, sg), where y is the base skill he agent chooses based on the current state and goal state. nspired by the conditional variational autoencoder (CVAE): log p(x|c) \u2265Eq(z|x,c)[log p(x|z, c)] \u2212DKL(q(z|x, c)||p(z|c)) (3) where c is a symbol to describe a general condition, we would ike to extend the above equation by integrating y which ndicates the base skill. The evidence we want to maximize\n(3)\n1: Given:\n1: Given: \u2022 D : {(Dplay, Dlang)}: Play Dataset and Language Dataset \u2022 F = {f\u03a6, f\u03bb, f\u03ba, f\u03c9, f\u03b8}. They are the encoder f\u03a6, the skill embedding selecter f\u03bb, the base skill locator f\u03ba, the base skill selector f\u03c9, the skill generator networks f\u03b8 with parameters \u03a6, \u03bb, \u03ba, \u03c9, and \u03b8, respectively. 2: Randomly initialize model parameters {\u03a6, \u03bb, \u03c9} 3: Initialize parameters \u03b8 and \u03ba with pre-trained skill generator and base skill locator 4: Freeze the parameters \u03b8 and \u03ba. 5: while not done do 6: L \u21900 7: for l in {play, lang} do 8: Sample a (demonstration, context) (xl, cl) \u223cDl 9: Encode the observation, goal, and plan embeddings, using the encoder network f\u03a6 10: Skill Embedding Selecter f\u03bb selects the skill embedding sequence 11: Determinate a sequence of base skill probabilities with Base Skill Selector f\u03c9. 12: Determinate base skill locations in the latent space with Base Skill Locator f\u03ba 13: Skill Generator f\u03b8 maps the skill embeddings to action sequences. 14: Calculate the loss function Ll according to (4) 15: Accumulate imitation loss L += Ll 16: end for 17: update parameters {\u03a6, \u03bb, \u03c9} w.r.t L 18: end while\nthen turns to p(x, y|c). We employ the approximate variational posterior q(y, z|x, c) to approximate the intractable true posterior p(y, z|x, c) where z indicates the skill embeddings in the skill latent space. We intend to find the ELBO by measuring the KL divergence between the true posterior and the posterior approximation (detailed theoretical motivation in the Appendix). We have\n(4)\n\u2212 \ufffd \ufffd\ufffd \ufffd ||| where c represents a combination of the current state and the goal state (sc, sg). z is skill embedding in the latent skill space. p\u03b8(x|z) is the skill generator network f\u03b8 with parameters \u03b8 and it is trained by VAEs discussed in the previous session and frozen during the imitation learning. f\u03c9 = q\u03c9(y|c) corresponds to the skill labeller with parameter \u03c9. q\u03a6,\u03bb(z|x, c) refers to the encoder network f\u03a6 plus the skill embedding selector network f\u03bb. Furthermore, p\u03ba(z|y) constitutes the base\nskill prior locater f\u03ba with parameter \u03ba. It is also trained by VAEs, as discussed in the previous section and frozen during the training process. Here, we use Huber loss as the metric for reconstructive loss. Intuitively, the base skill regularizer is used to regularize a skill embedding, depending on its base skill category. The categorial regularizer aims to regularize the base skill classification based on the prior categorical distribution of y. The overall algorithm can be seen in Algorithm 2.\n# IV. EXPERIMENTS\nIn this section, we present the experiments conducted to investigate the generalization ability of our model in comparison to other baselines. We choose the CALVIN [19] benchmark to evaluate our model. The CALVIN benchmark is introduced to facilitate learning language-conditioned tasks across four manipulation environments: A, B, C, and D. Each environment features a Franka Emika Panda robot arm equipped with a gripper and a desk that includes a sliding door and a drawer. Additionally, the desk has a button that can toggle a green light and a switch to control a light bulb. Note that each environment has a different desk with various of textures and the position of static elements such as the sliding door, drawer, light, switch, and button are different across each environment. Experiments are conducted in two settings: (1) a single environment where the training and testing environments are the same, and (2) zero-shot multi-environments where training occurs in the first three environments and testing takes place in the fourth, previously unseen environment. We choose long-horizon multi-task language control (LHMTLC) to evaluate the effectiveness of the learned multitask language-conditioned policy in accomplishing several language instructions in a row under the zero-shot multi environment. We also compare other skill-based reinforcement learning approaches to show the advantages of our approaches against theirs. We analyze the result of our model by comparing it to other baselines (shown in Table I). We evaluate the models with 1000 five-task chains. The columns labeled from one to five demonstrate the success rate of continuously completing that number of tasks in a row. The average length indicates the average number of tasks the agent can continuously complete when given five tasks in a row (The remaining tasks are not performed if one task fails in the middle). Subsequently, ablation studies on hyperparameters \u03b31,\u03b32 in (4) and the length of skill Nh (the default value is 5) are performed in the zeroshot multi-environment. Each model is evaluated three times across 3 random seeds.\n# A. Environment Result\nAs evidenced in Table I, our model substantially improves compared to our baselines HULC and MCIL in a zero-shot multi-environment setting. Compared to the current SOTA model HULC, the success rate of completing one to five tasks in a row has increased by 32.4% , 29.8%, 21.9 %, 12.8%, and 6.9 %, respectively. The overall average length increased from 0.67 to 1.71. Note that zero-shot multi-environment presents a challenging environment, as the agent must solve tasks in an unfamiliar environment. The performance in this setting\nEnvironment\nMethod\nTrain \u2192Test\nLH-MTLC\nNo. Instructions in a Row (1000 chains)\n1\n2\n3\n4\n5\nAvg. Len.\nZero-shot Multi\nEnvironment\nMCIL\nA,B,C \u2192D\n30.4%\n1.3%\n0.17 %\n0%\n0%\n0.31\nHULC\nA,B,C \u2192D\n41.8% (2.3)\n16.5% (2.5)\n5.7% (1.3)\n1.9% (0.9)\n1.1% (0.5)\n0.67 (0.1)\nSPIL (Ours)\nA,B,C \u2192D\n74.2% (1.4)\n46.3% (3.4)\n27.6% (3.4)\n14.7% (2.3)\n8.0% (1.7)\n1.71 (0.11)\n3D Diffuser Actor*\u2020\nA,B,C \u2192D\n92.2 %\n78.7 %\n63.9 %\n51.2 %\n41.2 %\n3.27\nGR-1*\u2020\nA,B,C \u2192D\n85.4 %\n71.2 %\n59.6 %\n49.7 %\n40.1 %\n3.06\nSuSIE*\nA,B,C \u2192D\n87.0 %\n69.0 %\n49.0 %\n38.0 %\n26.0 %\n2.69\nRoboFlamingo*\nA,B,C \u2192D\n82.4 %\n61.9 %\n46.6 %\n33.1 %\n23.5 %\n2.47\n\u03b31 = 1.0 \u00d7 10\u22122\nA,B,C \u2192D\n71.3% (1.4)\n45.8% (3.8)\n25.4% (2.3)\n13.1% (0.9)\n6.5% (0.5)\n1.62 (0.05)\n\u03b32 = 1.0 \u00d7 10\u22124\nA,B,C \u2192D\n70.6% (4.2)\n46.3% (3.2)\n25.1% (3.0)\n14.1% (1.0)\n7.3% (1.3)\n1.63 (0.08)\nNh = 4\nA,B,C \u2192D\n71.4% (2.1)\n41,0% (3,1)\n24.1% (1,2)\n12.1% (1.1)\n7.4% (0.6)\n1.58 (0.07)\nNh = 6\nA,B,C \u2192D\n74.0% (1.6)\n44.2% (2.6)\n25.2% (2.0)\n13.0% (2.4)\n7.9% (1.7)\n1.65 (0.08)\nw/o base skills\nA,B,C \u2192D\n57.5% (1.8)\n27.9% (2.1)\n12.2% (1.2)\n5.0% (1.1)\n2.2% (0.7)\n1.05 (0.06)\nSingle\nEnvironment\nMCIL\nD \u2192D\n76.4% (1.5)\n48.8% (4.1)\n30.1% (4.5)\n18.1% (3.0)\n9.3% (3.5)\n1.82 (0.2)\nHULC\nD \u2192D\n82.7% (0.3)\n64.9% (1.7)\n50.4% (1.5)\n38.5% (1.9)\n28.3% (1.8)\n2.64 (0.05)\nSPIL (Ours)\nD \u2192D\n84.6% (0.6)\n65.1% (1.3)\n50.8% (0.4)\n38.0% (0.6)\n28.6% (0.3)\n2.67 (0.01)\n* indicates that the model leverages pre-trained foundation models. \u2020 means that the model leverages extra proprioceptive state as input\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/74c9/74c9a537-e040-4134-a178-7ec0612609c9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e employ the multi-task language control (MTLC) setting in the CALVIN benchmark, en  the simulated CALVIN environment D and directly applied to the real-world setting.</div>\n<div style=\"text-align: center;\">Fig. 5. Real-world experiments. We employ the multi-task language control (MTLC) setting in the CALVIN benchmark, encompassing a total of 10 tasks as listed above. The agent is trained in the simulated CALVIN environment D and directly applied to the real-world setting.</div>\nrepresents the agent\u2019s ability to truly understand and connect the concepts in language instructions with real objects and actions. The performance of our model demonstrates a significant improvement, thus confirming our hypothesis that using skill priors to learn intermediate-level task composition can improve generalization capabilities. The other SOTA models - SuSIE [29], RoboFlamingo [30], GR-1 [31], 3D Diffuser Actor [32], which leverage pre-trained foundation models, as listed in Table I for reference. It is worth mentioning that our SPIL model also outperforms the baselines in the single environment setting.\n# B. Real-world Experiments\nTo investigate the viability of the policy trained in a simulated environment to real-world scenarios, we conduct a sim2real experiment without any additional specific adaptation (zero-shot), as shown in Figure 5. We designed the real-world environment to closely resemble the simulated CALVIN environment D. The rightmost part of Figure 4 illustrates that the real-world environment comprises one switch, one cabinet with a slider, one button, one drawer, and three blocks in red, pink, and blue colors. Additionally, two RGB cameras are employed to capture the static observation and gripper observations.\n<div style=\"text-align: center;\">TABLE II REAL-WORLD EXPERIMENT RESULTS</div>\nTasks\nHULC\nSPIL\nTask\nHULC\nSPIL\nopen drawer\n0%\n30%\nmove slider right\n0%\n30%\nclose drawer\n0%\n40%\npush button\n10%\n50%\ntoggle switch on\n10%\n40%\nlift red block\n0%\n20%\ntoggle switch off\n10%\n30%\nlift blue block\n0%\n20%\nmove slider left\n0%\n40%\nlift pink block\n0%\n30%\nAverage:\nHULC (3%)\nSPIL (33%)\nTable II lists the tasks performed and the corresponding success rate. The agent is trained in four CALVIN environments (A, B, C, D), and the trained policy is directly applied to a real-world environment. To mitigate the influence of the robot\u2019s initial position on the policies, we execute 10 rollouts for each task, maintaining identical starting positions. The table results demonstrate our model\u2019s effectiveness in handling the challenging zero-shot sim2real experiments. Despite the substantial differences between the simulation and real-world contexts, our model still achieves an average success rate of 33% in accomplishing the tasks. Conversely, the HULC model-trained agent struggles with these tasks, with a 3% average success rate, underscoring the difficulty of solving\nreal-world challenges. The results from real-world experiments further substantiate our claim that our proposed method exhibits superior generalization capabilities, enabling successful task completion even in unfamiliar environments.\n# V. CONCLUSION\nIn this paper, we introduced a novel imitation learning paradigm that integrates base skills into imitation learning. Our proposed SPIL model effectively improves the generalization ability compared to current baselines and substantially surpasses the SOTA models on the language-conditioned robotic manipulation CALVIN benchmark, especially under the challenging zero-shot multi environment setting. This work also aims to contribute towards the development of general-purpose robots that can effectively integrate human language with their perception and actions.\n# REFERENCES\n[1] H. Zhou et al., \u201cLanguage-conditioned learning for robotic manipulation: A survey,\u201d 2024. [Online]. Available: https://arxiv.org/abs/2312.10807 [2] D. Bahdanau, F. Hill, J. Leike, E. Hughes, P. Kohli, and E. Grefenstette, \u201cLearning to follow language instructions with adversarial reward induction,\u201d CoRR, vol. abs/1806.01946, 2018. [3] S. Nair, E. Mitchell, K. Chen, I. Brian, S. Savarese, and C. Finn, \u201cLearning language-conditioned robot behavior from offline data and crowd-sourced annotation,\u201d in Proceedings of the 5th Conference on Robot Learning, ser. Proceedings of Machine Learning Research, A. Faust, D. Hsu, and G. Neumann, Eds., vol. 164. PMLR, 08\u201311 Nov 2022, pp. 1303\u20131315. [Online]. Available: https://proceedings.mlr.press/v164/nair22a.html [4] P. Goyal, S. Niekum, and R. Mooney, \u201cPixl2r: Guiding reinforcement learning using natural language by mapping pixels to rewards,\u201d in Proceedings of the 2020 Conference on Robot Learning, ser. Proceedings of Machine Learning Research, J. Kober, F. Ramos, and C. Tomlin, Eds., vol. 155. PMLR, 16\u201318 Nov 2021, pp. 485\u2013497. [Online]. Available: https://proceedings.mlr.press/v155/goyal21a.html [5] Z. Bing, A. Koch, X. Yao, K. Huang, and A. Knoll, \u201cMeta-reinforcement learning via language instructions,\u201d in Proceedings of the IEEE International Conference on Robotics and Automation, London, UK, 2023. [6] X. Yao et al., \u201cLearning from symmetry: Meta-reinforcement learning with symmetrical behaviors and language instructions,\u201d in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023, pp. 5574\u20135581. [7] S. Stepputtis, J. Campbell, M. Phielipp, S. Lee, C. Baral, and H. Ben Amor, \u201cLanguage-conditioned imitation learning for robot manipulation tasks,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 13 139\u201313 150, 2020. [8] E. Jang et al., \u201cBC-z: Zero-shot task generalization with robotic imitation learning,\u201d in 5th Annual Conference on Robot Learning, 2021. [Online]. Available: https://openreview.net/forum?id=8kbp23tSGYv [9] C. Lynch et al., \u201cLearning latent plans from play,\u201d Conference on Robot Learning (CoRL), 2019. [10] C. Lynch and P. Sermanet, \u201cLanguage conditioned imitation learning over unstructured data,\u201d Robotics: Science and Systems, 2021. [Online]. Available: https://arxiv.org/abs/2005.07648 [11] E. Rosete-Beas, O. Mees, G. Kalweit, J. Boedecker, and W. Burgard, \u201cLatent plans for task-agnostic offline reinforcement learning,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2209.08959 [12] Z. J. Cui, Y. Wang, N. M. M. Shafiullah, and L. Pinto, \u201cFrom play to policy: Conditional behavior generation from uncurated robot data,\u201d arXiv preprint arXiv:2210.10047, 2022. [13] J. Borja-Diaz, O. Mees, G. Kalweit, L. Hermann, J. Boedecker, and W. Burgard, \u201cAffordance learning from play for sample-efficient policy learning,\u201d in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 6372\u20136378. [14] O. Mees, L. Hermann, and W. Burgard, \u201cWhat matters in language conditioned robotic imitation learning over unstructured data,\u201d IEEE Robotics and Automation Letters, vol. 7, no. 4, pp. 11 205\u201311 212, 2022. [15] O. Mees, J. Borja-Diaz, and W. Burgard, \u201cGrounding language with visual affordances over unstructured data,\u201d 2023.\n[16] L. X. Shi, J. J. Lim, and Y. Lee, \u201cSkill-based model-based reinforcement learning,\u201d in 6th Annual Conference on Robot Learning, 2022. [17] A. Nagabandi, K. Konolige, S. Levine, and V. Kumar, \u201cDeep dynamics models for learning dexterous manipulation,\u201d in Conference on Robot Learning. PMLR, 2020, pp. 1101\u20131112. [18] Wu et al., \u201cTidybot: Personalized robot assistance with large language models,\u201d Autonomous Robots, 2023. [19] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard, \u201cCalvin: A benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks,\u201d IEEE Robotics and Automation Letters (RAL), vol. 7, no. 3, pp. 7327\u20137334, 2022. [20] J. Pont-Tuset, J. Uijlings, S. Changpinyo, R. Soricut, and V. Ferrari, \u201cConnecting vision and language with localized narratives,\u201d in Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part V 16. Springer, 2020, pp. 647\u2013664. [21] J. Lu, D. Batra, D. Parikh, and S. Lee, \u201cVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,\u201d Advances in neural information processing systems, vol. 32, 2019. [22] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, \u201cWhat does BERT with vision look at?\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics, Jul. 2020, pp. 5265\u20135275. [Online]. Available: https://aclanthology.org/2020.acl-main.469 [23] M. Shridhar, D. Mittal, and D. Hsu, \u201cIngress: Interactive visual grounding of referring expressions,\u201d The International Journal of Robotics Research, vol. 39, no. 2-3, pp. 217\u2013232, 2020. [24] A. Magassouba, K. Sugiura, A. T. Quoc, and H. Kawai, \u201cUnderstanding natural language instructions for fetching daily objects using gan-based multimodal target\u2013source classification,\u201d IEEE Robotics and Automation Letters, vol. 4, no. 4, pp. 3884\u20133891, 2019. [25] W. Liu, C. Paxton, T. Hermans, and D. Fox, \u201cStructformer: Learning spatial structure for language-guided semantic rearrangement of novel objects,\u201d in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 6322\u20136329. [26] M. Shridhar, L. Manuelli, and D. Fox, \u201cCliport: What and where pathways for robotic manipulation,\u201d in Conference on Robot Learning. PMLR, 2022, pp. 894\u2013906. [27] Co-Reyes et al., \u201cGuiding policies with language via meta-learning,\u201d in International Conference on Learning Representations, 2018. [28] C. Lynch and P. Sermanet, \u201cLanguage conditioned imitation learning over unstructured data,\u201d arXiv preprint arXiv:2005.07648, 2020. [29] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine, \u201cZero-shot robotic manipulation with pretrained image-editing diffusion models,\u201d 2023. [Online]. Available: https: //arxiv.org/abs/2310.10639 [30] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing, W. Zhang, H. Liu et al., \u201cVision-language foundation models as effective robot imitators,\u201d arXiv preprint arXiv:2311.01378, 2023. [31] H. Wu, Y. Jing, C. Cheang, G. Chen, J. Xu, X. Li, M. Liu, H. Li, and T. Kong, \u201cUnleashing large-scale video generative pretraining for visual robot manipulation,\u201d 2023. [Online]. Available: https://arxiv.org/abs/2312.13139 [32] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki, \u201c3d diffuser actor: Policy diffusion with 3d scene representations,\u201d 2024. [Online]. Available: https://arxiv.org/abs/2402.10885 [33] K. Hausman, J. T. Springenberg, Z. Wang, N. Heess, and M. Riedmiller, \u201cLearning an embedding space for transferable robot skills,\u201d in International Conference on Learning Representations, 2018. [Online]. Available: https://openreview.net/forum?id=rk07ZXZRb [34] J. Merel et al., \u201cNeural probabilistic motor primitives for humanoid control,\u201d in International Conference on Learning Representations, 2019. [Online]. Available: https://openreview.net/forum?id=BJl6TjRcY7 [35] T. Kipf, Y. Li, H. Dai, V. F. Zambaldi, A. Sanchez-Gonzalez, E. Grefenstette, P. Kohli, and P. W. Battaglia, \u201cCompILE: Compositional Imitation Learning and Execution,\u201d in Proceedings of the 36th International Conference on Machine Learning. PMLR, 2019, pp. 3418\u20133428. [36] Y. Lee, J. Yang, and J. J. Lim, \u201cLearning to coordinate manipulation skills via skill behavior diversification,\u201d in International Conference on Learning Representations, 2020. [Online]. Available: https: //openreview.net/forum?id=ryxB2lBtvH [37] K. Pertsch, Y. Lee, Y. Wu, and J. J. Lim, \u201cDemonstration-guided reinforcement learning with learned skills,\u201d 5th Conference on Robot Learning, 2021. [38] K. Pertsch, Y. Lee, and J. J. Lim, \u201cAccelerating reinforcement learning with learned skill priors,\u201d in Conference on Robot Learning (CoRL), 2020.\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The paper addresses the limitations of existing language-conditioned robot manipulation approaches, particularly their inability to adapt to unfamiliar environments. The historical context includes advancements in deep reinforcement learning and imitation learning, which have shown promise but still face challenges in generalization and data efficiency.",
            "purpose of benchmark": "The benchmark is intended to evaluate the performance of various language-conditioned robotic manipulation models, focusing on their ability to generalize across different environments and tasks."
        },
        "problem": {
            "definition": "The benchmark is designed to assess the capability of models to execute complex manipulation tasks based on natural language instructions in both familiar and unfamiliar environments.",
            "key obstacle": "Existing benchmarks often require extensive expert demonstration data, which is labor-intensive to collect, and they struggle to ensure generalization to unseen environments."
        },
        "idea": {
            "intuition": "The inspiration behind the benchmark comes from the need to reduce reliance on expert demonstrations and enhance the generalization ability of robotic agents in novel settings by incorporating base skill priors.",
            "opinion": "The authors believe that the benchmark is crucial for advancing the field of language-conditioned robot manipulation, as it provides a means to evaluate and compare different approaches under challenging conditions.",
            "innovation": "The benchmark introduces a novel approach that integrates base skill priors into imitation learning, enabling agents to learn intermediate-level policies that enhance generalization capabilities.",
            "benchmark abbreviation": "SPIL"
        },
        "dataset": {
            "source": "The dataset is sourced from human demonstrations in simulation environments, with a small portion labeled with language instructions, significantly reducing the effort required for data collection.",
            "desc": "The dataset consists of play data obtained through interactions in virtual reality environments, allowing for a larger and more diverse training set.",
            "content": "The dataset includes sequences of actions corresponding to manipulation tasks, labeled with base skills such as translation, rotation, and grasping.",
            "size": "1,000,000",
            "domain": "Robotics",
            "task format": "Long-horizon multi-task language control"
        },
        "metrics": {
            "metric name": "Success Rate, Average Task Length",
            "aspect": "The metrics measure the effectiveness of the models in completing tasks and the average number of tasks successfully executed in sequence.",
            "principle": "The metrics were chosen to reflect both the accuracy of task completion and the efficiency of the learned policies in handling multiple instructions.",
            "procedure": "Models are evaluated based on their performance in executing sequences of tasks, with statistical significance assessed through multiple trials."
        },
        "experiments": {
            "model": "The models tested include state-of-the-art approaches like HULC and MCIL, along with the proposed SPIL model.",
            "procedure": "Experiments were conducted in both single and zero-shot multi-environment settings, comparing the performance of different models across various tasks.",
            "result": "SPIL significantly outperformed HULC and MCIL in terms of success rates and average task length, demonstrating superior generalization capabilities.",
            "variability": "Variability in results was accounted for by conducting multiple trials across different random seeds and environments."
        },
        "conclusion": "The experiments confirm that the SPIL model enhances generalization in language-conditioned robotic manipulation, outperforming existing benchmarks and suggesting its potential for real-world applications.",
        "discussion": {
            "advantage": "The benchmark allows for a comprehensive evaluation of model performance in challenging, real-world-like conditions, contributing to advancements in language-conditioned robotics.",
            "limitation": "Potential limitations include the reliance on simulation data, which may not fully capture the complexities of real-world environments.",
            "future work": "Future research could explore further enhancements to the benchmark, including additional task types and environments to broaden its applicability."
        },
        "other info": {
            "info1": "The benchmark focuses on integrating human-like skill composition into robotic learning.",
            "info2": {
                "info2.1": "The SPIL model achieved a 30% improvement in zero-shot evaluations compared to previous state-of-the-art models.",
                "info2.2": "The methodology emphasizes the importance of skill priors in enhancing the learning process."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the limitations of existing language-conditioned robot manipulation approaches, particularly their inability to adapt to unfamiliar environments."
        },
        {
            "section number": "1.2",
            "key information": "The historical context includes advancements in deep reinforcement learning and imitation learning, which have shown promise but still face challenges in generalization and data efficiency."
        },
        {
            "section number": "1.3",
            "key information": "The benchmark is intended to evaluate the performance of various language-conditioned robotic manipulation models, focusing on their ability to generalize across different environments and tasks."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark is designed to assess the capability of models to execute complex manipulation tasks based on natural language instructions in both familiar and unfamiliar environments."
        },
        {
            "section number": "2.2",
            "key information": "The benchmark introduces a novel approach that integrates base skill priors into imitation learning, enabling agents to learn intermediate-level policies that enhance generalization capabilities."
        },
        {
            "section number": "3.1",
            "key information": "The dataset consists of play data obtained through interactions in virtual reality environments, allowing for a larger and more diverse training set."
        },
        {
            "section number": "3.2",
            "key information": "The metrics measure the effectiveness of the models in completing tasks and the average number of tasks successfully executed in sequence."
        },
        {
            "section number": "3.3",
            "key information": "SPIL significantly outperformed HULC and MCIL in terms of success rates and average task length, demonstrating superior generalization capabilities."
        },
        {
            "section number": "6",
            "key information": "The benchmark allows for a comprehensive evaluation of model performance in challenging, real-world-like conditions, contributing to advancements in language-conditioned robotics."
        },
        {
            "section number": "8",
            "key information": "The experiments confirm that the SPIL model enhances generalization in language-conditioned robotic manipulation, outperforming existing benchmarks and suggesting its potential for real-world applications."
        }
    ],
    "similarity_score": 0.5999557284377085,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-14-1834_natur/papers/Language-Conditioned Imitation Learning with Base Skill Priors under Unstructured Data.json"
}