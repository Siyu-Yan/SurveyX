{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1805.10014",
    "title": "KONG: Kernels for ordered-neighborhood graphs",
    "abstract": " Abstract\nAbstract\nWe present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets. In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.\n[cs.LG]\narXiv:1805.10014v2 \n# 1 Introduction\nGraphs are ubiquitous representations for structured data and have found numerous applications in machine learning and related fields, ranging from community detection in online social networks [10] to protein structure prediction [28]. Unsurprisingly, learning from graphs has attracted much attention from the research community. Graphs kernels have become a standard tool for graph classification [18]. Given a large collection of graphs, possibly with node and edge attributes, we are interested in learning a kernel function that best captures the similarity between any two graphs. The graph kernel function can be used to classify graphs using standard kernel methods such as support vector machines. Graph similarity is a broadly defined concept and therefore many different graph kernels with different properties have been proposed. Previous works have considered graph kernels for different graph classes distinguishing between simple unweighted graphs without node or edge attributes, graph",
    "bib_name": "draief2018kongkernelsorderedneighborhoodgraphs",
    "md_text": "# ONG: Kernels for ordered-neighborhood g\nMoez Draief1, Konstantin Kutzkov\u22172, Kevin Scaman1, and Milan Vojnovic2\n1Noah\u2019s Ark Lab Paris, Huawei Technologies Ltd 2London School of Economics, London, UK\n 29 May 2018\nMay 30, 2018\n# Abstract\nAbstract\nWe present novel graph kernels for graphs with node and edge labels that have ordered neighborhoods, i.e. when neighbor nodes follow an order. Graphs with ordered neighborhoods are a natural data representation for evolving graphs where edges are created over time, which induces an order. Combining convolutional subgraph kernels and string kernels, we design new scalable algorithms for generation of explicit graph feature maps using sketching techniques. We obtain precise bounds for the approximation accuracy and computational complexity of the proposed approaches and demonstrate their applicability on real datasets. In particular, our experiments demonstrate that neighborhood ordering results in more informative features. For the special case of general graphs, i.e. graphs without ordered neighborhoods, the new graph kernels yield efficient and simple algorithms for the comparison of label distributions between graphs.\n[cs.LG]\narXiv:1805.10014v2 \n# 1 Introduction\nGraphs are ubiquitous representations for structured data and have found numerous applications in machine learning and related fields, ranging from community detection in online social networks [10] to protein structure prediction [28]. Unsurprisingly, learning from graphs has attracted much attention from the research community. Graphs kernels have become a standard tool for graph classification [18]. Given a large collection of graphs, possibly with node and edge attributes, we are interested in learning a kernel function that best captures the similarity between any two graphs. The graph kernel function can be used to classify graphs using standard kernel methods such as support vector machines. Graph similarity is a broadly defined concept and therefore many different graph kernels with different properties have been proposed. Previous works have considered graph kernels for different graph classes distinguishing between simple unweighted graphs without node or edge attributes, graphs with discrete node and edge labels, and graphs with more complex attributes such as real-valued vectors and partial labels. For evolving graphs, the ordering of the node neighborhoods can be indicative for the graph class. Concrete examples include graphs that describe user web browsing patterns, evolving networks such as social graphs, product purchases and reviews, ratings in recommendation systems, co-authorship networks, and software API calls used for malware detection. The order in which edges are created can be informative about the structure of the original data. To the best of our knowledge, existing graph kernels do not consider this aspect. Addressing the gap, we present a novel framework for graph kernels where the edges adjacent to a node follow specific order. The proposed algorithmic framework KONG, referring to Kernels for Ordered-Neighborhood Graphs, accommodates highly efficient algorithms that scale to both massive graphs and large collections of graphs. The key ideas are: (a) representation of each node neighborhood by a string using a tree traversal method, and (b) efficient\n\u2217Corresponding author: kutzkov@gmail.com\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8c4e/8c4e9a60-ce0f-47f4-b55b-4da745cfc066.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: An illustrative example of an order-neighborhood graph: the neighhbor order is the letter alphabetical order.</div>\nFigure 1: An illustrative example of an order-neighborhood graph: the neighhbor or order.\n<div style=\"text-align: center;\">Figure 1: An illustrative example of an order-neighborhood graph: the neighhbor order is the letter alphabetical</div>\ncomputation of explicit graph feature maps based on generating k-gram frequency vectors of each node\u2019s string without explicitly storing the strings. The latter enables to approximate the explicit feature maps of various kernel functions using sketching techniques. Explicit feature maps correspond to k-gram frequency vectors of node strings, and sketching amounts to incrementally computing sketches of these frequency vectors. The proposed algorithms allow for flexibility in the choice of the string kernel and the tree traversal method. In Figure 1 we present a directed labeled subgraph rooted at node v. A breadth first-search traversal would result in the string ABCDGEFHG but other traversal approaches might yield more informative strings. Related work Many graph kernels with different properties have been proposed in the literature. Most of them work with implicit feature maps and compare pairs of graphs, we refer to [18] for a study on implicit and explicit graph feature maps. Most related to our work is the Weisfeiler-Lehman kernel [30] that iteratively traverses the subtree rooted at each node and collects the corresponding labels into a string. Each string is sorted and the strings are compressed into unique integers which become the new node labels. After h iterations we have a label at each node. The convolutional kernel that compares all pairs of node labels using the Dirac kernel (indicator of an exact match of node labels) is equivalent to the inner product of the label distributions. However, this kernel might suffer from diagonal dominance where most nodes have unique labels and a graph is similar to itself but not to other graphs in the dataset. The shortcoming was addressed in [33]. The kernel between graphs G and G\u2032 is computed as \u03ba(G, G\u2032) = \u03a6(G)T M\u03a6(G\u2032) where M is a pre-computed matrix that measures the similarity between labels. The matrix can become huge and the approach is not applicable to large-scale graphs. While the Weisfeiler-Lehman kernel applies to ordered neighborhoods, for large graphs it is likely to result in many unique strings and comparing them with the Dirac kernel might yield poor results, both in terms of accuracy and scalability. In a recent work [21] presented an unsupervised learning algorithm that generates feature vectors from labeled graphs by traversing the neighbor edges in a predefined order. Even if not discussed in the paper, the generated vectors correspond to explicit feature maps for convolutional graph kernels with Dirac base kernels. Our approach provides a highly-scalable algorithmic framework that allows for different base kernels and different tree traversal methods. Another line of research related to our work presents algorithms for learning graph vector representations [25, 12, 23]. Given a collection of labeled graphs, the goal is to map the graphs (or their nodes) to a feature space that best represents the graph structure. These approaches are powerful and yield the state-of-theart results but they involve the optimization of complex objective functions and do not scale to massive graphs.\n\u2022 To the best of our knowledge, this is the first work to focus and formally define graph kernels for graphs with ordered node neighborhoods. Extending upon string kernels, we present and formally analyse a family of graph kernels that can be applied to different problems. The KONG framework presents algorithms that are efficient with respect to two parameters, the total number of graphs N and the total number of edges\nM. We propose approaches that compute an explicit feature map for each graph which enables the use of linear SVMs for graph classification, thus avoiding the computation of a kernel matrix of size O(N 2). Leveraging advanced sketching techniques, an approximation of the explicit feature map for a graph with m edges can be computed in time and space O(m) or a total O(M). We also present an extension to learning from graph streams using sublinear space o(M).1 \u2022 For general labeled graphs without neighbor ordering our approach results in new graph kernels that compare the label distribution of subgraphs using widely used kernels such as the polynomial and cosine kernels. We argue that the approach can be seen as an efficient smoothing algorithm for node labelling kernels such as the Weisfeiler-Lehman kernel. An experimental evaluation on real graphs shows that the proposed kernels are competitive with state-of-the-art kernels, achieving better accuracy on some benchmark datasets and using compact feature maps. \u2022 The presented approach can be viewed as an efficient algorithm for learning compact graph representations. The primary focus of the approach is on learning explicit feature maps for a class of base kernels for the convolutional graph kernel. However, the algorithms learn vector embeddings that can be used by other machine learning algorithms such as logistic regression, decision trees and neural networks as well as unsupervised methods. Paper outline The paper is organised as follows. In Section 2 we discuss previous work, provide motivating examples and introduce general concepts and notation. In Section 3 we first give a general\nM. We propose approaches that compute an explicit feature map for each graph which enables the use of linear SVMs for graph classification, thus avoiding the computation of a kernel matrix of size O(N 2) Leveraging advanced sketching techniques, an approximation of the explicit feature map for a graph with m edges can be computed in time and space O(m) or a total O(M). We also present an extension to learning from graph streams using sublinear space o(M).1\n For general labeled graphs without neighbor ordering our approach results in new graph kernels tha compare the label distribution of subgraphs using widely used kernels such as the polynomial and cosin kernels. We argue that the approach can be seen as an efficient smoothing algorithm for node labellin kernels such as the Weisfeiler-Lehman kernel. An experimental evaluation on real graphs shows tha the proposed kernels are competitive with state-of-the-art kernels, achieving better accuracy on som benchmark datasets and using compact feature maps.\n The presented approach can be viewed as an efficient algorithm for learning compact graph representations The primary focus of the approach is on learning explicit feature maps for a class of base kernels for the convolutional graph kernel. However, the algorithms learn vector embeddings that can be used by other machine learning algorithms such as logistic regression, decision trees and neural networks as well as unsupervised methods.\nPaper outline The paper is organised as follows. In Section 2 we discuss previous work, provide motivating examples and introduce general concepts and notation. In Section 3 we first give a genera overview of the approach and discuss string generation and string kernels and then present theoretical results Experimental evaluation is presented in Section 4. We conclude in Section 5.\n# 2 Preliminaries\nNotation and problem formulation The input is a collection G of tuples (Gi, yi) where Gi is a graph and yi is a class. Each graph is defined as G = (V, E, \u2113, \u03c4) where \u2113: V \u2192L is a labelling function for a discrete set L and \u03c4 defines ordering of node neighborhoods. For simplicity of exposition, we consider only node labels but all presented algorithms naturally apply to edge labels as well. The neighborhood of a node v \u2208V is Nv = {u \u2208V : (v, u) \u2208E}. The ordering function \u03c4v : Nv \u2192\u03a0(Nv) defines a fixed order permutation on Nv, where \u03a0(Nv) denotes the set of all permutations of the elements of Nv. Note that the order is local, i.e., two nodes can have different orderings for same neighborhood sets. Kernels, feature maps and linear support vector machines A function \u03ba : X \u00d7 X \u2192R is a valid kernel if \u03ba(x, y) = \u03ba(y, x) for x, y \u2208X and the kernel matrix K \u2208Rm\u00d7m defined by K(i, j) = \u03ba(xi, xj) for any x1, . . . , xm \u2208X is positive semidefinite. If the function \u03ba(x, y) can be represented as \u03c6(x)T \u03c6(y) for an explicit feature map \u03c6 : X \u2192Y where Y is an inner product feature space, then \u03ba is a valid kernel. Also, a linear combination of kernels is a kernel. Thus, if the base kernel is valid, then the convolutional kernel is\nNotation and problem formulation The input is a collection G of tuples (Gi, yi) where Gi is a graph and yi is a class. Each graph is defined as G = (V, E, \u2113, \u03c4) where \u2113: V \u2192L is a labelling function for a discrete set L and \u03c4 defines ordering of node neighborhoods. For simplicity of exposition, we consider only node labels but all presented algorithms naturally apply to edge labels as well. The neighborhood of a node v \u2208V is Nv = {u \u2208V : (v, u) \u2208E}. The ordering function \u03c4v : Nv \u2192\u03a0(Nv) defines a fixed order permutation on Nv, where \u03a0(Nv) denotes the set of all permutations of the elements of Nv. Note that the order is local, i.e., two nodes can have different orderings for same neighborhood sets.\nNotation and problem formulation The input is a collection G of tuples (Gi, yi) where Gi is a graph and yi is a class. Each graph is defined as G = (V, E, \u2113, \u03c4) where \u2113: V \u2192L is a labelling function for a discrete set L and \u03c4 defines ordering of node neighborhoods. For simplicity of exposition, we consider only node labels but all presented algorithms naturally apply to edge labels as well. The neighborhood of a node v \u2208V is Nv = {u \u2208V : (v, u) \u2208E}. The ordering function \u03c4v : Nv \u2192\u03a0(Nv) defines a fixed order permutation on Nv, where \u03a0(Nv) denotes the set of all permutations of the elements of Nv. Note that the order is local, i.e., two nodes can have different orderings for same neighborhood sets. Kernels, feature maps and linear support vector machines A function \u03ba : X \u00d7 X \u2192R is a valid kernel if \u03ba(x, y) = \u03ba(y, x) for x, y \u2208X and the kernel matrix K \u2208Rm\u00d7m defined by K(i, j) = \u03ba(xi, xj) for any x1, . . . , xm \u2208X is positive semidefinite. If the function \u03ba(x, y) can be represented as \u03c6(x)T \u03c6(y) for an explicit feature map \u03c6 : X \u2192Y where Y is an inner product feature space, then \u03ba is a valid kernel. Also, a linear combination of kernels is a kernel. Thus, if the base kernel is valid, then the convolutional kernel is also valid. We will consider base kernels where X = Rn and \u03c6 : Rn \u2192RD. Note that D can be very large or even infinite. The celebrated kernel trick circumvents this limitation by computing the kernel function for all support vectors. But this means that for training one needs to explicitly compute a kernel matrix of size N 2 for N input examples. Also, in large-scale applications, the number of support vectors often grows linearly and at prediction time one needs to evaluate the kernel function for O(N) support vectors. In contrast, linear support vector machines [15], where the kernel is the vector inner product, run in linear time of the number of examples and prediction needs O(D) time. An active area of research has been the design of scalable algorithms that compute low-dimensional approximation of the explicit feature map z : RD \u2192Rd such that d \u226aD and \u03ba(x, y) \u2248z(\u03c6(x))T z(\u03c6(y)) [27, 19, 26].\nKernels, feature maps and linear support vector machines A function \u03ba : X \u00d7 X \u2192R is a valid kernel if \u03ba(x, y) = \u03ba(y, x) for x, y \u2208X and the kernel matrix K \u2208Rm\u00d7m defined by K(i, j) = \u03ba(xi, xj) for any x1, . . . , xm \u2208X is positive semidefinite. If the function \u03ba(x, y) can be represented as \u03c6(x)T \u03c6(y) for an explicit feature map \u03c6 : X \u2192Y where Y is an inner product feature space, then \u03ba is a valid kernel. Also, a linear combination of kernels is a kernel. Thus, if the base kernel is valid, then the convolutional kernel is also valid. We will consider base kernels where X = Rn and \u03c6 : Rn \u2192RD. Note that D can be very large or even infinite. The celebrated kernel trick circumvents this limitation by computing the kernel function for all support vectors. But this means that for training one needs to explicitly compute a kernel matrix of size N 2 for N input examples. Also, in large-scale applications, the number of support vectors often grows linearly and at prediction time one needs to evaluate the kernel function for O(N) support vectors. In contrast, linear support vector machines [15], where the kernel is the vector inner product, run in linear time of the number of examples and prediction needs O(D) time. An active area of research has been the design of scalable algorithms that compute low-dimensional approximation of the explicit feature map z : RD \u2192Rd such that d \u226aD and \u03ba(x, y) \u2248z(\u03c6(x))T z(\u03c6(y)) [27, 19, 26].\nare implementation and data are available at https://github.com/kokiche/KO\nConvolutional graph kernels Most known graph kernels are instances of the family of convolutional kernels [13]. In their simplified form, the convolutional kernels work by decomposing a given graph G into a set of (possibly overlapping) substructures \u0393(G). For example, \u0393(G) can be the set of 1-hop subtrees rooted at each node. The kernel between two graphs G and H is defined as K(G, H) = \ufffd g\u2208\u0393(G),h\u2208\u0393(H) \u03ba(g, h) where \u03ba(g, h) is a base kernel comparing the parts g and h. For example, \u03ba can be the inner product kernel comparing the label distribution of the two subtrees. Known graph kernels differ mainly in the way the graph is decomposed. Notable examples include the random walk kernel [11], the shortest path kernel [4], the graphlet kernel [31] and the Weisfeiler-Lehman kernel [30]. The base kernel is usually the Dirac kernel comparing the parts g and h for equality. Building upon efficient sketching algorithms, we will compute explicit graph feature maps. More precisely, let \u03c6\u03ba be the explicit feature map of the base kernel \u03ba. An explicit feature map \u03a6\u03ba is defined such that for any two graphs G and H:\nWhen clear from the context, we will omit \u03ba and write \u03c6(g) and \u03a6(G) for the explicit maps of the substru g and the graph G.\nString kernels The strings generated from subtree traversal will be compared using string kernels. Let \u03a3\u2217 be the set of all strings that can be generated from the alphabet \u03a3, and let \u03a3\u2217 k \u2282\u03a3\u2217be the set of strings with exactly k characters. Let t \u2291s denote that the string t is a substring of s, i.e., a nonempty sequence of consecutive characters from s. The spectrum string kernel compares the distribution of k-grams between strings s1 and s2: \ufffd\nwhere #t(s) = |{x : x \u2291s and x = t}|, i.e., the number of occurrences of t in s [20]. The explicit feature map for the spectrum kernel is thus the frequency vector \u03c6(s) \u2208N|\u03a3\u2217 k| such that \u03c6i(s) = #t(s) where t is the i-th k-gram in the explicit enumeration of all k-grams. We will consider extensions of the spectrum kernel with the polynomial kernel for p \u2208N: for a constant ,\npoly(s1, s2) = (\u03c6(s1)T \u03c6(s2) + c)p.\nCount-Sketch and Tensor-Sketch Sketching is an algorithmic tool for the summarization of massive datasets such that key properties of the data are preserved. In order to achieve scalability, we will summarize the k-gram frequency vector distributions. In particular, we will use Count-Sketch [6] that for vectors u, v \u2208Rd computes sketches z(u), z(v) \u2208Rb such that z(u)T z(v) \u2248uT v and b < d controls the approximation quality. A key property is that Count-Sketch is a linear projection of the data and this will allow us to incrementally generate strings and sketch their k-gram distribution. For the polynomial kernel poly(x, y) = (xT y + c)p and x, y \u2208Rd, the explicit feature map of x and y is their p-level tensor product, i.e. the dp-dimensional vector formed by taking the product of all subsets of p coordinates of x or y. Hence, computing the explicit feature map and then sketching it using Count-Sketch requires O(dp) time. Instead, using Tensor-Sketch [26], we compute a sketch of size b for a p-level tensor product in time O(p(d + b log b)).\n# 3 Main results\nIn this section we first describe the proposed algorithm, discuss in detail its components, and then present theoretical approximation guarantees for using sketches to approximate graph kernels.\nAlgorithm The proposed algorithm is based on the following key ideas: (a) representation of each node v\u2019s neighborhood by a string Sv using a tree traversal method, and (b) approximating the k-gram frequency vector of string Sv using sketching in a way does not require storing the string Sv. The algorithm steps are described in more detail as follows. Given a graph G, for each node v we traverse the subtree rooted at v using the neighbor ordering \u03c4 and generate a string. The subtrees represent the graph decomposition of the convolutional kernel. The algorithm allows for flexibility in choosing different alternatives for the subtree traversal. The generated strings are compared by a string kernel. This string kernel is evaluated by computing an explicit feature map for the string at each node. Scalability is achieved by approximating explicit feature maps using sketching techniques so that the kernel can be approximated within a prescribed approximation error. The sum of the node explicit feature maps is the explicit feature map of the graph G. The algorithm is outlined in Algorithm 1.\nAlgorithm 1: ExplicitGraphFeatureMap\nInput: Graph G = (V, E, \u2113, \u03c4), depth h, labeling \u2113: V \u2192L, base kernel \u03ba\nfor v \u2208V do\nTraverse the subgraph Tv rooted at v up to depth h\nCollect the node labels \u2113(u) : u \u2208Tv in the order specified by \u03c4v into a string Sv\nSketch the explicit feature map \u03c6\u03ba(Sv) for the base string kernel \u03ba (without storing Sv)\n\u03a6\u03ba(G) \u2190\ufffd\nv\u2208V \u03c6\u03ba(Sv)\nreturn \u03a6\u03ba(G)\nTree traversal and string generation There are different options for string construction from each node neighborhood. We present a general class of subgraph traversal algorithms that iteratively collect the node strings from the respective neighborhood. Definition 1 Let Sh v denote the string collected at node v after h iterations. A subgraph traversal algorithm is called a composite string generation traversal (CSGT) if Sh v is a concatenation of a subset of the strings s0 v, . . . , sh v. Each si v is computed in the i-th iteration and is the concatenation of the strings si\u22121 u for u \u2208Nv, in the order given by \u03c4v. The above definition essentially says that we can iteratively compute the strings collected at a node v from strings collected at v and v\u2019s neighbors in previous iterations, similarly to the dynamic programming paradigm. As we formally show later, this implies that we will be able to collect all node strings Sh v by traversing O(m) edges in each iteration and this is the basis for designing efficient algorithm for computing the explicit feature maps. Next we present two examples of CSGT algorithms. The first one is the standard iterative breadth-first search algorithm that for each node v collects in h + 1 lists the labels of all nodes within exactly i hops, for 0 \u2264i \u2264h. The strings si v collect the labels of nodes within exactly i hops from v. After h iterations, we concatenate the resulting strings, see Algorithm 2. In the toy example in Figure 1, the string at the node with label A is generated as S2 v = s0 vs1 vs2 v resulting in A|BCDG|EF|H|G (s0 v = A, s1 v = BCDG and s2 v = EFGH). Another approach, similar to the WL labeing algorithm [30], is to concatenate the neighbor labels in the order given by \u03c4v for each node v into a new string. In the i-th iteration we set \u2113(v) = si v, i.e., si v becomes v\u2019s new label. We follow the CSGT pattern by setting Sh v = sh v, as evident from Algorithm 3. In our toy example, we have s0 v = A and s1 v = ABCD and s2 v = ABEFCHDGG generated from the neighbor strings s1 u, u \u2208Nv: BEF, CH, DG and G.\nAlgorithm 2: Breadth-first search\nInput: Graph G = (V, E, \u2113, \u03c4), depth h,\nlabeling \u2113: V \u2192S\nfor v \u2208V do\ns0\nv = \u2113(v)\nfor i = 1 to h do\nfor v \u2208V do\nsi\nv = $\n//$ is the empty string\nfor u \u2208\u03c4v(Nv) do\nsi\nv \u2190si\nv.append(si\u22121\nu\n)\nfor v \u2208V do\nSh\nv = $\nfor i = 0 to h do\nSh\nv \u2190Sh\nv .append(si\nv)\nAlgorithm 3: Weisfeiler-Lehman\nInput: Graph G = (V, E, \u2113, \u03c4), depth h,\nlabeling \u2113: V \u2192S\nfor v \u2208V do\nfor i = 1 to h do\nsi\nv \u2190\u2113(v)\nfor i = 1 to h do\nfor v \u2208V do\nfor u \u2208\u03c4v(Nv) do\nsi\nv \u2190si\nv.append(si\u22121\nu\n)\nfor v \u2208V do\nSh\nv \u2190sh\nv\nAlgorithm 2: Breadth-first search\nInput: Graph G = (V, E, \u2113, \u03c4), depth h,\nlabeling \u2113: V \u2192S\nfor v \u2208V do\ns0\nv = \u2113(v)\nfor i = 1 to h do\nfor v \u2208V do\nsi\nv = $\n//$ is the empty string\nfor u \u2208\u03c4v(Nv) do\nsi\nv \u2190si\nv.append(si\u22121\nu\n)\nfor v \u2208V do\nSh\nv = $\nfor i = 0 to h do\nSh\nv \u2190Sh\nv .append(si\nv)\nString kernels and WL kernel smoothing: After collecting the strings at each node we have to compare them. An obvious choice would be the Dirac kernel which compares two strings for equality. This would yield poor results for graphs of larger degree where most collected strings will be unique, i.e., the diagonal dominance problem where most graphs are similar only to themselves. Instead, we consider extensions of the spectrum kernel [20] comparing the k-gram distributions between strings, as discussed in Section 2. Setting k = 1 is equivalent to collecting the node labels disregarding the neighbor order and comparing the label distribution between all node pairs. In particular, consider the following smoothing algorithm for the WL kernel. In the first iteration we generate node strings from neighbor labels and relabel all nodes such that each string becomes a new label. Then, in the next iteration we again generate strings at each node but instead of comparing them for equality with the Dirac kernel, we compare them with the polynomial or cosine kernels. cos(s1, s2)p decreases faster with p for dissimilar strings, thus p can be seen as a smoothing parameter.\nSketching of k-gram frequency vectors The explicit feature maps for the polynomial kernel for p > 1 can be of very high dimensions. A solution is to first collect the strings Sh v at each node, then incrementally generate k-grams and feed them into a sketching algorithm that computes compact representation for the explicit feature maps of polynomial kernel. However, for massive graphs with high average degree, or for a large node label alphabet size, we may end up with prohibitively long unique strings at each node. Using the key property of the incremental string generation approach and a sketching algorithm, which is a linear projection of the original data onto a lower-dimensional space, we will show how to sketch the k-gram distribution vectors without explicitly generating the strings Sh v . More concretely, we will replace the line si v \u2190si v.append(si\u22121 u ) in Algorithms 2 and 3 with a sketching algorithm that will maintain the k-gram distribution of each si v as well as si v\u2019s (k \u22121)-prefix and (k \u22121)-suffix. In this way we will only keep track of newly generated k-grams and add up the sketches of the k-gram distribution of the si v strings computed in previous iterations. Before we present the main result, we show two lemmas that state properties of the incremental string generation approach. Observing that in each iteration we concatenate at most m strings, we obtain the following bound on the number of generated k-grams.\nProof: By definition, the string si v generated by CSGT at a node v in the i-th iteration is the concatenatio of strings generated at its neighbor nodes in the i \u22121-th iteration. Therefore, new k-grams can only b\nAlgorithm 3: Weisfeiler-Lehman\nInput: Graph G = (V, E, \u2113, \u03c4), depth h,\nlabeling \u2113: V \u2192S\nfor v \u2208V do\nfor i = 1 to h do\nsi\nv \u2190\u2113(v)\nfor i = 1 to h do\nfor v \u2208V do\nfor u \u2208\u03c4v(Nv) do\nsi\nv \u2190si\nv.append(si\u22121\nu\n)\nfor v \u2208V do\nSh\nv \u2190sh\nv\n# created when concatenating two strings. For a node v there are |Nv| \u22121 string concatenations, each of them resulting in at most k \u22121 k-grams. Thus, the total number of newly created k-grams is at most\ncreated when concatenating two strings. For a node v there are |Nv| \u22121 string concatenations, each of th resulting in at most k \u22121 k-grams. Thus, the total number of newly created k-grams is at most\n\ufffd v\u2208V (k \u22121)(|Nv| \u22121) = O(mk).\n\u25a1 The next lemma shows that in order to compute the k-gram distribution vector we do not need to explicitly store each intermediate string si v but only keep track of the substrings that will contribute to new k-grams and si v\u2019s k-gram distribution. This allows us to design efficient algorithms by maintaining sketches for k-gram distribution of the si v strings.\nLemma 2 The k-gram distribution vector of the strings si v at each node v can be updated after an iteration of CSGT from the distribution vectors of the strings si\u22121 v and explicitly storing substrings of total length O(mk).\nProof: We need to explicitly store only the substrings that will contribute to new k-grams. Consider a string si v. We need to concatenate the |Nv| strings si\u22121 u . Since we need to store the k \u22121-prefix and k \u22121-suffix of each si\u22121 u , for all u \u2208V , it follows that the total length of the stored state is at most\nThe following theorem is our main result that accommodates both polynomial and cosine kernels. We define cosh k(u, v)p to be the cosine similarity to the power p between the k-gram distribution vectors collected at nodes u and v after h iterations.\nTheorem 1 Let G1, . . . , GM be a collection of M graphs, each having at most m edges and n nodes. Let K be a convolutional graph kernel with base kernel the polynomial or cosine kernel with parameter p, and let \u02c6K be K\u2019s approximation obtained by using size-b sketches of explicit feature maps. Consider an arbitrary pair of graphs Gi and Gj. Let T<\u03b1 denote the number of node pairs vi \u2208Gi, vj \u2208Gj such that cosh k(vi, vj)p < \u03b1 and R be an upper bound on the norm of the k-gram distribution vector at each node. Then, we can choose a sketch size b = O( log M+log n \u03b12\u03b52 log 1 \u03b4) such that \u02c6K(Gi, Gj) has an additive error of at most \u03b5(K(Gi, Gj) + R2p\u03b1T<\u03b1) with probability at least 1 \u2212\u03b4, for \u03b5, \u03b4 \u2208(0, 1). A graph sketch can be computed in time O(mkph + npb log b) and space O(nb).\nProof: First we note that for non-homogeneous polynomial kernel (xT y + c)p and c > 0, we can add an extra dimension with value \u221ac to the k-gram frequency vector of each string. Therefore in the following w.l.o.g. we assume c = 0. We first show how to incrementally maintain a sketch of the k-gram frequency vector of each si v. In the first iteration, we generate a string s1 v at each node v from the labels \u2113(u) for u \u2208Nv. We then generate the k-grams and feed them into sketchv and keep the k \u22121-prefix and k \u22121-suffix of each s1 v. By Lemma 2, we can compute the k-gram frequency vector of s2 v from the prefixes and suffixes of s1 u, for u \u2208Nv and the k-gram frequency vector of s1 u. A key property of Count-Sketch is that it is a linear transformation, i.e. it holds CS(x+y) = CS(x)+CS(y) for x, y \u2208Rd. Thus, we have that\nwhere K(\u03c4v(Nv)) denotes the newly created k-grams from the concatenation of the strings si\u22121 u . By Lemmas 1 and 2, we can thus compute a single Count-Sketch that summarizes the k-gram frequency vector of Sh v for all v \u2208V in time O(mkh) and space O(nb) for sketch size b.\nFor the cosine kernel with parameter p = 1, we extend the above to summarizing the normalized k-gram frequency vectors as follows. As discussed, Count-Sketch maintains b bins. After processing all k-grams of a string s, it holds cntj = \ufffd t\u2208\u03a3\u2217 k:h(t)=j #t(s), where #t(s) is the number of occurrences of string t in s. Instead, we want to sketch the values #t(s)/w where w is the 2-norm of the k-gram frequency vector of Si v. From each Count-sketch CS(si v) we can compute also an (1 \u00b1 \u03b5)-approximation \u02dcw of the norm of k-gram frequency vector [6]. Using that (1 + \u03f5)/(1 \u2212\u03f5) \u22651 + 2\u03f5 and (1 \u2212\u03f5)/(1 + \u03f5) \u22651 \u22122\u03f5 for \u03b5 \u22641/2, we can scale \u03b5 in order to obtain the desired approximation guarantee. Now consider the polynomial and cosine kernels with parameter p > 1. Let TS(x) denote the Tensor-Sketch of vector x. By the main result from [26], for a sketch size b = 1/(\u03b12\u03b52), TS(x)TS(y) is an approximation of (xT y)p such that the variance of the additive error is ((xT y)2p + (\u2225x\u2225\u2225y\u2225)2p)/b. For \u03b1 \u2264cos(x, y)p we thus have\nA standard application of Chebyshev\u2019s inequality yields an (1 \u00b1 \u03b5)-multiplicative approximation of (xT y)p with probability larger than 1/2. On the other hand, for \u03b1 > cos(x, y)p we bound the additive error to 2\u03b1\u03b5(\u2225x\u2225\u2225y\u2225)p = O(\u03b1\u03b5R2p). The bounds hold with probability \u03b4 by taking the median of log(1/\u03b4) independent estimators, and by the union bound \u03b4 can be scaled to \u03b4/(Mn2) such that the bounds hold for all node pairs for all graphs. The same reasoning applies to the cosine kernel where the norm of the vectors is bounded by 1. The Tensor-Sketch algorithm keeps p Count-sketches per node and we need to feed the k-gram distribution vector at each node into each sketch. After h iterations, the p sketches at each node are converted to a single sketch using the Fast Fourier transform in time O(pb log b). This shows the claimed time and space bounds. \u25a1 Note that for the cosine kernel it holds R = 1. Assuming that p, k and h are small constants, the running time per graph is linear and the space complexity is sublinear in the number of edges. The approximation error bounds are for the general worst case and can be better for skewed distributions.\nGraph streams We can extend the above algorithms to work in the semi-streaming graph model [9] where we can afford O(n polylog(n)) space. Essentially, we can store a compact sketch per each node but we cannot afford to store all edges. We sketch the k-gram distribution vectors at each node v in h passes. In the i-pass, we sketch the distribution of si v from the sketches si\u22121 u for u \u2208Nv and the newly computed k-grams. We obtain following result: Theorem 2 Let E be stream of labeled edges arriving in arbitrary order, each edge ei belonging to one of M graphs over N different nodes. We can compute a sketch of each graph Gi in h passes over the edges by storing a sketch of size b per node using O(Nb) space in time O(|E|hkp + b log b). The above result implies that we can sketch real-time graph streams in a single pass over the data, i.e. h = 1. In particular, for constants k and p we can compute explicit feature maps of dimension b for the convolutional kernel for real-time streams for the polynomial and cosine kernels for 1-hop neighborhood and parameter p in time O(|E| + Nb log b) using O(Nb) space.\n# 4 Experiments\nIn this section we present our evaluation of the classification accuracy and computation speed of our algorithm and comparison with other kernel-based algorithms using a set of real-world graph datasets. We first present evaluation for general graphs without ordering of node neighborhoods, which demonstrate that our algorithm achieves comparable and in some cases better classification accuracy than the state of the art kernel-based approaches. We then present evaluation for graphs with ordered neighborhoods that demonstrates that accounting for neighborhood ordering can lead to more accurate classification as well as the scalability of our algorithm.\nAll algorithms were implemented in Python 3 and experiments performed on a Windows 10 laptop with an Intel i7 2.9 GHz CPU and 16 GB main memory. For the TensorSketch implementation, we used random numbers from the Marsaglia Random Number CDROM [2]. We used Python\u2019s scikit-learn implementation [24] of the LIBLINEAR algorithm for linear support vector classification [8]. For comparison with other kernel-based methods, we implemented the explicit map versions of the Weisfelier-Lehman kernel (WL) [30], the shortest path kernel (SP) [4] and the k-walk kernel (KW) [17].\nGeneral graphs We evaluated the algorithms on widely-used benchmark datasets from various domains [16]. MUTAG [7], ENZYMES [29], PTC [14], Proteins [5] and NCI1 [32] represent molecular structures, and MSRC [22] represents semantic image processing graphs. Similar to previous works [23, 33], we choose the optimal number of hops h \u2208{1, 2} for the WL kernel and k \u2208{5, 6} for the k-walk kernel. We performed 10-fold cross-validation using 9 folds for training and 1 fold for testing. The optimal regularization parameter C for each dataset was selected from {0.1, 1, 10}. We ran the algorithms on 30 permutations of the input graphs and report the average accuracy and the average standard deviation. We set the parameter subtree depth parameter h to 2 and used the original graph labels, and in the second setting we obtained new labels using one iteration of WL. If the explicit feature maps for the cosine and polynomial kernel have dimensionality more than 5,000, we sketched the maps using TensorSketch with table size of 5,000. The results are presented in Table 1. In brackets we give the parameters for which we obtain the optimal value: the kernel, cosine or polynomial with or without relabeling and the power p \u2208{1, 2, 3, 4} (e.g. poly-rlb-1 denotes polynomial kernel with relabeling and p = 1). We see that among the four algorithms, KONG achieves the best or second best results. We would like to note that the methods are likely to admit further improvements by learning data-specific string generation algorithms but such considerations are beyond the scope of the paper. Graphs with ordered neigborhoods We performed experiments on three datasets of graphs with ordered neighborhoods (defined by creation time of edges). The first dataset was presented in [21] and consists of 600 web browsing graphs from six different classes over 89.77M edges and 5.04M nodes. We generated the second graph dataset from the popular MovieLens dataset [3] as follows. We created a bipartite graph with nodes corresponding to users and movies and edges connecting a user to a movie if the the user has rated the movie. The users are labeled into four categories according to age and movies are labeled with a genre, for a total of 19 genres. We considered only movies with a single genre. For each user we created a subgraph from its 2-hop neighborhood and set its class to be the user\u2019s gender. We generated 1,700 graphs for each gender. The total number of edges is about 99.09M for 14.3M nodes. The third graph dataset was created from the Dunnhumby\u2019s retailer dataset [1]. Similarly to the MovieLens dataset we created a bipartite graph for customer and products where edges represent purchases. Users are labeled in four categories according to their affluence, and products belong to one of nine categories. Transactions are ordered by timestamps and products in the same transaction are ordered in alphabetical order. The total number of graphs is 1,565, over 257K edges and 244K nodes. There are 7 classes corresponding to the user\u2019s life stage. The classes have unbalanced distribution, and we optimized the classifier to distinguish between a class with frequency 0.0945% and all other classes. The optimal C-value for SVM optimization was selected from 10i for i \u2208{\u22121, 0, 1, 2, 3, 4}. The average classification accuracies over 1,000 runs of different methods for different training-test size splits are shown in Figure 2. We exclude the SP kernel from the graph because either the running time was infeasible or the results were much worse compared to the other methods. For all datasets, for the k-walk kernel we obtained best results for k = 1, corresponding to collecting the labels of the endpoints of edges. We set h = 1 for both WL and KONG. We obtained best results for the cosine kernel with p = 1. The methods compared are those for 2 grams with ordered neighborhoods and shuffled neighborhoods, thus removing the information about order of edges. We also compare with using only 1 grams. Overall, we observe that accounting for the information about the order of neighborhoods can improve classification accuracy for a significant margin. We provide further results in Table 2 for training set sizes 80% showing also dimension of the explicit feature map D, computation time (including explicit feature map computation time and SVM training time), and accuracies and AUC metrics. We observe that explicit feature maps can be of large\nDataset\nKW\nSP\nWL\nKONG\nMutag\n83.7 \u00b1 1.2\n84.7 \u00b1 1.3\n84.9 \u00b1 2.1\n87.8 \u00b1 0.7\n(poly-rlb-1)\nEnzymes\n34.8 \u00b1 0.7\n39.6 \u00b1 0.8\n52.9 \u00b1 1.1\n50.1 \u00b1 1.1\n(cosine-rlb-2)\nPTC\n57.7 \u00b1 1.1\n59.1 \u00b1 1.3\n62.4 \u00b1 1.2\n63.7 \u00b1 0.8\n(cosine-2)\nProteins\n70.9 \u00b1 0.4\n72.7 \u00b1 0.5\n71.4 \u00b1 0.7\n73.0 \u00b1 0.6\n(cosine-rlb-1)\nNCI1\n74.1 \u00b1 0.3\n73.3 \u00b1 0.3\n81.4 \u00b1 0.3\n76.4 \u00b1 0.3\n(cosine-rlb-1)\nMSRC\n92.9 \u00b1 0.8\n91.2 \u00b1 0.9\n91.0 \u00b1 0.7\n95.2 \u00b1 1.3 (poly-1)\nBZR\n81.9 \u00b1 0.6\n81.4 \u00b1 1.2\n85.9 \u00b1 0.9\n85.1 \u00b1 1.1\n(poly-rlb-2)\nCOX2\n78.4 \u00b1 1.0\n79.6 \u00b1 1.1\n80.7 \u00b1 0.8\n81.8 \u00b1 2.1\n(poly-rlb-1)\nDHFR\n79.1 \u00b1 1.0\n79.2 \u00b1 0.7\n81.4 \u00b1 0.6\n80.1 \u00b1 0.5\n(poly-rlb-3)\nTable 1: Classification accuracies for general labeled graphs (the 1-gram case).\ndimension, which can result in large computation time; our method controls this by using k-grams.\nSketching We obtained best results for p = 1. However, we also sketched the explicit feature maps for the polynomial and cosine kernels for p = 2. (Note that the running time for KONG in Table 2 in the main body of the paper also include the time for sketching.) We present classification accuracy results for sketch sizes 100, 250, 500, 1000, 2500, 5000 and 10000 in Figure 3. As evident from the values, the values are close to the case p = 1 and also for quite small sketch sizes we obtain good accuracy. This indicates that sketching captures essential characteristics of the 2-gram frequency distribution also for small sketch sizes and can indeed yield compact feature maps.\n# 5 Conclusions\nWe presented an efficient algorithmic framework KONG for learning graph kernels for graphs with ordered neighborhoods. We demonstrated the applicability of the approach and obtained performance benefits for graph classification tasks over other kernel-based approaches. There are several directions for future research. An interesting research question is to explore how much graph classification can be improved by using domain specific neighbor orderings. Another direction is to obtain efficient algorithms that can generate explicit graph feature maps but compare the node strings with more complex string base kernels, such as mismatch or string alignment kernels.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c461/c4614e78-463f-4a6d-a840-b4fcc377be82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Comparison of classification accuracy for graphs with ordered neighborhoods.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fa38/fa3867ff-7318-4388-95fe-58e618082696.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Comparison of classification accuracy for graphs with ordered neighborhoods.</div>\n# References\n[1] Dunnhumby dataset. https://www.dunnhumby.com/sourcefiles. [2] Marsaglia random number cd-rom. https://web.archive.org/web/20160119150146/http://stat. fsu.edu/pub/diehard/cdrom/. [3] Movielens dataset. https://grouplens.org/datasets/movielens/. [4] Karsten M. Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of the 5th IEEE International Conference on Data Mining (ICDM 2005), pages 74\u201381, 2005. [5] Karsten M. Borgwardt, Cheng Soon Ong, Stefan Sch\u00f6nauer, S. V. N. Vishwanathan, Alexander J. Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. In Proceedings Thirteenth International Conference on Intelligent Systems for Molecular Biology 2005, pages 47\u201356, 2005. [6] Moses Charikar, Kevin C. Chen, and Martin Farach-Colton. Finding frequent items in data streams. Theor. Comput. Sci., 312(1):3\u201315, 2004. [7] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, , and C. Hansch. Structureactivity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. J. Med. Chem., 34:786\u2014797, 1991. [8] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871\u20131874, 2008. [9] Joan Feigenbaum, Sampath Kannan, Andrew McGregor, Siddharth Suri, and Jian Zhang. On graph problems in a semi-streaming model. Theor. Comput. Sci., 348(2\u20133):207\u2013216, 2005. [10] Santo Fortunato. Community detection in graphs. Physics reports, 486(3-5):75\u2013174, 2010.\n<div style=\"text-align: center;\">Web browsing</div>\nWeb browsing\nMovieLens\nDunnhumby\nMethod\nD\nTime\nAccuracy\nAUC\nD\nTime\nAccuracy\nAUC\nD\nTime\nAccuracy\nAUC\nSP\n\u2212\n> 24 hrs\n\u2212\n\u2212\n\u2212\n> 24 hrs\n\u2212\n\u2212\n228\n144\u201d\n74\u201d\n90.61 50.1\nKW\n82\n665\u201d\n116\u201d\n99.80\n99.94\n136\n120\u201d\n420\u201d\n66.98\n73.65\n56\n0.7\u201d\n134\u201d\n90.57\n58.47\nWL\n20,359\n48\u201d\n576\u201d\n99.92\n99.99\n> 2M\n492\u201d\n\u2212\n\u2212\n\u2212\n2,491\n22\u201d\n230\u201d\n90.52\n57.80\nK-1\n34\n206\u201d\n79\u201d\n99.88\n99.97\n21\n509\u201d\n197\u201d\n65.83\n71.00\n13\n42\u201d\n25\u201d\n90.53\n59.33\nK-2\nshuffled\n264\n220\u201d\n255\u201d\n99.81\n99.94\n326\n592\u201d\n497\u201d\n67.01\n73.31\n85\n48\u201d\n131\u201d\n90.57\n61.07\nK-2\n203\n217\u201d\n249\u201d\n99.95\n99.99\n326\n589\u201d\n613\u201d\n67.68\n73.20\n82\n46\u201d\n133\u201d\n90.56\n61.94\nTable 2: Comparison of the accuracy and speed of different methods for graphs with ordered neighborhoods we use the notation K-k to denote KONG using k grams; time shows explicit map computation time and SVM classification time.\n[11] Thomas G\u00e4rtner, Peter A. Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient alternatives. In 16th Annual Conference on Computational Learning Theory, COLT 2003, pages 129\u2013143, 2003. [12] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 855\u2013864, 2016. [13] David Haussler. Convolution kernels on discrete structures, 1999. [14] C. Helma, R. D. King, S. Kramer, and A. Srinivasan. The predictive toxicology challenge 2000\u20132001. Bioinformatics, 17:107\u2013108, 2001. [15] Thorsten Joachims. Training linear svms in linear time. In Proceedings of the Twelfth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006, pages 217\u2013226, 2006. [16] Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels, 2016. [17] Nils Kriege, Marion Neumann, Kristian Kersting, and Petra Mutzel. Explicit versus implicit graph feature maps: A computational phase transition for walk kernels. In 2014 IEEE International Conference on Data Mining, ICDM 2014, pages 881\u2013886, 2014. [18] Nils M. Kriege, Marion Neumann, Christopher Morris, Kristian Kersting, and Petra Mutzel. A unifying view of explicit and implicit feature maps for structured data: Systematic studies of graph kernels. CoRR, abs/1703.00676, 2017.\n<div style=\"text-align: center;\">Dunnhumby</div>\n<div style=\"text-align: center;\">MovieLens</div>\n[19] Quoc V. Le, Tam\u00e1s Sarl\u00f3s, and Alexander J. Smola. Fastfood - computing hilbert space expansions in loglinear time. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pages 244\u2013252, 2013. [20] C. Leslie, E. Eskin, and W. S. Noble. The spectrum kernel: A string kernel for SVM protein classification. In Proceedings of the Pacific Symposium on Biocomputing, volume 7, pages 566\u2013575, 2002. [21] Emaad A. Manzoor, Sadegh M. Milajerdi, and Leman Akoglu. Fast memory-efficient anomaly detection in streaming heterogeneous graphs. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016, pages 1035\u20131044, 2016. [22] Marion Neumann, Roman Garnett, Christian Bauckhage, and Kristian Kersting. Propagation kernels: efficient graph kernels from propagated information. Machine Learning, 102(2):209\u2013245, 2016. [23] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural networks for graphs. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, pages 2014\u20132023, 2016. [24] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825\u20132830, 2011. [25] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: online learning of social representations. In The 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2014, pages 701\u2013710, 2014. [26] Ninh Pham and Rasmus Pagh. Fast and scalable polynomial kernels via explicit feature maps. In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013, pages 239\u2013247, 2013. [27] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, pages 1177\u20131184, 2007. [28] Jean-Fran\u00e7ois Rual, Kavitha Venkatesan, Tong Hao, Tomoko Hirozane-Kishikawa, Am\u00e9lie Dricot, Ning Li, Gabriel F. Berriz, Francis D. Gibbons, Matija Dreze, Nono Ayivi-Guedehoussou, Niels Klitgord, Christophe Simon, Mike Boxem, Stuart Milstein, Jennifer Rosenberg, Debra S. Goldberg, Lan V. Zhang, Sharyl L. Wong, Giovanni Franklin, Siming Li, Joanna S. Albala, Janghoo Lim, Carlene Fraughton, Estelle Llamosas, Sebiha Cevik, Camille Bex, Philippe Lamesch, Robert S. Sikorski, Jean Vandenhaute, Huda Y. Zoghbi, Alex Smolyar, Stephanie Bosak, Reynaldo Sequerra, Lynn Doucette-Stamm, Michael E. Cusick, David E. Hill, Frederick P. Roth, and Marc Vidal. Towards a proteome-scale map of the human protein\u2013protein interaction network. Nature, 437:1173,2005. [29] I. Schomburg, A. Chang, C. Ebeling, M. Gremse, C. Heldt, G. Huhn, and D. Schomburg. Brenda, the enzyme database: updates and major new developments. Nucleic Acids Research, 32D:431\u2013433, 2004. [30] Nino Shervashidze, Pascal Schweitzer, Erik Jan van Leeuwen, Kurt Mehlhorn, and Karsten M. Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12:2539\u20132561, 2011. [31] Nino Shervashidze, S. V. N. Vishwanathan, Tobias Petri, Kurt Mehlhorn, and Karsten M. Borgwardt. Efficient graphlet kernels for large graph comparison. In Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics, AISTATS 2009, pages 488\u2013495, 2009. [32] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 2006), 18\u201322 December 2006, Hong Kong, China, pages 678\u2013689, 2006.\n[33] Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pages 1365\u20131374,\n[33] Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pages 1365\u20131374, 2015.\n[33] Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2015, pages 1365\u20131374 2015.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of graph classification for graphs with ordered neighborhoods, highlighting the limitations of existing graph kernels that do not account for the order in which edges are created. The authors argue that this order is significant for understanding the structure of evolving graphs, such as social networks and user browsing patterns.",
        "problem": {
            "definition": "The problem defined in this paper is the challenge of effectively classifying graphs that have ordered neighborhoods, where the order of neighbor nodes affects the graph's characteristics and classification.",
            "key obstacle": "The main difficulty is that existing graph kernels typically ignore the ordering of neighborhoods, which can lead to suboptimal feature extraction and classification performance."
        },
        "idea": {
            "intuition": "The proposed idea is inspired by the observation that the order of nodes in a neighborhood can provide valuable information about the graph's structure.",
            "opinion": "The idea entails developing a novel framework, KONG, which generates explicit graph feature maps that leverage the ordered neighborhoods for improved classification accuracy.",
            "innovation": "The innovation lies in integrating convolutional subgraph kernels with string kernels to create scalable algorithms that can effectively handle large graphs with ordered neighborhoods."
        },
        "method": {
            "method name": "KONG",
            "method abbreviation": "KONG",
            "method definition": "KONG is a framework for generating graph kernels that specifically accounts for the order of neighborhoods in graphs, enabling better feature extraction for graph classification tasks.",
            "method description": "KONG generates explicit feature maps for graphs by representing node neighborhoods as strings and applying sketching techniques to approximate kernel computations.",
            "method steps": [
                "Traverse the subtree rooted at each node using the defined neighbor ordering.",
                "Collect node labels into a string representation based on the traversal.",
                "Sketch the explicit feature map of the string using a base kernel.",
                "Aggregate the sketched feature maps for all nodes to form the graph's feature map."
            ],
            "principle": "The method is effective due to its ability to capture the informative structure of the graph through the ordering of node neighborhoods, which enhances the feature representation used in classification."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on various real-world datasets, including molecular structures and user interaction graphs, to evaluate the classification accuracy and computation speed of the proposed method compared to existing graph kernels.",
            "evaluation method": "The performance was assessed through 10-fold cross-validation, measuring classification accuracy and computational efficiency against baseline methods."
        },
        "conclusion": "The experiments demonstrated that the KONG framework significantly improves classification accuracy for graphs with ordered neighborhoods compared to traditional graph kernels, showcasing its potential for scalable and efficient graph analysis.",
        "discussion": {
            "advantage": "The key advantages of KONG include its ability to leverage the order of neighborhoods for more informative feature extraction and its scalability for large graph datasets.",
            "limitation": "A limitation of the method is its reliance on the defined ordering of neighborhoods, which may not always capture all relevant structural information, particularly in highly dynamic graphs.",
            "future work": "Future research could explore the impact of domain-specific neighbor orderings on classification performance and develop more complex string kernels for improved feature comparison."
        },
        "other info": {
            "repository": "The implementation and data are available at https://github.com/kokiche/KO"
        }
    },
    "mount_outline": [
        {
            "section number": "3.3",
            "key information": "The proposed KONG framework generates explicit graph feature maps that leverage the ordered neighborhoods for improved classification accuracy."
        },
        {
            "section number": "3.1",
            "key information": "KONG integrates convolutional subgraph kernels with string kernels to create scalable algorithms that can effectively handle large graphs with ordered neighborhoods."
        },
        {
            "section number": "4.1",
            "key information": "The key advantages of KONG include its ability to leverage the order of neighborhoods for more informative feature extraction."
        },
        {
            "section number": "7.1",
            "key information": "A limitation of the KONG method is its reliance on the defined ordering of neighborhoods, which may not always capture all relevant structural information."
        },
        {
            "section number": "7.4",
            "key information": "Future research could explore the impact of domain-specific neighbor orderings on classification performance."
        }
    ],
    "similarity_score": 0.599030737841003,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/KONG_ Kernels for ordered-neighborhood graphs.json"
}