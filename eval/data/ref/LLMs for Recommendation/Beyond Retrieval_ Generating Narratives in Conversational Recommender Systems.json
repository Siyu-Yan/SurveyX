{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.16780",
    "title": "Beyond Retrieval: Generating Narratives in Conversational Recommender Systems",
    "abstract": "The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.",
    "bib_name": "sayana2024retrievalgeneratingnarrativesconversational",
    "md_text": "# Beyond Retrieval: Generating Narratives in Conversational Recommender Systems\n# Krishna Sayana*, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert\u2020, Hubert Pham, Ambarish Jash, Sukhdeep Sodhi\nGoogle Research, Mountain View, California, USA\nAbstract\nThe recent advances in Large Language Model\u2019s generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions. First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.\narXiv:2410.16780v1\narXiv:24\n# 1 Introduction\nThe Web is evolving towards richer and more interactive experiences, and conversational recommender systems are at the forefront of this evolution. These systems leverage natural language generation to provide personalized recommendations in the form of engaging narratives, enhancing user interaction and satisfaction. This paper introduces REGEN, a dataset specifically created to advance research in this area and improve the Web\u2019s ability to provide personalized and interactive recommendations.\n*Correspondence to: ksayana@google.com \u2020Work done while an intern at Google\nLarge Language Models (LLMs) have ushered in a new era of possibilities in natural language processing, enabling machines to generate human-quality text, translate languages, write different kinds of creative content, and answer user questions in an informative way. This in turn has led to a surge of interest in applying LLMs to various domains, including the long-standing challenge of building conversational recommender systems (Lin et al. 2021; Zhang et al. 2022; Jannach et al. 2020). Conversational recommender systems aim to improve upon traditional recommender systems by engaging users in dialogue to better understand and cater to their preferences. While traditional systems simply provide a list of items, conversational systems could explain recommendations, ask clarifying questions, and create narratives connecting items to user preferences. However, realizing this vision requires overcoming a fundamental challenge: ensuring that the generated language is relevant to both the recommendation itself and the user\u2019s preferences. This necessitates effectively integrating a latent representation of the user and the recommender\u2019s internal state with the LLM. Traditional approaches, such as collaborative filtering, excel at leveraging user-item interaction data to predict preferences and provide recommendations (Koren, Bell, and Volinsky 2009). Yet, they often fall short in generating the rich, informative, and engaging language that characterizes human interactions. This gap between the structured world of recommender systems and the nuanced world of human language presents a significant hurdle. Bridging this gap requires a careful consideration of how to represent and integrate user interaction and item information into the LLM architecture. This poses several research and design questions that need to be addressed. Should we rely on external retrieval mechanisms to fetch relevant information, or should we directly encode user preferences and item representations into the LLM? How can we ensure that the generated language is not only informative but also aligned with the user\u2019s individual preferences and the underlying recommendation? Is it better to separately scale the recommender and language models, enabling efficient handling of large item spaces while benefiting from advancements in language modeling or should they be co-trained? How well do the fusion architectures similar to multimodal language models work on conversational recommendation tasks?\nTo help answer the questions articulated above, we believe that the research community will benefit from datasets specifically designed for conversational tasks. These datasets should include narratives that accurately reflect user preferences and behavior within the context of recommendations. However, many existing datasets either concentrate on next item recommendations, structured outputs or short summaries that lack the rich and varied conversational elements necessary for effectively training and evaluating the capabilities of these systems. To accelerate research in this direction, we present REGEN (Reviews Enhanced with GEnerative Narratives). REGEN augments the Amazon Reviews dataset (Ni, Li, and McAuley 2019) by incorporating natural language outputs relevant to conversational recommendations. This augmented dataset now includes diverse examples such as purchase reasons, explanations, product endorsements, user summaries & concise user profiles and available for public use (Sayana et al. 2024). Furthermore, we use an autorater LLM ((Longpre et al. 2024)) to assess the generated outputs across multiple attributes, with a particular focus on enhancing grounding and factuality based on the user\u2019s historical interactions. In our work, we use a Gemini Pro model with few shot prompting instead of finetuning as in (Longpre et al. 2024). Our aim is to deliver a comprehensive framework and dataset that can serve as a foundation for further research.\nFurther, we introduce a task and framework for conversational recommender systems: generating rich natural language outputs from user-item interaction signals that are consistent with user preferences. This is made possible by the REGEN dataset presented in this paper, and requires models to go beyond traditional recommendation tasks and produce engaging and informative narratives tailored to individual users.\nTo establish a strong baseline for this task, we propose a fusion architecture that seamlessly integrates collaborative filtering signals and content embeddings as input to an LLM. This approach allows the LLM to leverage both user-item interaction data and rich item content representations to generate outputs that are not only informative but also aligned with user preferences. Our experiments demonstrate the effectiveness of this fusion strategy in producing human-like conversational recommendations, showcasing its potential for creating more engaging and personalized user experiences.\n# Our key contributions can be summarized as follows:\n1. We generate a new dataset with a diverse set of narratives suitable for language recommendation tasks. The dataset is available for public use. It has been thoroughly evaluated for factuality, grounding, and accurately capturing user preferences and context .\n1. We generate a new dataset with a diverse set of narratives suitable for language recommendation tasks. The dataset is available for public use. It has been thoroughly evaluated for factuality, grounding, and accurately capturing user preferences and context . 2. We propose an efficient, and scalable architecture fusing embeddings from collaborative filtering and content representations as inputs to an LLM.\n3. We use this to benchmark the dataset and establish key language metrics using the state-of-the-art LLMs. We conduct ablation studies to demonstrate the importance of the combined CF and semantic signals on the quality of the narrative outputs. 4. We show that the model learns to construct rich incontext and aggregate narratives from history instead of simply memorizing using several examples. 5. We analyze the soft token embeddings to determine how the model combines content and collaborative filtering signals when generating rich narratives.\n# 2 Related Work 2.1 Conversational Datasets\n# 2.1 Conversational Datasets\nSeveral datasets have been used to explore the application of LLMs in recommender systems. ReDial (Li et al. 2017) is an annotated dataset of dialogues where users recommend movies to each other, consisting of over 10,000 conversations. MIND (Wu et al. 2020) is a large-scale dataset for news recommendation and personalization, containing news articles and user interactions for conversational news recommendation scenarios. E-ConvRec (Chen et al. 2022) provides a large-scale conversational recommendation dataset for e-commerce customer service, based on pre-sales dialogues. TG-ReDial (Zhang et al. 2021) extends ReDial by incorporating topic information to guide conversations and recommendations. DuRecDial (Liu et al. 2022b) is another conversational recommendation dataset based on the DuReader question-answering dataset, where users ask questions about products and receive recommendations. Some focus on short-form recommendations, while others lack the diverse conversational elements needed to evaluate recommender LLMs for generating extended, natural language responses. The \u201dJustifying Recommendations\u201d dataset by Ni et al. (Ni, Li, and McAuley 2019) and by Chen et al. (Chen et al. 2024) also focus on generating explanations from reviews. Motivated by these works, we further extend to more open ended narratives, prompting the LLM with the entire user history, and covering use cases beyond purchase explanations. Our work targets natural language outputs that reflect the rapidly evolving capabilities of LLMs, with longer, more nuanced, and potentially semi-structured or unstructured responses, both in context and in aggregate that target conversational recommenders.\n# 2.2 Recommender LMs\nWhile various approaches have been proposed to integrate LLMs with recommender systems, they can be broadly classified into three main categories:\nI. Retrieval-Augmented Generation (RAG): RAG enhances traditional recommender systems by employing a retrieval model to select relevant items from a catalog based on user context. A generation model, often an LLM, then produces the final recommendations. However, RAG\u2019s effectiveness hinges on the retrieval model\u2019s accuracy, which\nmay not be fully personalized. Moreover, RAG can introduce computational overhead during inference, potentially impacting real-time performance. Evaluating RAG systems also presents challenges in balancing compute costs and recommendation quality. Furthermore, RAG systems cannot be trained end-to-end using a gradient based approach. Notable works exploring RAG approaches for LLMs include Lewis et al. (Lewis et al. 2020), Izacard and Grave (Izacard and Grave 2021), Guu et al. (Guu et al. 2020), Borgeaud et al. (Borgeaud et al. 2021), Nakano et al. (Nakano et al. 2021), Thoppilan et al. (Thoppilan et al. 2022), and Izacard et al. (Izacard et al. 2022). II. Fully Language-Based Techniques Representing items and user interactions solely through language presents several challenges for recommender systems. While this approach allows for the use of powerful LLMs, it relies heavily on the richness of available text descriptions. Moreover, encoding user signals at the token level can lead to a loss of information compared to item-level encoding. Scalability is also a concern, as processing long interaction histories becomes computationally expensive. Finally, incorporating additional metadata, which is often crucial for effective recommendations, poses a significant hurdle. Examples of this approach include M6-Rec by Lin et al. (Lin et al. 2021), CALRec (Li et al. 2024) by Li et al. and the work of Zhang et al. (Zhang et al. 2022) .The RecSys 2022 workshop (rec 2022) further highlights this trend. III. Embedding Input-Based Language Models This approach focuses on utilizing item representations as direct embeddings inputs to an LLM (Hebert et al. 2024b; Tennenholtz et al. 2024; Ning et al. 2024; Doddapaneni et al. 2024; Yang et al. 2024). However, the generation is restricted to categorical variables such as genre or product categories (Hebert et al. 2024b) and some of the studies generate up to a single sentence, like a concise summary (Ning et al. 2024), which do not explore the generative capabilities of these systems. This work expands on those efforts by introducing conversational tasks that require generating more complex and nuanced narratives. Our analysis demonstrates that these tasks provide valuable insights into how LLMs learn and utilize recommender signals. Note that the embedding based approaches are further motivated by the softprompt based parameter efficient tuning of language models proposed in (Li and Liang 2021; Lester, Al-Rfou, and Constant 2021). We use the architecture motivated by III, which allows us to easily incorporate the upstream embeddings from a purely\nmay not be fully personalized. Moreover, RAG can introduce computational overhead during inference, potentially impacting real-time performance. Evaluating RAG systems also presents challenges in balancing compute costs and recommendation quality. Furthermore, RAG systems cannot be trained end-to-end using a gradient based approach. Notable works exploring RAG approaches for LLMs include Lewis et al. (Lewis et al. 2020), Izacard and Grave (Izacard and Grave 2021), Guu et al. (Guu et al. 2020), Borgeaud et al. (Borgeaud et al. 2021), Nakano et al. (Nakano et al. 2021), Thoppilan et al. (Thoppilan et al. 2022), and Izacard et al. (Izacard et al. 2022). II. Fully Language-Based Techniques Representing items and user interactions solely through language presents several challenges for recommender systems. While this approach allows for the use of powerful LLMs, it relies heavily on the richness of available text descriptions. Moreover, encoding user signals at the token level can lead to a loss of information compared to item-level encoding. Scalability is also a concern, as processing long interaction histories becomes computationally expensive. Finally, incorporating additional metadata, which is often crucial for effective recommendations, poses a significant hurdle. Examples of this approach include M6-Rec by Lin et al. (Lin et al. 2021), CALRec (Li et al. 2024) by Li et al. and the work of Zhang et al. (Zhang et al. 2022) .The RecSys 2022 workshop (rec 2022) further highlights this trend.\nmay not be fully personalized. Moreover, RAG can introduce computational overhead during inference, potentially impacting real-time performance. Evaluating RAG systems also presents challenges in balancing compute costs and recommendation quality. Furthermore, RAG systems cannot be trained end-to-end using a gradient based approach. Notable works exploring RAG approaches for LLMs include Lewis et al. (Lewis et al. 2020), Izacard and Grave (Izacard and Grave 2021), Guu et al. (Guu et al. 2020), Borgeaud et al. (Borgeaud et al. 2021), Nakano et al. (Nakano et al. 2021), Thoppilan et al. (Thoppilan et al. 2022), and Izacard et al. (Izacard et al. 2022).\nIII. Embedding Input-Based Language Models This approach focuses on utilizing item representations as direct embeddings inputs to an LLM (Hebert et al. 2024b; Tennenholtz et al. 2024; Ning et al. 2024; Doddapaneni et al. 2024; Yang et al. 2024). However, the generation is restricted to categorical variables such as genre or product categories (Hebert et al. 2024b) and some of the studies generate up to a single sentence, like a concise summary (Ning et al. 2024), which do not explore the generative capabilities of these systems. This work expands on those efforts by introducing conversational tasks that require generating more complex and nuanced narratives. Our analysis demonstrates that these tasks provide valuable insights into how LLMs learn and utilize recommender signals. Note that the embedding based approaches are further motivated by the softprompt based parameter efficient tuning of language models proposed in (Li and Liang 2021; Lester, Al-Rfou, and Constant 2021).\nWe use the architecture motivated by III, which allows us to easily incorporate the upstream embeddings from a purely collaborative model, as well as embeddings from a semantic understanding model seamlessly. Further, this results in far fewer tokens as input to the LM, as opposed to using text representations for history, thereby allowing faster serving in practice. While this independent encoding strategy is advantageous for our current investigation and improves reproducibility, future work should explore whether jointly training the encoders with the LM or fusing these embeddings on a recommendations task could yield further improvements, especially when dealing with high-dimensional\nembedding spaces with large item counts common in production datasets. Notably, several recent approaches have sought to enhance recommendation quality by incorporating item and/or semantic context, including hybrid models like HybridBERT4Rec (Sun et al. 2020), FLARE (Hebert et al. 2024a) and CALRec (Li et al. 2024), self-supervised methods like S3Rec (Wu et al. 2021), intent-based models like CaFe (Qian et al. 2021), and models leveraging crossattention (Liu et al. 2022a), pre-training (Qi et al. 2020), semantic item IDs and T5-style training (Yuan et al. 2022; Raffel et al. 2020) or aligning CF and content representations (Kim et al. 2024; Lin et al. 2024).\n# 3 REGEN\nThis section details the REGEN dataset, including the data generation process and evaluation methodology with a rater LLM.\n3.1 Amazon Reviews\n# 3.1 Amazon Reviews\nWe use datasets from Amazon Product Reviews (Ni, Li, and McAuley 2019) as a basis for augmentation with rich narratives. The Amazon Reviews dataset (2018) is a massive collection of customer reviews spanning various product categories. This version, released in 2018, contains over 233 million reviews, making it one of the largest publicly available datasets for sentiment analysis and recommendation systems. Each review includes item features like title, description, category, price and review features including text, timestamp, score and summary. User sequences can be created by sorting each user review by timestamp, creating a sequential recommendation task.\nFor our work, we use \u201cOffice Products\u201d and \u201cClothing, Shoes and Jewelry,\u201d verticals. These are chosen to be representative samples with different item counts, Office with 27k items and Clothing with 360k items. We plan to update these to other key verticals, and including the \u201cAll Reviews\u201d.\n# 3.2 Dataset Generation\nData Preprocessing User sequences Su = {i1, i2, ..., in} consisting of interactions i are first created by aggregating reviews per user, and sorting them by timestamps. We then truncate all the sequences to the most recent 50 items and filter items with a missing title. This results in 27K items, 101K users for Office with an average of 7-8 items/user and 370K items, 1.2m users for Clothing with an average of 9.2 items/user.\nPrompting and Narrative Generation Our objective is to create a dataset of natural language narratives that mirrors the diverse interactions within a conversational recommender system. To achieve this, we focus on generating outputs that vary across two key dimensions,\n\u2022 Contextualization: We generate outputs both with/without explicit contextual information (e.g., user summaries vs explanations of the most recent purchase, endorsement of a recommendation) to understand its influence on the quality and relevance of the generated language.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/72e3/72e3b258-ff00-4783-8d9e-4f0f94483e11.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Purchase reason by rated attributes</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cd25/cd25da67-a9da-4a0b-8d7f-c303daa6414c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: User summary by rated attributes.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5ca3/5ca36043-46bc-4f0b-9c45-208957c6bee8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Office Products</div>\n<div style=\"text-align: center;\">Figure 3: User richness scores and associated confidence</div>\n\u2022 Length: We further explore generating both short-form and long-form narratives for different conversational sce-\n<div style=\"text-align: center;\">(b) Clothing, Jewelry & Accessories</div>\n<div style=\"text-align: center;\">(b) Clothing, Jewelry & Accessories</div>\nnarios. We use LLMs to generate this data from user reviews, em-\nWe use LLMs to generate this data from user reviews, em-\nploying carefully designed prompts and iterative evaluation to enhance generation quality and minimize inaccuracies. In this work, we use Gemini 1.5 Flash model, which is a fast and efficient version of the Gemini 1.5 models (Gemini 2024). We leverage the large token lengths of the LLM to generate natural sounding outputs that process the entire user history Su, for each user u. The prompt is fairly straight forward, and uses a task prefix with instructions on the task and the output format, followed by the entire history of the user including the item metadata as well as the reviews, concatenated as text, and finally the instructions on the expected output format (see Appendix F for the prompt). We use the entire history of the user along with associated reviews to generate purchase reasons and summaries. However, for product endorsement, we withhold the last user review, so the model learns to endorse a newly recommended item. Furthermore, we utilize LLMs as automated evaluators to assess the generated outputs across multiple attributes. We demonstrate the effectiveness of our approach augmenting the Amazon product review dataset (Ni, Li, and McAuley 2019), and evaluating using the Gemini Pro LLM as the rater. Future work will extend this evaluation to other LLMs and include multiple outputs for each prompt to reduce LM bias and enable more robust benchmarking. Detailed description of individual outputs are included in Appendix B.\n# 3.3 Evaluation using an Auto Rater\nTo evaluate the quality of the generated data, we use Gemini Pro LLM as the rater. We give this rater LLM the task, the history, the generated outputs, and prompt it to score the outputs on different attributes. Overall Approach We use an ensemble rating process, repeating the rating process several times (10-15) and aggregate these scores to a final score. While this is computationally intensive, we find that it improves the attribute scores with more reliable results. We present distribution of scores across all conversations, and annotate users with corresponding scores in the released dataset. Evaluation Metrics and Confidence Scoring We prompt the rater LLM to score the generated data on up to seven attributes (veracity, grounding, clarity & specificity, foresight, personalization, user richness and confidence) using a seven point likert scale. The definitions are summarized in Appendix B. Further, we conduct the evaluation in four stages, prompting the LLM each time: \u2022 User richness and confidence: User richness captures the richness of the purchase history for understanding a user. \u2022 Purchase reason and explanation: These are scored together based on five key attributes: veracity, foresight, clarity, personalization, and grounding. \u2022 User Summaries: These are assessed on the same attributes as purchase reason, excluding foresight, which is only relevant with contextualized output. \u2022 Product endorsements: These are assessed on the same attributes as the purchase reasons.\nTo evaluate the quality of the generated data, we use Gemini Pro LLM as the rater. We give this rater LLM the task, the history, the generated outputs, and prompt it to score the outputs on different attributes.\nOverall Approach We use an ensemble rating process, repeating the rating process several times (10-15) and aggregate these scores to a final score. While this is computationally intensive, we find that it improves the attribute scores with more reliable results. We present distribution of scores across all conversations, and annotate users with corresponding scores in the released dataset.\nAn associated confidence score is generated for each stage. This multi-stage approach is used to avoid any correlation caused by Chain of Thought (CoT) bias across different feature types. For ensemble aggregation of individual attribute scores, we use a majority rule, where if over half of the ensemble runs agree on a score, that score becomes the final score. In cases where there\u2019s no majority agreement, i.e. with ambiguity, the average score (rounded down) is used. To determine the confidence scores per feature type, we employ an ensemble method using plurality and averaging. The overall confidence score is calculated by first identifying the most frequent score across all ensemble runs for each attribute. Then, we average these scores across all attributes for that feature type to produce a single, representative confidence score. This approach aims to provide a comprehensive and reliable assessment of the generated data\u2019s quality. Analysis of Purchase Reasons and Summaries Figures 1 and 2 show the distribution of rated attributes for purchase reason and user summaries. Lower foresight implies less or no post-purchase information is used, while a higher foresight score indicates more or some post-purchase information (i.e, the last review) is incorporated into the reason. Analysis of the \u201cforesight\u201d scores reveals distinct patterns in purchase behavior between Office Products and Clothing. Office product purchases, often routine and predictable, show a greater reliance on past history (scores 1-4). Conversely, clothing purchases, with their higher item count and potential for evolving trends, emphasize the current context (scores clustered around 4 and above). Generated summaries demonstrate slightly higher scores overall, particularly in veracity and personalization, likely due to the model\u2019s attention to the complete purchase history. Associated confidence scores are generally high (4 or above).\nAn associated confidence score is generated for each stage. This multi-stage approach is used to avoid any correlation caused by Chain of Thought (CoT) bias across different feature types. For ensemble aggregation of individual attribute scores, we use a majority rule, where if over half of the ensemble runs agree on a score, that score becomes the final score. In cases where there\u2019s no majority agreement, i.e. with ambiguity, the average score (rounded down) is used. To determine the confidence scores per feature type, we employ an ensemble method using plurality and averaging. The overall confidence score is calculated by first identifying the most frequent score across all ensemble runs for each attribute. Then, we average these scores across all attributes for that feature type to produce a single, representative confidence score. This approach aims to provide a comprehensive and reliable assessment of the generated data\u2019s quality.\nFigure 3 illustrates the user richness and confidence scores. Approximately 34% of office supply and 28% of clothing examples fall below average richness, with average or above-average confidence. The dataset is annotated with attributes rather than filtered to facilitate research on the impact of personalization and data quality on various techniques and methods. We will include the evaluations of product endorsements in a future revision of the paper.\n# 4 Problem Formulation & Benchmarking Methodology\nIn this section, we describe the experimental setup used to generate narratives. The model architecture employs a multistage approach to generate narrative recommendations by integrating collaborative filtering signals and semantic embeddings into an LLM.\n# 4.1 Task Definitions\nWe formalize the conversational recommendation task within the context of user interaction sequences. Consider a dataset comprising sequences S = {i1, i2, ..., in}, where each interaction i \u2208I is represented as a tuple (ID, T, P), where ID is the unique identifier of the item, T is a set of\nnatural language components T = {t1, t2, ..., tm} describing the item. This includes the title, detailed product description, product category, and other relevant textual metadata. P is the personalized text associated with the item in the context of a specific user. This could be a user review, rating, or other user-generated content related to the item.\nWithin this framework, we define three key tasks central to conversational recommendation:\n# 1. Narrative Generation from Recent Context: Generate\n Narrative Generation from Recent Context: natural language output T (in|S = {ij | n \u2212N < j \u2264 n}) corresponding to the recent interaction in, considering the history of interactions up to and including n (truncated to a certain window of observation N). This task focuses on capturing short-term user preferences and generating contextually relevant responses. 2. Narrative Generation from Aggregate Context: Generate natural language output T (S) corresponding to the entire interaction history. This task aims to capture longterm user preferences and generate a comprehensive narrative reflecting the user\u2019s overall experience. 3. Next Item Prediction: Predict the next item in+1 in the sequence given the user\u2019s past interactions S. This task represents the traditional recommendation objective. In this paper, we focus on the the first two tasks, which are better suited to the proposed dataset, while the third task is evaluated extensively in several recent studies on the reviews data.\n3. Next Item Prediction: Predict the next item in+1 in the sequence given the user\u2019s past interactions S. This task represents the traditional recommendation objective.\nIn this paper, we focus on the the first two tasks, which are better suited to the proposed dataset, while the third task is evaluated extensively in several recent studies on the reviews data.\n# 4.2 Model Architecture\nTo effectively integrate recommender system knowledge in LLMs for natural language generation, we propose a modular architecture as shown in Figure 4 that decouples the encoding of collaborative filtering (CF) signals and semantic representations of user interactions. This approach offers two primary benefits. First, it allows us to systematically investigate the impact of different signal types on the LLM\u2019s ability to generate informative and engaging narratives. Second, it serves as a clear and effective baseline for evaluating models trained on the proposed dataset.\nTo effectively integrate recommender system knowledge in LLMs for natural language generation, we propose a modular architecture as shown in Figure 4 that decouples the encoding of collaborative filtering (CF) signals and semantic representations of user interactions. This approach offers two primary benefits. First, it allows us to systematically investigate the impact of different signal types on the LLM\u2019s ability to generate informative and engaging narratives. Second, it serves as a clear and effective baseline for evaluating models trained on the proposed dataset. Further, by independently selecting best-in-class models for each encoding stage, we can leverage proven methods for generating high-quality item embeddings, such as those from matrix factorization (Yi et al. 2018; Koren, Bell, and Volinsky 2009), factorization machines (Rendle 2010), or neural collaborative filtering (Rendle et al. 2020), while simultaneously employing state-of-the-art sentence embedding models like Sentence-T5 (Ni et al. 2021), Universal Sentence Encoder (Cer et al. 2018), GECKO (Lee et al. 2024), Sentence-BERT (SBERT) (Reimers and Gurevych 2019), SimCSE (Gao, Yao, and Chen 2021), and InferSent (Conneau et al. 2017). We also note that this modular approach is also highly practical for real-world datasets with item counts orders of magnitude larger (O(1M-1B)) than those typically used in research, enabling efficient encoding of diverse data scales. Below, we describe the different components used in our multi-stage recommender LM.\nFurther, by independently selecting best-in-class models for each encoding stage, we can leverage proven methods for generating high-quality item embeddings, such as those from matrix factorization (Yi et al. 2018; Koren, Bell, and Volinsky 2009), factorization machines (Rendle 2010), or neural collaborative filtering (Rendle et al. 2020), while simultaneously employing state-of-the-art sentence embedding models like Sentence-T5 (Ni et al. 2021), Universal Sentence Encoder (Cer et al. 2018), GECKO (Lee et al. 2024), Sentence-BERT (SBERT) (Reimers and Gurevych 2019), SimCSE (Gao, Yao, and Chen 2021), and InferSent (Conneau et al. 2017). We also note that this modular approach is also highly practical for real-world datasets with item counts orders of magnitude larger (O(1M-1B)) than those typically used in research, enabling efficient encoding of diverse data scales. Below, we describe the different components used in our multi-stage recommender LM.\nCF Encoder This encoder processes the user-item interaction history. For each user u, the interaction sequence Su = {i1, i2, ..., in} consists of interactions i, where each interaction is represented as a tuple (idi, mi, si, ri) with item ID idi, item metadata mi, user rating/score for the item si and user review for the item ri.\naction history. For each user u, the interaction sequence Su = {i1, i2, ..., in} consists of interactions i, where each interaction is represented as a tuple (idi, mi, si, ri) with item ID idi, item metadata mi, user rating/score for the item si and user review for the item ri. We construct a user-item rating matrix R from the interaction history and apply Weighted Alternating Least Squares (WALS) (Hu, Koren, and Volinsky 2008) matrix factorization to obtain user embeddings U \u2208RNu\u00d7d and item embeddings V \u2208RNi\u00d7d, where Nu is the number of users, Ni is the number of items, and d is the embedding dimension of the user and item embeddings. This results in a user embedding uu for each user and an item embedding vi for each interaction. Semantic Encoder This captures the semantic information from item metadata and user reviews. For each interaction i, we concatenate the item metadata mi and user review ri to form a text representation ti. We then apply a pre-trained sentence embedding model (e.g., sT5, Gecko) to ti to obtain a semantic embedding si \u2208Rds, where ds is embedding size of the sentence embedding model. Embedding Fusion To combine the collaborative filtering and semantic information, we fuse the embeddings from both encoders. We note that the semantic embeddings si produced by the Gecko model are each L2 normalized to 1. However the WALS embeddings are not, and the norm of these embeddings is related to the popularity of the item. To keep this intact, we perform a normalization in expected value as v \u2032 i = vi/ \u00af \u2225v\u22252, where \u00af \u2225v\u22252 is the mean L2 norm of all item embeddings. For each interaction i, the item embedding v \u2032 i and the semantic embedding si are then combined using a simple concatenation, resulting in a combined embedding ci = [v \u2032 i, si] \u2208Rdc, dc = di + ds. While the model can compensate for the normalization to some extent, we find that this additional preprocessing step improves the metrics. Adapter Model The sequence of combined embeddings Cu = {c1, c2, ..., cn} for user u is fed into into an adapter model (e.g an MLP or a transformer). This adapter model generates a sequence of soft tokens Zu = {z1, z2, ..., zn},\nWe construct a user-item rating matrix R from the interaction history and apply Weighted Alternating Least Squares (WALS) (Hu, Koren, and Volinsky 2008) matrix factorization to obtain user embeddings U \u2208RNu\u00d7d and item embeddings V \u2208RNi\u00d7d, where Nu is the number of users, Ni is the number of items, and d is the embedding dimension of the user and item embeddings. This results in a user embedding uu for each user and an item embedding vi for each interaction. Semantic Encoder This captures the semantic information from item metadata and user reviews. For each interaction i, we concatenate the item metadata mi and user review ri to form a text representation ti. We then apply a pre-trained sentence embedding model (e.g., sT5, Gecko) to ti to obtain a semantic embedding si \u2208Rds, where ds is embedding size of the sentence embedding model. Embedding Fusion To combine the collaborative filtering and semantic information, we fuse the embeddings from both encoders. We note that the semantic embeddings si produced by the Gecko model are each L2 normalized to 1. However the WALS embeddings are not, and the norm of these embeddings is related to the popularity of the item. To keep this intact, we perform a normalization in expected value as v \u2032 i = vi/ \u00af \u2225v\u22252, where \u00af \u2225v\u22252 is the mean L2 norm of all item embeddings. For each interaction i, the item embedding v \u2032 i and the semantic embedding si are then combined using a simple concatenation, resulting in a combined embedding ci = [v \u2032 i, si] \u2208Rdc, dc = di + ds. While the model can compensate for the normalization to some extent, we find that this additional preprocessing step improves the metrics. Adapter Model The sequence of combined embeddings Cu = {c1, c2, ..., cn} for user u is fed into into an adapter model (e.g an MLP or a transformer). This adapter model generates a sequence of soft tokens Zu = {z1, z2, ..., zn}, where each zi \u2208Rdl matches the model dimension dl of the LLM. These soft tokens serve as a condensed representation of the user\u2019s interaction history, tailored for integration with the LLM. Large Language Model (LLM) The soft token sequence Zu from the adapter model is prepended to the text prompt token embeddings P = {p1, p2, ..., pm} to form the input sequence I = {z1, z2, ..., zn, p1, p2, ..., pm} for the LLM. This allows the LLM to generate natural language narratives conditioned on both the user\u2019s interaction history and\nWe construct a user-item rating matrix R from the interaction history and apply Weighted Alternating Least Squares (WALS) (Hu, Koren, and Volinsky 2008) matrix factorization to obtain user embeddings U \u2208RNu\u00d7d and item embeddings V \u2208RNi\u00d7d, where Nu is the number of users, Ni is the number of items, and d is the embedding dimension of the user and item embeddings. This results in a user embedding uu for each user and an item embedding vi for each interaction.\nSemantic Encoder This captures the semantic information from item metadata and user reviews. For each interaction i, we concatenate the item metadata mi and user review ri to form a text representation ti. We then apply a pre-trained sentence embedding model (e.g., sT5, Gecko) to ti to obtain a semantic embedding si \u2208Rds, where ds is embedding size of the sentence embedding model.\nEmbedding Fusion To combine the collaborative filtering and semantic information, we fuse the embeddings from both encoders. We note that the semantic embeddings si produced by the Gecko model are each L2 normalized to 1. However the WALS embeddings are not, and the norm of these embeddings is related to the popularity of the item. To keep this intact, we perform a normalization in expected value as v \u2032 i = vi/ \u00af \u2225v\u22252, where \u00af \u2225v\u22252 is the mean L2 norm of all item embeddings. For each interaction i, the item embedding v \u2032 i and the semantic embedding si are then combined using a simple concatenation, resulting in a combined embedding ci = [v \u2032 i, si] \u2208Rdc, dc = di + ds. While the model can compensate for the normalization to some extent, we find that this additional preprocessing step improves the metrics.\nAdapter Model The sequence of combined embeddings Cu = {c1, c2, ..., cn} for user u is fed into into an adapter model (e.g an MLP or a transformer). This adapter model generates a sequence of soft tokens Zu = {z1, z2, ..., zn}, where each zi \u2208Rdl matches the model dimension dl of the LLM. These soft tokens serve as a condensed representation of the user\u2019s interaction history, tailored for integration with the LLM.\nLarge Language Model (LLM) The soft token sequence Zu from the adapter model is prepended to the text prompt token embeddings P = {p1, p2, ..., pm} to form the input sequence I = {z1, z2, ..., zn, p1, p2, ..., pm} for the LLM. This allows the LLM to generate natural language narratives conditioned on both the user\u2019s interaction history and the specific prompt, which could indicate additional context. Notably, this just requires N soft tokens to represent N interactions.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c54e/c54e0a27-3d2d-4da1-b6f9-345a5ce8461f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Overview of the Model Architecture.</div>\n# 5 Experiments\nFor our initial benchmarks, we use i) WALS encoder generating CF embeddings of dimension 128, ii) Gecko 1B encoder generating semantic embeddings of dimension 768 and PaLM 2 XXS (PaLM2 2023) for the LLM, and an MLP layer as the adapter.\n# 5.1 Datasets and Tasks\nTo evaluate the models, we utilize the proposed REGEN dataset, focusing on the task of generating natural language narratives. We demonstrate the model\u2019s capabilities as follows:\nNarrative Generation The model takes a user\u2019s history of ratings and reviews as input and generates a comprehensive narrative summarizing their preferences and experiences. For the language input, we use a fixed prompt, essentially prompting the model to generate the corresponding output feature based on the observed history.\nEvaluation with Distinct Users The training and test sets contain different users. This forces the model to learn how to interpret user history from the provided embeddings, rather than memorizing specific users. This ensures the model can generalize to new users and accurately capture their preferences through the generated narratives. We generate benchmarks using REGEN dataset with i) Office Products: with 27K items and ii) Clothing, Shoes,& Jewelry: with 376K items, to evaluate the model\u2019s performance across both small and large item vocabularies.\n# 5.2 Evaluation Metrics We assess the model using the following complementary methods:\n\u2022 Traditional NLP Metrics: We report BLEU and ROUGE scores to establish baselines. These metrics offer a high-level view of performance trends. \u2022 Similarity Scores: We compare the generated narratives to the REGEN refeence narratives using similarity measures based on sentence embedding models (e.g., Gecko). This helps assess the semantic closeness to the ground truth.\n\u2022 Side-by-Side Evaluations with raters: This provides a more nuanced evaluation of the quality and relevance of the generated narratives, either using a human raters or a rater LLM. We do not have comprehensive evaluations using this approach, but show sampled evaluation of some examples SxS with our annotations in Appendix H.\n# 5.3 Key Results\n# 5.3 Key Results The experiment results are shown in Tabl\n5.3 Key Results The experiment results are shown in Table 1.\nNatural Language Tasks with Context These are tasks where the language outputs are based on immediate context, for example, explaining the reasoning and explanation behind the most recently purchased item. Combining collaborative filtering (CF) and semantic embeddings leads to up to 12%, +8%, and +8% gains in BLEU, ROUGE, and Semantic Similarity metrics, respectively, compared to using the best result using either of these embeddings alone.\nNatural Language Tasks without Context For tasks that don\u2019t rely on immediate context (e.g., generating a narrative based on an aggregate user profile), combining collaborative filtering and content embeddings offers no significant advantage over using content embeddings alone. This could be because the model does not need to focus on the last item specifically, which is more typical of a recommendation task, and content features are sufficient.\nOverall, we find that the BLEU/ROUGE scores are lower compared to those you\u2019d expect in structured generation tasks. However, our SxS examples in Appendix H show that the model outputs compare well to the reference outputs. This is also reflected in the higher similarity scores.\n# 6 Embedding Analysis\nTo analyze the learned embeddings, we consider a linear projection layer as the adapter, which yields comparable results (within 5%) of an MLP adapter. Let ei \u2208Rdi represent item embedding, es \u2208Rds as content embedding, and W \u2208Rdl\u00d7(di+ds) as the projection matrix (adapter layer). As we concatenate the item and semantic embeddings, this is equivalent to W1 \u2208Rdl\u00d7di submatrix of W operating on\nOutput Feature\nMetrics\nCF\nContent\nCF + Content\n(%) vs CF\n(%) vs Content\nPurchase reason\nBLEU\n17.71\n21.07\n21.92\n+23.8%\n+4.0%\nROUGE-LSum\n38.26\n42.63\n43.01\n+12.4%\n+0.9%\nSimilarity\n0.614\n0.664\n0.669\n+9.0%\n+0.8%\nPurchase reason explanation\nBLEU\n19.56\n21.68\n24.24\n+23.9%\n11.8%\nROUGE-LSum\n37.26\n38.05\n41.11\n+10.3%\n+8.0%\nSimilarity\n0.783\n0.787\n0.851\n+8.7%\n8.1%\nUser Summary (Brief)\nBLEU\n17.96\n19.3\n19.12\n+6.5%\n-0.9%\nROUGE-LSum\n36.5\n38.06\n38.16\n+4.5%\n+0.3%\nSimilarity\n0.861\n0.866\n0.864\n+0.3%\n-0.2%\nUser Summary (Long)\nBLEU\n16.96\n18.07\n18.09\n+6.7%\n+0.1%\nROUGE-LSum\n31.61\n33.35\n33.42\n+5.7%\n+0.2%\nSimilarity\n0.885\n0.895\n0.903\n+2.0%\n+0.9%\n<div style=\"text-align: center;\">Table 1: Performance benchmarks with using different embedding inputs to the PaLM2 XXS LLM. REGEN Office Products (|I| = 27K)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/752f/752f7125-923c-406d-a608-3cb3aebe5617.png\" style=\"width: 50%;\"></div>\nFigure 5: Comparing Soft prompt embeddings vs LLM token embeddings. Soft prompt embeddings generated for all items in Office Products. ei and W2 \u2208Rdl\u00d7ds submatrix of W operating on es. The new embedding can be simply represented as,\n<div style=\"text-align: center;\">Figure 5: Comparing Soft prompt embeddings vs LLM token embeddings. Soft prompt embeddings generated for all items in Office Products. ei and W2 \u2208Rdl\u00d7ds submatrix of W operating on es. The new embedding can be simply represented as,</div>\nEssentially, the soft embeddings in the token embedding space are expressed as a projected CF and projected content components, where these projections are learnt by the adapter layer. This framework allows us to get more insight into the contribution of collaborative filtering and content signals.\n# 6.1 Soft Prompt Embeddings vs LM Token Embeddings:\n# 6.1 Soft Prompt Embeddings vs LM Token\nWe first investigate the relationship between soft prompt embeddings (i.e projected embeddings en) and the language model\u2019s token embeddings. We train a CF + Content model and obtain the projection layer to compute the projected CF and content embeddings for all items. Visualizing these\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b45f/b45f6295-89f3-4f60-80f7-4987f7a31fe1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Comparing CF and Content contributions to Soft Prompts (Office Products).</div>\nembedding spaces with t-SNE (Figures 5) reveals that soft prompts, including their constituent CF and content embeddings (W1ei, W2es), occupy a distinct space (i.e a different cluster) rather than mapping to the language model\u2019s token space. While this might seem counter-intuitive at first, it\u2019s not a surprising result considering the sparsity of the language embedding space. The maximum number of embeddings possible in a high-dimensional space with a certain minimum degree of separation is related to the concept of the kissing number in sphere packing (Conway and Sloane 1999). For instance, in 25 dimensions, this number is on the order of 100,000 and further increases exponentially with dimensionality. Given that most language models have embedding dimensions in the order of 100s to few 1,000s and vocabulary sizes on the order of 10,000 to 100,000 tokens, the language embeddings utilize a very small fraction of the potential embedding space. Therefore, it is reasonable that the adapter learns a separate embedding space, especially when trained on smaller datasets and with smaller models. However, it is an open question whether this behavior would persist if recommender language models were pretrained on datasets comparable in scale to those used for general-purpose language models. Interestingly, this finding also aligns with concepts that treat items as new (discrete) tokens (Singh et al. 2024), effectively expanding the lan-\nguage model\u2019s vocabulary, but with the distinction here of utilizing the full unconstrained embedding space representation.\n# 6.2 CF vs Content Embeddings\nFigure 6 shows the distribution of relative contribution (in norm) of projected CF and content embeddings (W1ei, W2es) to the total norm of the soft prompt embeddings. This shows that the content based signals have more significant representation in the learnt soft tokens compared to the CF parts, though CF parts are not insignificant. This could explain the observations in our experiments, where we see Content only embedding model outperform the CF based embedding model, while combining the two yields the best results. In the Office Products dataset, most items appear in 10-20 users (see Appendix D), which does not give extensive CF signals. A similar trend is observed with item statistics in several categories, so it would be informative to repeat these experiments on different datasets with richer interaction data.\n# 7 Limitations and Future Work\nWhile we have utilized a state-of-the-art LLM as an automated evaluator for the generated narratives and observed high-quality results across various dimensions, we acknowledge that LLMs cannot fully replace human evaluation, particularly for tasks requiring subjective judgment (Chiang and yi Lee 2023). For instance, while LLMs excel at assessing summarization quality, they may struggle with more nuanced aspects like creativity and coherence in narrative generation. Nevertheless, we believe LLMs can effectively complement human evaluation, reducing the need for extensive human involvement and improving efficiency. Recent work like RoboRater (Goldman and Vasilevski 2024) further supports this notion, demonstrating promising results in automating the evaluation of task-oriented conversations. In future updates of REGEN, we plan to explore augmenting our automated evaluation with human assessments for a more comprehensive evaluation. For benchmarking, we employed a simple yet effective embedding inputbased LLM architecture to incorporate recommender and semantic knowledge. This approach allows for reproducible benchmarks and facilitates the study of how different signal types impact narrative generation. Future work could explore other architectures, such as employing co-training of encoders or techniques like cross-attention and pre-training, to potentially improve performance.\n# 8 Conclusions\nThis paper introduces REGEN, a new dataset designed to advance research in conversational recommender systems. By enhancing the Amazon Product Reviews dataset with richer user narratives, REGEN enables the development and evaluation of models capable of generating personalized explanations and summaries of user preferences. We highlighted the challenge of bridging the gap between traditional recommender systems and natural language generation, emphasizing the need for innovative methods that effectively combine\nLLMs with recommender system knowledge. Through automated evaluation and analysis of generated outputs, we demonstrated REGEN\u2019s capacity to capture user preferences and context. Furthermore, we presented a simple, yet effective fusion architecture that combines collaborative filtering signals and semantic embeddings as input to an LLM, showcasing its potential for generating informative and engaging conversational recommendations. Our experiments demonstrated the effectiveness of this approach in improving natural language generation metrics, particularly when combining both collaborative filtering and semantic information compared to using either of these individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.\n# 9 Acknowledgments\nThe authors thank James Ren, Ajit Apte, Dima Kuzmin, Walid Krichene who graciously offered their expert advice and feedback on this work.\n# References\nReferences 2022. Language Models as Recommender Systems: Evaluations and Limitations. In Proceedings of the 16th ACM Conference on Recommender Systems. ACM. Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Rutherford, E.; Millican, K.; Driessche, G. v.; Lespiau, J.-B.; Bosma, M.; Hall, J.; et al. 2021. Improving Language Models by Retrieving from Trillions of Tokens. arXiv preprint arXiv:2112.04426. Cer, D.; Yang, Y.; Kong, S.-y.; Hua, N.; Limtiaco, N.; John, R. S.; Constant, N.; Guajardo-Cespedes, M.; Yuan, S.; Tar, C.; et al. 2018. Universal sentence encoder. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 169\u2013174. Chen, L.; Chen, W.; Ren, J.; Huang, F.; Tang, J.; and Zhao, W. 2022. E-convrec: A large-scale conversational recommendation dataset for e-commerce customer service. In Proceedings of the ACM Web Conference 2022, 2164\u20132175. Chen, T.; Zuo, S.; Li, C.; Zhang, M.; Mei, Q.; and Bendersky, M. 2024. Unlocking the \u2018Why\u2019 of Buying: Introducing a New Dataset and Benchmark for Purchase Reason and Post-Purchase Experience. arXiv:2402.13417. Chiang, C.-H.; and yi Lee, H. 2023. Can Large Language Models Be an Alternative to Human Evaluations? arXiv:2305.01937. Conneau, A.; Kiela, D.; Schwenk, H.; Barrault, L.; and Bordes, A. 2017. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 conference on empirical methods in natural language processing, 670\u2013680. Conway, J.; and Sloane, N. 1999. Sphere Packings, Lattices and Groups. Grundlehren der mathematischen Wissenschaften. Springer-Verlag. ISBN 9780387985855. Doddapaneni, S.; Sayana, K.; Jash, A.; Sodhi, S.; and Kuzmin, D. 2024. User Embedding Model for Personalized Language Prompting. arXiv:2401.04858. Gao, T.; Yao, X.; and Chen, D. 2021. Simcse: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6894\u20136910. Gemini. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530. Gemma. 2024. Gemma: Open Models Based on Gemini Research and Technology. arXiv:2403.08295. Goldman, S.; and Vasilevski, Y. 2024. RoboRater: Automating Ratings of Task-Oriented Conversations using LLMs. In AAAI 2024 Spring Symposium on User-Aligned Assessment of Adaptive AI Systems. Guu, K.; Lee, K.; Tung, Z.; Chang, P. P.; and Gao, J. 2020. REALM: Retrieval-Augmented Language Model PreTraining. In International Conference on Machine Learning, 3728\u20133738. PMLR. Hebert, L.; Kyriakidi, M.; Pham, H.; Sayana, K.; Pine, J.; Sodhi, S.; and Jash, A. 2024a. FLARE: Fusing Language Models and Collaborative Architectures for Recommender Enhancement. arXiv:2409.11699.\nHebert, L.; Sayana, K.; Jash, A.; Karatzoglou, A.; Sodhi, S.; Doddapaneni, S.; Cai, Y.; and Kuzmin, D. 2024b. PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized Language Prompting. arXiv:2408.00960. Hu, Y.; Koren, Y.; and Volinsky, C. 2008. Collaborative Filtering for Implicit Feedback Datasets. In 2008 Eighth IEEE International Conference on Data Mining, 263\u2013272. Izacard, G.; Caron, M.; Hosseini, A.; Riedel, S.; Boissonnade, G.; Joulin, A.; and Grave, E. 2022. Atlas: Few-shot Learning with Retrieval Augmented Language Models. In International Conference on Learning Representations. Izacard, G.; and Grave, E. 2021. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In International Conference on Learning Representations. Jannach, D.; Kumar, A.; Lerche, L.; and Zanker, M. 2020. A survey on conversational recommender systems. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 3637\u20133638. Kim, S.; Kang, H.; Choi, S.; Kim, D.; Yang, M.; and Park, C. 2024. Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201924, 1395\u20131406. New York, NY, USA: Association for Computing Machinery. ISBN 9798400704901. Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factorization techniques for recommender systems. Computer, (8): 30\u201337. Lee, J.; Dai, Z.; Ren, X.; Chen, B.; Cer, D.; Cole, J. R.; Hui, K.; Boratko, M.; Kapadia, R.; Ding, W.; Luan, Y.; Duddu, S. M. K.; Abrego, G. H.; Shi, W.; Gupta, N.; Kusupati, A.; Jain, P.; Jonnalagadda, S. R.; Chang, M.-W.; and Naim, I. 2024. Gecko: Versatile Text Embeddings Distilled from Large Language Models. arXiv:2403.20327. Lester, B.; Al-Rfou, R.; and Constant, N. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv:2104.08691. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V.; Goyal, N.; K\u00a8uttler, H.; Zettlemoyer, L.; Devlin, J.; Chen, W.-t. Y.; et al. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems, 33: 19690\u201319701. Li, X.; Zhao, L.; He, X.; Zhang, M.; Wu, Q.; and Chang, P. P. 2017. Redial: A large-scale dataset for group recommendation. In Proceedings of the 40th international ACM SIGIR conference on research and development in information retrieval, 1157\u20131160. Li, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv:2101.00190. Li, Y.; Zhai, X.; Alzantot, M.; Yu, K.; Vuli\u00b4c, I.; Korhonen, A.; and Hammad, M. 2024. CALRec: Contrastive Alignment of Generative LLMs for Sequential Recommendation. Lin, C.-T.; Lee, H.-Y.; Sun, M.-F.; Hwang, W.-C.; et al. 2021. M6-Rec: Generative Pretrained Language Models for Conversational Recommendation. In Proceedings of the 15th ACM Conference on Recommender Systems, 629\u2013637.\nLin, X.; Wang, W.; Li, Y.; Feng, F.; Ng, S.-K.; and Chua, T.-S. 2024. Bridging Items and Language: A Transition Paradigm for Large Language Model-Based Recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD \u201924, 1816\u20131826. New York, NY, USA: Association for Computing Machinery. ISBN 9798400704901. Liu, D.; Lian, J.; Fu, Z.; Wang, Z.; and Xie, X. 2022a. CARCA: Content-aware representation for cross-attention based sequential recommendation. Proceedings of the ACM Web Conference 2022, 1195\u20131203. Liu, Z.; Huang, J.; Tang, J.; Chen, Y.; Zhu, J.; Zhu, X.; and Xie, X. 2022b. DuRecDial: A DuReader-based dataset for conversational recommendation. In Findings of the Association for Computational Linguistics: NAACL 2022, 2832\u2013 2842. Longpre, S.; Suri, A.; Nguyen, T. M.; Li, S. H.; Dai, A. M.; and Zhao, Z. 2024. Foundational Autoraters: Taming Large Language Models for Better Automatic Evaluation. arXiv preprint arXiv:2407.10817. Nakano, R.; Hilton, J.; Balaji, S.; Jain, J.; and Schulman, J. 2021. WebGPT: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Ni, J.; Constant, N.; Guajardo-Cespedes, M.; and Gao, J. 2021. Sentence-t5: Scalable sentence encoders from pretrained text-to-text models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 8133\u20138142. Ni, J.; Li, J.; and McAuley, J. 2019. Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 188\u2013197. Hong Kong, China: Association for Computational Linguistics. Ning, L.; Liu, L.; Wu, J.; Wu, N.; Berlowitz, D.; Prakash, S.; Green, B.; O\u2019Banion, S.; and Xie, J. 2024. User-LLM: Efficient LLM Contextualization with User Embeddings. arXiv:2402.13598. PaLM2. 2023. PaLM 2 Technical Report. arXiv:2305.10403. Qi, W.; Zhang, Y.; Huang, Y.; Wu, C.; Zhang, J.; and Zhao, W. 2020. U-BERT: Pre-training user representations for improved recommendation. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 2069\u20132078. Qian, Z.; Chen, W.-L.; Li, X.; Lin, B.; Huang, X.; and Yang, H. 2021. CaFe: Coarse-to-Fine Embedding for Sequential Recommendation. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, 3032\u20133040. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1): 5485\u20135551.\nReimers, N.; and Gurevych, I. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), 3982\u20133992. Rendle, S. 2010. Factorization machines. In 2010 IEEE International Conference on Data Mining, 995\u20131000. IEEE. Rendle, S.; Krichene, W.; Zhang, L.; and Anderson, J. 2020. Neural collaborative filtering vs. matrix factorization revisited. In Fourteenth ACM Conference on Recommender Systems, 240\u2013248. Sayana, K.; Vasudeva, R.; Vasilevski, Y.; Pine, J.; Hebert, L.; and Pham, H. 2024. REGEN: Reviews Enhanced with Generative Narratives. Singh, A.; Vu, T.; Mehta, N.; Keshavan, R.; Sathiamoorthy, M.; Zheng, Y.; Hong, L.; Heldt, L.; Wei, L.; Tandon, D.; Chi, E. H.; and Yi, X. 2024. Better Generalization with Semantic IDs: A Case Study in Ranking for Recommendations. arXiv:2306.08121. Sun, F.; Liu, J.; Wu, J.; Pei, C.; Lin, X.; Ou, W.; and Jiang, P. 2020. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. Proceedings of the ACM SIGKDD international conference on knowledge discovery and data mining, 1441\u20131450. Tennenholtz, G.; Chow, Y.; Hsu, C.-W.; Shani, L.; Liang, E.; and Boutilier, C. 2024. Embedding-Aligned Language Models. arXiv:2406.00024. Thoppilan, R.; De Freitas, D.; Bradley, J.; Chintagunta, S.; Collins, M.; Fedus, W.; Huang, M.; Joulin, A.; Kappler, M.; Keene, S.; et al. 2022. LaMDA: Language Models for Dialog Applications. arXiv preprint arXiv:2201.08239. Wu, F.; Cao, Y. Q.; Wu, L.; Hu, C.; Lian, J.; Xie, Y.; Chen, Y.; and Liu, H. 2020. MIND: A large-scale dataset for news recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in information retrieval, 2431\u20132434. Wu, K.; Liang, W. X.; Chen, L.; Yuan, B.; and Zhang, S. Y. 2021. S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization. Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1262\u2013 1271. Yang, L.; Subbiah, A.; Patel, H.; Li, J.; Song, Y.; Mirghaderi, R.; and Aggarwal, V. 2024. Item-Language Model for Conversational Recommendation. Yi, X.; Chen, Y.-F.; Ramesh, S.; Rajashekhar, V.; Hong, L.; Fiedel, N.; Seshadri, N.; Heldt, L.; Wu, X.; and Chi, E. H. 2018. Factorized Deep Retrieval and Distributed TensorFlow Serving. SysML. Yuan, F.; Yang, X.; He, X.; Karatzoglou, A.; and Zhang, L. 2022. TIGER: Transferable interactive graph-enhanced recommender. In Proceedings of the ACM Web Conference 2022, 398\u2013407. Zhang, Y.; Sun, Y.; Zhang, W.; Zhang, Y.; Chen, X.; and He, X. 2021. Topic-guided conversational recommendation.\nIn Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 450\u2013459. Zhang, Y.; Wu, C.; Zhang, L.; Sun, Z.; and Zhang, W. 2022. Generating Personalized Recommendations with Large Language Models. arXiv preprint arXiv:2206.01407.\n# A Reproducibility\nThe REGEN dataset presented in this paper is available for public use (Sayana et al. 2024). We have used WALS algorithm, which is well known for generating collaborative filtering embeddings. For content embedding, we use Gecko 1B embeddings, which are again publicly available. For benchmarks, we have used a PaLM2 XXS model that can be run with reasonable compute to generate the benchmarks. We also plan to update this with results from a Gemma model (Gemma 2024), which is also available for public use.\n# B Details of narrative outputs and eval attributes\nThe detailed descriptions of narrative outputs and eval attributes are provided in Tables 2 and 3.\nC Office vs Clothing\n# C Office vs Clothing\nIn the Clothing category (Table 4), we observe similar trends with gains when using the combined approach. Interestingly, the metrics for CF embeddings alone are much closer to those of the content-based approach. This suggests that the larger dataset, with increased diversity of items, may be contributing to the stronger performance of collaborative filtering in this category.\n# D Item Count distribution\nItem count distribution for Office Products is shown in Figure 7 with significant weight at 10-20 item counts.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e1a8/e1a8fdb9-e707-4ad0-a118-10b29d6e0bcd.png\" style=\"width: 50%;\"></div>\nFigure 7: Item count distribution for Office Products.\n<div style=\"text-align: center;\">Figure 7: Item count distribution for Office Products.</div>\n# E Comparing CF, Content and Language Embeddings\nFigure 8 illustrates CF, content embedding components of the soft prompts individually, along with the LM token em-\nbeddings plotted with tSNE.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cdf3/cdf307f0-f1c2-443f-8a55-897e6b7cec2b.png\" style=\"width: 50%;\"></div>\nFigure 8: Comparing Soft prompt embeddings vs LLM token embeddings. Soft prompt embeddings generated for all items in Office Products.\n# F Data generation prompt and Examples The data generation prompts are shown in Tables 5 and 6.\nG Auto Rater Prompts An example of auto rater prompt is show in Table 7.\n# G Auto Rater Prompts An example of auto rater prompt is show in Table 7.\nH Example traces\nThese traces are generated from the trained CF+Content embedding model with PaLM2 XXS as the LLM. Gecko embeddings are used to represent content. The distilled embedding LM demonstrates strong performance in generating concise narratives that capture the core user preferences. However, compared to the reference model, it may sometimes oversimplify complex scenarios or miss subtle details. Furthermore, a tendency to hallucinate information is sometimes observed, especially with longer outputs. Full user histories were used for SxS comparison, but are omitted here for brevity. Traces generated by the models are shown in Tables 9 and 10.\nGenerated output\nDescription\nProduct Endorsement\nA tailored product endorsement/sales pitch\ncrafted based on understanding user\u2019s\nunique purchase history and reviews\nPurchase Reasons\nConcise explanations of the reasons behind\na product recommendation.\nPurchase Reason Explana-\ntions\nDetailed and elaborate explanations of a\npurchase decision of the most recent item\nin context of the entire history.\nBrief User Summaries\nConcise summaries of a user\u2019s preferences\nand purchase history.\nDetailed User Summaries\nComprehensive summaries of a user\u2019s pref-\nerences and history, potentially including\nspecific examples and justifications.\nUser Profiles\nA short phrase describing the type of user\nin product/shopping context.\nTable 2: Generated outputs and their descriptions. Average length is mean number of words for each output.\nRating\nAttribute\nDescription\nVeracity\nAccuracy and truthfulness of the purchase reason, an-\nalyzing specific evidence and inconsistencies within the\nuser\u2019s purchase history.\nGrounding\nStrength of the supporting evidence for all individual\nclaims made in the purchase reason and explanation.\nClarity &\nSpecificity\nClarity and specificity of the purchase reason and its ex-\nplanation, and specificity of evidence.\nForesight\nHow much of the information used in the purchase reason\ncame from the post-purchase review of the last purchased\nitem.\nPersonalization\n(User needs vs Product descriptions) Extent to which the\nnarrative prioritizes information directly from the user\u2019s\nreviews and statements, as opposed to relying on product\ndescriptions or general assumptions.\nUser richness\nHow information-rich the user\u2019s purchase and review his-\ntory is for understanding their motivations and prefer-\nences.\nConfidence\nConfidence level in the evaluation, ranging from unre-\nliable to high agreement between different evaluation\nmethods.\n<div style=\"text-align: center;\">Table 3: Auto Rater: Rating Attributes.</div>\nOutput Feature\nMetrics\nCF\nContent\nCF + Content\n(%) vs CF\n(%) vs Content\nPurchase reason explanation\nBLEU\n17.81\n18.01\n18.96\n+6.5%\n+5.3%\nROUGE\n34.05\n34.73\n35.67\n+4.8%\n+2.7%\nSimilarity\n0.795\n0.82\n0.838\n+5.4%\n+2.2%\nUser Summary (Long)\nBLEU\n19.18\n18.92\n19.49\n+1.6%\n+3.0%\nROUGE\n33.01\n33.23\n33.88\n+2.6%\n+2.0%\nSimilarity\n0.908\n0.91\n0.913\n+0.6%\n+0.3%\nTable 4: Performance benchmarks with using different embedding inputs to the PaLM2 XXS LLM. REGEN Clothing, Shoes & Jewelry (|I| = 376K)\nNOTE: Few shot prompting is used for purchase reasons and user summaries, and a zero shot prompt for product endorsements. Prompt for purchase reasons and user summaries: You are an exceptional customer engagement specialist at Amazon. Your role is to understand customer purchase motivations and experiences. You will be provided with a sequence of product purchases made by a user and their corresponding reviews. Each sequence is ordered chronologically, from earliest to most recent purchase, marked by \u2019Sequence Item Position\u2019.\nNOTE: Few shot prompting is used for purchase reasons and user summaries, and a zero shot prompt for product endorsements. Prompt for purchase reasons and user summaries: You are an exceptional customer engagement specialist at Amazon. Your role is to understand customer purchase motivations and experiences. You will be provided with a sequence of product purchases made by a user and their corresponding reviews. Each sequence is ordered chronologically, from earliest to most recent purchase, marked by \u2019Sequence Item Position\u2019.\nYour task is two fold. - One, focus on the last product in their purchase sequence (marked as \u2019Final Item: True\u2019). Analyze the final purchase based on their prior purchases and reviews. - Two, identify purchase attributes of this customer\u2019s purchases using all items in purchase and review history. Provide the following information in JSON format: 1. **purchase reason**: The reason for purchase of the last product that can be inferred from the review, product description, and the user\u2019s purchase history. If not mentioned, leave null. 2. **purchase reason explanation**: Briefly justify your reasoning for the identified purchase reasons, considering the user\u2019s past purchases and reviews. provide an in depth user summary based on specific shopping interests 3. **brief user summary**: Based on this user\u2019s purchase history and reviews, provide a brief two sentence user summary based on specific shopping interests, including any specific items, brands, colors and attributes. 4. **long user summary**: Based on this user\u2019s purchase history and reviews, provide an in depth user summary based on specific shopping interests, including any specific items, brands, colors and attributes. 5. **User profile**: Provide a user profile in 4 to 6 words based on the user\u2019s past purchases and reviews.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/abc7/abc7199a-2baa-4323-9968-241e516f0954.png\" style=\"width: 50%;\"></div>\n# Example JSON Output:\njson { \u201dpurchase reason\u201d: \u201d........\u201d, \u201dpurchase reason explanation\u201d: \u201d........\u201d, \u201dbrief user summary\u201d: \u201d........\u201d, \u201dlong user summary\u201d: \u201d........\u201d, \u201duser profile\u201d: \u201d........\u201d } *Important:** **DO NOT** quote any part of the user\u2019s reviews verbatim in your answers. Focus on insights, not direct repetition. Be specific and relate to the context of each customer\u2019s purchase and review history. Here are examples of 7 reviewers and the expected answer for the last item in the sequence for each reviewer id reviewer id A01003458IEUPS8LQ1QU Sequence Item Position 1 Final Item False item title Rolodex Open Rotary Business Card File item desc Unique Stack-Lock feature keeps stacked trays together with locked-in stability Unique Stack-Lock feature keeps stacked trays together with locked-in stability item price 9.989999771118164 overall 5 review title Five Stars review text Nice trays to get organized. We buy these for work, and they work quite nicely. No complaints here! \u201d...\u201d \u201d...\u201d Sequence Item Position 6 Final Item True Evaluate purchase intent of the following product item title Avery Two-Pocket Folders, Green, Pack of 25 (47977) item desc Fasteners for punched sheets, pockets for loose sheets\u2013covers it all! Dual use makes it perfect for sales materials. Business card slot for convenient contact information. item price 17.219999313354492 overall 5 review title Five Stars review text Great presentation folders to have. I\u2019ll buy them again. End of Sequence\nSequence Item Position 6 Final Item True Evaluate purchase intent of the following product item title Avery Two-Pocket Folders, Green, Pack of 25 (47977) item desc Fasteners for punched sheets, pockets for loose sheets\u2013covers it all! Dual use makes it perfect for sales materials. Business card slot for convenient contact information. item price 17.219999313354492 overall 5 review title Five Stars review text Great presentation folders to have. I\u2019ll buy them again. End of Sequence\n# expected answer\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9dd7/9dd72d95-af02-443f-88aa-f03ee81df5fc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2552/255285b3-50b5-4c02-a57e-785024cdc828.png\" style=\"width: 50%;\"></div>\nGenerated Output 1 \u201dpurchase reason\u201d: \u201dThe customer wanted a stylish, relatively small tote bag with multiple pockets for organization.\u201d \u201dpurchase reason explanation\u201d: \u201dThe customer had previously purchased a dress and was looking for a stylish handbag that would match. They mentioned the bag was smaller than expected, but was still stylish and had multiple pockets. This suggests the customer is looking for a functional and fashionable handbag.\u201d \u201dbrief user summary\u201d: \u201dThis customer is a woman who enjoys shopping for both stylish and comfortable clothing. She is also interested in fashion accessories such as handbags.\u201d \u201dlong user summary\u201d: \u201dThis customer is a woman who has a taste for stylish and comfortable clothing. She has purchased sandals, dresses, and underwear, all items that suggest a preference for fashion and comfort. Her reviews also mention that she values quality, especially in clothing items like dresses. She is also interested in fashion accessories such as handbags. She seems to be an occasional buyer with a more practical and less frequent shopping approach. \u201d \u201duser profile\u201d: \u201dFashionable and practical woman\u201d \u201dproduct endorsement\u201d: \u201dThis Michael Kors Bedford Top Zip Pocket Tote Bag is a luxurious and functional choice, perfect for someone who appreciates quality and style. Based on your previous purchases of comfortable clothing and stylish sandals, this bag complements your taste for both everyday practicality and elegant accents. The durable leather, spacious interior with multiple pockets, and adjustable handles make it ideal for carrying your essentials in a sophisticated way. It\u2019s a great investment in a timeless piece that will elevate your everyday looks and be a reliable companion for years to come.\u201d\n# Generated Output 2\nGenerated Output 2 \u201dpurchase reason\u201d: \u201dThe customer wanted a new casual dress.\u201d \u201dpurchase reason explanation\u201d: \u201dThe customer has a history of purchasing various dresses and has also mentioned in a review that they like a dress that is flattering on a \u201cbuxom\u201d body. This purchase is likely for a casual dress that they want to wear.\u201d \u201dbrief user summary\u201d: \u201dThis customer buys a wide variety of clothing and accessories for herself, her children, and her husband. They often look for items with comfortable fit and durability.\u201d \u201dlong user summary\u201d: \u201dThis customer buys a wide variety of clothing and accessories for herself, her husband, and children. She often seeks items that are comfortable and durable. She\u2019s interested in items that are well-made and have good value for money. For herself, she enjoys stylish and functional items, especially dresses, tops, and accessories. She also prioritizes practicality and comfort in clothing for her kids. Her reviews often highlight comfort, fit, and durability as important factors, particularly when it comes to clothing for children. Her purchase history reveals a preference for clothing that suits various occasions and personal styles. She likes to express her personal style through clothing and accessories and is not afraid to experiment with different looks.\u201d \u201duser profile\u201d: \u201dFashionable, practical, family-oriented shopper\u201d \u201dproduct endorsement\u201d: \u201dThis casual A-line dress is perfect for a variety of occasions, from a casual day out to a more formal event. With its cap sleeves and V-neck, it is both stylish and comfortable. The dress is made from a lightweight and breathable fabric, making it perfect for warmer weather. And with its flattering A-line cut, it will look great on everyone. You\u2019ve previously purchased a variety of clothing items, ranging from formal dresses to everyday wear, and appreciated the quality of pieces that fit well. This dress is a great option for those who appreciate comfortable yet stylish clothing.\u201d\n# Generated Output 3\nGenerated Output 3 \u201dpurchase reason\u201d: \u201dThe customer was looking for a short-sleeve fishing shirt that is breathable and provides UV protection.\u201d \u201dpurchase reason explanation\u201d: \u201dThe customer purchased a similar fishing shirt previously and rated it 5 stars for its fit, quality and features. The review mentions that the shirt is breathable, dries fast and provides UV protection.\u201d \u201dbrief user summary\u201d: \u201dThis customer is a man who is looking for comfortable and well-made work wear, specifically work boots and fishing shirts. He also values good quality and is looking for items that will last.\u201d \u201dlong user summary\u201d: \u201dThis customer is a man who seems to enjoy outdoor activities such as fishing, as he has bought multiple fishing shirts. The customer is also interested in finding comfortable and well-made workwear, having purchased work boots and a work shirt. He is willing to pay for quality and looks for items that will last. While he seems to be satisfied with the quality of the fishing shirts, he doesn\u2019t seem to appreciate the quality of a work shirt he purchased before.\u201d \u201duser profile\u201d: \u201dOutdoor enthusiast, workwear shopper\u201d\nGenerated Output 3 \u201dpurchase reason\u201d: \u201dThe customer was looking for a short-sleeve fishing shirt that is breathable and provides UV protection.\u201d \u201dpurchase reason explanation\u201d: \u201dThe customer purchased a similar fishing shirt previously and rated it 5 stars for its fit, quality and features. The review mentions that the shirt is breathable, dries fast and provides UV protection.\u201d \u201dbrief user summary\u201d: \u201dThis customer is a man who is looking for comfortable and well-made work wear, specifically work boots and fishing shirts. He also values good quality and is looking for items that will last.\u201d \u201dlong user summary\u201d: \u201dThis customer is a man who seems to enjoy outdoor activities such as fishing, as he has bought multiple fishing shirts. The customer is also interested in finding comfortable and well-made workwear, having purchased work boots and a work shirt. He is willing to pay for quality and looks for items that will last. While he seems to be satisfied with the quality of the fishing shirts, he doesn\u2019t seem to appreciate the quality of a work shirt he purchased before.\u201d \u201duser profile\u201d: \u201dOutdoor enthusiast, workwear shopper\u201d\n\u201dproduct endorsement\u201d: \u201dThis Columbia Men\u2019s PFG Bahama II Short Sleeve Breathable Fishing Shirt is a great choice for anyone who wants a comfortable and durable shirt for fishing or other outdoor activities. You\u2019ve already shown a preference for high-quality, comfortable clothing, like the KEEN Utility Men\u2019s Pittsburgh Steel Toe Work Boot, and this shirt is no different. The shirt is made of lightweight, durable nylon, dries quickly, and has meshlined cape vents at the back shoulder to keep you cool. It also has built-in UV protection, making it a great choice for sunny days on the water. Plus, you\u2019ve already experienced the quality of Columbia clothing with your previous purchase, so you can be confident in the quality of this shirt.\u201d\n# Generated Output 4\nGenerated Output 4 \u201dpurchase reason\u201d: \u201dThe customer needed a comfortable running shoe with a good fit.\u201d \u201dpurchase reason explanation\u201d: \u201dThe customer needed a new pair of running shoes, and had to replace the original insole due to discomfort on the sides. They found that an insole from their old Saucony shoes solved the problem, making the new shoes comfortable.\u201d \u201dbrief user summary\u201d: \u201dThe customer is a diverse shopper interested in men\u2019s clothing, travel accessories, and women\u2019s shoes and lingerie. They have purchased items for themselves and others.\u201d \u201dlong user summary\u201d: \u201dThis customer enjoys shopping for various items, including men\u2019s neckties and socks, travel accessories, women\u2019s lingerie, and women\u2019s running shoes. They have purchased items for both themselves and others, such as the socks for a friend. The customer has expressed a preference for comfortable and well-fitting items, as seen in their reviews for the shoes and lingerie. They are also drawn to humorous and unique items, as demonstrated by the purchase of the \u2019Nuclear Physics\u2019 necktie.\u201d \u201duser profile\u201d: \u201dDiverse, Practical, Comfort-Oriented\u201d \u201dproduct endorsement\u201d: \u201dLooking for a running shoe that offers maximum comfort and energy return? The Saucony Women\u2019s Triumph ISO 3 Running Sneaker is designed to elevate your run with its increased Everun landing zone and dynamic ISOFIT system. This shoe provides a comfortable and supportive fit, adapting to your stride for a smooth and enjoyable running experience. You\u2019ve previously purchased items for yourself and others, focusing on both comfort and style, so you\u2019ll appreciate the Triumph ISO 3\u2019s blend of performance and everyday wearability. You\u2019ve also enjoyed durable and comfortable items, and the Triumph ISO 3 is made with high-quality materials to ensure long-lasting performance. Treat yourself to the ultimate running experience with the Saucony Women\u2019s Triumph ISO 3 Running Sneaker.\u201d\n# Table 7: Example Auto-Rater prompt used for evaluation (Rating Veracity on User Summaries)\nYou are a highly skilled customer behavior analyst AI. You will be provided with user purchase history, including the item details, the review text. Your task is to answer questions about the user\u2019s purchase behavior. \u2014 BEGIN USER PURCHASE HISTORY \u2014 User Purchase: On YYYY-MM-DD the following was purchased\n# \u2014 END USER PURCHASE HISTORY \u2014\nEvaluation Criteria: Your responses will be evaluated based on: * Accuracy: how well your judgment aligns with the true reason for purchase (if available) * Evidence: the quality and relevance of the examples cited from the review history * Reasoning: the clarity and logic of your analysis * Confidence: how well your confidence level reflects the strength of the evidence\n# Instructions:\nInstructions: * Analyze the user\u2019s review history carefully to identify patterns in preferences, needs, or dislikes * Compare the suggested reason and explanation against those patterns * Consider both positive and negative evidence * Be explicit in citing specific examples from the review history to support your assessment * If there\u2019s insufficient evidence to make a strong judgment, state so clearly and explain why * **Strictly penalize any purchase reasons that imply future knowledge of the user\u2019s experience with the p with very low rating.\n* Be explicit in citing specific examples from the review history to support your assessment * If there\u2019s insufficient evidence to make a strong judgment, state so clearly and explain why * **Strictly penalize any purchase reasons that imply future knowledge of the user\u2019s experience with the product** with very low rating.\n# Additional Considerations:\n* Pay close attention to the language used in user reviews, especially adjectives, adverbs, and any mentions of specific features or occasions. Analyze the sentiment expressed towards these aspects. * Evaluate each element of the provided purchase reason individually. If the reason mentions specific attributes (e.g., color, size, material), look for direct evidence of those preferences in the purchase history. * Prioritize user statements and reviews over product descriptions when evaluating the purchase reason. Only use product descriptions as supporting evidence if they are directly corroborated by user statements. * Analyze seemingly unrelated purchases for potential connections to the current purchase reason. For example, a past purchase of a gift might reveal information about the user\u2019s gifting preferences. * If evidence for a specific aspect of the reason is limited or based solely on assumptions, express lower confidence in the overall judgment.\n# Question:\nQuestion: Please evaluate the veracity of the provided user profile, user brief summary, and user long summary in light of the user\u2019s purchase history. \u2014 BEGIN USER PROFILE \u2014 . . . \u2014 END USER PROFILE \u2014 \u2014 BEGIN USER BRIEF SUMMARY \u2014 . . . \u2014 END USER BRIEF SUMMARY \u2014 \u2014 BEGIN USER LONG SUMMARY \u2014\nQuestion: Please evaluate the veracity of the provided user profile, user brief summary, and user long summary in light of the user\u2019s purchase history. \u2014 BEGIN USER PROFILE \u2014\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8beb/8beb1c4c-dd3e-4b06-9ed0-c1c192ba27fa.png\" style=\"width: 50%;\"></div>\nOutput: A detailed assessment of the veracity of the provided user profile, user brief summary, and user long summary. Your assessment should include: * Clear judgment on whether the summaries and profile are true, false, or uncertain, based on the evidence from the review history. * Specific examples from the review history that support your judgment. * You may use the entirety of the user\u2019s purchase history to validate the claims made in the summaries. * Keep in mind that even a negative review on a specific attribute can indicate that the user cares about that aspect. * Avoid making broad generalizations or inferences that are not directly supported by the evidence. * Focus on strong, direct evidence that clearly connects to the specific claims in the summaries. * Analysis of any potential inconsistencies or contradictions between the summaries and the user\u2019s past behavior. * Consideration of the full range of evidence, including potentially contradictory information. * Acknowledgment of complexities and uncertainties if present. * Overall confidence level in your assessment (high, medium, low). Remember, your end judgment must be one of: - Likely Very False - Likely False - Likely Somewhat False - Uncertain - Likely Somewhat True - Likely True - Likely Very True Please think aloud about how would you answer this question and provide your reasoning prior to answering. Answer:\nOutput: A detailed assessment of the veracity of the provided user profile, user brief summary, and user long summary. Your assessment should include: * Clear judgment on whether the summaries and profile are true, false, or uncertain, based on the evidence from the review history. * Specific examples from the review history that support your judgment. * You may use the entirety of the user\u2019s purchase history to validate the claims made in the summaries. * Keep in mind that even a negative review on a specific attribute can indicate that the user cares about that aspect. * Avoid making broad generalizations or inferences that are not directly supported by the evidence. * Focus",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The integration of recommendation systems with natural language generation has been challenging, particularly in ensuring that generated narratives are relevant to user preferences. Traditional recommendation systems often fail to provide engaging and informative language, which is essential for user satisfaction in conversational contexts.",
            "purpose of benchmark": "The benchmark aims to evaluate the effectiveness of conversational recommender systems in generating personalized narratives that enhance user engagement and satisfaction."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of generating natural language outputs that accurately reflect user preferences and behaviors in the context of recommendations.",
            "key obstacle": "Existing benchmarks often focus on structured outputs or short summaries, lacking the rich conversational elements necessary for evaluating generative models in recommendation tasks."
        },
        "idea": {
            "intuition": "The authors were inspired by the need for datasets that capture the nuances of user interactions and preferences in conversational settings, leading to the development of a more comprehensive dataset.",
            "opinion": "The authors believe that the REGEN dataset and its associated benchmarks are crucial for advancing the field of conversational recommendation systems and improving user experiences.",
            "innovation": "The REGEN dataset introduces a novel approach by augmenting existing product review datasets with rich narratives, enabling a more nuanced evaluation of generative models.",
            "benchmark abbreviation": "REGEN"
        },
        "dataset": {
            "source": "The dataset is an augmentation of the Amazon Product Reviews dataset, incorporating user-generated narratives and contextual information.",
            "desc": "REGEN consists of rich user narratives related to product preferences, endorsements, and summaries of purchase histories, designed to facilitate the evaluation of conversational recommendation systems.",
            "content": "The dataset includes text data, specifically user reviews and narratives that reflect user preferences and behaviors in a conversational context.",
            "size": "233,000,000",
            "domain": "Conversational Recommendation",
            "task format": "Natural Language Generation"
        },
        "metrics": {
            "metric name": "BLEU, ROUGE",
            "aspect": "Accuracy and relevance of generated narratives",
            "principle": "The metrics were chosen to evaluate the quality of generated narratives in terms of their linguistic fidelity and alignment with user preferences.",
            "procedure": "Model performance is evaluated using standard NLP metrics, along with side-by-side evaluations with human raters and automated assessments."
        },
        "experiments": {
            "model": "The experiments utilized a fusion architecture combining collaborative filtering and semantic embeddings, evaluated with state-of-the-art LLMs.",
            "procedure": "Models were trained using the REGEN dataset, focusing on narrative generation tasks while ensuring a clear separation of training and test user sets to enhance generalization.",
            "result": "The experiments showed significant improvements in narrative quality when combining collaborative filtering and semantic embeddings, with notable gains in BLEU and ROUGE scores.",
            "variability": "Variability in results was accounted for through multiple trials and evaluations using different subsets of the dataset."
        },
        "conclusion": "The REGEN dataset and its associated benchmarks significantly advance the understanding of conversational recommendation systems, demonstrating the potential for improved user engagement through personalized narrative generation.",
        "discussion": {
            "advantage": "The benchmark provides a comprehensive framework for evaluating conversational recommender systems, facilitating research in generating personalized narratives.",
            "limitation": "While the dataset is robust, it may not capture all nuances of user preferences, particularly in diverse or niche domains.",
            "future work": "Future research should explore the integration of additional contextual data and the application of the benchmark to various recommendation tasks beyond product reviews."
        },
        "other info": [
            {
                "info1": "The dataset is publicly available for further research.",
                "info2": {
                    "info2.1": "The authors propose additional evaluations with human raters to complement automated assessments.",
                    "info2.2": "Future iterations of the dataset may include more diverse product categories."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The integration of recommendation systems with natural language generation has been challenging, particularly in ensuring that generated narratives are relevant to user preferences."
        },
        {
            "section number": "2.1",
            "key information": "The benchmark addresses the challenge of generating natural language outputs that accurately reflect user preferences and behaviors in the context of recommendations."
        },
        {
            "section number": "2.2",
            "key information": "Existing benchmarks often focus on structured outputs or short summaries, lacking the rich conversational elements necessary for evaluating generative models in recommendation tasks."
        },
        {
            "section number": "3.2",
            "key information": "The experiments utilized a fusion architecture combining collaborative filtering and semantic embeddings, evaluated with state-of-the-art LLMs."
        },
        {
            "section number": "4.1",
            "key information": "The REGEN dataset introduces a novel approach by augmenting existing product review datasets with rich narratives, enabling a more nuanced evaluation of generative models."
        },
        {
            "section number": "5.1",
            "key information": "The REGEN dataset consists of rich user narratives related to product preferences, endorsements, and summaries of purchase histories."
        },
        {
            "section number": "8.1",
            "key information": "The benchmark provides a comprehensive framework for evaluating conversational recommender systems, facilitating research in generating personalized narratives."
        },
        {
            "section number": "10.1",
            "key information": "While the dataset is robust, it may not capture all nuances of user preferences, particularly in diverse or niche domains."
        }
    ],
    "similarity_score": 0.7686690889562119,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Beyond Retrieval_ Generating Narratives in Conversational Recommender Systems.json"
}