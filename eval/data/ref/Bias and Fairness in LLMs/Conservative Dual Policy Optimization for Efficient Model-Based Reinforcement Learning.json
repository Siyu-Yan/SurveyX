{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2209.07676",
    "title": "Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement Learning",
    "abstract": "Provably efficient Model-Based Reinforcement Learning (MBRL) based on optimism or posterior sampling (PSRL) is ensured to attain the global optimality asymptotically by introducing the complexity measure of the model. However, the complexity might grow exponentially for the simplest nonlinear models, where global convergence is impossible within finite iterations. When the model suffers a large generalization error, which is quantitatively measured by the model complexity, the uncertainty can be large. The sampled model that current policy is greedily optimized upon will thus be unsettled, resulting in aggressive policy updates and over-exploration. In this work, we propose Conservative Dual Policy Optimization (CDPO) that involves a Referential Update and a Conservative Update. The policy is first optimized under a reference model, which imitates the mechanism of PSRL while offering more stability. A conservative range of randomness is guaranteed by maximizing the expectation of model value. Without harmful sampling procedures, CDPO can still achieve the same regret as PSRL. More importantly, CDPO enjoys monotonic policy improvement and global optimality simultaneously. Empirical results also validate the exploration efficiency of CDPO.",
    "bib_name": "zhang2022conservativedualpolicyoptimization",
    "md_text": "# Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement Learning\nShenao Zhang Georgia Institute of Technology Atlanta, GA 30332 shenao@gatech.edu\n# Abstract\nProvably efficient Model-Based Reinforcement Learning (MBRL) based on optimism or posterior sampling (PSRL) is ensured to attain the global optimality asymptotically by introducing the complexity measure of the model. However, the complexity might grow exponentially for the simplest nonlinear models, where global convergence is impossible within finite iterations. When the model suffers a large generalization error, which is quantitatively measured by the model complexity, the uncertainty can be large. The sampled model that current policy is greedily optimized upon will thus be unsettled, resulting in aggressive policy updates and over-exploration. In this work, we propose Conservative Dual Policy Optimization (CDPO) that involves a Referential Update and a Conservative Update. The policy is first optimized under a reference model, which imitates the mechanism of PSRL while offering more stability. A conservative range of randomness is guaranteed by maximizing the expectation of model value. Without harmful sampling procedures, CDPO can still achieve the same regret as PSRL. More importantly, CDPO enjoys monotonic policy improvement and global optimality simultaneously. Empirical results also validate the exploration efficiency of CDPO.\narXiv:2209.07676v1\n# 1 Introduction\nModel-Based Reinforcement Learning (MBRL) involves acquiring a model by interacting with the environment and learning to make the optimal decision using the model [55, 32]. MBRL is appealing due to its significantly reduced sample complexity compared to its model-free counterparts. However, greedy model exploitation that assumes the model is sufficiently accurate lacks guarantees for global optimality. The policies can be suboptimal and get stuck at local maxima even in simple tasks [10]. As such, several provably-efficient MBRL algorithms have been proposed. Based on the principle of optimism in the face of uncertainty (OFU) [56, 49, 10], OFU-RL achieves the global optimality by ensuring that the optimistically biased value is close to the real value in the long run. Based on Thompson Sampling [62], Posterior Sampling RL (PSRL) [57, 42, 43] explores by greedily optimizing the policy in an MDP which is sampled from the posterior distribution over MDPs. Beyond finite MDPs, to obtain a general bound that permits sample efficiency in various cases, we need to introduce additional complexity measure. For example, [49, 43] provide an \ufffdO(\u221adET) regret for both OFU and PSRL with eluder dimension dE capturing how effectively the model generalizes. However, it is recently shown [13, 33] that the eluder dimension for even the simplest nonlinear models cannot be polynomially bounded. The effectiveness of the algorithms will thus be crippled. The underlying reasons for such ineffectiveness are the aggressive policy updates and the overexploration issue. Specifically, when a nonlinear model is used to fit complex transition functions, its generalizability will be poor compared to simple linear problems. If a random model is selected from the large hypothesis, e.g., optimistically chosen or sampled from the posterior, it is \u201cunsettled\".\n36th Conference on Neural Information Processing Systems (NeurIPS 2022).\nIn other words, the selected model can change dramatically between successive iterations. Policy updates under this model will also be aggressive and thus cause value degradation. What\u2019s worse, large epistemic uncertainty results in an unrealistic model, which drives agents for uninformative exploration. An exploration step can only eliminate an exponentially small portion of the hypothesis. In this work, we present Conservative Dual Policy Optimization (CDPO), a simple yet provable MBRL algorithm. As the sampling process in PSRL harms policy updates due to the unsettled model during training, we propose the Referential Update that greedily optimizes an intermediate policy under a reference model. It mimics the sampling-then-optimization procedure in PSRL but offers more stability since we are free to set a steady reference model. We show that even without a sampling procedure, CDPO can match the expected regret of PSRL up to constant factors for any proper reference model, e.g., the least squares estimate where the confidence set is centered at. The Conservative Update step then follows to encourage exploration within a reasonable range. Specifically, the objective of a reactive policy is to maximize the expectation of model value, instead of a single model\u2019s value. These two steps are performed in an iterative manner in CDPO. Theoretically, we show the statistical equivalence between CDPO and PSRL with the same order of expected regret. Additionally, we give the iterative policy improvement bound of CDPO, which guarantees monotonic improvement under mild conditions. We also establish the sublinear regret of CDPO, which permits its global optimality equipped with any model function class that has a bounded complexity measure. To our knowledge, the proposed framework is the first that simultaneously enjoys global optimality and iterative policy improvement. Experimental results verify the existence of the over-exploration issue and demonstrate the practical benefit of CDPO.\n# 2 Background\n# 2.1 Model-Based Reinforcement Learning\nWe consider the problem of learning to optimize an infinite-horizon \u03b3-discounted Markov Decision Process (MDP) over repeated episodes of interaction. Denote the state space and action space as S and A, respectively. When taking action a \u2208A at state s \u2208S, the agent receives reward r(s, a) and the environment transits into a new state according to probability s\u2032 \u223cf \u2217(\u00b7|s, a). Here, f \u2217is a dirac measure for deterministic dynamics and is a probability distribution for probabilistic dynamics. In model-based RL, the true dynamical model f \u2217is unknown and needs to be learned using the collected data through episodic (or iterative) interaction. The history data up to iteration t then forms Ht = {{sh,i, ah,i, sh+1,i}H\u22121 h=0 }t\u22121 i=1, where H is the actual timesteps agents run in an episode. The posterior distribution of the dynamics model is estimated as \u03c6(\u00b7|Ht). Alternatively, the frequentist model of the mean and uncertainty can also be estimated. Specifically, consider the model function class F = {f : S \u00d7A \u2192S} with size |F|, which contains the real model f \u2217\u2208F. The confidence set (or model hypothesis set) Ft \u2282F is introduced to represent the range of dynamics that is statistically plausible [49, 43, 10]. To ensure that f \u2217\u2208Ft with high probability, one way is to construct the confidence set as Ft := {f \u2208F | \u2225f \u2212\ufffdf LS t \u22252,Et \u2264\u221a\u03b2t}. Here, \u03b2t is an appropriately chosen confidence parameter (via concentration inequality), the cumulative empirical 2-norm is defined by \u2225g\u22252 2,Et := \ufffdt\u22121 i=1\u2225g(xi)\u22252 2. The least squares estimate is\n\ufffd Denote the state and state-action value function associated with \u03c0 on model f by V f \u03c0 : S \u2192R and Qf \u03c0 : S \u00d7 A \u2192R, respectively, which are defined as\n\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd The objective of RL is to learn a policy \u03c0\u2217= argmax\u03c0 J(\u03c0) that maximizes the expected return J(\u03c0). Denote the initial state distribution as \u03b6. Under policy \u03c0, the state visitation measure \u03bd\u03c0(s) over S and the state-action visitation measure \u03c1\u03c0(s, a) over S \u00d7 A in the true MDP are defined as \u03bd\u03c0(s) = (1 \u2212\u03b3) \u00b7 \u221e \ufffd h=0 \u03b3h \u00b7 P(sh = s), \u03c1\u03c0(s, a) = (1 \u2212\u03b3) \u00b7 \u221e \ufffd h=0 \u03b3h \u00b7 P(sh = s, ah = a), (2.2)\n\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd The objective of RL is to learn a policy \u03c0\u2217= argmax\u03c0 J(\u03c0) that maximizes the expected return J(\u03c0). Denote the initial state distribution as \u03b6. Under policy \u03c0, the state visitation measure \u03bd\u03c0(s) over S and the state-action visitation measure \u03c1\u03c0(s, a) over S \u00d7 A in the true MDP are defined as \u221e \ufffd \u221e \ufffd\n(2.1)\nwhere s0 \u223c\u03b6, ah \u223c\u03c0(\u00b7|sh) and sh+1 \u223cf \u2217(\u00b7|sh, ah). The objective J(\u03c0) is then J(\u03c0) = Es0\u223c\u03b6[V f \u2217 \u03c0 (s0)] = E(s,a)\u223c\u03c1\u03c0[r(s, a)]\n# 2.2 Cumulative Regret and Asymptotic Optimality\nA common criterion to evaluate RL algorithms is the cumulative regret, defined as the cumulative performance discrepancy between policy \u03c0t at each iteration t and the optimal policy \u03c0\u2217over the run of the algorithm. The (cumulative) regret up to iteration T is defined as:\n\ufffd In the Bayesian view, the model f \u2217, the learning policy \u03c0, and the regret are random variables that must be learned from the gathered data. The Bayesian expected regret is defined as: BayesRegret(T, \u03c0, \u03c6) := E [Regret(T, \u03c0, f \u2217) | f \u2217\u223c\u03c6] . (2.5)\nOne way to prove the asymptotic optimality is to show that the (expected) regret is sublinear in T, so that \u03c0t converges to \u03c0\u2217within sufficient iterations. To obtain the regret bound, the width of confidence set \u03c9t(s, a) is introduced to represent the maximum deviation between any two members in Ft:\n# 3 Provable Model-Based Reinforcement Learning\nIn this section, we analyze the central ideas and limitations of greedy algorithms as well as two popular theoretically justified frameworks: optimistic algorithms and posterior sampling algorithms.\nGreedy Model Exploitation. Before introducing provable algorithms, we first analyze greedy modelbased algorithms. In this framework, the agent takes actions assuming that the fitted model sufficiently accurately resembles the real MDP. Algorithms that lie in this category can be roughly divided into two groups: model-based planning and model-augmented policy optimization. For instance, Dyna agents [61, 20, 17] optimize policies using model-free learners with model-generated data. The model can also be exploited in first-order gradient estimators [18, 12, 9] or value expansion [15, 6]. On the other hand, model-based planning, or model-predictive control (MPC) [40, 41], directly generates optimal action sequences under the model in a receding horizon fashion. However, greedily exploiting the model without deep exploration [45] will lead to suboptimal performance. The resulting policy can suffer from premature convergence, leaving the potentially high-reward region unexplored. Since the transition data is generated by the agent taking actions in the real MDP, the dual effect [4, 27] that current action influences both the next state and the model uncertainty is not considered by greedy model-based algorithms. Optimism in the Face of Uncertainty. A common provable exploration mechanism is to adopt the principle of optimism in the face of uncertainty (OFU) [56, 49, 10]. With OFU, the agent assigns to its policy an optimistically biased estimate of virtual value by jointly optimizing over the policies and models inside the confidence set. At iteration, the OFU-RL policy is defined as:\nOptimism in the Face of Uncertainty. A common provable exploration mechanism is to adopt the principle of optimism in the face of uncertainty (OFU) [56, 49, 10]. With OFU, the agent assigns to its policy an optimistically biased estimate of virtual value by jointly optimizing over the policies and models inside the confidence set Ft. At iteration t, the OFU-RL policy \u03c0t is defined as:\nMost asymptotic analyses of optimistic RL algorithms can be abstracted as showing two properties: the virtual value V f \u03c0 is sufficiently high, and it is close to the real value V f \u2217 \u03c0 in the long run. However, in complex environments where the generalizability of nonlinear models is limited, large epistemic uncertainty will result in an unrealistically large optimistic return that drives agents for uninformative exploration. What\u2019s worse, such suboptimal exploration steps eliminate only a small portion of the model hypothesis [13], leading to a slow converging process and suboptimal practical performance. Posterior Sampling Reinforcement Learning. An alternative exploration mechanism is based on Thompson Sampling (TS) [62, 52], which involves selecting the maximizing action from a statistically\n(2.4)\n(2.5)\n(2.6)\n(3.1)\nplausibly set of action values. These values can be associated with the MDP sampled from its posterior distribution, thus giving its name posterior sampling for reinforcement learning (PSRL) [57, 42, 43]. The algorithm begins with a prior distribution of f \u2217. At each iteration t, a model ft is sampled from the posterior \u03c6(\u00b7|Ht), and \u03c0t is updated to be optimal under ft:\nThe insight is to keep away from actions that are unlikely to be optimal in the real MDP. Exploration is guaranteed by the randomness in the sampling procedure. Unfortunately, executing actions that are optimally associated with a single sampled model can cause similar over-exploration issues [52, 51]. Specifically, an imperfect model sampled from the large hypothesis can cause aggressive policy updates and value degradation between successive iterations. The suboptimality degree of the resulting policies depends on the epistemic model uncertainty. Besides, executing \u03c0t is not intended to offer performance improvement for follow-up policy learning, but only to narrow down the model uncertainty. However, this elimination procedure will be slow when the model suffers a large generalization error, which is quantitatively formulated in the model complexity measure below. Complexity Measure and Generalization Bounds. In RL, we seek to have the sample complexity for finding a near-optimal policy or estimating an accurate value function. When given access to a generative model (i.e., an abstract sampling model) in finite MDPs, it is known that the (minimax) number of transitions the agent needs to observe can be sublinear in the model size, i.e. smaller than O(|S|2|A|). Beyond finite MDPs where the number of states is large (or countably or uncountably infinite), we are interested in the learnability or generalization of RL. Unfortunately, it is impossible for agnostic reinforcement learning that finds the best hypothesis in some given policy, value, or model hypothesis class: the number of needed samples depends exponentially on the problem horizon [24]. Despite of the structural assumptions, e.g. linear MDPs [66, 22, 65] or low-rank MDPs [21, 38], we focus on the generalization bounds that can cover various cases. This can be done with additional complexity measure, e.g. eluder dimension [49], witness rank [60], or bilinear rank [14]. By introducing the eluder dimension dE [49], previous work [43, 44] established regret \ufffdO(\u221adET) for both OFU-RL and PSRL. Intuitively, the eluder dimension captures how effectively the model learned from observed data can extrapolate to future data, and permits sample efficiency in various (linear) cases. Nevertheless, it is shown in [13, 33] that even the simplest nonlinear models do not have a polynomially-bounded eluder dimension. The following result is from Thm. 5.2 in Dong et al. [13] and similar results are also established in [33]. Theorem 3.1 (Eluder Dimension of Nonlinear Models [13]). The eluder dimension dimE(F, \u03b5) (c.f. Definition 5.6) of one-layer ReLU neural networks is at least \u2126(\u03b5\u2212(d\u22121)), where d is the state-action dimension, i.e. (s, a) \u2208Rd. With more layers, the requirement of ReLU activation can be relaxed. As a result, additional complexity is hidden in the eluder dimension, e.g. when we choose \u03b5 = T \u22121, regret \ufffdO(\u221adET) contains dE = \u2126(T d\u22121) and is no longer sublinear in T. In this case, previous provable exploration mechanisms will lose the desired property of global optimality and sample efficiency, which is the underlying reason for the over-exploration issue.\n# 4 Conservative Dual Policy Optimization\nWhen using nonlinear models, e.g. neural networks, the over-exploration issue causes unfavorable performance in practice, in terms of slow convergence and suboptimal asymptotic values. To tackle this challenge, the key is to abandon the sampling process and have guarantees during training. In this regard, we propose Conservative Dual Policy Optimization (CDPO) that is simple yet provably efficient. By optimizing the policy following two successive update procedures iteratively, CDPO simultaneously enjoys monotonic policy value improvement and global optimality properties.\n# 4.1 CDPO Framework\nTo begin with, consider the problem of maximizing the expected value, \u03c0t = argmax\u03c0 E[V f \u2217 \u03c0 | Ht], where E[V f \u2217| Ht] denotes the expected values over the posterior. Obviously, we have the expected value improvement guarantee E[V f \u2217 \u03c0t | Ht] \u2265E[V f \u2217 \u03c0t\u22121 | Ht]. We can also perform expected value\n(3.2)\nmaximization in a trust-region to guarantee iterative improvement under any f \u2217. However, such updates will lose the desired global convergence guarantee and may get stuck at local maxima even with linear models. For this reason, we propose a dual procedure of policy optimization. Referential Update. The first update step returns an intermediate policy, denoted as qt. This step is a greedy one in the sense that qt is optimal with respect to the value of a single model \ufffdft, which we call a reference model. Selecting a reference model and optimizing a policy w.r.t. it imitates the sampling-optimization procedure of PSRL. We will show in Section 5.1 that if we pose the constraint \ufffdft \u2208Ft, then CDPO achieves the same expected regret as PSRL, which implies global optimality. More importantly, policy optimization under \ufffdft is more stable and can avoid the over-exploration issue in PSRL since we are free to set it as a steady reference between successive iterations. For example, we fix the reference model \ufffdft as the least squares estimate \ufffdf LS t defined in (2.1), instead of a random model sampled from the large hypothesis that causes aggressive policy update. This gives us:\nConstrained Conservative Update. The conservative update then follows as the second stage of CDPO, which takes input qt and returns the reactive policy \u03c0t+1:\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd where DTV(\u00b7, \u00b7) stands for the total variation distance and \u03b7 is the hyperparameter that characterizes the trust-region constraint and controls the degree of exploration.\nCompared with OFU-RL and PSRL, the above exploration and policy updates are conservative since the policy maximizes the expectation of the model value, instead of a single model\u2019s value (i.e. the optimistic model in OFU-RL and the sampled model in PSRL). The conservative update (4.2) avoids the pitfalls when the optimistic model or the posterior sampled model suffers large bias, which leads to aggressive policy updates and over-exploration during training. Notably, the term conservative in our work differs from previous use, e.g. Conservative Policy Iteration [23, 53]. While the latter refers to policy updates with constraints, ours is to emphasize the conservative range of randomness and the reduction of unnecessary over-exploration by shelving the sampling process. In our analysis, we follow previous work [43, 59, 10, 35] and assume access to a policy optimization oracle. In practice, the problem of finding an optimal policy under a given model can be approximately solved by model-based solvers listed below. More fine-grained analysis can be obtained by applying off-the-shelf results established for policy gradient or MPC for specific policy or model function classes. This, however, is beyond the scope of this paper.\n# 4.2 Practical Algorithm\nAlgorithm 1 Practical CDPO Algorithm\nInput: Prior \u03c6, model-based policy optimization\nsolver MBPO(\u03c0, f, J ).\n1: for iteration t = 1, ..., T do\n2:\nqt \u2190MBPO(\u00b7, \ufffdf LS\nt\n, (4.1))\n3:\nSample N models {ft,n}N\nn=1\n4:\n\u03c0t \u2190MBPO(qt, {ft,n}N\nn=1, (4.2))\n5:\nExecute \u03c0t in the real MDP\n6:\nUpdate Ht+1 = Ht \u222a{sh,t, ah,t, sh+1,t}h\n7:\nUpdate \ufffdf LS\nt+1 and \u03c6\n8: end for\n9: return policy \u03c0T\nThe pseudocode of CDPO is in Alg. 1. The model-based solver MBPO(\u03c0, f, J ) outputs the policy (qt or \u03c0t) that optimizes the objective J with access to model f. Several different types of solvers can be leveraged, e.g., model-augmented model-free policy optimization such as Dyna [61], model-based reparameterization gradient [18, 9], or model-predictive control [63]. Details of different optimization choices can be found in Appendix E. In experiments, we use Dyna and MPC solvers. With Pinsker\u2019s inequality, the total variation constraint in (4.2) is replaced by the KL divergence [53, 2] in experiments. We follow\n(4.1)\n(4.2)\n# 5 Analysis\nIn this section, we first show the statistical equivalence between CDPO and PSRL in terms of the same BayesRegret bound. Then we give the iterative policy value bound with monotonic improvement. Finally, we prove the global convergence of CDPO. The missing proofs can be found in the Appendix.\n# 5.1 Statistical Equivalence between CDPO and PSRL\nSketch proof. We first sketch the general strategy in the PSRL analysis. Recall the definition of the Bayesian expected regret BayesRegret(T, \u03c0, \u03c6) := E[\ufffdT t=1 Rt], where Rt = V f \u2217 \u03c0\u2217\u2212V f \u2217 \u03c0t . PSRL breaks down Rt by adding and subtracting V ft \u03c0ft , the value of the imagined optimal policy \u03c0ft under a sampled model ft, i.e. \u03c0ft = argmax\u03c0 V ft \u03c0 .\n\u2212 \u2212 \u2212  \u2212 where the second equality follows from the definition of the PSRL policy. Following the law of total expectation and the Posterior Sampling Lemma (e.g. Lemma 1 in [42]), we have E[V f \u2217 \u03c0\u2217\u2212V ft \u03c0ft ] = 0 by noting that f \u2217and ft are identically distributed conditioned upon Ht. Then we obtain\n \u2212 \ufffd where the first inequality follows from the simulation lemma under the L-Lipschitz value assumption [43]. The second inequality follows from the definition of \u03c9t in (2.6) and the construction of confidence set such that P(f \u2217\u2208\ufffdFt) \u22651 \u22122\u03b4 and P(ft \u2208\ufffdFt, f \u2217\u2208\ufffdFt) \u22651 \u22124\u03b4 via a union bound. As more data is collected, the model uncertainty is reduced and the sum of confidence set width \u03c9t will be sublinear in T (c.f. Lemma B.5 and B.6), indicating sublinear regret. When it comes to CDPO, we decompose the regret as\n  where the CDPO policy \u03c0t is defined in (4.2). Since E[V f \u2217 \u03c0\u2217\u2212V ft \u03c0ft ] = 0,\n\ufffd  \u2212 \ufffd where the first inequality follows from the greediness of qt and \u03c0t in the dual update steps, i.e., V \ufffd ft \u03c0ft \u2264V \ufffd ft qt for any \u03c0ft as well as E[V ft \u03c0t ] \u2265E[V ft qt ]. The 8\u03b4T term is introduced since \ufffdft \u2208Ft and P(ft \u2208\ufffdFt, \ufffdft \u2208\ufffdFt) \u22651 \u22122\u03b4.\n(5.1)\n(5.2)\n(5.3)\n\nOne motivation for the conservative update is that it maximizes (thus improves) the expected value over the posterior. In this section, we are interested in the policy value improvement under any unknown f \u2217. Namely, we seek to have the iterative improvement bound J(\u03c0t) \u2212J(\u03c0t\u22121), where the true objective J is defined in (2.3). We impose the following regularity conditions on the underlying MDP transition and the state-action visitation. Assumption 5.2 (Regularity Condition on MDP Transition). Assume that the MDP transition function f \u2217: S \u00d7 A \u2192S is with additive \u03c3-sub-Gaussian noise and bounded norm, i.e., \u2225s\u22252 \u2264C. Assumption 5.3 (Regularity Condition on State-Action Visitation). We assume that there exists \u03ba > 0 such that for any policy \u03c0t, t \u2208[1, T],\n\ufffd \ufffd where d\u03c1qt+1/d\u03c1\u03c0t is the Radon-Nikodym derivative of \u03c1qt+1 with respect to \u03c1\u03c0t. Theorem 5.4 (Policy Iterative Improvement). Suppose we have \u2225\ufffdf(\u00b7, \u00b7)\u2225\u2264C for \ufffdf \u2208F where the model class F is finite. Define \u03b9 := maxs,a |Af \u2217 \u03c0 (s, a)|, where Af \u2217 \u03c0 is the advantage function defined as Af \u2217 \u03c0 (s, a) := Qf \u2217 \u03c0 (s, a) \u2212V f \u2217 \u03c0 (s). With probability at least 1 \u2212\u03b4, the policy improvement between successive iterations is bounded by\nwhere \u2206(t) := Es\u223c\u03b6 \ufffd V \ufffd ft qt (s) \u2212V \ufffd ft qt\u22121(s) \ufffd \u22650 due to the greediness of qt.\n\ufffd  \ufffd  \ufffd \ufffd The above theorem provides the iterative improvement bound following the CDPO algorithm. When H is large enough, the policy value improvement is at least \u2206(t) by choosing a properly small \u03b7.\n\ufffd     \ufffd The above theorem provides the iterative improvement bound following the CDPO algorithm. When H is large enough, the policy value improvement is at least \u2206(t) by choosing a properly small \u03b7. In particular, the first term \u2206(t) characterizes the policy improvement brought by the greedy exploitation in (4.1), and \u2206(t) \u22650 since qt is optimal under the reference model \ufffdft. The second term in (5.6) accounts for the generalization error of least square methods. Specifically, model \ufffdft = \ufffdf LS t \u2208Ft is trained to fit the history samples. However, we seek to have the model error bound over the stateaction visitation measure, which requires the deviation from the empirical mean to its expectation using Bernstein\u2019s inequality and union bound. Finally, the trust-region constraint in (4.2) brings the 4\u03b7\u03b1/(1 \u2212\u03b3) term, which reduces to zero if \u03b7 is small. This makes intuitive sense as \u03b7 controls the degree of conservative exploration.\n# 5.3 Global Optimality of CDPO\nWe now analyze the global optimality of CDPO by studying its expected regret. As discussed in Section 3, agnostic reinforcement learning is impossible. Without structural assumptions, additional complexity measure is required for a generalization bound beyond finite settings. For this reason, we adopt the notation of eluder dimension [49, 43], defined as follows: Definition 5.5 ((F, \u03b5)-Dependence). If we say (s, a) \u2208S\u00d7A is (F, \u03b5)-dependent on {(si, ai)}n i=1 \u2286 S \u00d7 A, then\n\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd \ufffd\ufffd Conversely, (s, a) \u2208S \u00d7 A is (F, \u03b5)-independent of {(si, ai)}n i=1 if and only if it does not satisfy the definition for dependence. Definition 5.6 (Eluder Dimension). The eluder dimension dimE(F, \u03b5) is the length of the longest possible sequence of elements in S\u00d7A such that for some \u03b5\u2032 \u2265\u03b5, every element is (F, \u03b5\u2032)-independent of its predecessors. We make the following assumption on the Lipschitz continuity of the value function.\n(5.6)\nAssumption 5.7 (Lipschitz Continuous Value). At iteration t, assume the value function V ft \u03c0 for any policy \u03c0 is Lipschitz continuous in the sense that |V ft \u03c0 (s1) \u2212V ft \u03c0 (s2)| \u2264Lt\u2225s1 \u2212s2\u22252. Notably, Assumption 5.7 holds under certain regularity conditions of the MDP, e.g. when the transition and rewards are Lipschitz continuous [5, 47]. Under this assumption, many RL settings can be satisfied [13], e.g., nonlinear models with stochastic Lipschitz policies and Lipschitz reward models, and is thus adopted by various model-based RL work [35, 7, 13]. We now study the global optimality of CDPO by the following expected regret theorem, which can be seen as a direct consequence of Theorem 5.1 that states the statistical equivalence between CDPO and PSRL. Theorem 5.8 (Expected Regret of CDPO). Let N(F, \u03b1, \u2225\u00b7\u22252) be the \u03b1-covering number of F. Denote dE := dimE(F, T \u22121) for the eluder dimension of F at precision 1/T. Under Assumption 5.2 and 5.7, the cumulative expected regret of CDPO in T iterations is bounded by \ufffd \ufffd\n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd Here, the covering number is introduced since we are considering F that may contain infinitely many functions, for which we cannot simply apply a union bound. Besides, \u03b2 is the confidence parameter that contains f \u2217with high probability (via concentration inequality). To clarify the asymptotics of the expected regret bound, we introduce another measure of dimensionality that captures the sensitivity of F to statistical overfitting. Corollary 5.9 (Asymptotic Bound). Define the Kolmogorov dimension w.r.t. function class F as\n \ufffd \ufffd The sublinear regret result permits the global optimality and sample efficiency for any model class with a reasonable complexity measure. Meanwhile, the iterative improvement theorem guarantees efficient exploration and good performance even when the model class is highly nonlinear.\n# 6 Empirical Evaluation\n# 6.1 Understanding Different Exploration Mechanisms\nWe first provide insights and evidence of why CDPO exploration can be more efficient in the tabular N-Chain MDPs, which have optimal right actions and suboptimal left actions at each of the N states. Settings and full results are provided in Appendix F.2. In Figure 1, we compare the posterior of CDPO and PSRL at the state that is the furthest away from the initial state, i.e. the state that is the hardest for the agents to reach and explore.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/71b6/71b64315-4855-436f-9582-2838baabafe6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: CDPO and PSRL posterior on an 8-Chain MDP and a 15-Chain MDP, where the right actions are optimal. Figure 2: Regret curve of CDPO and PSRL when N = 8 and N = 15.</div>\nWhen training starts, both algorithms have a large variance of value estimation. However, as training progresses, CDPO gives more accurate and certain estimates, but only for the optimal right actions not\n(5.7)\nfor the suboptimal left actions, while PSRL agents explore both directions. This verifies the potential over-exploration issue in PSRL: as long as the uncertainty contains unrealistically large values, PSRL agents can perform uninformative exploration by acting suboptimally according to an inaccurate sampled model. In contrast, CDPO replaces the sampled model with a stable mean estimate and cares about the expected value, thus avoiding such pitfalls. We see in Figure 2 that although CDPO has much larger uncertainty for the suboptimal left actions, its regret is lower.\nIn finite MDPs, PSRL-style agents can specify and try every possible action to finally obtain an accurate high-confidence prediction. However, our discussion in Section 3 indicates that a similar over-exploration issue in more complex environments can lead to less informative exploration steps, which only eliminate an exponentially small portion of the uncertainty.\nTo see its impact on the training performance, we report the results of provable algorithms with nonlinear models on several MuJoCo tasks in Figure 3. For OFU-RL, we mainly evaluate HUCRL [10], a deep algorithm proposed to deal with the intractability of the joint optimization. We observe that all algorithms achieve asymptotic optimality in the inverted pendulum. Since the dimension of the pendulum task is low, learning an accurate (and thus generalizable) model poses no actual challenge However, in higher dimensional tasks such as half-cheetah, CDPO achieves a higher asymptotic value with faster convergence. Implementation details and hyperparameters are provided in Appendix F.1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ce76/ce76234d-66d1-4b49-b718-f51e7fb68c9f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) 7-DOF Pusher.</div>\n<div style=\"text-align: center;\">(a) Inverted Pendulum.</div>\n<div style=\"text-align: center;\">Figure 3: Performance of CDPO, PSRL, and HUCRL equipped with nonlinear models in several MuJoCo tasks: inverted pendulum swing-up, pusher goal-reaching, and half-cheetah locomotion.</div>\n# 6.3 Comparison with Prior RL Algorithms\nWe also examine a broader range of MBRL algorithms, including MBPO [20], SLBO [35], and ME-TRPO [30]. The model-free baselines include SAC [16], PPO [54], and MPO [2]. The results are shown in Figure 4. We observe that CDPO achieves competitive or higher asymptotic performance while requiring fewer samples compared to both the model-based and the model-free baselines.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1adf/1adf2ae6-df53-4d80-adfa-e01f3f5d24eb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"> Comparison between CDPO and model-free, model-based RL baseline </div>\nWe conduct ablation studies to provide a better understanding of the components in CDPO. One can observe from Figure 5 that the policies updated with only Referential Update or Conservative Update lag behind the dual framework. We also test the necessity and sensitivity of the constraint hyperparameter \u03b7. We see that a constant \u03b7 and a time-decayed \u03b7 achieve similar asymptotic values with a similar convergence rate, showing the robustness of CDPO. However, removing the constraint will lose the policy improvement guarantee, thus causing degradation. Ablation on different choices of MBPO solver (Dyna and POPLIN-P [63]) shows the generalizability of CDPO.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/28d8/28d85f6f-7df0-4702-9523-a7364bde4ab7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Half-Cheetah.</div>\n<div style=\"text-align: center;\">(a) Inverted Pendulum.</div>\nFigure 5: Ablation studies on the effect of the dual update steps and the trust-region constraint. The robustness and generalizability of the CDPO framework are demonstrated by the results of different choices of the constraint threshold and different solvers.\n# 7 Conclusions & Future Work\nIn this work, we present Conservative Dual Policy Optimization (CDPO), a simple yet provable modelbased algorithm. By iterative execution of the Referential Update and Conservative Update, CDPO explores within a reasonable range while avoiding aggressive policy update. Moreover, CDPO gets rid of the harmful sampling procedure in previous provable approaches. Instead, an intermediate policy is optimized under a stable reference model, and the agent conservatively explore the environment by maximizing the expected policy value. With the same order of regret as PSRL, the proposed algorithm can achieve global optimality while monotonically improving the policy. Considering our naive choice of the reference model, other more sophisticated designs should be a fruitful future direction. It will also be interesting to explore different choices of the MBPO solvers, which we would like to leave as future work.\n# References\n<div style=\"text-align: center;\">(c) Half-Cheetah.</div>\n# Checklist\n# A Proofs\n# A.1 Proof of Theorem 5.4\nProof. We lay out the proof in two major steps. Firstly, we characterize the performance difference between J(qt) and J(\u03c0t\u22121), which can be done by applying Lemma B.3. Specifically, we set \u03c01, \u03c02 in Lemma B.3 to qt, \u03c0t\u22121 and set f as the reference model \ufffdft. Then we obtain J(qt) \u2212J(\u03c0t\u22121) \ufffd \ufffd\nwhere \u2206(t) := Es\u223c\u03b6 \ufffd V \ufffd ft qt (s) \u2212V \ufffd ft \u03c0t\u22121(s) \ufffd \u22650 due to the optimality of qt under \ufffdft, i.e., qt = argmaxq V \ufffd ft q . Recall that the reference model is the least squares estimate, i.e.,\n\ufffd \ufffd \ufffd\ufffd where Ht\u22121 is the trajectory in the real environment when following policy \u03c0t\u22121. From the simulation property of continuous distribution, we have the following equivalence between the direct and indirect ways of drawing samples:\nwhere p(\u03f5) is some noise distribution. Therefore, according to the Gaussian noise assumption, we obtain from the least squares generalization bound in Lemma B.4 that\n\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd \ufffd where \u03f5approx = 0 in the generalization bound as the realizability is guaranteed since \ufffdf LS t and f \u2217are from the same function class F. Similarly, we have for the intermediate policy qt that\nE\u03c1qt \ufffd\ufffd\ufffd\ufffdft(s, a) \u2212f \u2217(\u00b7|s, a) \ufffd\ufffd 1 \ufffd \u2264E\u03c1\u03c0t\u22121 \ufffd\ufffd\ufffd\ufffdft(s, a) \u2212f \u2217(\u00b7|s, a) \ufffd\ufffd 1 \ufffd \u00b7 \ufffd E\u03c1\u03c0t\u22121 \ufffd\ufffdd\u03c1qt d\u03c1\u03c0t\u22121 (s) \ufffd2\ufffd\nNow we can bound (A.1) by\nJ(qt) \u2212J(\u03c0t\u22121) \u2265\u2206(t) \u2212(1 + \u03ba) \u00b7 22\u03b3C2 ln(|F|/\u03b4) (1 \u2212\u03b3)H .\nThe second step of the proof is to characterize the performance difference between J(\u03c0t) and J(qt). From the Performance Difference Lemma B.2, we obtain\n\ufffd\ufffd (A.1)\n(A.2)\n(A.4)\n(A.5)\nwhere recall that \u03b9 := maxs,a |Af \u2217 \u03c0 (s, a)| and the third equality holds due to Ea\u223c\u03c0t \ufffd Af \u2217 \u03c0t (s, a) \ufffd = 0 for any s. By the definition of the total variation distance, we can further bound the absolute difference as\n\ufffd \ufffd By the definition of the total variation distance, we can further bound the absolute difference as\n\ufffd \ufffd By the definition of the total variation distance, we can further bound the absolute difference as\nThus, we have J(\u03c0t) \u2212J(qt) \u2265\u22122\u03b7\u03b9/(1 \u2212\u03b3) and similarly J(qt\u22121) \u2212J(\u03c0t\u22121) \u2265\u22122\u03b7\u03b9/(1 \u2212\u03b3). Combining with (A.4) gives us the iterative improvement bound as follows:\n# A.2 Proof of Theorem 5.8\nProof. We are interested in the expected regret defined as BayesRegret(T, \u03c0, \u03c6) := E[\ufffdT t=1 Rt], where Rt = V f \u2217 \u03c0\u2217\u2212V f \u2217 \u03c0t . Recall the definition of the reactive policy \u03c0t in CDPO (i.e. (4.2)) and the imagined best-performing policy \u03c0ft under a sampled model ft, i.e., \u03c0ft = max\u03c0 V ft \u03c0 . From the Posterior Sampling Lemma, we know that if \u03c8 is the distribution of f \u2217, then for any sigma-algebra \u03c3(Ht)-measurable function g, E[g(f \u2217) | Ht] = E[g(ft) | Ht]. (A.8)\nProof. We are interested in the expected regret defined as BayesRegret(T, \u03c0, \u03c6) := E[\ufffdT t=1 R where Rt = V f \u2217 \u03c0\u2217\u2212V f \u2217 \u03c0t .\nRecall the definition of the reactive policy \u03c0t in CDPO (i.e. (4.2)) and the imagined best-performing policy \u03c0ft under a sampled model ft, i.e., \u03c0ft = max\u03c0 V ft \u03c0 . From the Posterior Sampling Lemma, we know that if \u03c8 is the distribution of f \u2217, then for any sigma-algebra \u03c3(Ht)-measurable function g, E[g(f \u2217) | Ht] = E[g(ft) | Ht]. (A.8)\nThe PS Lemma together with the law of total expectation gives us E[V f \u2217 \u03c0\u2217\u2212V ft \u03c0ft ] = 0,\nwhere the equality holds since the true f \u2217and the sampled ft are identically distributed when conditioned on Ht. Therefore, we obtain the expected regret for CDPO as\n\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd where the first equation follows from Lemma B.1 and \ufffd\u03c1\u03c0 is the state-action visitation measure under model \ufffdft, the second inequality follows the simulation property of continuous distribution and the Lipschitz value function assumption.\n(A.6)\n(A.7)\n(A.8)\n(A.9)\n(A.10)\n(A.11)\n\ufffd \ufffd We can further know from the construction of the confidence set (c.f. Lemma B.5) that P \ufffd f \u2217\u2208 \ufffd t Ft \ufffd \u22651 \u22122\u03b4 and P(A) \u22651 \u22122\u03b4 since ft, f \u2217are identically distributed and P \ufffd\ufffdft \u2208Ft \ufffd = 1 as Ft is centered at the least squares model for all t. Besides, we have for\nPlugging into (A.21), we have\nSumming over T iterations gives us\nBy setting \u03b4 = 1/(2T), we obtain\n \u2212  \ufffd  \u2212 \ufffd \ufffd where the last inequality follows from Lemma B.6 to bound the sum of the set width. We denote dE := dimE(F, T \u22121) for notation simplicity. Since (A.16) holds for all policy \u03c0, we have the bound for E[V ft \u03c0ft \u2212V \ufffd ft \u03c0ft ] and the bound for E[V \ufffd ft qt \u2212 V ft qt ]. What remains in the expected regret (A.19) is the E[V ft \u03c0t \u2212V f \u2217 \u03c0t ] term, which can be bounded similarly. Specifically, we define another event B = \ufffd f \u2217\u2208\ufffd t Ft, ft \u2208\ufffd t Ft \ufffd . Since by construction P \ufffd f \u2217\u2208\ufffd t Ft \ufffd \u22651 \u22122\u03b4 and P \ufffd ft \u2208\ufffd t Ft \ufffd \u22651 \u22122\u03b4, we have P(B) \u22651 \u22124\u03b4 via a union bound. This implies the following bound\n(A.12)\n(A.13)\n(A.14)\nwhere the second inequality follows from the choice of \u03b4, i.e., \u03b4 = 1/(2T) Plugging (A.16) and (A.17) into (A.19), we obtain the expected regret as\nBy setting \u03b1 = 1/(T 2) and \u03b4 = 1/(2T) in Lemma B.5, we have the following confidence parameter that can guarantee that f \u2217is contained in the confidence set with high probability: \ufffd \ufffd \ufffd\n# \ufffd A.3 Proof of Theorem 5.1\nProof. Denote the imagined optimal policy \u03c0ft under a sampled model ft as \u03c0ft = max\u03c0 V ft \u03c0 . For PSRL, its expected regret can be decomposed as\n\ufffd where the second equality holds since the PSRL policy \u03c0t := \u03c0ft for a sampled ft. The third equality follows from (A.9), obtained by the Posterior Sampling Lemma and the law of total expectation. Similar with the proof in A.2, we obtain from the Simulation Lemma B.1 that \ufffd \ufffd \ufffd \ufffd\n\ufffd  \u2212  \ufffd\ufffd\ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd where the second inequality follows from the construction of confidence set that P \ufffd f \u2217\u2208\ufffd t Ft \ufffd \u2265  \u22122\u03b4 and thus P(E) \u22651 \u22124\u03b4.\n\ufffd\ufffd \ufffd\ufffd where the second inequality follows from the construction of confidence set that P \ufffd f \u2217\u2208\ufffd t Ft \ufffd \u2265 1 \u22122\u03b4 and thus P(E) \u22651 \u22124\u03b4.\n(A.19)\n(A.20)\n(A.21)\n\ufffd \ufffd From the proof in A.2, the expected regret of CDPO is bounded by\nThe claim is thus established.\n# B Useful Lemmas\nLemma B.1 (Simulation Lemma). For any policy \u03c0 and transition f1, f2, we have\nProof. Denote the expected reward under policy \u03c0 as r\u03c0. Let f \u03c0 be the transition matrix on stateaction pairs induced by policy \u03c0, defined as f \u03c0 (s,a),(s\u2032,a\u2032) := P(s\u2032|s, a)\u03c0(a\u2032|s\u2032).\nSince \u03b3 < 1, it is easy to verify that I \u2212\u03b3f \u03c0 is full rank and thus invertible. Therefore, we can write V\u03c0 = (I \u2212\u03b3f \u03c0)\u22121r\u03c0. (B.2)\nTherefore, we conclude the proof by\nLemma B.2 (Performance Difference Lemma). For all policies \u03c0, \u03c0\u2217and distribution \u00b5 over S, w have\nLemma B.2 (Performance Difference Lemma). For all policies \u03c0, \u03c0\u2217and distribution \u00b5 over S, we ave\nProof. This lemma is widely adopted in RL. Proof can be found in various previous works, e.g. Lemma 1.16 in [3]. Let P\u03c0(\u03c4|s0 = s) denote the probability of observing trajectory \u03c4 starting at state s0 and then following \u03c0. Then the value difference can be written as\n(A.23)\n\n(B.1)\n(B.2)\n(B.3)\n\ufffd \ufffd \ufffd where the third equation rearranges terms in the summation via telescoping, and the fourth equality follows from the law of total expectation. From the definition of objective J(\u03c0) in (2.3), we obtain\nLemma B.3 (Performance Difference and Model Error). For any two policies \u03c01 and \u03c02, it holds that\n\ufffd \ufffd \ufffd \ufffd Proof. The proof can be established by combining the Performance Difference Lemma and the Simulation Lemma. We refer to Corollary 3.1 in [48] or Lemma A.3 in [59] for a detailed proof. Lemma B.4 (Least Squares Generalization Bound). Given a dataset H = {xi, yi}n i=1 where xi \u2208X and xi, yi \u223c\u03bd, and yi = f \u2217(xi)+\u03f5i. Suppose |yi| \u2264Y and \u03f5i is independently sampled noise. Given a function class F : X \u2192[0, Y ], we assume approximate realizable, i.e., minf\u2208F Ex\u223c\u03bd \ufffd |f \u2217(x) \u2212 f(x)|2\ufffd \u2264\u03f5approx. Denote \ufffdf as the least square solution, i.e., \ufffdf = argminf\u2208F \ufffdn i=1 \ufffd f(xi) \u2212yi \ufffd2. With probability at least 1 \u2212\u03b4, we have\n\ufffd\ufffd \ufffd Proof. The result is standard and can be proved by using the Bernstein\u2019s inequality and union boun Detailed proof can be found at Lemma A.11 in [3]. Lemma B.5 (Confidence sets with high probability). If the control parameter \u03b2t(\u03b4, \u03b1) is set to \u03b2(\u03b4, \u03b1) = 8\u03c32 log(N(F, \u03b1, \u2225\u00b7\u2225)/\u03b4) + 2\u03b1t \ufffd 8C + \ufffd 8\u03c32 log(4t2/\u03b4) \ufffd , (B.7\n\ufffd\ufffd \ufffd Proof. The result is standard and can be proved by using the Bernstein\u2019s inequality and union bound. Detailed proof can be found at Lemma A.11 in [3]. Lemma B.5 (Confidence sets with high probability). If the control parameter \u03b2t(\u03b4, \u03b1) is set to \u03b2t(\u03b4, \u03b1) = 8\u03c32 log(N(F, \u03b1, \u2225\u00b7\u22252)/\u03b4) + 2\u03b1t \ufffd 8C + \ufffd 8\u03c32 log(4t2/\u03b4) \ufffd , (B.7) then for all \u03b4 > 0, \u03b1 > 0 and t \u2208N, the confidence set Ft = Ft(\u03b2t(\u03b4, \u03b1)) satisfies: \ufffd \ufffd \ufffd\nProof. See [43] Proposition 6 for a detailed proof.\n(B.4)\n(B.6)\n(B.7)\n(B.8)\n\n(B.9)\n# C Limitations of Eluder Dimension\nIn Theorem 5.8, the eluder dimension dE appears in the Bayes expected regret bound to capture how effectively the observed samples can extrapolate to unobserved transitions. For some specific function classes, Osband et al. [43] provide the corresponding eluder dimension bound, e.g., for (generalized) linear function classes, quadratic function class, and for finite MDPs, c.f. Proposition 1-4 in [43]. However, for non-linear models, Dong et al. [13] show that the \u03b5-eluder dimension of one-layer neural networks is at least exponential in model dimension. Similar results are also established in [33]. We refer to Section 5 in [13] or Section 4 in [33] for details and more explanations.\n# D Additional Related Work\nSome MBRL work also concerns iterative policy improvement. SLBO [35] provides a trust-region policy optimization framework based on OFU. However, the conditions for monotonic improvement cannot be satisfied by most parameterized models [35, 13], which leads to a greedy algorithm in practice. Prior work that shares similarities with ours contains DPI [59] and GPS [31, 39] as dual policy optimization procedures are adopted. Both DPI and GPS leverage a locally accurate model and use different objectives for imitating the intermediate policy within a trust-region. However, the policy imitation procedure updates the policy parameter in a supervised manner, which poses additional challenges for effective exploration, resulting in unknown convergence results even with a simple model class. In contrast, CDPO by taking the epistemic uncertainty into consideration can be shown to achieve global optimality. In fact, greedy model exploitation is provably optimal only in very limited cases, e.g., linear-quadratic regulator (LQR) settings [36]. OFU-RL has shown to achieve an optimal sublinear regret when applied to online LQR [1], tabular MDPs [19] and linear MDPs [22]. Among them, HUCRL [10] is a deep algorithm proposed to deal with the joint optimization intractability in (3.1). Besides, Russo and Van Roy [49, 50] unify the bounds in various settings (e.g., finite or linear MDPs) by introducing an additional model complexity measure \u2014 eluder dimension. Other complexity measure include witness rank [60], linear dimensionality [66] and sequential Rademacher complexity [13].\n# E Algorithm Instantiations\nThe model-based policy optimization solver MBPO(\u03c0, {f}, J ) in Algorithm 1 can be instantiated as one of the following algorithms, Dyna-style policy optimization in Algorithm 2, model-based back-propagation in Algorithm 3, and model predictive control policy optimization in Algorithm 4. By default, MBPO is instantiated as the Dyna solver (i.e. Algorithm 2) in our MuJoCo experiments and as the policy iteration solver in our N-Chain MDPs experiments. We note that the instantiations are not restricted to the listed algorithms, and many other MBPO algorithms that augment policy learning with a predictive model can also be leveraged, e.g., model-based value expansion [15, 6]. In the Referential Update step where no input policy exists in MBPO(\u00b7, \ufffdf LS t , (4.1), we initialize policy \u03c0 = \u03c0t\u22121, i.e. the reactive policy from the last iteration. Dyna. Dyna involves model-generated data and optimizing the policy with any model-free RL method, e.g., REINFORCE or actor-critic [28]. The state-action value can be estimated by learning a critic function or unrolling the model. In Constrained Conservative Update, the input objective function J is (4.2), which is with constraints. Thus, the Lagrangian multiplier is introduced, similar to the model-free trust-region algorithms [53, 54, 2]. Back-Propagation Through Time. BPTT [30, 64] is a first-order model-based policy optimization framework based on pathwise gradient (or reparameterization gradient) [58]. There are also several variants including Stochastic Value Gradients (SVG) [18], Model-Augmented Actor-Critic (MAAC) [9], and Probabilistic Inference for Learning COntrol (PILCO) [12]. Specifically, the policy parameters are updated by directly computing the derivatives of the performance with respect to the parameters. When the optimization of objective function is constrained, the accumulating step (Algorithm 3\nAlgorithm 2 Dyna Model-Based Policy Optimization\nInput: Policy \u03c0, model set {f}, objective function J .\n1: Initialize a simulation data buffer \ufffdD\n2: Sample a batch of initial states from the initial distribution \u03b6\n3: \u25b7Data simulation\n4: for initial state sample s0 do\n5:\nfor model f in model set {f} do\n6:\nfor timestep h = 1, ..., H do\n7:\nSample action \ufffdah \u223c\u03c0(\u00b7|\ufffdsh)\n8:\nSample simulation state \ufffdsh+1 \u223cf(\ufffdsh, \ufffdah)\n9:\nAppend simulation data to buffer \ufffdD = \ufffdD \u222a(\ufffdsh, \ufffdah, rh, \ufffdsh+1)\n10:\nend for\n11:\nend for\n12: end for\n13: \u25b7Policy optimization with any model-free algorithm ModelFree\n14: Objective optimization of policy on the simulated data \u03c0 \u2190ModelFree( \ufffdD, \u03c0)\nAlgorithm 3 Model-Based Back-Propagation Policy Optimization\nInput: Policy \u03c0, model set {f}, objective function J .\n1: Initialize a simulation data buffer \ufffdD\n2: Start from initial state s0\n3: Reset L \u21900\n4: \u25b7Data simulation\n5: for model f in model set {f} do\n6:\nfor timestep h = 1, ..., H do\n7:\nSample action \ufffdah \u223c\u03c0(\u00b7|\ufffdsh)\n8:\nSample simulation state \ufffdsh+1 \u223cf(\ufffdsh, \ufffdah)\n9:\nAccumulate reward and constraint to L\n10:\nend for\n11: end for\n12: \u25b7Policy optimization\n13: Compute policy gradient with back-propagation through time\n14: Objective optimization of policy \u03c0 \u2190PolicyGradient\nModel Predictive Control Policy Optimization. MPC is a planning framework that directly generates optimal action sequences under the model. Different from the above model-augmented policy optimization methods, MPC policy optimization directly generates optimal action sequences under the model and then distills the policy. Specifically, the pseudocode in Algorithm 4 begins with initial actions generated by the policy. Then with a shooting method, e.g., the cross-entropy method (CEM), the actions are refined and the policy that generates these optimal actions are distilled. Below, the algorithm to obtain the refined actions EliteActions can be CEM with action noise added to the action or policy parameter, i.e., POPLIN-A and POPLIN-P in [63]. The policy can be updated by UpdatePolicy using behavior cloning. Policy Iteration for Tabular MDPs. In tabular settings where the state space S and action space A are discrete and countable, we can perform policy iteration under each model in the model set {f}. Here, the model is the tabular representation instead of function approximators. Based on the state-action values under various models, the optimal action at each state is the one that maximizes the weighted average of the values within the constraint of total variation distance.\nAlgorithm 4 Model Predictive Control Policy Optimization\nInput: Policy \u03c0, model set {f}, objective function J , algorithm to update actions EliteActions,\nalgorithm to update policy UpdatePolicy.\n1: Start from initial state s0\n2: Reset J \u21900\n3: \u25b7Model-based planning\n4: for model f in model set {f} do\n5:\nfor timestep h = 1, ..., H do\n6:\nSample action \ufffdah \u223c\u03c0(\u00b7|\ufffdsh)\n7:\nSample simulation state \ufffdsh+1 \u223cf(\ufffdsh, \ufffdah)\n8:\nAccumulate reward and constraint to J\n9:\nend for\n10: end for\n11: a \u2190EliteActions(J, \ufffda1:N)\n12: \u25b7Policy distillation\n13: \u03c0 \u2190UpdatePolicy(a)\n# F Experimental Settings and Results in N-Chain MDPs\nF.1 Settings of MuJoCo Experiments\nIn the MuJoCo experiments, we use a 5-layer neural network to approximate the dynamical model. We use deterministic ensembles [8] to capture the model epistemic uncertainty. Specifically, different ensembles are learned with independent transition data to construct the 1-step ahead confidence interval at every timestep. Each ensemble is separately trained using Adam [26]. And the number of ensemble heads can be set to 3, 4, or, 5, each of which is shown to be able to provide considerable performance in our experiments. All the experiments are repeated with 6 random seeds. Since neural networks are not calibrated in general, i.e., the model uncertainty set is not guaranteed to contain the real dynamics, we follow HUCRL [10] to re-calibrate [29] the model. Our MuJoCo code is also built upon the HUCRL GitHub repository. When using the Dyna model-based policy optimization, the number of gradient steps for each optimization procedure in an iteration is set to 20. And we empirically find that the KL divergence (or total variance) constraint makes the algorithm more efficient when computing the argmax in the optimization step, since optimizing from \u03c0t\u22121 at iteration t needs fewer policy gradient steps if the policy update is constrained within a certain trust region. The task-specific and task-common settings and parameters are listed below in Table 1.\n<div style=\"text-align: center;\">Table 1: Experimental parameters. Inverted Pendulum Pushe</div>\nInverted Pendulum\nPusher\nHalf-Cheetah\nepisode length H\n200\n150\n1000\ndimension of state\n4\n23\n18\ndimension of action\n1\n7\n6\naction penalty\n0.001\n0.1\n0.1\nhidden nodes\n(200, 200, 200, 200, 200)\nactivation function\nSwish\noptimizer\nAdam\nlearning rate\n10\u22123\n# F.2 Experiments in N-Chain MDPs\nBesides the experiments in MuJoCo, we also conduct tabular experiments in the N-Chain environment that is proposed in [37]. Specifically, there are in total 2 actions and N states in an MDP. The initial state is s1 and the agent can choose to go left or right at each of the N states. The left action always succeeds and moves the agent to the left state, giving reward r \u223cN(0, \u03b42). Taking the right action at\nstate s1, . . . , sN\u22121 gives reward r \u223cN(\u2212\u03b4, \u03b42) and succeeds with probability 1 \u22121/N, moving the agent to the right state and otherwise moving the agent to the left state. Taking the right action at sN gives reward r \u223cN(1, \u03b42) and moves the agent back to s1 with probability 1 \u22121/N. We set \u03b4 = 0.1 exp (\u2212N/4), such that going right is the optimal action at least up to N = 40. As the number of states N is increasing, the agent needs deep exploration (e.g. guided by uncertainty) instead of dithering exploration (e.g. epsilon-greedy exploration), such that the agent can keep exploring despite receiving negative rewards [45].\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/af5c/af5cdfe6-c5bc-46ac-9580-07e8aa602ab1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Illustration of the N-Chain MDP. Blue arrows correspond to action right (optimal) and red arrows correspond to action left (suboptimal). The figure is copied from [37].</div>\nFor this reason, we evaluate the proposed algorithm CDPO and compare it with other Bayesian RL algorithms, including Bayesian Q-Learning (BQL) [11], Posterior Sampling for RL (PSRL) [42], the Uncertainty Bellman Equation (UBE) [46] and Moment Matching (MM) approach [37]. For CDPO, the dual optimization steps are solved by policy iteration, and the conservative update is performed within the total variation distance \u03b7 = 0.2 (c.f. Policy Iteration for Tabular MDPs in Appendix E). We choose conjugate priors to represent the posterior distribution: we use a Categorical-Dirichlet model for discrete transition distribution at each (s, a), and a Normal-Gamma (NG) model for continuous reward distribution at each (s, a, s\u2032).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b045/b045d231-fb02-4dd5-ab7c-ddcf1051e202.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Posterior evolution of CDPO algorithm in the 8-Chain MDP.</div>\nEvolution of Posterior. Figure 7 demonstrates the evolution of the posterior of the CDPO algorithm in an 8-Chain MDP. As training progresses the posteriors concentrate on the true optimal state-action values and the behavior policy converges on the optimal one. The fast reduction of uncertainty is central to achieving principled and efficient exploration. Compared to the posterior evolution of the PSRL algorithm corresponding to the optimal actions, i.e. the bottom row of curves in Figure 8, the expected value estimates of CDPO are closer to the ground-truth, and the variance is also smaller. Notably, the variance of CDPO might be higher for suboptimal actions, e.g., s = 8, a = left (the last image of the first row in Figure 7). It is due to the conservative nature of CDPO that it only cares about the expected value, instead of the value of a sampled (imperfect) model as in PSRL. In other words, as long as the uncertainty is large, the PSRL agents can take suboptimal actions to explore the uninformative regions, which causes the inefficient over-exploration issue.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3860/38609666-77bd-45b4-abe1-c96be30f6f5e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Posterior evolution of PSRL algorithm in the 8-Chain MDP.</div>\nCumulative Regret. We compare CDPO and previous algorithms on the N-Chain MDPs with various state sizes N by measuring the cumulative regret of an oracle agent following the optimal policy. The results are shown in Figure 9. To make the performances comparable on the same scale, we also provide the normalized regret in Figure 10. We observe that when the size of state space N is relatively smaller, e.g. N \u22645, CDPO, PSRL, BQL, and MM algorithms achieve sublinear regret. The performances of these algorithms are also comparable, showing the necessity of deep exploration. On the contrary, Q-Learning which only relies on dithering exploration mechanisms fail to find the optimal strategy. However, as N is increasing, where the exploration must be effective for the agent to continually explore despite receiving negative rewards, the CDPO agents offer significantly lower cumulative regret and faster convergence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bab4/bab4f02b-d3af-4958-8b11-b898c2ecf051.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Comparison of cumulative regret.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7aa1/7aa157d1-6c22-46c3-812d-a56f550c7cbc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: Performance comparison in terms of regret to the oracle.</div>\n# G Algorithmic Comparisons between MBRL Algorithm\nWe provide algorithmic comparisons of four MBRL frameworks, including greedy model exploitation algorithms, OFU-RL, PSRL, and the proposed CDPO algorithm.\nalgorithms, OFU-RL, PSRL, and the proposed CDPO algorithm. The differences mainly lie in the model selection and policy update procedures. The high-level pseudocode is given in Algorithm 5, 6, 7 and 8. Among them, the greedy model exploitation algorithm is a naive instantiation, where other instantiations can include the ones that augment Algorithm 5 with e.g., a dual framework that involves a locally accurate model and a supervised imitating procedure [59, 31]. In Algorithm 5, \ufffdft can either be a probabilistic model or a deterministic model (with additive noise), which can be estimated via Maximum Likelihood Estimation (MLE) or minimizing the Mean Squared Error (MSE), respectively.\nAlgorithm 5 Naive Greedy Model Exploitation\n1: for iteration t = 1, ..., T do\n2:\nEstimate model \ufffdft via MLE or MSE\n3:\nCompute \u03c0t = argmax\u03c0 V \ufffd\nft\n\u03c0\n4:\nExecute \u03c0t in the real MDP\n5:\nHt+1 = Ht \u222a{sh,t, ah,t, sh+1,t}h\n6: end for\n7: return policy \u03c0T\nAlgorithm 7 PSRL Algorithm\n1: for iteration t = 1, ..., T do\n2:\nSample ft \u223c\u03c6(\u00b7 | Ht)\n3:\nCompute \u03c0t = argmax\u03c0 V ft\n\u03c0\n4:\nExecute \u03c0t in the real MDP\n5:\nHt+1 = Ht \u222a{sh,t, ah,t, sh+1,t}h\n6: end for\n7: return policy \u03c0T\n# H Societal Impact\nFor real-world applications, interactions with the system imply energy or economic costs. With practical efficiency, CDPO reduces the training investment and is aligned with the principle of responsible AI. However, as an RL algorithm, CDPO is unavoidable to introduce safety concerns, e.g., self-driving cars make mistakes during RL training. Although CDPO does not explicitly address them, it may be used in conjunction with safety controllers to minimize negative impacts, while drawing on its powerful MBRL roots to enable efficient learning.\nAlgorithm 6 OFU-RL Algorithm\n1: for iteration t = 1, ..., T do\n2:\nConstruct confidence set Ft\n3:\nCompute \u03c0t = argmax\u03c0,f\u223cFt V ft\n\u03c0\n4:\nExecute \u03c0t in the real MDP\n5:\nHt+1 = Ht \u222a{sh,t, ah,t, sh+1,t}h\n6: end for\n7: return policy \u03c0T\nAlgorithm 8 CDPO Algorithm\n1: for iteration t = 1, ..., T do\n2:\nReferential Update qt following (4.1)\n3:\nConservative Update \u03c0t following (4.2)\n4:\nExecute \u03c0t in the real MDP\n5:\nHt+1 = Ht \u222a{sh,t, ah,t, sh+1,t}h\n6: end for\n7: return policy \u03c0T\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of over-exploration and aggressive policy updates in Model-Based Reinforcement Learning (MBRL), particularly when using nonlinear models. Previous methods like Posterior Sampling Reinforcement Learning (PSRL) have shown limitations due to large generalization errors, which necessitate a new approach to ensure stability and efficiency in exploration.",
        "problem": {
            "definition": "The problem is defined as the inefficiency of existing MBRL methods, particularly PSRL, which can lead to suboptimal policies due to aggressive updates and over-exploration when models are inaccurately estimated.",
            "key obstacle": "The main obstacle is the large epistemic uncertainty associated with nonlinear models, which causes the selected model to change dramatically between iterations, resulting in value degradation and uninformative exploration."
        },
        "idea": {
            "intuition": "The idea behind Conservative Dual Policy Optimization (CDPO) stems from the need to stabilize policy updates by separating the optimization process into two distinct updates, ensuring that exploration remains within a controlled range.",
            "opinion": "CDPO is proposed as a method that optimizes the policy iteratively through a Referential Update and a Conservative Update, aiming to achieve stability and efficiency without the harmful sampling procedures of PSRL.",
            "innovation": "The key innovation of CDPO lies in its dual update mechanism, which allows for both monotonic policy improvement and global optimality, contrasting with existing approaches that may sacrifice one for the other."
        },
        "method": {
            "method name": "Conservative Dual Policy Optimization",
            "method abbreviation": "CDPO",
            "method definition": "CDPO is defined as a model-based reinforcement learning algorithm that iteratively optimizes policy using a reference model and a conservative approach to exploration.",
            "method description": "CDPO operates by executing two types of updates iteratively: a Referential Update that stabilizes policy optimization and a Conservative Update that limits exploration to a reasonable range.",
            "method steps": [
                "1. Perform a Referential Update to compute an intermediate policy based on a reference model.",
                "2. Execute a Conservative Update to adjust the policy while maximizing the expected model value.",
                "3. Repeat the process iteratively to improve the policy."
            ],
            "principle": "The effectiveness of CDPO is grounded in its ability to maintain stability during policy updates while ensuring exploration remains bounded, thus preventing aggressive updates that could degrade performance."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted in various environments, including N-Chain MDPs and MuJoCo tasks, comparing CDPO against baseline methods like PSRL and other MBRL algorithms.",
            "evaluation method": "Performance was assessed by measuring cumulative regret and policy value improvement over multiple iterations across different tasks."
        },
        "conclusion": "CDPO demonstrates significant improvements in exploration efficiency and policy stability compared to existing methods, achieving global optimality while maintaining monotonic policy improvement.",
        "discussion": {
            "advantage": "CDPO's main advantages include reduced over-exploration, stable policy updates, and the ability to achieve global optimality without sacrificing exploration efficiency.",
            "limitation": "A limitation of the proposed method is its reliance on the choice of the reference model, which could impact performance if not selected appropriately.",
            "future work": "Future work should explore more sophisticated designs for the reference model and investigate various instantiations of the model-based policy optimization solver."
        },
        "other info": {
            "arxiv_id": "2209.07676v1",
            "authors": [
                {
                    "name": "Shenao Zhang",
                    "affiliation": "Georgia Institute of Technology",
                    "email": "shenao@gatech.edu"
                }
            ],
            "conference": "36th Conference on Neural Information Processing Systems (NeurIPS 2022)"
        }
    },
    "mount_outline": [
        {
            "section number": "5.3",
            "key information": "The effectiveness of Conservative Dual Policy Optimization (CDPO) is grounded in its ability to maintain stability during policy updates while ensuring exploration remains bounded, thus preventing aggressive updates that could degrade performance."
        },
        {
            "section number": "3.2",
            "key information": "The paper reviews existing methods like Posterior Sampling Reinforcement Learning (PSRL) and discusses their limitations due to large generalization errors, which necessitate a new approach to ensure stability and efficiency in exploration."
        },
        {
            "section number": "6.1",
            "key information": "CDPO is defined as a model-based reinforcement learning algorithm that iteratively optimizes policy using a reference model and a conservative approach to exploration."
        },
        {
            "section number": "8.1",
            "key information": "Future work should explore more sophisticated designs for the reference model and investigate various instantiations of the model-based policy optimization solver."
        },
        {
            "section number": "4.1",
            "key information": "CDPO aims to achieve stability and efficiency without the harmful sampling procedures of PSRL, highlighting the importance of ethical considerations in the development of AI algorithms."
        }
    ],
    "similarity_score": 0.5740082962219122,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Conservative Dual Policy Optimization for Efficient Model-Based Reinforcement Learning.json"
}