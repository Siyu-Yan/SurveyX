{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2111.02080",
    "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
    "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",
    "bib_name": "xie2022explanationincontextlearningimplicit",
    "md_text": "# An Explanation of In-context Learning as Implicit Bayesian Inference\nSang Michael Xie Stanford University xie@cs.stanford.edu\nPercy Liang Stanford University pliang@cs.stanford.edu\n# Abstract\nLarge language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning1. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.\n# 1 Introduction\nLarge language models (LMs) such as GPT-3 (Brown et al., 2020, Lieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021) are pretrained on massive text corpora to predict the next word given previous words. They demonstrate the surprising ability to do in-context learning, where an LM \u201clearns\u201d to do a task simply by conditioning on a prompt containing input-output pairs, achieving SOTA results on LAMBADA (Paperno et al., 2016) and TriviaQA (Joshi et al., 2017) tasks (18% and 3% over previous SOTA (Brown et al., 2020)). For example, consider the task of predicting nationalities from names. A prompt (Figure 1) is constructed by concatenating independent \u201ctraining\u201d examples (e.g., \u201cAlbert Einstein was German\u201d) followed by a \u201ctest example\u201d (\u201cMarie Curie was\u201d). Conditioning on this prompt, GPT-3 places the largest probability on the correct output p(\u201cPolish\u201d | \u201cAlbert Einstein was German \\n Mahatma Gandhi was Indian \\n Marie Curie was\u201d)\nAditi Raghunathan Stanford University aditir@stanford.edu\nTengyu Ma Stanford University tengyuma@cs.stanford.edu\n\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de08/de085847-337f-4cf5-a756-fc5f07982bb0.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\"></div>\nFigure 1: In-context learning can emerge from modeling long-range coherence in the pretraining data. During pretraining, the language model (LM) implicitly learns to infer a latent concept (e.g., wiki bios, which typically transition between name (Albert Einstein) \u2192nationality (German) \u2192 occupation (physicist) \u2192...) shared across sentences in a document. Although prompts are unnatural sequences that concatenate independent examples, in-context learning occurs if the LM can still infer the shared concept across examples to do the task (name \u2192nationality, which is part of wiki bios).\nby inferring the task from examples. Intruigingly, GPT-3 was not explicitly pretrained to learn from examples, and the distribution of prompts (which concatenate independent examples) is quite different from natural language. Our understanding of in-context learning is limited since (i) real pretraining data is messy and (ii) in-context learning has so far required large-scale datasets and models. In this paper, we introduce a simple pretraining distribution where in-context learning emerges. To generate a document, we first draw a latent concept \u03b8, which parameterizes the transitions of a Hidden Markov Model (HMM) (Baum and Petrie, 1966), then sample a sequence of tokens from the HMM (Figure 9). This latent variable structure is common in topic models such as LDA (Blei et al., 2003, Gruber et al., 2007). During pretraining, the LM must infer the latent concept across multiple sentences to generate coherent continuations. When conditioning on a prompt, in-context learning occurs when the LM also infers a shared prompt concept across examples to make a prediction. We assume the LM fits the pretraining distribution p exactly with enough data and expressivity, so that the question of in-context learning becomes characterizing the conditional distribution of completions given prompts p(output|prompt) under the pretraining distribution, where the prompt is generated from a different distribution pprompt. This conditional distribution, which is the posterior predictive distribution, marginalizes out the latent concepts:\np(output|prompt) = \ufffd concept p(output|concept, prompt)p(concept|prompt)d(concept). (1\n(1)\nIf p(concept|prompt) concentrates on the prompt concept with more examples, then the LM learns via marginalization by \u201cselecting\u201d the prompt concept. Thus, in-context learning can be viewed as the LM implicitly performing Bayesian inference. The main challenge is that prompts are sampled from a different distribution than the pretraining distribution. The canonical Bayesian asymptotic tool is the Bernstein-von Mises theorem (Gunst and Shcherbakova, 2008, Kleijn and van der Vaart, 2012, van der Vaart, 1998), which asserts (under regularity conditions) that the posterior distribution of a latent variable concentrates on the maximum likelihood estimate. However, Bernstein-von Mises typically assumes observations are independent and/or drawn from the same distribution as the model, both of which are not satisfied. We prove that despite the distribution mismatch, the asymptotic prediction error of in-context learning is optimal when the signal about the latent concept in each prompt example is larger than the error due to the distribution mismatch. Additionally, we prove that the in-context learning error decreases with the length of each example\u2014thus, information in the inputs, not just the input-output mapping, can be useful for in-context learning. As a companion to this theory, we created the Generative IN-Context learning dataset (GINC), which is a small-scale synthetic dataset for studying in-context learning. We find that both Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter and Schmidhuber, 1997) trained on GINC exhibit in-context learning. We verify intuitions from the theory, showing that the accuracy of incontext learning improves with the number of examples and example length. Ablations of the GINC dataset show that the latent concept structure in the pretraining distribution is crucial to the emergence of in-context learning. The experiments also bring up open questions which go beyond our theory, which only studies the pretraining distribution. We find that scaling up the number of model parameters steadily improves the in-context accuracy despite achieving the same pretraining loss, showing that larger models may improve in-context learning beyond increasing the capacity for memorizing the training data better. Previously observed in-context learning phenomena such as sensitivity to example ordering (Zhao et al., 2021) and the existence of settings where zero-shot is better than one/fewshot learning (Brown et al., 2020) are also mirrored in GINC.\n# 2 In-context learning setting\nPretraining distribution. In our framework, a latent concept \u03b8 from a family of concepts \u0398 defines a distribution over observed tokens o from a vocabulary O. To generate a document, we first sample a concept from a prior p(\u03b8) and then sample the document given the concept. Each pretraining document is a length T sequence: \ufffd\nWe assume p(o1, . . . , oT |\u03b8) is defined by a Hidden Markov Model (HMM). The concept \u03b8 determines he transition probability matrix of the HMM hidden states h1, . . . , hT from a hidden state set H.\nWe assume p(o1, . . . , oT |\u03b8) is defined by a Hidden Markov Model (HMM). The concept \u03b8 determines the transition probability matrix of the HMM hidden states h1, . . . , hT from a hidden state set H. Prompt distribution. The prompt distribution pprompt generates prompts for in-context learning. The prompt is a concatenation of n independent training examples and 1 test input xtest, which are all conditioned on a shared prompt concept \u03b8\u2217. The goal is to predict the test output ytest by predicting the next token. A prompt example is composed of an input token sequence x (e.g., Albert Einstein was) followed by an output token y (e.g., German). In particular, the i-th training example Oi consists of an input\nPrompt distribution. The prompt distribution pprompt generates prompts for in-context learning. The prompt is a concatenation of n independent training examples and 1 test input xtest, which are all conditioned on a shared prompt concept \u03b8\u2217. The goal is to predict the test output ytest by predicting the next token. A prompt example is composed of an input token sequence x (e.g., Albert Einstein was) followed by an output token y (e.g., German). In particular, the i-th training example Oi consists of an input\n(2)\n1. Generate a start hidden state h i from a prompt start distribution pprompt. 2. Given hstart i , generate the example sequence Oi = [xi, yi] from p(Oi|hstart i , \u03b8\u2217), the pretraining distribution conditioned on a prompt concept \u03b8\u2217.\nThe test input xtest = xn+1 is sampled similarly. Between each example, there is a special delim token odelim. The prompt consists of a sequence of training examples (Sn) followed by the  example xtest:\n[Sn, xtest] = [x1, y1, odelim, x2, y2, odelim, . . . , xn, yn, odelim, xtest] \u223cpprompt.\nMismatch between prompt and pretraining distributions. Since transitions between independent examples can be unnatural, the prompts are low probability sequences under the pretraining distribution. We provide a simple illustration using the names to nationalities example. Suppose that wiki bio documents in the pretraining data typically transition between name \u2192nationality \u2192occupation \u2192. . . . In the prompt, the examples transition between name \u2192nationality \u2192name \u2192nationality \u2192. . . , which contains low-probability transitions such as \u201cGerman\u201d \u2192\u201cMahatma Gandhi\u201d. The prompt formatting (e.g., choice of delimiter) can also be a source of mismatch. We aim to show that despite this mismatch, large LMs can infer the prompt concept from examples.\n\ufffd \ufffd where h test denotes the hidden state corresponding to the first token of xtest. We analyze the in-context predictor fn(xtest) = arg maxy p(y|Sn, xtest), which outputs the most likely prediction over the pretraining distribution conditioned on the prompt from the prompt distribution3. We study the in-context predictor and its expected 0-1 error with n examples L0-1(fn) = Extest,ytest\u223cpprompt[1[fn(xtest) \u0338= ytest]].\n# 2.1 Assumptions\nWe detail the assumptions in our framework, including the structure of delimiters and regularity assumptions. We first assume that there exists a subset of delimiter hidden states D which generate the special delimiter token odelim deterministically.\nAssumption 1 (Delimiter hidden states). Let the delimiter hidden states D be a subset of H. For an hdelim \u2208D and \u03b8 \u2208\u0398, p(odelim|hdelim, \u03b8) = 1 and for any h /\u2208D, p(odelim|h, \u03b8) = 0.\nThus, observing the delimiter odelim reveals that the corresponding hidden state is in D, but does not reveal which element of D it is. The delimiter is usually a token that can appear in a broad range of contexts (e.g., newline). The delimiter ideally does not distract from the examples \u2014 for example, an adversarial delimiter could look like part of the input x. To mitigate these scenarios we assume that no delimiter (e.g., newline) is significantly more likely under one concept rather\n(3)\n(4)\nAssumption 2 (Bound on delimiter transitions). For any delimiter state hdelim \u2208D and any hidden state h \u2208H, the probability of transitioning to a delimiter hidden state under \u03b8 is upper bounded p(hdelim|h, \u03b8) < c2 for any \u03b8 \u2208\u0398 \\ {\u03b8\u2217}, and is lower bounded p(hdelim|h, \u03b8\u2217) > c1 > 0 for \u03b8\u2217. Additionally, the start hidden state distribution for delimiter hidden states is bounded as p(hdelim|\u03b8) \u2208[c3, c4]. The choice of prompt start distribution can be a source of distribution shift which is separate from the distribution shift from concatenating independent examples. We make an assumption that limits how much distribution shift is introduced by the prompt start distribution. Assumption 3 (Distribution shift from prompt start distribution). We assume that the prompt start distribution pprompt is close in TV distance to all hidden transition distributions (under \u03b8\u2217) starting from a delimiter hidden state: maxhdelim\u2208D TV (pprompt(h)\u2225p(h|hdelim, \u03b8\u2217)) < \u2206/4. Here, \u2206= pprompt(ymax|xtest) \u2212 maxy\u0338=ymax pprompt(y|xtest) is the margin between the most likely label ymax = arg maxy pprompt(y|xtest) and the second most likely label. Note that even when the maximum TV distance is 0, there is still distribution shift from concatenating independent examples. We also assume the prompt concept \u03b8\u2217is in the family \u0398, which is a broad set of concepts. Assumption 4 (Well-specification). The prompt concept \u03b8\u2217is in \u0398.\nEven though the pretraining distribution is broad, the prompt is still low probability under the pretraining distribution since it concatenates independent examples. Finally, if the prompt has zero probability under the prompt concept \u03b8\u2217, then Bayesian inference will not be able to infer the prompt concept as in Section 3.1. The following are regularity assumptions which mainly ensure that the prompt is not zero probability under \u03b8\u2217.\nAssumption 5 (Regularity). The pretraining distribution p satisfies: 1) Lower bound on transition probability for the prompt concept \u03b8\u2217: for any pair of hidden states h, h\u2032 \u2208H, p(h|h\u2032, \u03b8\u2217) > c5 > 0. 2) Start hidden state is lower bounded: for any h \u2208H, p(h|\u03b8\u2217) \u2265c8 > 0. 3) All tokens can be emitted: for every symbol o, there is some hidden state h \u2208H such that p(o|h, \u03b8\u2217) > c6 > 0, 4) The prior p(\u03b8) has support over the entire concept family \u0398 and is bounded above everywhere.\n# 3 Theoretical analysis\nWe prove that in the limit of infinite examples, the error of the in-context predictor is optimal if a distinguishability condition holds \u2014 the prompt concept \u03b8\u2217is distinct enough from the other concepts in \u0398 (e.g., when \u0398 is a discrete set). When distinguishability does not hold (e.g, \u0398 is continuousvalued), we show that the expected error still decreases with the length of each example, showing that information in both the inputs and the input-output mapping contribute to in-context learning.\n# 3.1 High-level approach\nOur goal is to show that arg maxy p(y|Sn, xtest) \u2192arg maxy pprompt(y|xtest) as the number of exam ples n grows. In the following, assume that the prompt has non-zero probability under the pretrain ing distribution p given \u03b8\u2217, meaning that p(Sn, xtest|\u03b8\u2217) > 0. We expand p(y|Sn, xtest) to analyze its\n\u221d \ufffd p(y|Sn, xtest, \u03b8)p(Sn, xtest|\u03b8)p(\u03b8)d\u03b8 (Bayes\u2019 rule, drop the constant p(\nwhere rn(\u03b8) = 1 n log p(Sn,xtest|\u03b8) p(Sn,xtest|\u03b8\u2217). In Theorem 1, we prove that under a distinguishability condition, exp(n \u00b7 rn(\u03b8)) \u21920 for all concepts \u03b8 except the prompt concept \u03b8\u2217, where exp(n \u00b7 rn(\u03b8\u2217)) = 1. The only nonzero term in the integral is when \u03b8 = \u03b8\u2217, and thus the prompt concept is \u201cselected\u201d as a consequence of Bayesian inference4. Lemma 1 shows that the argmax after restricting to \u03b8\u2217is the same as the most likely label under pprompt(y|xtest) (using Assumption 3). Putting these together with Equation 6, the in-context predictor infers the prompt concept \u03b8\u2217:\n# 3.2 Heuristic derivation\nRecall from Section 3.1 that if exp(n \u00b7 rn(\u03b8)) \u21920 for all \u03b8 \u0338= \u03b8\u2217, then Bayesian inference \u201cselects\u201d the prompt concept through marginalization. To do this, we focus on showing that rn(\u03b8), the average log-likelihood ratio between \u03b8 and \u03b8\u2217, converges to a negative constant, and thus nrn goes to \u2212\u221e. The main technical challenge is to handle the sequence-of-examples structure of the prompt, which makes all the examples dependent with respect to the pretraining distribution. Our approach uses properties of delimiter tokens to approximately factorize the examples, with constant error per example. We let Oex i = [odelim i\u22121 , Oi] be the i-th input-output pair and the previous delimiter together for i > 1 and define Oex 1 = O1. Expanding the likelihood term inside rn(\u03b8), our goal is to show\n\ufffd how this, we expand p(Sn|\u03b8) with the chain rule, and with Assumption 5 (to bound p(xtest|Sn, \u03b8) ) it can be shown that\n\ufffd To show this, we expand p(Sn|\u03b8) with the chain rule, and with Assumpti by O(1)) it can be shown that\n\ufffd We then marginalize p(Oex i |Oex 1:i\u22121, \u03b8) over the hidden state hdelim i\u22121 corresponding to the delimiter in ex delim\n(5)\n(6)\n(7)\n(8)\n(9)\n(10)\nWhile summing over H above would be a trivial equality, we can replace H with the set of delimiter hidden states D since p(h|Oex 1:i\u22121, \u03b8) = 0 for non-delimiter hidden states h /\u2208D (Assumption 1). We used in the first equality that Oex 1:i\u22121 \u2192hdelim i\u22121 \u2192Oex i forms a Markov chain and p(odelim i\u22121 |hdelim i\u22121 ) = 1 (Assumption 1) to change Oex i to Oi. Finally, we can show using properties of delimiter hidden states (Assumption 2) that p(hdelim i\u22121 |Oex 1:i\u22121, \u03b8) = O(1) and \ufffd hdelim i\u22121 \u2208D p(Oi|hdelim i\u22121 , \u03b8) \u2248O(1)p(Oi|\u03b8) in the second step. Therefore, we can upper bound rn(\u03b8) as\n\ufffd The expectation term can be written as the difference of two KL divergences, KL(pprompt(O)\u2225p(O|\u03b8\u2217)) \u2212KL(pprompt(O)\u2225p(O|\u03b8)). We bound the first KL term by a constant using Assumption 5 \u2014 intuitively for one example, pprompt and p(\u00b7|\u03b8\u2217) are close. We break the second term into a sum of negative KL divergences over k tokens. There are O(k) KL terms and only O(1) other error terms, which come from the distribution mismatch between the prompt and pretraining distributions. If the KL terms are larger than the error terms, then rn(\u03b8) has a negative limit. If this holds for all \u03b8 \u0338= \u03b8\u2217, then we have exp(n \u00b7 rn(\u03b8)) \u21920 for all \u03b8 \u0338= \u03b8\u2217, enabling in-context learning.\n# 3.3 Formal results\n# 3.3.1 In-context learning under distinguishability\nWe define a distinguishability condition which formalizes when in-context learning occurs. Letting pj \u03b8(o) := p(O[j] = o|O[1 : j \u22121], \u03b8) be the output distribution of the j-th token given the previous tokens and pj prompt(o) := pprompt(O[j] = o|O[1 : j \u22121]) be the analogous distribution under the prompt distribution, the distinguishability condition depends on the KL divergence between pj prompt (which represents \u03b8\u2217) and pj \u03b8 as well as error terms \u03f5\u03b8 start and \u03f5\u03b8 delim coming from the distribution mismatch between the prompt and pretraining distributions at the start and delimiter token for each example:\n \u2212 \u2212 Condition 1 (Distinguishability). We define \u03b8\u2217to be distinguishable if for all \u03b8 \u2208\u0398, \u03b8 \u0338= \u03b8\u2217,\nWhen the signal from KL divergence (LHS) is larger than the error terms, Equation 14 is satisfied (Figure 2). For larger example lengths k, the LHS increases, improving distinguishability. Intuitively, larger example lengths increase the proportion of the prompt sampled from the pretraining distribution by providing more evidence for Bayesian inference. Under Condition 1, the in-context predictor asymptotically achieves the optimal expected error. Theorem 1. Assume the assumptions in Section 2.1 hold. If Condition 1 holds, then as n \u2192\u221ethe prediction according to the pretraining distribution is\nThus, the in-context predictor fn achieves the optimal 0-1 risk: limn\u2192\u221eL0-1(fn) = inff L0-1(f).\n(11)\n(14)\n(15)\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\"></div>\n\nFigure 2: When the signal about the prompt concept within each example (green) is greater than the error from low-probability transitions between examples, in-context learning succeeds in our latent concept setting (Theorem 1). Increasing the example length k increases the signal. The signal for in-context learning comes from tokens in both the inputs and the input-output mapping.\n# 3.3.2 Non-distinguishable case\nThe distinguishability condition (Condition 1) fails when there is some \u03b8 \u0338= \u03b8\u2217for which the KL divergence between \u03b8 and \u03b8\u2217is less than the error terms. However, this also means that the output distributions of \u03b8 and \u03b8\u2217are close in KL. We leverage this to prove that the expected 0-1 error decreases with the example length k under two different settings where distinguishability does not hold.\nContinuity. Our first result relies on a continuity assumption between the concept parameter and its corresponding output distribution. Our assumption is based on prior works (Kleijn and van der Vaart, 2012), where the KL divergence is assumed to have a 2nd-order Taylor expansion. Theorem 2. Let the set of \u03b8 which does not satisfy Equation 14 in Condition 1 to be B. Assume that KL divergences have a 2nd-order Taylor expansion around \u03b8\u2217:\nwhere Ij,\u03b8\u2217is the Fisher information matrix of the j-th token distribution with respect to \u03b8\u2217. Let \u03b3\u03b8\u2217= maxj \u03bbmax(Ij,\u03b8\u2217) min j\u03bbmin(Ij,\u03b8\u2217) where \u03bbmax, \u03bbmin return the largest and smallest eigenvalues. Then for k \u22652 and as n \u2192\u221e, the 0-1 risk of the in-context learning predictor fn is bounded as\nwhere g(\u03b4) = 1 2((1 \u2212\u03b4) log(1 \u2212\u03b4) + (1 + \u03b4) log(1 + \u03b4)) is a calibration function (Steinwart, 2007, \u00c1vila Pires and Szepesv\u00e1ri, 2016) for the multiclass logistic loss for \u03b4 \u2208[0, 1), assuming that the minimizers of th 0-1 risk and multiclass logistic risk are the same.\nwhere g(\u03b4) = 1 2((1 \u2212\u03b4) log(1 \u2212\u03b4) + (1 + \u03b4) log(1 + \u03b4)) is a calibration function (Steinwart, 2007, \u00c1vila Pires and Szepesv\u00e1ri, 2016) for the multiclass logistic loss for \u03b4 \u2208[0, 1), assuming that the minimizers of the 0-1 risk and multiclass logistic risk are the same. Since the inverse calibration function g\u22121 is roughly linear in \u03f5 for \u03f5 \u22640.7, the excess risk roughly decreases as O(1/k). When the \u201cworst-case condition number\u201d \u03b3\u03b8\u2217of the Fisher information matrices is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction to vary \u03b8\u2217in which the output distribution will sharply change. As a consequence, the concepts \u03b8 that are not distinguishable from the prompt concept \u03b8\u2217parameterize distributions that produce similar outputs to the prompt concept and thus achieve a small error.\nSince the inverse calibration function g\u22121 is roughly linear in \u03f5 for \u03f5 \u22640.7, the excess risk roughly decreases as O(1/k). When the \u201cworst-case condition number\u201d \u03b3\u03b8\u2217of the Fisher information matrices is smaller (well-conditioned), the error decreases. Intuitively, this means that there is no direction to vary \u03b8\u2217in which the output distribution will sharply change. As a consequence, the concepts \u03b8 that are not distinguishable from the prompt concept \u03b8\u2217parameterize distributions that produce similar outputs to the prompt concept and thus achieve a small error.\n(16)\n(17)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2f3f/2f3fb45e-bde8-4284-906a-103d199e8185.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: In-context accuracy (95% intervals) of Transformers (left) and LSTMs (right) on the GINC dataset. Accuracy increases with number of examples n and length of each example k.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ca2/2ca25266-ee0c-46da-a5c2-537f6f6f1ba7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Ablation studies for 4 layer Transformers on the GINC dataset with vocab size 50. (Left) When pretrained with only one concept, in-context learning fails. (Middle) When the pretraining data has random transitions, the model sees all token transitions but in-context learning fails. (Right) When prompts are from random unseen concepts, in-context learning fails to extrapolate.</div>\nInstead of measuring only the error at the k-th token, we average the prediction error on the 2nd to k-th tokens. However, we leave bridging the mismatch between training examples, which ar consistently length k, and test examples, which have random length, to future work.\n# 4 Simulations\nWe generate the GINC dataset and show that Transformers (Vaswani et al., 2017) and LSTMs (Hochreiter and Schmidhuber, 1997) trained on GINC exhibit in-context learning. In the theory, we assumed that the pretrained LM fits the pretraining distribution exactly. Here, we pretrain LMs to approximate the pretraining distribution, showing that the in-context learning properties of the pretraining distribution transfer to the LM.\n(18)\nGINC dataset. We construct the GINC dataset according to our theory (see Appendix F.1). For pretraining, we define a uniform mixture of HMMs over a family \u0398 of 5 concepts to generate 1000 pretraining documents with \u223c10 million tokens total. For prompting, we generate prompts with 0 to 64 training examples and example lengths k \u2208{3, 5, 8, 10} (2500 prompts for each setting). The target token ytest is taken to be the most likely output arg maxy pprompt(y|xtest) instead of sampling so that the intrinsic error is 0. Main result. We train GPT-2-based Transformers (Radford et al., 2019) and LSTMs on three versions of the GINC dataset with vocabulary sizes 50, 100, and 150, then evaluate the in-context accuracy (see Appendix F.2, F.3). We average all results over 5 pretraining runs. Figure 3 shows that for both Transformer and LSTMs, in-context accuracy improves as the number of prompt examples n and the example length k increase, verifying our theory. Ablations on the latent concept structure. We ablate the role of the mixture-of-concepts structure in GINC. In Figure 4 (left), we pretrain a 4 layer Transformer on data with only one concept (removing the prior) from \u0398, resulting in flat in-context learning curves. Figure 4 (middle) shows that pretraining on random pretraining data, which contains all possible token transitions, in-context learning also fails. Therefore, the mixture-of-concepts structure is important and simply seeing diverse token transitions does not enable in-context learning. Extrapolation to unseen concepts. Full generative control of GINC allows for experimentation with latent variables in the pretraining distribution. For example, in large-scale datasets, it is difficult to test whether a concept or task is in the pretraining data. We test this in GINC by testing the in-context accuracy of a 4 layer Transformer on prompts generated from 5 random concepts that are not in the pretraining family of concepts. Figure 4 (right) shows that in-context learning also fails for these novel concepts. Effect of model size and architecture. Figure 5 shows that increasing the size of the Transformer (4, 12, 16 layers) steadily increases the in-context accuracy, corroborating the results of Brown et al. (2020). Table 6 shows that even though larger Transformers may have the same pretraining loss (e.g., 12 and 16 layer Transformers both get 1.33 validation loss for vocab size 50), the in-context accuracy still improves (81% to 85% from 12 to 16 layers), suggesting that larger models can improve in-context learning beyond improving pretraining perplexity. This may be related to phenomena from overparameterization and overtraining (Power et al., 2021, Zhang et al., 2017). Finally, the model architecture also plays a role \u2014 LSTMs consistently outperform Transformers on GINC despite having fewer parameters, perhaps due to the similarity between HMMs and LSTMs. We leave analysis of the effect of model scaling and model architecture as open questions. Sensitivity to example ordering. In Figure 7 (left), we test the sensitivity of in-context accuracy on GINC to the ordering of the prompt examples, following Zhao et al. (2021). For this experiment, we consider prompts generated from a single concept and prompt start distribution. We sample 10 different sets (leading to 10 training set IDs) of 4 examples and generate all 24 possible permutations for each example set. We consider the in-context accuracy of the 4 layer Transformer trained on GINC with vocabulary size 50. Similarly to the behavior of GPT-3 (Zhao et al., 2021), there is a significant variation (10\u201340% difference) between permutations of the same set of examples. Zero-shot is sometimes better than few-shot. In some settings in GINC, we find that zero-shot performance can be better than few-shot performance. This mirrors GPT-3 on some datasets (e.g., LAMBADA, HellaSwag, PhysicalQA, RACE-m, CoQA/SAT analogies for smaller models (Brown\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6973/69738509-530a-4f39-a94f-993169aa58ec.png\" style=\"width: 50%;\"></div>\nModel\n# Params\nTrain loss\n(pretraining)\nVal loss\n(pretraining)\nIn-context Acc\nVocab size 50, k = 10, n = 64\nTransformer (4 layer)\n29M\n1.49\n1.50\n60.2 \u00b1 5.7\nTransformer (12 layer)\n85M\n1.31\n1.33\n81.2 \u00b1 7.1\nTransformer (16 layer)\n115M\n1.31\n1.33\n84.7 \u00b1 3.4\nLSTM\n28M\n1.31\n1.35\n95.8 \u00b1 1.11\nVocab size 100, k = 10, n = 64\nTransformer (4 layer)\n29M\n1.58\n1.59\n67.4 \u00b1 4.7\nTransformer (12 layer)\n85M\n1.40\n1.42\n84.6 \u00b1 3.0\nTransformer (16 layer)\n115M\n1.41\n1.43\n88.7 \u00b1 1.6\nLSTM\n28M\n1.43\n1.44\n95.8 \u00b1 1.54\nVocab size 150, k = 10, n = 64\nTransformer (4 layer)\n29M\n1.44\n1.45\n92.8 \u00b1 1.9\nTransformer (12 layer)\n85M\n1.27\n1.28\n98.4 \u00b1 0.4\nTransformer (16 layer)\n115M\n1.27\n1.28\n98.1 \u00b1 0.5\nLSTM\n28M\n1.26\n1.31\n99.2 \u00b1 1.06\nFigure 5: In-context accuracy (95% intervals) of Transformers improves as model size increases on the GINC dataset for vocabulary sizes 50, 100, and 150.\nFigure 6: In-context accuracies (95% intervals) on GINC with vocab sizes (50, 100, 150) for Transformers and LSTMs. Accuracy improves with scale even though the pretraining loss may be the same.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/96d6/96d684c0-7021-4e7b-91fb-09d75e22a46b.png\" style=\"width: 50%;\"></div>\net al., 2020)). This occurs especially when the transition probabilities in GINC are lower entropy (controlled via a temperature parameter). For this experiment, we consider GINC with transition matrix temperature parameter 0.01 (instead of 0.1), 12 concepts, and vocabulary size 100. Figure 7 (right) shows that here, few-shot accuracy is initially worse than zero-shot accuracy, but can recover with more examples. We hypothesize that the distracting prompt structure initially decreases the accuracy in this setting.\n# 5 Discussion and related work\nLearning via Bayesian inference and extrapolation. The canonical Bernstein-von Mises theorem (van der Vaart, 1998) does not apply for in-context learning since the prompt examples are not independent under the pretraining distribution. Gunst and Shcherbakova (2008) show a Bernsteinvon Mises-type result for observations from an HMM, but do not handle observations from a dif-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b0eb/b0ebf626-a18e-4a98-8f5b-3cc5d2915eb0.png\" style=\"width: 50%;\"></div>\nferent distribution. Future directions include more precise asymptotic results about the posterior distribution and results under misspecification/extrapolation (Kleijn and van der Vaart, 2012). A possible avenue for extrapolation to some types of unseen concepts is to factorize the latent concept into semantics and syntax. While the pretraining data may contain only some semantics-syntax pairs, the language model could generalize to unseen pairs if it learns generalizable syntactical operations such as copying or reordering. Topic models and HMMs. Topic models such as LDA (Blei et al., 2003) also have document-level latent variables, but learning is typically relies on algorithms such as EM (Dempster et al., 1977), variational inference (Jordan et al., 1999), or MCMC (Hastings, 1970, Metropolis et al., 1953). We focus on learning as a natural result of Bayesian inference without an explicit inference algorithm. Wei et al. (2021a) also use an HMM model in their pretraining analysis. However, they analyze how pre-trained representations learned with masked LMs (Clark et al., 2020, Devlin et al., 2019, Lewis et al., 2020, Liu et al., 2019) can improve optimization-based downstream learning (Lester et al., 2021, Li and Liang, 2021) rather than in-context learning. Bridging the mismatch between pretraining and prompting. Prior works support our theoretical intuitions that reducing the prompt distribution mismatch would improve in-context learning. Finetuning LMs on text with a prompting format improves its zero-shot performance (Sanh et al., 2021, Wei et al., 2021b) and optimizing prompt templates improves few-shot finetuning (Gao et al., 2021, Jiang et al., 2020, Schick and Sch\u00fctze, 2021, Shin et al., 2020). Holtzman et al. (2021), Zhao et al. (2021) improve in-context accuracy via calibration or renormalization, a form of adaptation to the prompt distribution.\nMeta-learning. Meta-learning methods can also train a sequence model to learn from exam ples (Ravi and Larochelle, 2017). However, meta-learning models are trained to learn, while in context learning emerges from LM pretraining.\nStudying large-scale phenomena at a small scale. We can study in-context learning, a large scale phenomenon, at a small scale in GINC because the complexity of the pretraining distribution (HMM hidden state size, number of latent concepts) is small, such that the data and models are relatively larger. Since GINC is synthetic, we can also control the latent data properties (e.g., unseen concepts) to make predictions about large LMs while working at a small scale.\n# 6 Conclusion\nWe cast in-context learning as implicit Bayesian inference, where the pretrained LM implicitly infers a concept when making a prediction. We show that in-context learning occurs when the pretraining distribution is a mixture of HMMs. Our work provides a first step towards understanding in-context learning, which we hope will provide insight for improving pretraining and prompting.\n# Acknowledgements\nWe thank Tianyi Zhang, Frieda Rong, Lisa Li, Colin Wei, Shibani Santurkar, Tri Dao, Ananya Kumar, and Shivam Garg for helpful discussions and feedback. SMX is supported by an NDSEG Fellowship. The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL at Stanford University. TM acknowledges support of Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, and JD.com. Toyota Research Institute provided funds to support this work.\nLeonard E Baum and Ted Petrie. Statistical inference for probabilistic functions of finite state markov chains. The annals of mathematical statistics, 37(6):1554\u20131563, 1966.\nD. Blei, Andrew Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine Learnin Research (JMLR), 3:993\u20131022, 2003.\n# A Framework details\nPrompt distribution details. For in-context learning, we sample a prompt from a new distribution pprompt, which consists of n independent training examples and 1 test example. We first sample n hidden segments H of length k by sampling the first element hstart = H[1] from a prompt start distribution pprompt. Then, we sample the rest of the segment Hseg = H[2 : k] from the hidden transition distribution of the pretraining distribution p corresponding to a particular concept \u03b8\u2217:\nTo end each example (except the test example), we sample n delimiters hdelim \u2208D from pdelim prompt:\nConditioned on hidden variables Hi and hdelim i , we sample the observed tokens Oi = [oi,1, . . . , oi,k] and odelim i respectively from the pre-training distribution:\nThe \u201cinput\u201d for each example is xi = Oi[1 : k \u22121] and the \u201coutput\u201d is yi = Oi[k]. Taking S to be the sequence of training examples (without the test example), the resulting prompt sequence is [Sn, xtest] = [O1, odelim 1 , . . . , On, odelim n , xtest] = [x1, y1, odelim 1 , x2, y2, odelim 2 , . . . , xn, yn, odelim n , xtest] \u223cppromp (24) where xtest = xn+1 = On+1[1 : k \u22121] is sampled via the same process but with k \u22121 elements.\n# B Propositions for Theorem 1\nThe following propositions, which lower bound the probability of a delimiter token and probability of an example under \u03b8\u2217, are direct corollaries of the assumptions. Proposition 1. For all i, we have p(hdelim i |O1, odelim 1 , . . . , Oi, \u03b8\u2217) > c1 and p(hdelim i |O1, odelim 1 , . . . , Oi, \u03b8) < c2. Proof. By Assumption 2,\nThe following propositions, which lower bound the probability of a delimiter token and probability of an example under \u03b8\u2217, are direct corollaries of the assumptions.\nSimilarly,\n(19) (20)\n(21)\n(22) (23)\n(25)\n(26)\n(27)\n(28)\nfor some Hi. We have\np(Hi|hstart i , hj,l, \u03b8\u2217) = p(hj,l|H, hstart i , \u03b8\u2217)p(H|hstart i , \u03b8\u2217) p(hj,l|hstart i , \u03b8\u2217) > c2 5\nwhich lower bounds the terms in the numerator by c5 (marginalizing over previous hidden states), and upper bounding the denominator by 1. Setting c7 = (c6)kc2 5 finishes the proof.\n# C Convergence of the in-context predictor\nUnder Assumption 3, we show that the in-context predictor fn(xtest) = arg maxy p(y|Sn, xtest) converges when abstracting away the Bayesian inference component (the selection of \u03b8\u2217from \u0398) of the in-context predictor. We will complete the argument for the convergence of the in-context predictor in the proof of Theorem 1. Lemma 1. Suppose the prompt Sn and the test input xtest are given. Under Assumption 3, we show that the argmax of the averaged predictive distribution conditioned on\u2217and a prompt is the same as the argmax\nUnder Assumption 3, we show that the in-context predictor fn(xtest) = arg maxy p(y|Sn, xtest) converges when abstracting away the Bayesian inference component (the selection of \u03b8\u2217from \u0398) of the in-context predictor. We will complete the argument for the convergence of the in-context predictor in the proof of Theorem 1.\nLemma 1. Suppose the prompt Sn and the test input xtest are given. Under Assumption 3, we show that the argmax of the averaged predictive distribution conditioned on \u03b8\u2217and a prompt Sn is the same as the argmax of the prompt predictive distribution:\nProof. First, we note by definition that\nExpanding the last term, we have\n  which is proportional to a constant in xtest. On the other hand, analyzing one term inside the LHS of the lemma statement, we have\n  which is proportional to a constant in xtest and Sn. The quantities differ in the last term, which we expand below and put in matrix form. Let T \u2208R|H|\u00d7|D| be the matrix that represents the transition probabilities starting from a delimiter state: p(hstart test |hdelim) for hstart test \u2208H and hdelim \u2208D. As a result,\nwhere hdelim n is the delimiter hidden state before hstart test .\n(29)\n(30)\n(31)\n(32)\n(33)\n(34)\n(35)\n(36)\nLet W \u2208R|Y|\u00d7|H| be the matrix that represents the probabilities p(y|xtest, hstart test , \u03b8\u2217)p(xtest|hstart test , \u03b8\u2217) for all the possible y \u2208Y and hstart test \u2208H. Overall, we can write\nwhere u \u2208R|H| is the vector of probabilities that corresponds to the prompt start distribution pprompt. Bounding the difference between the two predictive distributions,\n# Using Assumption 3, we can further bound this by \u2206/2:\nSince the probability of any output does not change by more than \u2206/2 and the margin between the most likely label and the second most likely label is \u2206, the argmax\u2019s are the same, showing the result.\n(37)\n(37) (38)\n(39)\n(40)\n(41)\n(42)\n(43)\n(44)\n(45)\n(47)\n# D Proof of Theorem 1\nProof. We analyze the most likely prediction over the pretraining distribution conditioned on the prompt arg maxy p(y|Sn, xtest). \ufffd\nDefining the following quantity,\n| we will show that under distinguishability for all \u03b8 \u0338= \u03b8\u2217, rn(\u03b8) converges to a negative constant such that\n| for \u03b8 \u0338= \u03b8\u2217, whereas this ratio is always 1 for \u03b8 = \u03b8\u2217. This will then \u201cselect\u201d the desired promp concept through marginalization. Supposing that Equation 53 holds, we show that the theorem statement holds. Let\nand let \u03f5 < (\u2206/2 \u2212\u2206\u2032)p(\u03b8\u2217). Then for n large enough (due to Equation 53),\nwhere \u03f5\u03b8(y) \u2264\u03f5/2 for all y \u2208Y. By Lemma 1, the argmax of the first term of Equation 57 is the same as arg maxy pprompt(y|xtes where the margin between the most likely label and the second most likely is at least \u2206/2 \u2212\u2206 Since \ufffd\n \u2264 \u2208Y By Lemma 1, the argmax of the first term of Equation 57 is the same as arg maxy pprompt(y|xtest), where the margin between the most likely label and the second most likely is at least \u2206/2 \u2212\u2206\u2032.\nfor all y \u2208Y, the argmax of Equation 57 is also the same as arg max pprompt(y|xtest\n(48)\n(50)\n(51)\n(52)\n(53)\n(54)\n(55)\n(57)\n(58)\nNow it remains to show that rn(\u03b8) converges to a negative constant for \u03b8 \u0338= \u03b8\u2217. Let Oex i = [odelim i\u22121 , Oi] be the i-th observation segment and the previous delimiter together for i > 1 and define Oex 1 = O1. Expanding the numerator of the ratio in rn(\u03b8), we have\n\ufffd hdelim n \u2208D p(odelim n |hdelim n )p(hdelim n |Oex 1:n, \u03b8) n \ufffd i=1 \ufffd hdelim i\u22121 \u2208D p(Oi|hdelim i\u22121 , \u03b8)p(hdelim i\u22121 |Oex 1:i\u22121, \u03b8)\nNote that in the last line, the inner sum is over the set of delimiter states D by using the assumption that observing a delimiter odelim implies that the corresponding hidden state hdelim must be in D. We also see that \ufffd hdelim n p(hdelim n |Oex 1:n, \u03b8) = 1. We restrict our attention to \u03b8 where p(Sn, xtest|\u03b8) > 0, since otherwise \u03b8 does not affect the prediction. Expanding rn(\u03b8), we have the following upper bound:\n\ufffd In the above steps, we used both Propositions 1 and 2 in the terms involving c2, c1 (bounding the probability of hdelim hidden states) and c7 (bounding the probability of xtest). Note that in the second line, the sum can must be over the set of delimiter states D by using the assumption that observing a delimiter odelim implies that the corresponding hidden state hdelim must be in D.\n(59)\n(61)\n(64)\n(65)\n(66)\n(69)\nFocusing on the numerator of the ratio term and summing over the start hidden example, \ufffd \ufffd \ufffd\n  where the last step applies Bayes\u2019 rule. We can lower and upper bound the following quant any \u03b8 using Assumption 2:\n  re the last step applies Bayes\u2019 rule. We can lower and upper bound the following quantity for  using Assumption 2:\nThis implies that\nPlugging in these bounds, we have \ufffd\nwhere we set\n\u03f5\u03b8 delim = 2(log(c2) \u2212log(c1)) + log(c4) \u2212log(c3). Next, we convert the expectation in the bound into a KL divergence. We have \ufffd \ufffd \ufffd\n(70)\n(71)\n(72)\n(73)\n(74)\n(75)\n(76)\n(77)\n(81)\n(82)\n(83)\n\ufffd \ufffd which differ in only the hidden start distribution. Using Assumption 5, we have that p(h|\u03b8\u2217) \u2265c8 for any h \u2208H, which implies that\n\u2225\u00b7| \u2264\u2212 This term is non-negative since c8 \u22641. Aiming to decompose the second KL term into a sum over the k tokens, we write pj \u03b8(o) = p(O[j] = o|O[1 : j \u22121], \u03b8) and pj prompt(o) = pprompt(O[j] = o|O[1 : j \u22121]). We have\nThen we have that\n\ufffd The second term (set \u03f5\u03b8 start = log( 1 c8 )) is an error term that depends on how different the starting prompt distribution pprompt (which is part of pprompt) is to the pretraining distribution. The third term is an error term that comes from the delimiter transitions. The bound is negative when the sum of KL terms is larger in magnitude than the error terms. Note that as k becomes larger, the number of observations of \u03b8\u2217\u201coverpowers\u201d the distracting transitions in the prompt distribution This condition is equivalent to the disinguishability condition (Condition 1). By assumption, for \u03b8 \u0338= \u03b8\u2217the Condition 1 holds, and thus\n\u2192\u221e | \u2192\u221e  negative, constant limit. Note that exp(n \u00b7 rn(\u03b8\u2217)) = 1 for \u03b8\u2217\n(85)\n(86)\n(87)\n(88)\n(89)\n(90)\n(91)\n(92)\n(93)\n(94)\n\n# E Non-distinguishable case\nWhen Condition 1 is unsatisfied, Equation 14), gives an upper bound on the sum of KL divergences for the next token distributions given different-length histories. In contrast, the in-context task only measures the accuracy of the last (k-th) token. The main challenge is to relate the different-length histories to each other to give a more precise bound for the error on the in-context task (last token). Before addressing this challenge, we give the following lemma, which leverages the result of Steinwart (2007), \u00c1vila Pires and Szepesv\u00e1ri (2016) to relate a bound on the KL divergence to 0-1 loss. Lemma 2. Let the set of \u03b8 which does not satisfy Condition 1 to be B. Assume that KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8) is bounded above for all \u03b8 and that \u03b8\u2217minimizes the multiclass logistic risk LCE(\u03b8) = \u2212Extest\u223cpprompt[pprompt(ytest|xtest) log p(ytest|xtest, \u03b8)]. If Extest\u223cpprompt[KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8))] \u2264\u03f5\u03b8 for all \u03b8 \u2208B, (95)\nWhen Condition 1 is unsatisfied, Equation 14), gives an upper bound on the sum of KL divergences for the next token distributions given different-length histories. In contrast, the in-context task only measures the accuracy of the last (k-th) token. The main challenge is to relate the different-length histories to each other to give a more precise bound for the error on the in-context task (last token). Before addressing this challenge, we give the following lemma, which leverages the result of Steinwart (2007), \u00c1vila Pires and Szepesv\u00e1ri (2016) to relate a bound on the KL divergence to 0-1 loss. Lemma 2. Let the set of \u03b8 which does not satisfy Condition 1 to be B. Assume that\nthen\nwhere\nwhere in the last step we use that since the output space of fn is discrete and the probabilities that the in-context predictor takes an argmax over converges, then for N large enough, fN(xtest) = limn\u2192\u221efn(xtest). Note that for every input xtest, the limiting in-context learning predictor outputs the argmax of a predictive distribution which can be a mixture of predictive distributions over B:\nfor some distribution q over B. The KL divergence between this mixture and the prompt concept is bounded by the KL divergence of any one \u03b8 \u2208B, due to the convexity of KL:\nwhere we can exchange the order of expectations since the KL is bounded (dominated conver gence).\n(95)\n(96)\n(97)\n(98)\n(101)\n(102) (103) (104) (105)\nExtest\u223cpprompt[KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8))] = LCE(\u03b8) \u2212LCE(\u03b8\u2217) \u2264sup \u2208B \u03f5\u03b8\nwhere LCE(\u03b8) = \u2212Extest\u223cpprompt[pprompt(ytest|xtest) log p(ytest|xtest, \u03b8)] is the multiclass logistic risk, and LCE(\u03b8\u2217) is the optimal risk over \u03b8 \u2208\u0398 by assumption. Applying Theorem 2.2 and 5.11 of \u00c1vila Pires and Szepesv\u00e1ri (2016), g is a calibration function for the multiclass logistic loss, and allows us to convert the surrogate risk bound to a bound on the 0-1 loss, giving the result. Note that we have zero approximation error here, since \u03b8\u2217\u2208\u0398.\n# Note that g\u22121 is roughly linear in \u03f5 for \u03f5 smaller than 0.7, where the bound is non-vacuous.\n# E.1 Proof of Theorem 2\nuse this to bound the last KL term by plugging it in below:\nExtest\u223cpprompt[KL(pprompt(ytest|xtest)\u2225p(ytest|xtest, \u03b8))] \u2264(\u03f5\u03b8 start + \u03f5\u03b8 delim)(maxj \u03bbmax(Ij,\u03b8\u2217) + O(1)) (k \u22121) minj \u03bbmin(Ij,\u03b8\u2217) (\nPlugging into Lemma 2 gives the result.\n# E.2 Proof of Theorem 3\nNote that Condition 1 ensures that the sum of KL divergences between positions within a k-length input is bounded. This means that we have a bound over not only the last-position KL divergence, but also for all the intermediate tokens. Intuitively, the random length test example allows the incontext predictor to \u201ctake credit\u201d for fitting the intermediate tokens. The proof is immediate given the KL bound and Lemma 2, given that the length of xtest is uniformly random between 2 to k.\n(106)\n(107)\n(108)\n(109)\n(110)\n(111)\n(112)\n\nProof. Let the set of \u03b8 that does not satisfy Condition 1 to be B. We have for any \u03b8 in B that\neorem 1 and Condition 1. Plugging this into Lemma 2 gives \n# F Experimental details\n# F.1 GINC dataset\nPretraining distribution. We consider a pretraining distribution from a mixture of HMMs with an interpretable hidden state structure and emission distribution. The HMM hidden state ht = [st, vt] at time t is composed of an entity vt \u2208{1, . . . , |V|} (e.g., Einstein) and a property st \u2208{1, . . . , |S|} (e.g., nationality, first name, last name, other grammatical tokens). We model the entities and properties as independent Markov chains (i.e., a factorial HMM (Ghahramani and Jordan, 1997)), while the emissions depend on both. In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while and the properties of the entity (e.g., their nationality) change quickly with some pattern to generate natural sentences. We implement this by ensuring that the probability of transitioning to the same entity index in the next step is at least 0.9. The emission distribution depends on a memory matrix M with |V| rows and |S| columns (Figure 9). At step t we use the entity vt and property st to index into the memory matrix. In particular, the observed tokens are deterministic with p(ot|ht) = 1 if ot = M[vt, st]. This construction satisfies the structure on delimiter states (Assumption 1). We ensure that all the transitions have nonzero probability and use a uniform prior over concepts, satisfying Assumptions 2 and 5.\n(114)\n(116)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bba5/bba588a3-91f2-431f-90b1-0757869a938a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 9: The GINC dataset generates sequences from a mixture of HMMs. The HMM hidden states consist of entities (v) and properties (s), which index into a memory matrix to produce the observed token. The entity and property sequences are sampled from independent Markov chains. The concept parameter \u03b8 is the transition matrix for properties, which defines relations between properties. In this example, the sequence of properties [2,3,5,4] relates names to nationalities, defining the in-context task. The blue color represents hidden states/observations sampled from the prompt distribution, and the purple color represents hidden states/observations sampled from the pretraining distribution.\nConcept parameter. The concept parameter is the property transition matrix, while the entity transition matrix is fixed for all concepts. The prompt start distribution and the concept together determine the in-context task. We define a uniform mixture of HMMs over a family \u0398 of 5 concepts to generate 1000 documents with \u223c10 million tokens total.\nTransition matrix for properties. We generate 5 property transition matrices, one for each com ponent of the HMM mixture. We generate each transition matrix via a convex combination of 10 random permutation matrices. The weights of the convex combination are randomly generated a\n(117)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/19c2/19c2f491-e47b-4185-a040-5ec514ed2bca.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 10: In-context accuracy curve of the 4 layer Transformer on the GINC dataset when the entity transition matrix does not have an additional identity component, for vocabulary sizes 50 (left), 100 (middle), and 150 (right). In-context learning is still generally successful.</div>\nTransition matrix for entities. The entity transition matrix is shared between all the HMMs that consistute the mixture. The entity transition matrix is generated in the same way as the property transition matrices, except with one additional step. Letting T be a transition matrix sampled in the same way as a property transition matrix, In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while and the properties of the entity (e.g., their occupation) change quickly with some pattern to generate natural sentences. We implement this by ensuring that the probability of transitioning to the same entity index in the next step is at least 0.9. The final entity transition matrix is then 0.1T +0.9I where I is the identity matrix. Although we add the diagonal component for added realism, we also consider not adding this component. Figure 10 shows in-context learning curves for a small (4 layer) Transformer trained on data that does not add the diagonal component (we check this for vocabulary sizes 50, 100, and 150). In-context learning still works in this case, although not as well for the 50 vocab size case. Start distribution. The starting distribution for the hidden states in all HMMs in the mixture are close to uniform. We generate the start distribution as softmax((u \u22120.5)/t) for random vector u with entries uniformly from [0, 1] and temperature t = 10. In the pretraining documents, we only sample from the start distribution in the beginning of the document. Prompt distribution. We generate prompts with 0 to 64 training examples and example lengths k \u2208{3, 5, 8, 10} (2500 prompts for each setting). The target token ytest is taken to be the most likely output arg maxy pprompt(y|xtest) instead of sampling so that the intrinsic error is 0. Prompt distribution. To generate the prompts, we first sample a concept \u03b8 uniformly at random from \u0398 (well-specification, Assumption 4), then use it to generate all the prompt examples. The prompt start distribution is chosen to be uniform over entities but with a fixed starting property that is chosen randomly for each prompt, for consistency in the task. This may not satisfy Assumption 3, but we found this to still work empirically and is simpler. Given the starting property, we sample k tokens from the HMM defined by the concept \u03b8. Finally, we append the delimiter token for the example. We repeat this process for each example in the prompt, concatenating all examples. The label is generated as\nTransition matrix for entities. The entity transition matrix is shared between all the HMMs that consistute the mixture. The entity transition matrix is generated in the same way as the property transition matrices, except with one additional step. Letting T be a transition matrix sampled in the same way as a property transition matrix, In pretraining documents, we expect that the entities (e.g., Einstein) change slowly over time while and the properties of the entity (e.g., their occupation) change quickly with some pattern to generate natural sentences. We implement this by ensuring that the probability of transitioning to the same entity index in the next step is at least 0.9. The final entity transition matrix is then 0.1T +0.9I where I is the identity matrix. Although we add the diagonal component for added realism, we also consider not adding this component. Figure 10 shows in-context learning curves for a small (4 layer) Transformer trained on data that does not add the diagonal component (we check this for vocabulary sizes 50, 100, and 150). In-context learning still works in this case, although not as well for the 50 vocab size case. Start distribution. The starting distribution for the hidden states in all HMMs in the mixture are close to uniform. We generate the start distribution as softmax((u \u22120.5)/t) for random vector u with entries uniformly from [0, 1] and temperature t = 10. In the pretraining documents, we only sample from the start distribution in the beginning of the document.\nPrompt distribution. We generate prompts with 0 to 64 training examples and example lengths k \u2208{3, 5, 8, 10} (2500 prompts for each setting). The target token ytest is taken to be the most likely output arg maxy pprompt(y|xtest) instead of sampling so that the intrinsic error is 0.\nPrompt distribution. To generate the prompts, we first sample a concept \u03b8 uniformly at random from \u0398 (well-specification, Assumption 4), then use it to generate all the prompt examples. The prompt start distribution is chosen to be uniform over entities but with a fixed starting property that is chosen randomly for each prompt, for consistency in the task. This may not satisfy Assumption 3, but we found this to still work empirically and is simpler. Given the starting property, we sample k tokens from the HMM defined by the concept \u03b8. Finally, we append the delimiter token for the example. We repeat this process for each example in the prompt, concatenating all examples. The label is generated as\nunder the prompt concept \u03b8\u2217. This differs from the theory, which samples ytest instead of taking it to be the most likely token. However, there can be a large amount of intrinsic error that sampling\n(118)\nExample of prompt generation. In the example in Figure 8 (right), the starting property is fixed to be 5 (for example). The first token (l) is generated by sampling a random entity index (3), and indexing into the memory matrix returns l. Running the hidden state chain of the HMM forward gives the next pair of property and entity. Since the entity Markov chain changes slowly, the entity is still 3 in the next step \u2013 however, the property has changed to 4, and indexing into the memory matrix outputs the next token (aw). Following this same process to generate the third token (the output for the first example), we finish generating one example. To end the example, we append a delimiter (backslash). We repeat this example generation process for all the examples, except for the test example at the end, where we do not generate the last token. We condition the HMM on the generated prompt to compute the posterior distribution over the next token pprompt(y|xtest). We take the argmax of this distribution to be the ground truth label. Dataset details. The dataset contains 1000 training documents and 100 validation documents, where training documents have 10240 tokens and validation documents have 1024 tokens. Each document is generated by first selecting one of the HMMs from the mixture uniformly at random, then generating 10240 tokens from the HMM. We also generate 2500 in-context prompts for each (example length,number of examples) pair, for example lengths k = [3, 5, 8, 10] and number of examples n = [0, 1, 2, 4, 8, 16, 32, 64]. Each prompt is generated using a random HMM in the mixture.\n# F.2 Transformer details\nOur Transformer models are based on the GPT-2 architectures with 4, 12, and 16 layers respectively, with 12 attention heads, 768 dimensional embeddings, residual/embedding/attention dropout set to 0.1, and a context window of 1024. Other than the number of layers, the other parameters are the default settings from the HuggingFace library (Wolf et al., 2019). We train for 5 epochs using the AdamW optimizer (Kingma and Ba, 2015, Loshchilov and Hutter, 2019) with a batch size of 8 and a linear learning rate schedule (with 1000 step warmup) up to a learning rate of 8e-4 for the 4 layer and 12 layer model, while for the 16 layer model we start with a constant learning rate of 8e-4 and reduce by a factor of 0.25 whenever the best validation loss does not improve. We tried both learning rate strategies for all models and take the most consistent. We tuned these models so that the training loss curves between seeds have smaller variability between the runs in terms of the curve shape and when the loss decreases \u2013 we found that this is an important indication of stable results. The models took 50 minutes, 2 hours, 3 hours to train respectively. The hardware was mainly Titan Xp GPUs, trained and evaluated using 16-bit precision. All the results are reported with 5 pretraining runs (5 different seeds).\n# F.3 LSTM details\nWe train an LSTM language model with embedding size 768, hidden layer size 768, and 6 layers. We use dropout 0.2 and weight decay 1e-5. The optimizer is AdamW starting with a learning rate of 1e-3, then reducing by a factor of 0.25 whenever the best validation loss does not go down. We train for a total of 10 epochs, with gradient clipping at norm 1.0. We use a batch size of 8 and backpropagate through time for 1024 steps (each pretraining data segment is also 1024 tokens). Each model takes roughly 2 hours to train on Titan Xp GPUs.\n# F.4 Varying the vocabulary size\nTo do well on the in-context learning task, the model must both infer the prompt concept and the last HMM hidden state. In general, increasing the number of observable symbols makes the incontext task easier by making the inference of the HMM hidden state easier. With more symbols, each hidden state is more likely to output a different symbol, making the inference problem easier. This improvement comes despite the number of output classes in the problem (same as the vocabulary size) increasing. Figures 11, 12, 13, 14 show in-context learning curves for vocabulary sizes 50, 100, and 150, keeping other hyperparmeters of the dataset the same.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/be1b/be1bf4ec-5b6f-42de-8119-18108c2dea10.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ure 11: In-context accuracy of the 4 layer Transformer on the GINC dataset for vocabulary sizes left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size inases.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ff6e/ff6e56c3-ca0e-43e4-a5d5-1f875d09a425.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 12: In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.</div>\n<div style=\"text-align: center;\">Figure 12: In-context accuracy of the 12 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b8eb/b8ebe483-8419-4dbd-898c-0b184d37c71e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 13: In-context accuracy of the 16 layer Transformer on the GINC dataset for vocabulary sizes 50 (left), 100 (middle) and 150 (right). Accuracies generally improve as the vocabulary size increases.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe99/fe99b1e5-010a-4559-b5b7-f843329de8ef.png\" style=\"width: 50%;\"></div>\nPrompt example length\nTest Acc (200\u2013300 chars)\n5 examples\nShort (200\u2013300 chars)\n69.8\nLong (500\u2013600 chars)\n70.7\n10 examples\nShort, duplicated examples\n69.6\nShort, independent examples\n71.4\nTable 1: Accuracies for 5-shot in-context learning of GPT-3 on a filtered LAMBADA test set with short examples (200\u2013300 characters). Even though there is distribution mismatch with the test set, having longer examples improves the accuracy, supporting theoretical intuitions. The first two rows use 5 training examples in the prompt, while the last two rows use 10 training examples to equalize the total length.\n# F.5 Experiment on GPT-3\nWe conduct an additional experiment which shows that longer examples improve in-context lear ing in GPT-3 on the LAMBADA (Paperno et al., 2016) completion task.\nData. In this experiment, we define a short version of the LAMBADA test dataset (LAMBADA test-short) which contains only test examples with up to 200\u2013300 characters in length. We also define two \u201ctraining\u201d datasets from which to sample examples for the in-context prompts from. The short training dataset (LAMBADA train-short) contains examples from the training set that are 200\u2013300 characters in length, which matches the distribution of test-short. The long training dataset (LAMBADA train-long) contains training examples that are 500\u2013600 characters long. We cut the number of examples in the larger of the two training datasets so that the two training datasets are equally sized (47 examples). For each test example, we sample 5 random training examples (5-shot learning). We also consider equalizing the total length of the prompts in two ways. First, we consider duplicating the 5 short examples (if the examples are [1,2,3,4,5], duplicating refers to [1,2,3,4,5,1,2,3,4,5]). This allows for equalizing the total length without increasing the number of examples. As a skyline comparison, we also consider sampling 10 independent short examples, which contains more input-output pairs for the task.\nResult. Table 1 shows that when evaluating only on LAMBADA test-short, 5-shot in-context learn ing using LAMBADA train-long improves the test accuracy by almost 1% compared to LAMBADA\ntrain-short, despite the long/short distribution mismatch between train and test. This supports intuitions from our theory. In comparison, simply increasing the total prompt length by duplicating the short examples does not improve the accuracy. Intuitively, the longer examples have additional information that is not directly related to mapping between the input and output, but can be leveraged to improve incontext learning by helping the model infer the latent concept. Using 5 long examples (as opposed to 5 short examples) closes about 56% of the gap between using 5 short examples and 10 independent short examples despite not adding additional examples or task-related information.\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of in-context learning in large language models (LMs) like GPT-3, which can perform tasks by conditioning on prompts of input-output examples without explicit pretraining for these tasks. The phenomenon of in-context learning raises questions about its underlying mechanisms and the role of document-level coherence during pretraining.",
        "problem": {
            "definition": "The primary problem is understanding how in-context learning is possible in LMs, particularly when pretraining documents exhibit long-range coherence and how this affects the model's ability to infer a shared latent concept from prompts.",
            "key obstacle": "A significant challenge is that the distribution of prompts, which concatenate independent examples, differs from the natural language distribution used during pretraining, complicating the inference process."
        },
        "idea": {
            "intuition": "The idea was inspired by the need to understand how LMs can generalize from examples presented in prompts, despite the challenges posed by the distribution mismatch.",
            "opinion": "The authors propose that in-context learning can be viewed as an implicit form of Bayesian inference, where the LM infers a latent concept shared across examples in a prompt.",
            "innovation": "The main innovation of this work is the introduction of a simple pretraining distribution modeled as a mixture of Hidden Markov Models (HMMs) that facilitates the emergence of in-context learning, contrasting with the messy large-scale datasets typically used."
        },
        "Theory": {
            "perspective": "The theoretical perspective revolves around treating in-context learning as Bayesian inference, where the LM infers latent concepts from examples to predict outputs.",
            "opinion": "The authors assume that with enough data and model capacity, LMs can fit the pretraining distribution and effectively perform inference on prompts despite distributional differences.",
            "proof": "The authors provide a proof showing that even with distribution mismatches, the in-context learning error decreases as the length of examples increases, suggesting that the information in the inputs contributes to learning."
        },
        "experiments": {
            "evaluation setting": "The experiments utilize a synthetic dataset called Generative IN-Context learning dataset (GINC), comprising 1000 pretraining documents generated from a mixture of HMMs and various prompting configurations (0 to 64 examples with lengths of 3, 5, 8, or 10 tokens).",
            "evaluation method": "The evaluation involves measuring the in-context accuracy of Transformers and LSTMs trained on the GINC dataset, analyzing the impact of example length and number on learning performance."
        },
        "conclusion": "The study concludes that in-context learning can be effectively modeled as implicit Bayesian inference, facilitated by coherent pretraining distributions. The findings suggest that improvements in model scaling and example structuring can enhance in-context learning capabilities.",
        "discussion": {
            "advantage": "The paper provides a clear theoretical framework for understanding in-context learning, supported by empirical results that demonstrate the effectiveness of the proposed approach.",
            "limitation": "A limitation is that the theory primarily focuses on the pretraining distribution and does not fully account for the complexities of real-world data distributions and their impact on learning.",
            "future work": "Future work could explore refining the theoretical framework to incorporate more complex distributions and further investigate the effects of example ordering and prompt design on in-context learning performance."
        },
        "other info": [
            {
                "info1": "The authors acknowledge contributions from various individuals for discussions and feedback.",
                "info2": {
                    "info2.1": "SMX is supported by an NDSEG Fellowship.",
                    "info2.2": "The work is partially supported by an Open Philanthropy Project Award, SDSI, and SAIL at Stanford University."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of in-context learning in large language models (LMs) like GPT-3, which can perform tasks by conditioning on prompts of input-output examples without explicit pretraining for these tasks."
        },
        {
            "section number": "1.3",
            "key information": "The phenomenon of in-context learning raises questions about its underlying mechanisms and the role of document-level coherence during pretraining."
        },
        {
            "section number": "2.1",
            "key information": "The primary problem is understanding how in-context learning is possible in LMs, particularly when pretraining documents exhibit long-range coherence and how this affects the model's ability to infer a shared latent concept from prompts."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective revolves around treating in-context learning as Bayesian inference, where the LM infers latent concepts from examples to predict outputs."
        },
        {
            "section number": "3.3",
            "key information": "The main innovation of this work is the introduction of a simple pretraining distribution modeled as a mixture of Hidden Markov Models (HMMs) that facilitates the emergence of in-context learning."
        },
        {
            "section number": "6.1",
            "key information": "A limitation is that the theory primarily focuses on the pretraining distribution and does not fully account for the complexities of real-world data distributions and their impact on learning."
        },
        {
            "section number": "7",
            "key information": "The study concludes that in-context learning can be effectively modeled as implicit Bayesian inference, facilitated by coherent pretraining distributions."
        }
    ],
    "similarity_score": 0.7595873037686681,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/An Explanation of In-context Learning as Implicit Bayesian Inference.json"
}