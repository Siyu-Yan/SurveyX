{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2210.05983",
    "title": "Model-based clustering in simple hypergraphs through a stochastic blockmodel",
    "abstract": "We propose a model to address the overlooked problem of node clustering in simple hypergraphs. Simple hypergraphs are suitable when a node may not appear multiple times in the same hyperedge, such as in co-authorship datasets. Our model generalizes the stochastic blockmodel for graphs and assumes the existence of latent node groups and hyperedges are conditionally independent given these groups. We first establish the generic identifiability of the model parameters. We then develop a variational approximation ExpectationMaximization algorithm for parameter inference and node clustering, and derive a statistical criterion for model selection. To illustrate the performance of our R package HyperSBM, we compare it with other node clustering methods using synthetic data generated from the model, as well as from a line clustering experiment and a co-authorship dataset.",
    "bib_name": "brusa2024modelbasedclusteringsimplehypergraphs",
    "md_text": "# Model-based clustering in simple hypergraphs through a stochastic blockmodel\nLuca Brusa1 and Catherine Matias2\n Department of Statistics and Quantitative Methods, University of Milano-Bicocca, Vi Bicocca degli Arcimboldi 8, 20100 Milano, Italy. Email: luca.brusa@unimib.it 2 Sorbonne Universit\u00e9, Universit\u00e9 de Paris Cit\u00e9, Centre National de la Recherche Scientifique, Laboratoire de Probabilit\u00e9s, Statistique et Mod\u00e9lisation, 4 place Jussieu, 75252 PARIS Cedex 05, France. Email: catherine.matias@cnrs.fr\n# Abstract\nWe propose a model to address the overlooked problem of node clustering in simple hypergraphs. Simple hypergraphs are suitable when a node may not appear multiple times in the same hyperedge, such as in co-authorship datasets. Our model generalizes the stochastic blockmodel for graphs and assumes the existence of latent node groups and hyperedges are conditionally independent given these groups. We first establish the generic identifiability of the model parameters. We then develop a variational approximation ExpectationMaximization algorithm for parameter inference and node clustering, and derive a statistical criterion for model selection. To illustrate the performance of our R package HyperSBM, we compare it with other node clustering methods using synthetic data generated from the model, as well as from a line clustering experiment and a co-authorship dataset.\nKeywords: co-authorship network, high-order interactions, latent variable model, line clustering, non-uniform hypergraph, variational expectation-maximization.\n# 1 Introduction\nOver the past two decades, a wide range of models has been developed to capture pairwise interactions represented in graphs. However, modern applications in various fields have highlighted the necessity to consider high-order interactions, which involve groups of three or more nodes. Simple examples include triadic and larger group interactions in social networks (whose importance has been recognized early on, see Wolff, 1950), scientific co-authorship (Estrada and Rodr\u00edguez-Vel\u00e1zquez, 2006), interactions among more than two species in ecological systems\n(Muyinda et al., 2020; Singh and Baruah, 2021), or high-order correlations between neurons in brain networks (Chelaru et al., 2021). To formalize these high-order interactions, hypergraphs provide the most general framework. Similar to a graph, a hypergraph consists of a set of nodes and a set of hyperedges, where each hyperedge is a subset of nodes involved in an interaction. In this context, it is important to distinguish simple hypergraphs from multiset hypergraphs, where hyperedges can contain repeated nodes. Multisets are a generalization of sets, allowing elements to appear with varying multiplicities. Recent reviews on high-order interactions can be found in the works of Battiston et al. (2020), Bick et al. (2023), and Torres et al. (2021).\nprovide the most general framework. Similar to a graph, a hypergraph consists of a set of nodes and a set of hyperedges, where each hyperedge is a subset of nodes involved in an interaction. In this context, it is important to distinguish simple hypergraphs from multiset hypergraphs, where hyperedges can contain repeated nodes. Multisets are a generalization of sets, allowing elements to appear with varying multiplicities. Recent reviews on high-order interactions can be found in the works of Battiston et al. (2020), Bick et al. (2023), and Torres et al. (2021). Despite the increasing interest in high-order interactions, the statistical literature on this topic remains limited. Some graph-based statistics, such as centrality or clustering coefficient, have been extended to hypergraphs to aid in understanding the structure and extracting information from the data (Estrada and Rodr\u00edguez-Vel\u00e1zquez, 2006). However, these statistics do not fulfill the need for random hypergraph models. Early analyses of hypergraphs have relied on their embedding into the space of bipartite graphs (see, e.g., Battiston et al., 2020). Hypergraphs with self-loops and multiple hyperedges (weighted hyperedges with integer-valued weights) are equivalent to bipartite graphs. However, bipartite graph models were not specifically designed for hypergraphs and may introduce artifacts; we refer to Section A in the Supplementary Material for more details. Generalizing Erd\u0151s-R\u00e9nyi\u2019s model of random graphs leads to uniformly random hypergraphs. This model involves drawing uniformly at random from the set of all m-uniform hypergraphs (hypergraphs with hyperedges of fixed cardinality m) over a set of n nodes. However, similar to Erd\u0151s-R\u00e9nyi\u2019s model for graphs, this hypergraph model is too simplistic and homogeneous to be used for statistical analysis of real-world datasets. In the configuration model for random graphs, the graphs are generated by drawing uniformly at random from the set of all possible graphs over a set of n nodes, while satisfying a given prescribed degree sequence. In the context of hypergraphs, configuration models were proposed in Ghoshal et al. (2009), focusing on tripartite and 3-uniform hypergraphs. Later, Chodrow (2020) extended the configuration model to a more general hypergraph setup. In these references, both the node degrees and the hyperedge sizes are kept fixed (a consequence of the fact that they rely on bipartite representations of hypergraphs). The configuration model is useful for sampling (hyper)graphs with the same degree sequence (and hyperedge sizes) as an observed dataset through shuffling algorithms. Therefore, it is often employed as a null model in statistical analyses. However, sampling exactly (rather than approximately) from this model poses challenges, particularly in the case of hypergraphs. For a comprehensive discussion on this issue, we refer readers to Section 4 in Chodrow (2020). Another popular approach for extracting information from heterogeneous data is clustering.\n(Frank and Harary, 1982; Holland et al., 1983) and have since evolved in various directions. These models assume that nodes are grouped into clusters, and the probabilities of connections between nodes are determined by their cluster memberships. Variants of SBMs have been developed to handle weighted graphs and degree-corrected versions, among others. In the context of hypergraphs, Ghoshdastidar and Dukkipati (2017) studied a spectral clustering approach based on a hypergraph Laplacian, and obtained its weak consistency under a Hypergraph SBM (HSBM) under certain restrictions on the model parameters. More recently, Deng et al. (2024) established the strong consistency of the basic spectral clustering under the degree-corrected HSBM (DCHSBM) in the sparse regime where the maximum expected hyperdegree might be of order \u2126(log n) and n is the number of nodes. By introducing hypergraphons, Balasubramanian (2021) extended the ideas of hypergraph SBMs to a nonparametric setting. In a parallel vein, Turnbull et al. (2021) proposed a latent space model for hypergraphs, generalizing random geometric graphs to hypergraphs, although it was not specifically designed to capture clustering. An approach linked to SBMs is presented in Vazquez (2009), where nodes belong to latent groups and participate in a hyperedge with a probability that depends on both their group and the specific hyperedge. Modularity is a widely used criterion for clustering entities in the context of interaction data. It aims to identify specific clusters, known as communities, characterized by high withingroup connection probabilities and low between-group connection probabilities (Ghoshdastidar and Dukkipati, 2014). However, in the hypergraph context, the definition of modularity is not unique. In particular, Kami\u0144ski et al. (2019) introduced a \u201cstrict\u201d modularity criterion, where only hyperedges with all their nodes belonging to the same group contribute to an increase in modularity. Their criterion measures the deviation of the number of these homogeneous hyperedges from a new null model called the configuration-like model for hypergraphs, where the average values of the degrees are fixed. Building upon this, Chodrow et al. (2021) proposed a degree-corrected hypergraph SBM and introduced two new modularity criteria. Similar to Kami\u0144ski et al. (2019), one of these criteria utilizes an \u201call-or-nothing\u201d affinity function that distinguishes whether a given hyperedge is entirely contained within a single cluster or not. In this setup, they established a connection between approximate maximum likelihood estimation and their modularity criterion. This work is reminiscent of the work of Newman (2016) in the graph context. However, the estimators proposed by Chodrow et al. (2021) do not guarantee maximum likelihood estimation, as the parameter space is constrained by assuming a symmetric affinity function. We refer to Poda and Matias (2024) for an empirical comparison of these modularity-based methods. It is important to highlight that the developments presented in Kami\u0144ski et al. (2019) and Chodrow et al. (2021) are specifically conducted in the context of multiset hypergraphs, where hyperedges can contain repeated nodes with certain multiplicities. The use of multiset hypergraphs simplifies some of the challenges associated with computing modularity. However, to the best of our knowledge, modularity approaches still lack instantiation in the case of simple hypergraphs where each node can only appear once in a hyperedge. More specifically, the null model\nused in hypergraph modularity criteria relies on a model for multiset hypergraphs, similar to how the null model used in classical graph modularity is based on graphs with self-loops. While it is known in the case of graphs that this assumption is inadequate, as it induces a stronger deviation than expected and affects sparse networks as well (Massen and Doye, 2005; Cafieri et al., 2010; Squartini and Garlaschelli, 2011), the assumption of multisets has not yet been discussed in the context of hypergraph modularity. In the context of community detection, random walk approaches have also been utilized for hypergraph clustering (Swan and Zhan, 2021). Additionally, low-rank tensor decompositions have been explored (Ke et al., 2020). The misclassification rate for the community detection problem in hypergraphs and its limits have been analyzed in various contexts (see, for instance, Ahn et al., 2018; Chien et al., 2019; Cole and Zhu, 2020). It is worth mentioning that a recent approach has been proposed to cluster hyperedges instead of nodes (Ng and Murphy, 2022). However, our focus in this work is on clustering nodes. The literature on high-order interactions often discusses simplicial complexes alongside hypergraphs (Battiston et al., 2020). However, the unique characteristic of simplicial complexes, where each subset of an occurring interaction should also occur, places them outside the scope of this introduction, which is specifically focused on hypergraphs. In this article, our focus is on model-based clustering for simple hypergraphs, specifically studying stochastic hypergraph blockmodels. We formulate a general stochastic blockmodel for simple hypergraphs, along with various submodels (Section 2.1). We provide the first result on the generic identifiability of parameters in a hypergraph stochastic blockmodel (Section 2.2). Parameter inference and node clustering are performed using a variational Expectation-Maximization (VEM) algorithm (Section 2.3) that approximates the maximum likelihood estimator. Model selection for the number of groups is based on an integrated classification likelihood (ICL) criterion (Section 2.4). To illustrate the performance of our method, we conduct experiments on synthetic sparse hypergraphs, including a comparison with hypergraph spectral clustering (HSC) and modularity approaches (Section 3). Notably, the line clustering experiment (Section 3.4) highlights the significant differences between our approach and the one proposed by Chodrow et al. (2021). We also analyze a co-authorship dataset, presenting conclusions that differ from spectral clustering and bipartite stochastic blockmodels (Section 4). We discuss (Section 5) our approach, its advantages, current limitations and possible extensions. An R package, HyperSBM, which implements our method in efficient C++ code, as well as all associated scripts, are available online (see Section 6). This manuscript is accompanied by a Supplementary Material (SM) that contains additional information and experiments, as well as the proofs of all theoretical results.\nused in hypergraph modularity criteria relies on a model for multiset hypergraphs, similar to how the null model used in classical graph modularity is based on graphs with self-loops. While it is known in the case of graphs that this assumption is inadequate, as it induces a stronger deviation than expected and affects sparse networks as well (Massen and Doye, 2005; Cafieri et al., 2010; Squartini and Garlaschelli, 2011), the assumption of multisets has not yet been discussed in the context of hypergraph modularity. In the context of community detection, random walk approaches have also been utilized for hypergraph clustering (Swan and Zhan, 2021). Additionally, low-rank tensor decompositions have been explored (Ke et al., 2020). The misclassification rate for the community detection problem in hypergraphs and its limits have been analyzed in various contexts (see, for instance, Ahn et al., 2018; Chien et al., 2019; Cole and Zhu, 2020). It is worth mentioning that a recent approach has been proposed to cluster hyperedges instead of nodes (Ng and Murphy, 2022). However, our focus in this work is on clustering nodes. The literature on high-order interactions often discusses simplicial complexes alongside hypergraphs (Battiston et al., 2020). However, the unique characteristic of simplicial complexes, where each subset of an occurring interaction should also occur, places them outside the scope of this introduction, which is specifically focused on hypergraphs.\nused in hypergraph modularity criteria relies on a model for multiset hypergraphs, similar to how the null model used in classical graph modularity is based on graphs with self-loops. While it is known in the case of graphs that this assumption is inadequate, as it induces a stronger deviation than expected and affects sparse networks as well (Massen and Doye, 2005; Cafieri et al., 2010; Squartini and Garlaschelli, 2011), the assumption of multisets has not yet been discussed in the context of hypergraph modularity. In the context of community detection, random walk approaches have also been utilized for hypergraph clustering (Swan and Zhan, 2021). Additionally, low-rank tensor decompositions have been explored (Ke et al., 2020). The misclassification rate for the community detection problem in hypergraphs and its limits have been analyzed in various contexts (see, for instance, Ahn et al., 2018; Chien et al., 2019; Cole and Zhu, 2020). It is worth mentioning that a recent approach has been proposed to cluster hyperedges instead of nodes (Ng and Murphy, 2022). However, our focus in this work is on clustering nodes. The literature on high-order interactions often discusses simplicial complexes alongside hypergraphs (Battiston et al., 2020). However, the unique characteristic of simplicial complexes, where each subset of an occurring interaction should also occur, places them outside the scope of this introduction, which is specifically focused on hypergraphs. In this article, our focus is on model-based clustering for simple hypergraphs, specifically studying stochastic hypergraph blockmodels. We formulate a general stochastic blockmodel for simple hypergraphs, along with various submodels (Section 2.1). We provide the first result on the generic identifiability of parameters in a hypergraph stochastic blockmodel (Section 2.2). Parameter inference and node clustering are performed using a variational Expectation-Maximization (VEM) algorithm (Section 2.3) that approximates the maximum likelihood estimator. Model selection for the number of groups is based on an integrated classification likelihood (ICL) criterion\nIn this article, our focus is on model-based clustering for simple hypergraphs, specifically studying stochastic hypergraph blockmodels. We formulate a general stochastic blockmodel for simple hypergraphs, along with various submodels (Section 2.1). We provide the first result on the generic identifiability of parameters in a hypergraph stochastic blockmodel (Section 2.2). Parameter inference and node clustering are performed using a variational Expectation-Maximization (VEM) algorithm (Section 2.3) that approximates the maximum likelihood estimator. Model selection for the number of groups is based on an integrated classification likelihood (ICL) criterion (Section 2.4). To illustrate the performance of our method, we conduct experiments on synthetic sparse hypergraphs, including a comparison with hypergraph spectral clustering (HSC) and modularity approaches (Section 3). Notably, the line clustering experiment (Section 3.4) highlights the significant differences between our approach and the one proposed by Chodrow et al. (2021). We also analyze a co-authorship dataset, presenting conclusions that differ from spectral clustering and bipartite stochastic blockmodels (Section 4). We discuss (Section 5) our approach, its advantages, current limitations and possible extensions. An R package, HyperSBM, which implements our method in efficient C++ code, as well as all associated scripts, are available online (see Section 6). This manuscript is accompanied by a Supplementary Material (SM) that contains additional information and experiments, as well as the proofs of all theoretical results.\n# 2 A stochastic blockmodel for hypergraphs\n# 2.1 Model formulation\nLet H = (V, E) represent a binary hypergraph, where V = {1, . . . , n} is a set of n nodes and E is the set of hyperedges. In this context, a hyperedge of size m \u22652 is defined as a collection of m distinct nodes from V. We do not allow for hyperedges to be multisets or self-loops. Let M = max e\u2208E |e| denote the largest possible size of hyperedges in E, with M \u22652 (for graphs, M = 2). We define the sets of (unordered) node subsets, (ordered) node tuples, and hyperedges of size m as\nV(m) = \ufffd {i1, . . . , im} : i1, . . . , im \u2208V are all distinct \ufffd , Vm = \ufffd (i1, . . . , im) : i1, . . . , im \u2208V are all distinct \ufffd , E(m) = \ufffd {i1, . . . , im} \u2208V(m) : {i1, . . . , im} \u2208E \ufffd ,\n# \ufffd \ufffd respectively. Obviously E = \ufffdM m=2 E(m) \u2286\ufffdM m=2 V(m). For each node subset {i1, . . . , im} \u2208V(m), we define the indicator variable:\n  Yi1,...,im = 1{i1,...,im}\u2208E = \uf8f1 \uf8f2 \uf8f3 1 if {i1, . . . , im} \u2208E, 0 if {i1, . . . , im} /\u2208E.\n\uf8f3 We represent a random hypergraph as Y = (Yi1,...,im)i1,...,im\u2208V(m),2\u2264m\u2264M. Similar to the formulation of the stochastic blockmodel (SBM) for graphs, we assume that the nodes in the hypergraph belong to Q unobserved groups. We use Z1, . . . , Zn to denote n independent and identically distributed latent variables, where Zi follows a prior distribution \u03c0q = P(Zi = q) for each q = 1, . . . , Q. The values \u03c0q satisfy \u03c0q \u22650 and \ufffdQ q=1 \u03c0q = 1. To simplify notation, we sometimes represent Zi as a binary vector Zi = (Zi1, . . . , ZiQ) \u2208{0, 1}Q, where only one element, Ziq, equals 1. We also define Z = (Z1, . . . , Zn). Every m-subset of nodes {i1, . . . , im} in V(m) is associated to a latent configuration, namely a set {Zi1, . . . , Zim} = {q1, . . . , qm} of latent groups to which these nodes belong. The values of the latent groups within a configuration may be repeated, so that each {q1, . . . , qm} is a multiset. Now, given the latent variables Z, all indicator variables Yi1,...,im are assumed to be independent and to follow a Bernoulli distribution whose parameter depends on the latent configuration:\nYi1,...,im|{Zi1 = q1, . . . , Zim = qm} \u223cB(B(m,n) q1,...,qm), for any {i1, . . . , im} \u2208V(m).\nYi1,...,im|{Zi1 = q1, . . . , Zim = qm} \u223cB(B(m,n) q1,...,qm),\nHere B(m,n) q1,...,qm = B(m,n) q1,...,qm represents the probability that m unordered nodes, with latent configuration {q1, . . . , qm}, are connected into a hyperedge. To simplify notation, we drop the superscript (m, n). However, the model may account for 2 possible sparse settings. First, as the number of nodes n increases, it is natural to assume that the probability of a hyperedge may decrease;\notherwise, we would only observe dense hypergraphs. Second, it is likely that real data contain fewer hyperedges of larger size m. Each B is a fully symmetric tensor of rank m, namely\notherwise, we would only observe dense hypergraphs. Second, it is likely that real data contain fewer hyperedges of larger size m. Each B is a fully symmetric tensor of rank m, namely B(m,n) q1,...,qm = Bq\u03c3(1),...,q\u03c3(m), \u2200q1, . . . , qm and \u2200\u03c3 permutation of {1, . . . , m}. (1) We denote the parameter vector as \u03b8 = (\u03c0q, B(m,n) q1,...,qm)q,m,q1\u2264\u00b7\u00b7\u00b7\u2264qm and the corresponding probability distribution and expectation as P\u03b8, E\u03b8, respectively. Lemma 1. The number of different parameters in each tensor B = (B(m,n) q1,...,qm)1\u2264q1\u2264\u00b7\u00b7\u00b7\u2264qm\u2264Q is \ufffdQ+m\u22121 m \ufffd . As a result, the total number of parameters in our hypergraph stochastic blockmodel (HSBM) is given by: \ufffd \ufffd\nWe denote the parameter vector as \u03b8 = (\u03c0q, B(m,n) q1,...,qm)q,m,q1\u2264\u00b7\u00b7\u00b7\u2264qm and the corresponding probability distribution and expectation as P\u03b8, E\u03b8, respectively. Lemma 1. The number of different parameters in each tensor B = (B(m,n) q1,...,qm)1\u2264q1\u2264\u00b7\u00b7\u00b7\u2264qm\u2264Q is \ufffdQ+m\u22121 m \ufffd . As a result, the total number of parameters in our hypergraph stochastic blockmodel (HSBM) is given by: \ufffd \ufffd\nAs shown in Table 1, the number of B(m,n) q1,...,qm parameters increases rapidly as the values of Q and m grow. Note that the number of parameters (of the order O(MQM+Q)) remains small compared to the number of observations (\ufffdM m=2 \ufffdn m \ufffd = O(nM)). So we do have enough statistical information to estimate all parameters. Nonetheless, to reduce the complexity of the model, we introduce submodels by assuming equality of certain conditional probabilities B(m,n) q1,...,qm. In particular, we consider two affiliation submodels given by\nand\n\uf8f3 The number of parameters is dropped to (Q \u22121) + 2(M \u22121) and to (Q \u22121) + 2 under Assumptions (Aff-m) and (Aff), respectively. These submodels align with the concepts discussed in Kami\u0144ski et al. (2019) and Chodrow et al. (2021), where they propose that only hyperedges with nodes belonging to the same group should contribute to the increase in modularity. Additionally, when \u03b1(m) > \u03b2(m) (resp. \u03b1 > \u03b2) these submodels correspond to the scenarios in which Ghoshdastidar and Dukkipati (2014, 2017) obtained their results. A summary of the manuscript notation is given in Table 2.\n# 2.2 Parameter identifiability\nWe first establish the generic identifiability of the parameter in a HSBM that is restricted to simple m-uniform hypergraphs for any m \u22652. In a parametric context, generic identifiability implies\n(1)\n(Aff-m)\n(Aff)\n<div style=\"text-align: center;\">Table 1: Number \ufffdQ+m\u22121 m \ufffd of connectivity parameters B(m,n) q1,...,qm of the full HSBM for given value of Q (number of latent groups) and m (hyperedge size).</div>\nQ\nm\n2\n3\n4\n5\n6\n7\n3\n4\n10\n20\n35\n56\n84\n4\n5\n15\n35\n70\n126\n210\n5\n6\n21\n56\n126\n252\n462\n6\n7\n28\n84\n210\n462\n924\n7\n8\n36\n120\n330\n792\n1716\n<div style=\"text-align: center;\">Table 2: Notation summary.</div>\nhypergraph with V = {1, . . . , n} set of nodes and E collection of (simple) hyperedges largest hyperedge size and number of clusters node subsets (unordered) of size 2 \u2264m \u2264M node tuples (ordered) of size 2 \u2264m \u2264M hyperedges of size 2 \u2264m \u2264M observations (presence/absence of a hyperedge at each node subset) latent configurations (latent clusters), each Zi \u2208{1, . . . , Q} clusters proportions, such that \ufffdQ q=1 \u03c0q = 1 probability of a hyperedge at a size-m node subset with latent configuration {q1, . . . , qm}, for 1 \u2264q1 \u2264\u00b7 \u00b7 \u00b7 \u2264qm \u2264Q model parameter within-clusters and between-clusters probabilities in the affiliation sub-model (Aff-m) (resp. (Aff)) variational distribution on the latent configurations Z variational probability that node i belongs to cluster q, such that \ufffdQ q=1 \u03c4iq = 1 for all i \u2208{1, . . . , n} Bernoulli density at y with parameter b evidence lower bound (ELBO) entropy and Kullback-Leibler divergence\nthat the distribution P\u03b8 of a hypergraph over a set of n nodes uniquely defines the parameter \u03b8, except possibly for some parameters in a subset of dimension strictly smaller than the full parameter space. In other words, if we randomly select a parameter \u03b8 \u2208\u0398 according to the Lebesgue measure, the distribution P\u03b8 uniquely characterizes the parameter \u03b8, for a large enough number of nodes n. Identifiability is established up to label switching on the node groups, as is common in discrete latent variable models. For the case of m = 2, the identifiability result corresponds to Theorem 2 in Allman et al. (2011). Our proof follows similar principles, building upon a key result by Kruskal (1977). In our case, we crucially additionally rely on a sufficient condition for a sequence of nonnegative integers to represent the degree sequence of a simple m-uniform hypergraph, as established by Behrens et al. (2013). Theorem 2. For any m \u22652 and any integer Q, the parameter \u03b8 = (\u03c0q, B(m,n) q1,...,qm)1\u2264q\u2264Q,1\u2264q1\u2264\u00b7\u00b7\u00b7\u2264qm\u2264 of the HSBM restricted to m-uniform simple hypergraphs over n nodes, is generically identifiable, up to label switching on the node groups, as soon as n \u2265Q2\ufffd m!Qm + m \u22121 \ufffd2/(m\u22121) . Moreover, the result remains valid when the group proportions \u03c0q are fixed. This result does not provide specific insights into the identifiability in the affiliation cases (Aff-m) and (Aff). Indeed, it does not explicitly characterize the subspace of the parameter space where identifiability may not hold, although we know that its dimension is smaller than that of the full parameter space (and that the possible restrictions apply only on the part of the parameter space concerning the probabilities of connection B(m,n) q1,...,qm). The result we have established for m-uniform hypergraphs also implies a similar result for non-uniform simple hypergraphs, as shown in the following corollary. Corollary 3. For any integer Q, the parameter \u03b8 = (\u03c0q, B(m,n) q1,...,qm)1\u2264q\u2264Q,1\u2264q1\u2264\u00b7\u00b7\u00b7\u2264qm\u2264Q,2\u2264m\u2264M of the HSBM for simple hypergraphs over n nodes is generically identifiable, up to label switching on the node groups, as soon as n \u2265Q2\ufffd M!QM + M \u22121 \ufffd2/(M\u22121) . Our proof of Corollary 3 relies on the assumption that all the \u03c0q\u2019s are distinct, which is a generic condition. This condition is not explicitly stated in the corollary, but it is required for the proof to hold. Consequently, the result of generic identifiability does not bring any insight in cases where the group proportions are equal, as it is not sufficient to identify the parameters separately for each value of m. Additional technical work is thus needed to establish whether a HSBM with equal group proportions, or whether the affiliation submodels have identifiable parameters. As a final note, we mention that there is no direct link between parameter identifiability and detectability thresholds for clusters recovery (Dumitriu et al., 2022; Stephan and Zhu, 2022).While clusters recovery is an asymptotic result with guarantees when the sample size increases, parameter identifiability is a theoretical result stating that the distribution of the observations (for a minimal sample size) fully characterizes the parameter. It is theoretical in the sense that it does\n\ufffd \ufffd Our proof of Corollary 3 relies on the assumption that all the \u03c0q\u2019s are distinct, which is a generic condition. This condition is not explicitly stated in the corollary, but it is required for the proof to hold. Consequently, the result of generic identifiability does not bring any insight in cases where the group proportions are equal, as it is not sufficient to identify the parameters separately for each value of m. Additional technical work is thus needed to establish whether a HSBM with equal group proportions, or whether the affiliation submodels have identifiable parameters. As a final note, we mention that there is no direct link between parameter identifiability and detectability thresholds for clusters recovery (Dumitriu et al., 2022; Stephan and Zhu, 2022).While clusters recovery is an asymptotic result with guarantees when the sample size increases, parameter identifiability is a theoretical result stating that the distribution of the observations (for a minimal sample size) fully characterizes the parameter. It is theoretical in the sense that it does\nnot deal with inference, though the property has consequences on inference results. Parameter identifiability is a basic requirement for consistency results of maximum likelihood estimators to hold in parametric settings and it is also required for proofs of clusters exact recovery.\n# 2.3 Parameter estimation via variational Expectation-Maximization\nThe likelihood of the model is given as a marginal distribution\nThe computation of the model likelihood is generally intractable. Equation (2) involves a summation over all possible Qn different latent configurations of the nodes, which becomes computationally prohibitive when n and Q are large. In the context of latent variable models, the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) is commonly used to address this issue. However, the EM algorithm cannot be directly applied to SBMs. This is because the E-step, which involves computing the conditional posterior distribution of the latent variables P(Z|Y ), is itself intractable in SBMs (see, e.g., Matias and Robin, 2014). One possible solution is to employ variational approximations of the EM algorithm, known as Variational EM (VEM, Jordan et al., 1999). Below, we recall the classical approach for the VEM algorithm. We denote the density of the Bernoulli distribution with parameter b as\n\u2200y \u2208{0, 1}, f(y, b) := y log b + (1 \u2212y) log(1 \u2212b).\nThen, the complete data log-likelihood is\n\u2113c n(\u03b8) = log P\u03b8(Y , Z) = log P\u03b8(Z) + log P\u03b8(Y |Z)\nNote that the final equality ensures that each parameter value appears only once. The key principle underlying the variational method is to adopt the same iterative two-step structure\n(2)\n(3)\n(4)\nas the EM algorithm but replace the intractable posterior distribution P\u03b8(Z|Y ) with the best approximation, in terms of Kullback-Leibler divergence, from a class of simpler distributions, often factorized. We introduce a class of probability distributions Q\u03c4 over Z = (Z1, . . . , Zn) that factorize over the set of nodes, thus given by\nwhere the variational parameter \u03c4iq = Q\u03c4(Zi = q) \u2208[0, 1] satisfies \ufffdQ q=1 \u03c4iq = 1 for any i = 1, . . . , n. The expectation under distribution Q\u03c4 is denoted as EQ\u03c4 , and H(Q\u03c4) represents the entropy of Q\u03c4. Now we define the evidence lower bound (ELBO):\nJ (\u03b8, \u03c4) = EQ\u03c4 [log P\u03b8(Y , Z)] + H(Q\u03c4) = EQ\u03c4 [log P\u03b8(Y , Z)] \u2212EQ\u03c4 [log Q\u03c4(Z)]\n= Q \ufffd q=1 n \ufffd i=1 \u03c4iq log \u03c0q \u03c4iq + M \ufffd m=2 \ufffd q1\u2264q2\u2264\u00b7\u00b7\u00b7\u2264qm \ufffd (i1,...,im)\u2208Vm \u03c4i1q1 \u00b7 \u00b7 \u00b7 \u03c4imqmf(Yi1...im, B(m,n) q1,...,qm).\nIt can be observed that J (\u03b8, \u03c4) satisfies the following relation: J (\u03b8, \u03c4) = log P\u03b8(Y ) \u2212KL(Q\u03c4(Z)||P\u03b8(Z|Y ))\nIt can be observed that J (\u03b8, \u03c4) satisfies the following relation\nJ (\u03b8, \u03c4) = log P\u03b8(Y ) \u2212KL(Q\u03c4(Z)||P\u03b8(Z|Y )),\nwhere KL(\u00b7||\u00b7) denotes the Kullback-Leibler divergence. Equation (6) is at the core of the EM algorithm and its variational approximation. In the classical EM approach, at the t-th iteration of the algorithm, the variational distribution Q\u03c4 is chosen as the distribution P\u03b8(t)(Z|Y ) of the latent variables given the observations at the current parameter value \u03b8(t). This cancels the Kullback-Leibler term and the ELBO equals the log-likelihood. When the distribution P\u03b8(Z|Y ) is not factorized, such a choice would prevent from an efficient computation of the expectation EQ\u03c4 [log P\u03b8(Y , Z)] of the complete log-likelihood under Q\u03c4 appearing in Equation (5). Thus, the variational approximation searches for the best approximation of the true P\u03b8(Z|Y ) in a class of simplified (in general, factorized) variational distributions. As a consequence, the KullbackLeibler divergence term in (6) is non null in general and the ELBO J serves as a lower bound for the model log-likelihood log P\u03b8(Y ). The VEM algorithm iterates between the following two steps until a suitable convergence criterion is met:\n\u2022 VE-Step maximizes J (\u03b8, \u03c4) with respect to \u03c4\n# \u2022 VE-Step maximizes J (\u03b8, \u03c4) with respect to \u03c4\n\ufffd\u03c4 (t) = arg max \u03c4 J (\u03b8(t\u22121), \u03c4), s.t. \ufffdQ q=1 \ufffd\u03c4 (t) iq = 1 \u2200i = 1, . . . , n.\n\ufffd  \ufffd  \ufffd This step involves finding the best approximation of the conditional distribution P\u03b8(Z|Y ) by minimizing the Kullback-Leibler divergence term in (6).\n(5)\n(6)\n(7)\n\u2022 M-Step maximizes J (\u03b8, \u03c4) with respect to \u03b8\n\ufffd\u03b8(t) = arg max \u03b8 J (\u03b8, \u03c4 (t\u22121)), s.t. \ufffdQ q=1 \ufffd\u03c0(t) q = 1.\nIn the following we provide the solutions of the two maximization problems in Equations (7) and (8). Proposition 4 (VE-Step). Given the current model parameters \u03b8 = (\u03c0q, B(m,n) q1,...,qm)q,m,q1\u2264\u00b7\u00b7\u00b7\u2264qm at any iteration of the VEM algorithm, the corresponding optimal values of the variational parameters (\ufffd\u03c4iq)i,q defined in Equation (7) satisfy the fixed point equation:\nIn the following we provide the solutions of the two maximization problems in Equations (7) and (8).\n\ufffd log \ufffd\u03c4iq = log \u03c0q+ M\u22121 \ufffd m=1 \ufffd 1\u2264q1\u2264\u00b7\u00b7\u00b7\u2264qm\u2264Q \ufffd (i1,...,im)\u2208Vm s.t.{i,i1,...,im}\u2208V(m+1) \ufffd\u03c4i1q1 \u00b7 \u00b7 \u00b7 \ufffd\u03c4imqmf(Yii1...im, B(m qq\nfor any 1 \u2264i \u2264n and 1 \u2264q \u2264Q and where ci are normalising constants such that \ufffd q \ufffd\u03c4iq = 1. Equation (9) relates the variational probability \ufffd\u03c4iq that a node i belongs to a cluster q to the other variational parameters (as well as the observations and current parameter value \u03b8). The sum starts at m = 1 and deals with (m + 1)-tuples of nodes {i, , i1, . . . , im} that contain node i and whose latent configuration is given by some multiset {q, q1, . . . , qm}. Remark. From Proposition 4, the \u03c4i\u2019s are obtained using a fixed point algorithm. Although in all the situations we experienced, the algorithm converged in a reasonable number of iterations, we have no guarantee about existence nor uniqueness of a solution to (9). Proposition 5 (M-Step). Given the variational parameters (\u03c4iq)i,q at any iteration of the VEM algorithm, the corresponding optimal values of the model parameters (\ufffd\u03c0q, \ufffdBq1...qm)q,m,q1\u2264\u00b7\u00b7\u00b7\u2264qm defined in Equation (8) are given by\n\ufffd\u03c0q = 1 n n \ufffd i=1 \u03c4iq and \ufffdBq1...qm = \ufffd (i1,...,im)\u2208Vm \u03c4i1q1 . . . \u03c4imqmYi1...im \ufffd (i1,...,im)\u2208Vm \u03c4i1q1 . . . \u03c4imqm .\n\ufffd \ufffd \ufffd We now express the solutions of the M-Step under the submodels given by (Aff-m) and (Aff). Note that the VE-Step is unchanged under these settings.\nProposition 6 (M-Step, affiliation setups). In the particular affiliation submodels given by (Aff-m) and (Aff) respectively, given variational parameters (\u03c4iq)i,q, at any iteration of the VEM algorithm, the corresponding optimal values of (\ufffd\u03b1(m), \ufffd\u03b2(m))m and \ufffd\u03b1, \ufffd\u03b2 maximising J as in Equation (8) are given by\n(8)\n(7)\n\u2022 Under Assumption (Aff),\nAlgorithm initialization. We choose to begin the algorithm with its M-step, which requires an initial value for \u03c4. This allows us to leverage smart initialization strategies based on a preliminary clustering of the nodes. Specifically, we employ three different initialization strategies and select the best result that maximizes the ELBO criterion J : Random initialization: This naive method involves drawing each (\u03c4iq)1\u2264q\u2264Q uniformly from (0, 1) for every node i and normalizing the vector \u03c4i. \u201cSoft\u201d spectral clustering: We utilize Algorithm 1 from Ghoshdastidar and Dukkipati (2017) combined with soft k-means. In this approach, we compute a hypergraph Laplacian and construct the column matrix X consisting of its leading Q orthonormal eigenvectors. The rows of X are then normalized to have unit norm (following steps 1 to 3 in Algorithm 1 from Ghoshdastidar and Dukkipati, 2017). We subsequently perform a soft k-means algorithm on the rows of X to obtain \u03c4iq, which represents the posterior probability of node i belonging to cluster q. Graph-component absolute spectral clustering: This strategy focuses on edges in the hypergraph (m = 2) and the corresponding adjacency matrix. We apply the absolute spectral clustering method (Rohe et al., 2011) to this adjacency matrix. The absolute spectral clustering method introduces a graph Laplacian with both positive and negative eigenvalues and focuses on the ones with largest magnitude, thus capturing both communities and dis-assortative structures. It should be noted that this initialization only uses information from hyperedges of size m = 2, excluding hyperedges with larger sizes. However, absolute spectral clustering is considered superior to spectral clustering as it captures disassortative groups. In Section F.2 from the SM, we include a comparison of different initialization strategies. In general, there won\u2019t be an initialization strategy that is always superior, so we recommend always using different strategies and selecting the best criteria.\nFixed point. The VE-Step is achieved through a fixed-point algorithm. In practice, at iteration t of the VEM algorithm, starting from the previous values of the variational and model parameters \u03c4 (t\u22121) iq and \u03b8(t\u22121) respectively, we iterate over some index u to compute the values of \u03c4 (t,u) iq according to Equation (9). This generates a sequence of values \u03c4 (t,u) iq . We terminate these iterations either when we reach the maximum number of fixed-point iterations (u > Umax) or when the variational parameters have converged (max iq |\u03c4 (t,u\u22121) iq \u2212\u03c4 (t,u) iq | \u2264\u03b5), where \u03b5 is a small tolerance threshold. Stopping criteria. The iterations of the VEM algorithm should be terminated when the ELBO J and the sequence of model parameter vectors \u03b8(t) = (\u03b8(t) s )s have converged. However, in practice, we have observed that sometimes the algorithm stops prematurely when the VE-Step still requires a few iterations to reach a fixed point. In such cases, continuing with the VEM iterations often leads to higher values of the ELBO function and better parameter estimates. To address this, we enforce the condition that the fixed point in the VE-Step is reached in its first iteration. This reduces the chance of converging to local maxima of J . If these convergence conditions are not met, we stop the algorithm if the maximum number of iterations has been reached. To summarize, we stop the algorithm whenever:\nFixed point. The VE-Step is achieved through a fixed-point algorithm. In practice, at iteration t of the VEM algorithm, starting from the previous values of the variational and model parameters \u03c4 (t\u22121) iq and \u03b8(t\u22121) respectively, we iterate over some index u to compute the values of \u03c4 (t,u) iq according to Equation (9). This generates a sequence of values \u03c4 (t,u) iq . We terminate these iterations either when we reach the maximum number of fixed-point iterations (u > Umax) or when the variational parameters have converged (max iq |\u03c4 (t,u\u22121) iq \u2212\u03c4 (t,u) iq | \u2264\u03b5), where \u03b5 is a small tolerance threshold.\nStopping criteria. The iterations of the VEM algorithm should be terminated when the ELBO J and the sequence of model parameter vectors \u03b8(t) = (\u03b8(t) s )s have converged. However, in practice, we have observed that sometimes the algorithm stops prematurely when the VE-Step still requires a few iterations to reach a fixed point. In such cases, continuing with the VEM iterations often leads to higher values of the ELBO function and better parameter estimates. To address this, we enforce the condition that the fixed point in the VE-Step is reached in its first iteration. This reduces the chance of converging to local maxima of J . If these convergence conditions are not met, we stop the algorithm if the maximum number of iterations has been reached. To summarize, we stop the algorithm whenever:\n\ufffd |J (\u03b8(t\u22121)) \u2212J (\u03b8(t))| |J (\u03b8(t))| \u2264\u03b5 and max s |\u03b8(t\u22121) s \u2212\u03b8(t) s | \u2264\u03b5 and max iq |\u03c4 (t,0) iq \u2212\u03c4 (t,1) iq |  or {t > Tmax}. Section E in SM contains additional details about the algorithm\u2019s implementation.\n\ufffd |J (\u03b8(t\u22121)) \u2212J (\u03b8(t))| |J (\u03b8(t))| \u2264\u03b5 and max s |\u03b8(t\u22121) s \u2212\u03b8(t) s | \u2264\u03b5 and max iq |\u03c4 (t,0) iq \u2212\u03c4 (t,1) iq | \u2264\u03b5 \ufffd or {t > Tmax}.\n{} Section E in SM contains additional details about the algorithm\u2019s implementation.\nSection E in SM contains additional details about the algorithm\u2019s implementation.\nAlgorithm complexity and choice of M. The complexity of our algorithm is of the order O(nQM\ufffdn M \ufffd ), which can be quite prohibitive for large datasets, especially when M becomes large. It is important to emphasize that when analyzing a dataset, the value of M is not necessarily the maximum observed size of the hyperedges, but rather a modeling choice. Indeed, while an occurring hyperedge Yi1,...,im with node clusters {q1, . . . , qm} contributes log B(m,n) q1,...,qm to the likelihood, a non occurring one contributes log(1 \u2212B(m,n) q1,...,qm) and the statistical information that they bring to the parameter is the same (see Equations (3) and (4)). Now let\u2019s consider for e.g. a co-authorship dataset where we observe n authors and at most 3 co-authors per paper. The absence of hyperedges of size 4 provides as much information for a HSBM as if all possible size-4 hyperedges were present. Similarly, the information contained in a dataset with all but 5 possible size-4 hyperedges present is the same as the information contained in a dataset with only 5 occurring size-4 hyperedges. In other words, 0 and 1 values play a symmetric role. As a consequence, the choice of M is left to the discretion of the statistician, depending on the characteristics of the dataset and the available computational resources. In particular, if there are hyperedges with very large sizes M, the statistician may decide not to consider them, just\nas it is justified not to take into account the absence of hyperedges of size M + 1, where M is the largest observed size. It is important to note that choosing M > 2 already represents an improvement in terms of considering more information compared to a graph analysis of the data. Therefore, for large datasets, we recommend limiting the analysis to smaller values of M, such as M = 3 or M = 4, to reduce computational burden and improve efficiency.\n# 2.4 Model selection\nWhile Ghoshdastidar and Dukkipati (2017) propose a method for selecting the number of groups based on the spectral gap, our approach relies on a statistical framework to construct a model selection criterion. After obtaining the estimated parameters \u02c6\u03b8 and (\u02c6\u03c4i)i from the VEM algorithm, we assign each node i to its estimated group \u02c6Zi = arg maxq \u02c6\u03c4iq. We then define the integrated classification likelihood (ICL, Biernacki et al., 2000) for the full model and the submodels (Aff-m) and (Aff) as follows:\nICLaff(q) = log P\u02c6\u03b8(Y , \u02c6Z) \u22121 2(q \u22121) log n \u2212 M \ufffd log \ufffdn m \ufffd .\nThese criteria are constructed as the complete log-likelihood (computed at the estimated parameter value and clusters), penalized by a BIC-like term that accounts for the number of parameters and the corresponding \u201ceffective\u201d sample size (n for the parameters related to the nodes and \ufffdn m \ufffd for the size-m interaction parameters B(m,n) q1,...,qm). ICL criteria have been widely used in the context of SBMs. Their theoretical properties have never been established, though they exhibit very good empirical results on synthetic SBMs datasets (e.g. Daudin et al., 2008). Recently, Cerqueira and Leonardi (2020) obtained a first consistency result for a related criterion in the graph SBM, relying on a penalized version of the exact ICL (C\u00f4me and Latouche, 2015), also known in the information theory literature as Krichevsky-Trofimov (KT) estimator. While the literature of order estimation focuses on minimal penalties as these will lead to minimum underestimation probability (see for e.g. van Handel, 2011), these KT penalties are generally heavier than what is thought to be sufficient to consistently estimate the number of groups. We determine the number of groups \u02c6q as the value that maximizes the corresponding ICL criterion: \u02c6q = arg maxq ICL(q).\n# 3 Synthetic experiments\n# 3.1 Synthetic data\nWe conducted a simulation study to evaluate the performance of the HyperSBM package. We generated hypergraphs under the HSBM with two or three latent groups (Q = 2 or Q = 3). The group proportions were non-uniform, with \u03c0 = (0.6, 0.4) for Q = 2 and \u03c0 = (0.4, 0.3, 0.3) for Q = 3. We set the largest hyperedge size M to 3, and we considered different numbers of nodes, n \u2208{50, 100, 150, 200}. To simplify the latent structure, we assumed the (Aff-m) submodel, and we parametrized the model through the ratios \u03c1(m) of within-group size-m hyperedges over between-groups sizem hyperedges (assumed constant with n, see Section F.1 in SM for details). We analyzed two different scenarios:\nA. Communities: in this scenario, we focus on community detection and consider the case o more within-group than between-groups size-m hyperedges \u03c1(m) > 1 for m = 2, 3.\nB. Disassortative: in this scenario, we focus on disassortative behaviour and consider the cas of less within-group than between-groups size-m hyperedges \u03c1(m) < 1 for m = 2, 3.\nEach setting is a combination of a scenario (X=A,B) and number of groups (Q = 2, 3) and is denoted XQ. In each setting, values of \u03b1(m) = \u03b1(m,n) and \u03b2(m) = \u03b1(m,n) (we here emphasize the dependence on the number of nodes n) decrease with increasing n so as to maintaining constant the quantities n\u03b1(2,n) and n\u03b2(2,n) as well as n2\u03b1(3,n) and n2\u03b2(3,n). This implies that the number of size-m hyperedges (both within and between groups) grows linearly with n. We have explored a total of 5 different settings, denoted by A2, A3, B2, B3 and A3\u2019 and we present below the most striking results. In the case of scenarios A (communities) with Q = 3 groups, we pushed the limits and explore two different settings (namely settings A3 and A3\u2019), with setting A3 being highly sparse, i.e. sparser than the already sparse setting A3\u2019. Details of the parametrization, specific parameter values and number of hyperedges are fully given in Section F.1 in SM, while Section F.2 in SM contains additional results. For each setting and each value of n, we randomly draw 50 different hypergraphs. We estimate the parameters using the full HSBM formulation with our VEM algorithm, relying on soft spectral clustering (for Scenario A) and graph-component absolute spectral clustering (for Scenario B) initializations (see paragraph \u201cAlgorithm initialization\u201d above).\n# 3.2 Clustering and estimation under HSBM with a fixed number of groups\nIn this part we focus on clustering and parameter estimation with a known number of groups. The performance of HyperSBM is evaluated based on its ability to accurately recover the true clustering and estimate the original parameters. We also compare our results with hypergraph\nspectral clustering, relying on Algorithm 1 from Ghoshdastidar and Dukkipati (2017), denoted\nspectral clustering, relying on Algorithm 1 from Ghoshdastidar and Dukkipati (2017), denote HSC below.\nClustering results. The performance of correct classification is evaluated using the Adjusted Rand Index (ARI, Hubert and Arabie, 1985). The ARI measures the similarity between the true node clustering and the estimated clustering. It is upper bounded by 1, where a value of 1 indicates perfect agreement between the clusterings, and negative values indicate less agreement than expected by chance.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ffa7/ffa7ebe3-9b6b-4e6c-9e28-d31bd1ee75bd.png\" style=\"width: 50%;\"></div>\nFigure 1: Boxplots of Adjusted Rand Indexes for different settings XQ (where X=A, B is the scenario and Q = 2, 3 is the number of groups), number of nodes n (along x-axis) and 2 methods: our HyperSBM (left boxplot) and HSC (right boxplot). First row (resp. first column) shows scenario A with communities (resp. Q = 2) while second row (resp. second column) shows scenario B with disassortative behaviour (resp. Q = 3).\nFigure 1 displays the boxplot values of the ARI for settings A2 to B3. It is evident that our HyperSBM consistently outperforms HSC, obtaining higher ARI values overall and significantly lower variances in most cases, except for setting B3, where HyperSBM exhibits a larger variance but still yields substantially better results compared to HSC. We also observe that increasing the\nnumber of nodes n does not appear to significantly enhance the clustering results of HyperSBM. This behavior could be attributed to our simulation setting, where the numbers of size-m hyperedges (m = 2, 3) are kept linearly increasing with n. However, it is worth noting that the variances of the ARI obtained by HyperSBM tend to decrease with an increasing number of nodes n. One final comment pertains to the relatively poor clustering performance obtained by both methods in setting B3: this setting appears to be particularly challenging. Parameter estimation accuracy. We also evaluate the accuracy of parameter estimation. As the parameter values may be extremely small (see Section F.1 in SM), we choose to compute the Mean Squared Relative Error (MSRE) between the true parameters (in the full model) and the estimated values, both for the prior probabilities \u03c0q and the probabilities of hyperedge occurrence B(m) q1,...,qm. Specifically, we compute the aggregated MSRE over all the components of \u03b8 using the following formula:\nMSRE = 1 nrep nrep \ufffd i=1 \ufffdQ\u22121 \ufffd q=1 \ufffd\u02c6\u03c0i q \u2212\u03c0q \u03c0q \ufffd2 + M \ufffd m=2 \ufffd q1\u2264\u00b7\u00b7\u00b7\u2264qm \ufffd\u02c6Bi q1,...,qm \u2212B(m,n) q1,...,qm B(m,n) q1,...,qm \ufffd2\ufffd ,\nwhere (\u02c6\u03c0i 1, . . . , \u02c6\u03c0i Q\u22121, { \u02c6Bi q1,...,qm}m,q1,...,qm) is the set of free parameters estimated on the i-th dataset by the full model and nrep = 50 is the number of replicates. The corresponding results are summarized through the boxplots in Figure 2. The relative errors are rather small, decreasing and showing a lower variance as the number of nodes increases. Note that the absolute values of MSRE cannot be compared between the cases Q = 2 (first column) and Q = 3 (second column), with very different scales on the y-axis. Indeed, in the first case, the relative error is cumulated over a total of 1+3 +4=8 free parameters (in the full model), while this increases to 2+ 6+10=18 free parameters when Q = 3.\n# 3.3 Performance of model selection\nIn this section we assess the performance of ICL as a model selection criterion. The simulated data is fitted with our HyperSBM with a number of latent states ranging from 1 to 5. In Table 3 we show the frequency of the selected number of groups for setting A3\u2019. The correct model is selected in 74% of cases for n = 50, in 98% of cases for n = 100 and in 100% of cases for n = 150, 200. We also compute the value of ARI of the classification obtained with 3 clusters depending on the selected number of latent groups. This value is always equal to 1 when the correct model is recovered, thus confirming the optimal behavior of HyperSBM already shown in Section 3.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d9b9/d9b9fb8c-d4b0-4ef6-8c9f-a0c0ca361602.png\" style=\"width: 50%;\"></div>\nFigure 2: Boxplots of Mean Squared Relative Errors between true and estimated model parameters for different settings XQ (where X=A, B is the scenario and Q = 2, 3 is the number of groups) and number of nodes n (along x-axis). First row (resp. first column) shows scenario A with communities (resp. Q = 2) while second row (resp. second column) shows scenario B with disassortative behaviour (resp. Q = 3).\nTable 3: Frequency (as a percentage) of the selected number of groups Q for setting A3\u2019. Model selection is carried out by means of the ICL criterion. Results are computed over 50 samples for\nTable 3: Frequency (as a percentage) of the selected number of groups Q for setting A3\u2019. Mode selection is carried out by means of the ICL criterion. Results are computed over 50 samples for each value of n.\nQ\nn = 50\nn = 100\nn = 150\nn = 200\n1\n0%\n0%\n0%\n0%\n2\n26%\n2%\n0%\n0%\n3\n74%\n98%\n100%\n100%\n4\n0%\n0%\n0%\n0%\n5\n0%\n0%\n0%\n0%\n# 3.4 Line clustering through hypergraphs\nFollowing Leordeanu and Sminchisescu (2012); Kami\u0144ski et al. (2019) and earlier references, we here explore the use of hypergraphs to detect line clusters of points in R2. Similarly to the construction of pairwise similarity measures, we here resort on third order affinity measures to detect alignment of points since pairwise measures would be useless to detect alignment. Thus, for any triplet of points {i, j, k}, we use the mean distance to the best fitting line as a dissimilarity measure d(i, j, k) and transform this through a Gaussian kernel to a similarity measure. We performed two different experiments, with either 2 or 3 lines. In each setting, we randomly generate the same number of points per line in the range [\u22120.5, 0.5]2 and perturbed with centered Gaussian noise with standard deviation 0.01. We then add noisy points, generated from uniform distribution on [\u22120.5, 0.5]2. The particular settings of each experiment are described in Table 4 and Figure 3 shows the resulting sets of points.\n<div style=\"text-align: center;\">Table 4: Description settings for the line clustering experimen</div>\nNumber of points\nper line\nNumber of\nnoisy points\nTotal number\nof points\nMean number\nof hyperedges\n2 lines\n30\n40\n100\n1070.84\n3 lines\n30\n60\n150\n587.70\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2ef7/2ef7c840-e1dd-4ade-af2c-1b23bde2d2a1.png\" style=\"width: 50%;\"></div>\nFigure 3: Sets of points from the line clustering experiments. Left: 2 lines (green dots and red triangles) plus noise (black crosses). Right: 3 lines (green dots, red triangles and blue diamonds) plus noise (black crosses). For both settings, we generated 100 different 3-uniform hypergraphs using the following procedure. We randomly selected 3 points {i, j, k} and calculated the mean distance d(i, j, k) to the best-fitting line. We then measured their similarity using a Gaussian kernel exp(\u2212d(i, j, k)2/\u03c32)\n<div style=\"text-align: center;\">Figure 3: Sets of points from the line clustering experiments. Left: 2 lines (green dots and red triangles) plus noise (black crosses). Right: 3 lines (green dots, red triangles and blue diamonds) plus noise (black crosses).</div>\nFor both settings, we generated 100 different 3-uniform hypergraphs using the following procedure. We randomly selected 3 points {i, j, k} and calculated the mean distance d(i, j, k) to the best-fitting line. We then measured their similarity using a Gaussian kernel exp(\u2212d(i, j, k)2/\u03c32)\nwith \u03c32 = 0.04. If the similarity was greater than a threshold \u03f5 = 0.999, we constructed a hyperedge {i, j, k}. This procedure resulted in both signal hyperedges, where all points belonged to the same line cluster, and noise hyperedges, where the points were sufficiently aligned without belonging to the same line. The signal-to-noise ratio of hyperedges was set to 2 for each hypergraph. We specifically simulated sparse hypergraphs, and the average number of hyperedges is presented in Table 4. Additionally, isolated nodes in the hypergraph were excluded from the clustering analysis. We applied our HyperSBM algorithm to cluster the nodes of these 3-uniform and sparse hypergraphs, and we compared the results with three different modularity-based approaches. The first two approaches, referred to as Chodrow_Symm and Chodrow_AON, are from Chodrow et al. (2021) and are based on their general symmetric and all-or-nothing modularity, respectively. The third approach, referred to as Kaminski, is from Kami\u0144ski et al. (2019). The modularity-based methods automatically select the number of groups, and for HyperSBM, we performed model selection using Q \u2208{1, . . . , 6}. Figure 4 displays the ARI obtained from the clustering results. We can observe that the modularity-based methods fail to accurately recover the true original line clusters, resulting in lower ARIs. In contrast, HyperSBM shows good performance in this task, achieving higher ARIs. This difference in performance can be attributed to the tendency of modularity-based methods, especially the one by Kami\u0144ski et al. (2019), to select a larger number of groups in this particular context, as evidenced in Figure 5.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4efd/4efdb874-bfb9-4267-9b72-5dc1d2838d0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Boxplots of the ARI obtained by the different clustering methods on the line clusterin problem. Left: 2 lines, right: 3 lines.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dec8/dec83821-aca7-4012-acef-b75d269c44fa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Estimated number of groups \u02c6Q on the line clustering problem. Left: 2 lines (true valu of Q is 3), right: 3 lines (true value of Q is 4).</div>\nThis experiment highlights the distinct behavior of HyperSBM compared to the modularitybased clustering methods, including the approach proposed by Chodrow et al. (2021), despite both methods being based on a Stochastic Block Model (SBM) framework with a maximum-likelihood approach.\n# 4 Analysis of a co-authorship dataset\n# 4.1 Dataset description\nWe analyze a co-authorship dataset available at http://vlado.fmf.uni-lj.si/pub/networks/ data/2mode/Sandi/Sandi.htm. The dataset originates from the bibliography of the book \u201cProduct Graphs: Structure and recognition\u201d by Imrich and Klavz\u0103r and is provided as a bipartite author/article graph. To construct the hypergraph, following the approach of Estrada and Rodr\u00edguez-Vel\u00e1zquez (2006), we consider authors as nodes and create hyperedges that link authors who have collaborated on the same paper. Further details regarding the dataset pre-treatment can be found in Section G of the SM, along with additional analyses. In our analysis, we set M = 4 and focused on the main connected component of the hypergraph, which consists of 79 authors and 76 hyperedges. Among these hyperedges, 68.5% have a size of 2, while 29% have a size of 3, and 2.5% have a size of 4.\n# 4.2 Analysis with HyperSBM\nWe conducted an analysis of this dataset using our HyperSBM package. The model selection based on the ICL criterion determined that there are 2 groups ( \u02c6Q = 2). One group consists of only 8 authors, while the remaining 71 authors belong to the second group. Table 5 displays the distribution of the number of distinct co-authors per author. Within the first group of 8 authors, 6 of them have the highest number of distinct co-authors, while the remaining 2 authors each have 4 distinct co-authors. Table 5: Distribution of the number of distinct co-authors per author. The first group contains the 6 authors having the largest number of distinct co-authors (between 7 and 12) plus 2 authors with 4 co-authors each.\nNb co-authors 1 2 3 4 5 6 7 8 10 11 12 Count 23 27 13 6 2 2 1 1 2 1 1\nComing back to the bipartite graph of authors and (co-authored) papers, we looked at the degree distribution of the authors, given in Table 6. This corresponds to the distribution of the number of co-authored papers per author. We observed that 5 of the 8 authors from our first group are the ones that co-published the most, the three others having also high degree (one of degree 5 and two of degree 4). Thus, our first group is made of authors (among) the most collaborative ones, which are also (among) the most prolific ones. Table 6: Degree distribution of authors in the bipartite graph. Our first group contains the 5 most collaborating authors, one of the sixth, plus 2 authors with degree equal to 4.\nNeither the first nor the second group inferred by HyperSBM are communities. Indeed we obtained the following estimated values from the size-2 hyperedges: \u02c6B11 \u22434.2% is of the same order as \u02c6B12 \u22435.1% while \u02c6B22 \u22430.8% is around five times smaller. This means that the first group contains authors that have written with authors from the two groups while the second group is made of authors who have less co-authored papers with people of their own group. Looking now at size-3 hyperedges, we get that \u02c6B111 \u22432 \u00b7 10\u22124 ; \u02c6B112 \u224318 \u00b7 10\u22124 ; \u02c6B122 \u22437 \u00b7 10\u22124 and \u02c6B222 \u22430.6\u00b710\u22124. The most important estimated frequency is \u02c6B112 that concerns 2 authors of the small first group co-authoring a paper with one author of the large second group. The second most important estimated frequency is \u02c6B122 and is obtained for one author from small first group co-authoring a paper with two authors of the large second group. The remaining frequencies of size-3 hyperedges are negligible. This characterizes further the first groups as being composed by authors that do co-author with their own group as well as with authors from the second one.\nFinally, looking now at size-4 hyperedges, the only non negligible estimated frequency is obtained for \u02c6B1222 \u22434 \u00b7 10\u22126. We note here that the frequencies \u02c6B\u2019s with m = 3 or 4 are intrinsically on different scales, as also happens with m = 2 or 3. So again, authors from group 1 co-authored with the others authors. (Note that the first group is not large enough for a size-4 \u02c6B frequency with at least 2 authors in that group 1 to be non negligible).\n# 4.3 Comparison with 2 other methods\nWe first compared our approach with the hypergraph spectral clustering (HSC) algorithm proposed in Ghoshdastidar and Dukkipati (2017). Let us recall that spectral clustering does not come with a statistical criterion to select the number of groups. Looking at the partition obtained with Q = 2 groups, spectral clustering output groups with sizes 24 and 55, respectively. These groups are neither characterized by the number of co-authors nor their degrees in the bipartite graph (see details in SM). Indeed, in our case the best clusters are not communities and their sizes are very different, while we recall that spectral clustering tends to: i) extract communities ; ii) favor groups of similar size.\nWe then analysed the same dataset as a bipartite graph of authors/papers with the R package SBM through the function estimateBipartiteSBM (Chiquet et al., 2022). This method infers a latent blockmodel (that in fact corresponds to a SBM for bipartite graphs) and automatically selects a number of groups on both parts (authors and papers). The method relies on the same core VEM algorithm as ours, adapted to the bipartite graphs context. Hereafter, we refer to this method as the Bipartite-SBM implementation. Let us underline here that while the bipartite stochastic blockmodel can be written as a particular case of a HSBM, the converse is not true (see Section A.3 in the SM). In particular, our hypergraph SBM is not constrained by the need to cluster the set of hyperedges. The Bipartite-SBM also selected 2 groups of authors (and 1 group of papers). There was one small group with 4 authors, entirely contained in our first small group; it corresponds to authors that have the highest degree in the bipartite graph and the highest number of co-authors. So, Bipartite-SBM output a very small group of the most prolific and the most collaborative authors in this dataset. Further details about the distinctions between these groups and the ones obtained by HyperSBM are given in SM. As a conclusion, we see that while the outputs of Bipartite-SBM and HyperSBM may seem close on this specific dataset, they are nonetheless different. On the other hand, and still on this specific dataset, the spectral clustering approach outputs results that are completely different from those of HyperSBM.\nWe have proposed a hypergraph stochastic block model for simple hypergraphs and general clusters types, i.e. our work is not limited to community detection and/or equally-sized clusters. This is in sharp contrast with most existing approaches. For example, Ghoshdastidar and Dukkipati (2014, 2017) obtained error bounds that converge to zero only for the (Aff-m) model with equally-sized groups and assuming moreover that \u03b1(m) > \u03b2(m). Moreover, references such as Ke et al. (2020); Ahn et al. (2018); Chien et al. (2019) primarily focus on community detection, which means they only identify clusters that correspond to communities. Our inference procedure is based on a maximum-likelihood approach, which should in principle provide some statistical guarantees. While consistency and asymptotic normality of the variational and the maximum likelihood estimators in our HSBM is left for future work, we believe that such results could be obtained following approaches used in the context of graphs SBMs (Celisse et al., 2012; Bickel et al., 2013). It is worth noting that while Chodrow et al. (2021) initially employ a maximum likelihood approach, they deviate from that setting for their inference procedure. In contrast, our method retains the maximum likelihood framework throughout the inference process. The maximum likelihood approach also enables the use of a penalized criterion for model selection. The SBM for hypergraphs presented in Balasubramanian (2021) is highly general. However, their least-squares estimator for a hypergraphon model is computationally infeasible. Additionally, their Algorithm 1 is dedicated to community detection and does not provide general cluster recovery. Our model can accommodate self-loops without significant changes by allowing for m = 1. Furthermore, it can be easily extended to handle multiple hypergraphs (with or without self-loops) by incorporating a zero-inflated or deflated Poisson distribution on the conditional distribution of the hyperedges. In a more general setting, the conditional Bernoulli distribution can be replaced with any parametric distribution to handle weighted hypergraphs, and it could also easily incorporate covariates. This flexibility allows for the adaptation of our model to various types of hypergraph data. While an important challenge is to reduce the complexity of our approach, some gain could be provided by constraining the parameter set. For instance, Contisciani et al. (2022) consider a Poisson HSBM, where the connectivity parameter is non-zero only between nodes in the same cluster. While this assumption is quite restrictive, it is mitigated by the introduction of overlapping clusters. In the same way, Ruggeri et al. (2023) propose a similar model where the connectivity parameter is the sum of nodes-pairs contributions, resulting in a model that differs from what could be obtained through a clique reduction graph (namely, the graph obtained from hyperedges transformed into cliques). In both cases, these constraints on the parameters considerably reduce the complexity of the inference procedure which is based on a variational-like approach (but does not rely on an evidence lower bound). We believe that similar techniques\ncould be useful in our case and plan to explore that in future works.\n# 6 Codes availability\nThe algorithm implementation in C++ is available as an R package called HyperSBM at https:// github.com/LB1304/HyperSBM. The Supplementary Material, the files to reproduce the synthetic experiments and the dataset analysis are available at https://github.com/LB1304/Hypergraph-Stochastic-Blockmodel.\n# Acknowledgements\nFunding was provided by the French National Research Agency (ANR) grant ANR-18-CE020010-01 EcoNet. L. Brusa thanks the financial support from the grant \u201cHidden Markov Models for Early Warning Systems\u201d of the University of Milano-Bicocca (2022-ATEQC-0031)\n# References\nAhn, K., K. Lee, and C. Suh (2018). Hypergraph spectral clustering in the weighted stochastic block model. IEEE J Sel Top Signal Process 12(5), 959\u2013974. Allman, E., C. Matias, and J. Rhodes (2011). Parameters identifiability in a class of random graph mixture models. Journal Statist Plann Inf 141, 1719\u20131736. Balasubramanian, K. (2021). Nonparametric modeling of higher-order interactions via hypergraphons. J Mach Learn Res 22(146), 1\u201335. Battiston, F., G. Cencetti, I. Iacopini, V. Latora, M. Lucas, A. Patania, J.-G. Young, and G. Petri (2020). Networks beyond pairwise interactions: Structure and dynamics. Phys Rep 874, 1\u201392. Behrens, S., C. Erbes, M. Ferrara, S. G. Hartke, B. Reiniger, H. Spinoza, and C. Tomlinson (2013). New results on degree sequences of uniform hypergraphs. Electron J Comb 20(4), research paper p14, 18. Bick, C., E. Gross, H. A. Harrington, and M. T. Schaub (2023). What are higher-order networks? SIAM Review 65(3), 686\u2013731. Bickel, P., D. Choi, X. Chang, and H. Zhang (2013, 08). Asymptotic normality of maximum likelihood and its variational approximation for stochastic blockmodels. Ann Statist 41(4), 1922\u20131943. Biernacki, C., G. Celeux, and G. Govaert (2000). Assessing a mixture model for clustering with the Integrated Completed Likelihood. IEEE Trans Pattern Anal Machine Intel 22(7), 719\u2013725.\nCafieri, S., P. Hansen, and L. Liberti (2010). Loops and multiple edges in modularity maximization of networks. Phys Rev E 81, 046102. Celisse, A., J.-J. Daudin, and L. Pierre (2012). Consistency of maximum-likelihood and variational estimators in the stochastic block model. Electron J Statist 6, 1847\u20131899. Cerqueira, A. and F. Leonardi (2020). Estimation of the number of communities in the stochastic block model. IEEE Transactions on Information Theory 66(10), 6403\u20136412. Chelaru, M. I., S. Eagleman, A. R. Andrei, R. Milton, N. Kharas, and V. Dragoi (2021). Highorder correlations explain the collective behavior of cortical populations in executive, but not sensory areas. Neuron 109(24), 3954\u20133961. Chien, I. E., C.-Y. Lin, and I.-H. Wang (2019). On the minimax misclassification ratio of hypergraph community detection. IEEE Trans Inf Theory 65(12), 8095\u20138118. Chiquet, J., S. Donnet, gro\u00dfBM team, and P. Barbillon (2022). sbm: Stochastic Blockmodels. R package version 0.4.4. Chodrow, P. S. (2020). Configuration models of random hypergraphs. J Complex Networks 8(3), cnaa018. Chodrow, P. S., N. Veldt, and A. R. Benson (2021). Generative hypergraph clustering: From blockmodels to modularity. Sci Adv 7(28), eabh1303. Cole, S. and Y. Zhu (2020). Exact recovery in the hypergraph stochastic block model: A spectral algorithm. Linear Algebra Appl 593, 45\u201373. Contisciani, M., F. Battiston, and C. De Bacco (2022). Inference of hyperedges and overlapping communities in hypergraphs. Nat Commun 13, 7229. C\u00f4me, E. and P. Latouche (2015). Model selection and clustering in stochastic block models based on the exact integrated complete data likelihood. Statistical Modelling 15(6), 564\u2013589. Daudin, J.-J., F. Picard, and S. Robin (2008). A mixture model for random graphs. Stat. Comput. 18(2), 173\u2013183. Dempster, A., N. Laird, and D. Rubin (1977). Maximum likelihood from incomplete data via the EM algorithm (with discussion). J R Stat Soc, Ser B 39, 1\u201338. Deng, C., X.-J. Xu, and S. Ying (2024). Strong consistency of spectral clustering for the sparse degree-corrected hypergraph stochastic block model. IEEE Transactions on Information Theory 70(3), 1962\u20131977.\nDumitriu, I., H. Wang, and Y. Zhu (2022). Partial recovery and weak consistency in the nonuniform hypergraph stochastic block model. Technical report, arXiv:2112.11671. Estrada, E. and J. A. Rodr\u00edguez-Vel\u00e1zquez (2006). Subgraph centrality and clustering in complex hyper-networks. Physica A 364, 581\u2013594. Frank, O. and F. Harary (1982). Cluster inference by using transitivity indices in empirical graphs. J Amer Statist Assoc 77(380), 835\u2013840. Ghoshal, G., V. Zlati\u0107, G. Caldarelli, and M. E. J. Newman (2009). Random hypergraphs and their applications. Phys Rev E 79, 066118. Ghoshdastidar, D. and A. Dukkipati (2014). Consistency of spectral partitioning of uniform hypergraphs under planted partition model. In Advances in Neural Information Processing Systems, Volume 27. Ghoshdastidar, D. and A. Dukkipati (2017). Consistency of spectral hypergraph partitioning under planted partition model. Ann Statist 45(1), 289 \u2013 315. Holland, P., K. Laskey, and S. Leinhardt (1983). Stochastic blockmodels: some first steps. Social networks 5, 109\u2013137. Hubert, L. and P. Arabie (1985). Comparing partitions. J Classif 2, 193\u2013218. Jordan, M. I., Z. Ghahramani, T. S. Jaakkola, and L. K. Saul (1999). An introduction to variational methods for graphical models. Mach Learn 37(2), 183\u2013233. Kami\u0144ski, B., V. Poulin, P. Pra\u0142at, P. Szufel, and F. Th\u00e9berge (2019). Clustering via hypergraph modularity. PLoS One 14(11), e0224307. Ke, Z. T., F. Shi, and D. Xia (2020). Community detection for hypergraph networks via regularized tensor power iteration. Technical report, arXiv:1909.06503. Kruskal, J. (1977). Three-way arrays: rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear Algebra Appl 18(2), 95\u2013138. Leordeanu, M. and C. Sminchisescu (2012). Efficient hypergraph clustering. In N. D. Lawrence and M. Girolami (Eds.), Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, Volume 22 of Proceedings of Machine Learning Research, pp. 676\u2013 684. PMLR. Massen, C. P. and J. P. K. Doye (2005). Identifying communities within energy landscapes. Phys Rev E 71, 046101.\nMatias, C. and S. Robin (2014). Modeling heterogeneity in random graphs through latent space models: a selective review. ESAIM, Proc Surv 47, 55\u201374. Muyinda, N., B. De Baets, and S. Rao (2020). Non-king elimination, intransitive triad interactions, and species coexistence in ecological competition networks. Theor Ecol 13, 385\u2013397. Newman, M. E. J. (2016). Equivalence between modularity optimization and maximum likelihood methods for community detection. Phys Rev E 94, 052315. Ng, T. and T. Murphy (2022). Model-based clustering for random hypergraphs. Adv Data Anal Classif 16, 691\u2013723. Poda, V. and C. Matias (2024). Comparison of modularity-based approaches for nodes clustering in hypergraphs. Peer Community Journal 4. Rohe, K., S. Chatterjee, and B. Yu (2011). Spectral clustering and the high-dimensional stochastic blockmodel. Ann Statist 39(4), 1878 \u2013 1915. Ruggeri, N., M. Contisciani, F. Battiston, and C. D. Bacco (2023). Community detection in large hypergraphs. Science Advances 9(28), eadg9159. Singh, P. and G. Baruah (2021). Higher order interactions and species coexistence. Theor Ecol 14, 71\u201383. Squartini, T. and D. Garlaschelli (2011). Analytical maximum-likelihood method to detect patterns in real networks. New J Phys 13(8), 083001. Stephan, L. and Y. Zhu (2022). Sparse random hypergraphs: Non-backtracking spectra and community detection. In 2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS), pp. 567\u2013575. Swan, M. and J. Zhan (2021). Clustering hypergraphs via the MapEquation. IEEE Access 9, 72377\u201372386. Torres, L., A. S. Blevins, D. Bassett, and T. Eliassi-Rad (2021). The why, how, and when of representations for complex systems. SIAM Rev 63(3), 435\u2013485. Turnbull, K., S. Lunag\u00f3mez, C. Nemeth, and E. Airoldi (2021). Latent space modelling of hypergraph data. Technical report, arXiv:1909.00472. van Handel, R. (2011). On the minimal penalty for Markov order estimation. Probability Theory and Related Fields 150(3), 709\u2013738. Vazquez, A. (2009). Finding hypergraph communities: a Bayesian approach and variational solution. J Stat Mech: Theory Exp 2009(07), P07006.\nWolff, K. H. (1950). The sociology of Georg Simmel. The free press, New York.\n# Supplementary Material to: Model-based clustering in simple hypergraphs through a stochastic blockmodel By Luca Brusa & Catherine Matias\nA.1 Bipartite graphs and multiple hypergraphs with self-loops equivalence\nSome early analyses of hypergraphs rely on the embedding of the former into the space of bipartite graphs (see for e.g. Battiston et al., 2020). Indeed, any hypergraph H = (V, E) where V is the set of nodes and E the set of hyperedges may be represented as a bipartite graph with two parts. The top part is simply the set V of hypergraph nodes, while the bottom part is the set E of hyperedges and there is a link between v \u2208V and e \u2208E whenever node v belongs to hyperedge e in the original hypergraph H. Now, it is possible to define a \u201cconverse\u201d application from bipartite graphs to hypergraphs. Indeed, any bipartite graph can be projected into two distinct hypergraphs, by choosing one of the two parts as the nodes set and forming a hyperedge with any set of nodes that are neighbors (in the bipartite graph) of the same node (belonging to the second part). A major difference appears whether we consider simple hypergraphs or multiple hypergraphs with self-loops. In multiple hypergraphs (not to be confused with multiset hypergraphs) hyperedges may appear several time so that these are weighted hypergraphs with integer valued weights. We also allow for self-loops, i.e hyperedges of cardinality 1. Then, this application from bipartite graphs to hypergraphs slightly differs depending on whether we allow the image of a bipartite graph to be a multiple hypergraphs with self-loops or a simple hypergraph. In the first case, all the information from the bipartite graph will be encoded in the multiple hypergraphs with self-loops; while in the second case, part of the information will be lost. This is illustrated on a toy example in Figure 6. The embedding of the simple hypergraphs space into the bipartite graphs space is not the inverse of the natural projection of bipartite graphs into simple hypergraphs. Thus, models of bipartite graphs are inappropriate to handle simple hypergraphs, as the former generally put mass on any bipartite graph, notwithstanding the fact that not all of these may be realized as the image of a simple hypergraph. For the same reason, preferential attachment models of bipartite graphs (Guillaume and Latapy, 2004) may not be directly used for simple hypergraphs as they would produce unconstrained bipartite graphs that do not necessarily come from simple hypergraphs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4783/47831104-3578-4f99-b6d7-2269e25a1bbd.png\" style=\"width: 50%;\"></div>\nFigure 6: (a) A bipartite graph G; (b) Projection of G into the multiple hypergraphs with self-loops space, choosing the top nodes as the new set of nodes. Hyperedges are {a}, {a, b}, {a, b}, {a, b, c}. The applications from (a) to (b) are invertible bijections, one being the inverse of the other; (c) Projection of G on the simple hypergraphs subspace. Hyperedges are {a, b}, {a, b, c}. (d) Embedding of the simple hypergraph in (c) in the bipartite graphs space. Note that (a) and (d) are not the same bipartite graph.\n(c)\nIn order to view a bipartite graph as a hypergraph, one first needs to select the top and bottom parts. Swapping the role of the two parts will in general give another hypergraph. Statistical models of bipartite graphs handle the two parts symmetrically and do not differentiate between a top and a bottom part. They are thus inadequate for modeling hypergraphs. One may also note that most random bipartite graphs models are designed for fixed parts sizes, which induces, on top of a fixed number of nodes, a fixed number of hyperedges in the corresponding hypergraph model, an artifact which is not always desirable. For instance the uniformly random hypergraphs model allows for any possible density on the hyperedges. A last example of inadequacy is given by configuration models on bipartite graphs that induce configuration models on hypergraphs. In these models, the degree distributions in each part are kept fixed. When projected in the hypergraphs space, that means that the degrees of the nodes and the sizes of the hyperedges are kept fixed. Then, relying on shuffling algorithms to explore the space of this configuration model, one will loose the labels on the bottom part (the hyperedges part) as these are automatically induced by the new edges of the bipartite graph and the labelling of the top part (the nodes part). As a consequence, if a specific node tends to take part in large size hyperedges, this information is lost in the configuration model issued from bipartite graphs. To our knowledge, there is no configuration model on hypergraphs that only keeps the nodes degrees sequence fixed. We mention that Section 4 from Chodrow (2020) provides a discussion about the limitations of the embedding approach in terms of the types of hypergraph null models from which we can conveniently sample. In particular, Chodrow (2020) establishes that there is no obvious route for vertex-label sampling in hypergraphs through bipartite random graphs.\n# A.3 HSBM is not a bipartite SBM\nIn this section, we briefly outline that (i) while the bipartite stochastic blockmodel can be seen as a particular case of HSBM, (ii) the converse is not true in",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of clustering nodes in simple hypergraphs, which has been largely overlooked in the statistical literature despite the increasing relevance of hypergraphs in modeling high-order interactions. Previous methods, primarily based on pairwise interactions in graphs, have not adequately captured the complexities introduced by hyperedges, necessitating a new approach to effectively analyze such data.",
        "problem": {
            "definition": "The paper aims to solve the problem of identifying latent groups of nodes in simple hypergraphs, where hyperedges can connect more than two nodes, and each node appears only once in any hyperedge.",
            "key obstacle": "Existing methods fail to effectively capture the structure of simple hypergraphs due to their reliance on pairwise interactions, which do not account for the high-order correlations present in hypergraph data."
        },
        "idea": {
            "intuition": "The idea stems from the recognition that high-order interactions in data, such as those found in social networks and ecological systems, require a more complex modeling framework than traditional graph-based methods.",
            "opinion": "The proposed model generalizes the stochastic blockmodel to hypergraphs, allowing for the identification of latent node groups while assuming that hyperedges are conditionally independent given these groups.",
            "innovation": "The primary innovation lies in the introduction of a stochastic blockmodel specifically designed for simple hypergraphs, addressing the limitations of existing methods by allowing for the analysis of high-order interactions."
        },
        "method": {
            "method name": "Hypergraph Stochastic Block Model (HSBM)",
            "method abbreviation": "HSBM",
            "method definition": "HSBM is a statistical model that identifies latent groups of nodes in simple hypergraphs by assuming that hyperedges are formed independently based on these latent group memberships.",
            "method description": "The method utilizes a variational Expectation-Maximization algorithm for parameter inference and clustering of nodes within hypergraphs.",
            "method steps": [
                "Formulate the hypergraph and define the latent group structure.",
                "Apply the variational EM algorithm to maximize the evidence lower bound.",
                "Estimate model parameters and classify nodes into groups based on their probabilities of belonging to each latent group."
            ],
            "principle": "The effectiveness of HSBM in solving the problem is supported by its ability to model the probability of hyperedge formation based on latent group memberships, capturing the complexities of high-order interactions."
        },
        "experiments": {
            "evaluation setting": "The performance of HSBM was evaluated using synthetic hypergraphs generated under the model with varying numbers of latent groups and compared against existing clustering methods such as hypergraph spectral clustering.",
            "evaluation method": "The evaluation involved measuring clustering accuracy using the Adjusted Rand Index (ARI) and assessing parameter estimation accuracy through the Mean Squared Relative Error (MSRE)."
        },
        "conclusion": "The experiments demonstrated that HSBM outperforms existing methods in accurately recovering true clustering structures in hypergraphs, highlighting its potential for broader applications in analyzing complex data with high-order interactions.",
        "discussion": {
            "advantage": "HSBM effectively captures the latent structure in hypergraphs, providing a flexible framework for modeling high-order interactions that traditional methods cannot adequately address.",
            "limitation": "The computational complexity of HSBM can be prohibitive for large datasets, particularly when the maximum hyperedge size is large, necessitating careful consideration of model parameters.",
            "future work": "Future research may focus on reducing the complexity of the approach and exploring extensions to handle weighted hypergraphs and additional covariates."
        },
        "other info": {
            "info1": "The R package HyperSBM implementing the proposed method is available online.",
            "info2": {
                "info2.1": "Funding was provided by the French National Research Agency grant ANR-18-CE020010-01 EcoNet.",
                "info2.2": "Additional supplementary material is available, containing proofs and further experimental details."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of clustering nodes in simple hypergraphs, highlighting the need for a new approach to analyze data with high-order interactions."
        },
        {
            "section number": "1.2",
            "key information": "The motivation behind the development of the Hypergraph Stochastic Block Model (HSBM) stems from the inadequacy of existing methods that rely on pairwise interactions, failing to capture the complexities of hypergraphs."
        },
        {
            "section number": "1.3",
            "key information": "The main objective of the paper is to identify latent groups of nodes in simple hypergraphs using the proposed HSBM."
        },
        {
            "section number": "2.1",
            "key information": "Key terms defined in the paper include 'simple hypergraphs,' which are structures where hyperedges can connect more than two nodes and each node appears only once in any hyperedge."
        },
        {
            "section number": "3.1",
            "key information": "The proposed HSBM generalizes the stochastic blockmodel to hypergraphs, enabling the identification of latent node groups while assuming hyperedges are conditionally independent given these groups."
        },
        {
            "section number": "4.1",
            "key information": "The paper emphasizes the importance of accurately capturing the latent structure in hypergraphs, which is critical for effective data labeling and clustering."
        },
        {
            "section number": "7.1",
            "key information": "The paper discusses the computational complexity of HSBM as a challenge, particularly for large datasets, indicating the need for future work to reduce this complexity."
        },
        {
            "section number": "8",
            "key information": "The conclusion highlights that HSBM outperforms existing methods in accurately recovering true clustering structures in hypergraphs, underscoring its potential for broader applications."
        }
    ],
    "similarity_score": 0.5800522687071505,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Model-based clustering in simple hypergraphs through a stochastic blockmodel.json"
}