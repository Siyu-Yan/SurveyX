{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.02800",
    "title": "Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation",
    "abstract": "A common scenario of Multilingual Neural Machine Translation (MNMT) is that each translation task arrives in a sequential manner, and the training data of previous tasks is unavailable. In this scenario, the current methods suffer heavily from catastrophic forgetting (CF). To alleviate the CF, we investigate knowledge distillation based life-long learning methods. Specifically, in one-tomany scenario, we propose a multilingual distillation method to make the new model (student) jointly learn multilingual output from old model (teacher) and new task. In many-to one scenario, we find that direct distillation faces the extreme partial distillation problem, and we propose two different methods to address it: pseudo input distillation and reverse teacher distillation. The experimental results on twelve translation tasks show that the proposed methods can better consolidate the previous knowledge and sharply alleviate the CF.",
    "bib_name": "zhao2022lifelonglearningmultilingualneural",
    "md_text": "# Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation\nYang Zhao1,2, Junnan Zhu1,2, Lu Xiang1,2, Jiajun Zhang\nYu Zhou1,3, Feifei Zhai1,3, and Chengqing Zong1,2 National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China 2University of Chinese Academy of Sciences, Beijing, China 3Fanyu AI Research, Beijing Fanyu Technology Ltd., Beijing, China {yang.zhao, junnan.zhu, lu.xiang, jjzhang, yzhou, cqzong} @nlpr.ia.ac.cn feifeizhai@zkyf.com\nA common scenario of Multilingual Neural Machine Translation (MNMT) is that each translation task arrives in a sequential manner, and the training data of previous tasks is unavailable. In this scenario, the current methods suffer heavily from catastrophic forgetting (CF). To alleviate the CF, we investigate knowledge distillation based life-long learning methods. Specifically, in one-tomany scenario, we propose a multilingual distillation method to make the new model (student) jointly learn multilingual output from old model (teacher) and new task. In many-toone scenario, we find that direct distillation faces the extreme partial distillation problem, and we propose two different methods to address it: pseudo input distillation and reverse teacher distillation. The experimental results on twelve translation tasks show that the proposed methods can better consolidate the previous knowledge and sharply alleviate the CF.\narXiv:2212.02800v1\n# 1 Introduction\nRecently, multilingual neural machine translation (MNMT) (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018; Aharoni et al., 2019) draws much attention due to its remarkable improvement on low-resource language pairs and simple implementation, especially the approach with one universal encoder and decoder (Johnson et al., 2017; Aharoni et al., 2019). The current MNMT methods are studied in the conventional setting that bilingual pairs for all the translation tasks are available at training time. In practice, however, we always face an incremental scenario where each task arrives in a sequential manner. Assuming that we have already built an MNMT model (old model) from English to Italy and Dutch (EN\u21d2IT, NL), and we hope to extend the model with English-to-Romanian (EN\u21d2RO)\nModel\nEN\u21d2IT\nEN\u21d2NL\nEN\u21d2RO\nInitial\n28.11\n29.79\n\u223c\nFine-tuning\n1.08\n0.99\n25.96\nJoint training\n30.46\n31.48\n27.55\nTable 1: The BLEU scores of fine-tuning and joint training method. Fine-tuning method suffers from CF.\ntranslation. Generally, two basic methods can be adopted: i) Fine-tuning. We can fine-tune the old system with the new translation data.This method suffers from severe degradation on previous translation tasks, this phenomenon is known as Catastrophic Forgetting (CF) (McCloskey and Cohen, 1989). Table 1 shows the results, where after fine-tuning with EN\u21d2RO task, BLEU scores of the previous tasks sharply drop from 28.51 to 1.08 (EN\u21d2IT) and from 29.79 to 0.99 (EN\u21d2NL), respectively. ii) Joint training. We can train a new task jointly with the previous and new training data. This method can achieve good performance. While as the number of translation tasks grows, storing and retraining on all training data becomes infeasible and cumbersome (Li and Hoiem, 2017). More seriously, in many cases, the training data for previously learned tasks is unavailable due to the data privacy and protection (Shokri and Shmatikov, 2015; Yang et al., 2019), and it is impossible to jointly train an MNMT model under this situation. Life-long learning aims at adapting a learned model to a new task while retaining the previous knowledge without accessing the previous training data. Among them, knowledge distillation based methods (Li and Hoiem, 2017; Hou et al., 2018; Belouadah and Popescu, 2019) are very common ways. In these methods when a new task arrives, the new model (student) is jointly learned by the old model\u2019s output (teacher) and new task. However, these methods are specifically designed for image classification (Li and Hoiem, 2017; Hou\net al., 2018; Aharoni et al., 2019), or object detection (Shmelkov et al., 2017), the incremental MNMT scenarios are not studied. Therefore, in this paper, we focus on the lifelong learning for incremental MNMT, and the scenarios we study here are setting as follows: i) Each task arrives in a sequential manner. ii) Training data of previous tasks is unavailable. We can only access the training data of new translation task and a learned old MNMT model. iii) A single MNMT model needs to perform well on all tasks after learning a new one. Specifically, two common incremental MNMT scenarios are considered: Incremental one-to-many scenario: An MNMT model incrementally learns to translate one same source language into different target languages. Incremental many-to-one scenario: An MNMT model incrementally learns to translate different source languages into one same target language1. In incremental one-to-many scenario, we propose a multilingual distillation method, in which the old model is treated as a teacher and the new model is treated as a student. To distillate the multilingual knowledge in the teacher, we first add the corresponding indicator of learned languages in the beginning of source sentence, which then be fed into the teacher model to get the multilingual outputs. Finally, the student model is jointly learned by the new task and multilingual distillation results. In incremental many-to-one scenario, we find that direct distillation faces the Extreme Partial Distillation problem: Extreme Partial Distillation: Given an old many-to-one model (such as IT, NL\u21d2EN), and a new task (such as RO\u21d2EN), if we treat the old model as a teacher and directly input the new source sentences (RO) into it, the teacher model actually is fed by a sentence filled with UNKs due to the UNK replacing strategy2. Ideally, we hope that the student model could learn the knowledge from teacher on various tokens (whole knowledge) of previous source languages (IT,NL). While in this situation, the student model can only learn from teacher how to handle UNKs (partial knowledge).\nTherefore, we define this problem as extreme partial distillation. To address this problem in many-to-one scenario, we propose two methods: 1) pseudo input distillation, and 2) reverse teacher distillation. In the former one, we still utilize the old many-to-one MNMT model as a teacher. While instead of directly inputting the new source sentences (RO) into it, we first construct pseudo inputs by replacing the new tokens (RO) with learned tokens (IT,NL) via a frequency mapping. Then pseudo inputs are utilized to distillate the knowledge from teacher. In the later one, we utilize the reversed one-tomany model (EN \u21d2IT,NL) as a teacher. When a new task arrives (RO\u21d2EN), we input the target language (EN) into the teacher and get the multilingual source outputs (IT,NL). We test the proposed methods on twelve different translation tasks. The experimental results show that the proposed methods can sharply alleviate the CF. The contributions of this paper are listed as follows: i) We focus on the incremental MNMT scenario and investigate the knowledge distillation based life-long learning method. ii) In one-to-many scenario, we propose a multilingual distillation method to make the new model jointly learn multilingual output from old model and new task. iii) In many-to-one scenario, we find that direct distillation faces the extreme partial distillation problem, and propose two different methods (pseudo input distillation and reverse teacher distillation) to address it.\n# 2 Multilingual NMT\nTo make full use of multilingual data within a single system, various MNMT methods are proposed (Firat et al., 2016; Johnson et al., 2017; Gu et al., 2018), where (Johnson et al., 2017) propose a simple while effective MNMT method. In this method, it is no need to change the network architecture. The only modification is that they introduce a special indicator at the beginning of the source sentence to indicate source and target language. For example, consider the following English-toItaly sentence pair: you probably saw it on the news . \u2192forse lo avete visto sui notiziari . It will be modified to: <en2it> you probably saw it on the news . \u2192\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b9d/4b9d33c8-12ab-4a17-8065-5234752d3a8c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(c)  Reverse Teacher Distillation</div>\nFigure 1: The framework of proposed methods, where multilingual distillation (a) is proposed for incremental one to-many scenario. Pseudo input distillation (b) and reverse teacher distillation (c) are proposed for incremental many-to-one scenario.\nforse lo avete visto sui notiziari . where <en2it> is an indicator to show that the source is English and the target is Italy. Notation: We denote a one-to-many MNMT model by \u03b8(X\u21d2Y1,...,Yn), where X is a source language, and Y1, ..., Yn denotes n different target languages. Similarly, a many-to-one MNMT model is denoted by \u03b8(X1,...,Xn\u21d2Y). We denote a translation task from X to Y by X \u21d2Y, whose training sentence pairs are denoted by DX\u21d2Y = {(X, Y )}, where X is the source sentence and Y is the target sentence. When we adding a indicator <X2Yj> for target language Yj into a source sentence X, we denote the source sentence by X+<X2Yj>.\n# 3 Method Description\nIn incremental one-to-many scenario, given an old one-to-many model \u03b8(X\u21d2Y1,...,Yn) and the training data DX\u21d2Yn+1 = {(X, Yn+1)} of a new task X \u21d2Yn+1, our goal is to get a new model \u03b8(X\u21d2Y1,...,Yn+1). To achieve this, we propose a multilingual distillation method to let the new model (student) jointly learn the new task and multilingual knowledge in the old model (teacher). Fig. 1 (a) illustrates the framework, which contains three\nsteps: Step 1: For each learned target language Yi \u2208 (Y1, ..., Yn), we first add the indicator <X2Yi> into source sentence X. We denote the source sentence with indicator by X+<X2Yi>. Step 2: For each X+<X2Yi>, i \u2208[1, n], we input it into the old model \u03b8(X\u21d2Y1,...,Yn) (teacher) and get the corresponding result Yi with beam search:\nStep 3: Train a new model \u03b8(X\u21d2Y1,...,Yn+1 (student) by maximizing the following objective function:\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd where the first n items denote the loss of previous tasks produced by the teacher. The last one denotes the loss of new task. Discussion. We call this method as multilingual distillation, since the new model could learn multilingual knowledge in the old model. During distillation, we can also utilize the k-best sentence distillation (k > 1) (Kim and Rush, 2016; Tan et al., 2019) or greedy search distillation. The experimental results can be found in Sec. 5.1.\n# 3.2 Incremental Many-to-one Scenario\nIn this scenario, our goal is to get a new model \u03b8(X1,...,Xn+1\u21d2Y) with an old many-to-one model \u03b8(X1,...,Xn\u21d2Y) and training sentence pairs DXn+1\u21d2Y = {(Xn+1, Y )} of a new task Xn+1 \u21d2 Y. As we mentioned before, direct distillation faces the extreme partial distillation problem. To address this, we propose two different methods: 1) pseudo input distillation (Fig. 1 (b)), and 2) reverse teacher distillation (Fig. 1 (c)).\n# 3.2.1 Pseudo Input Distillation\nIn this method, instead of inputting Xn+1 into the teacher, we first construct a pseudo input by transforming the new token of Xn+1 into the previous tokens. Then the pseudo input is utilized to distillate the knowledge. Specifically, this method contains four steps: Step 1: For each source language Xi \u2208 (X1, ..., Xn+1), we sort its vocabulary VXi in descending order by the frequency as follows:\n(3)\nN(sj Xi) represents the frequency of token sj Xi in the corresponding training data, and j is the frequency ranking of this token in all vocabulary. Then we can construct a mapping between a new token sj Xn+1 and a learned token sj Xi, if these two tokens have the same ranking j. Formally,\n(4)\nwhere sj Xn+1 is a new token in language Xn+1 which ranked jth in VXn+1 and sj Xi is a learned token in language Xi which also ranked jth in VXi Step 2: Given a new source sentence Xn+1, we replace the tokens in Xn+1 with learned tokens in languages Xi with the mapping M(Xn+1\u2192Xi) (Eq. (4)) by\n(5)\nwhere Xp i is the pseudo input, which contains the tokens of language Xi. Step 3: For each pseudo input Xp i \u2208 (Xp 1, ..., Xp n), we input it into the old model \u03b8(X1,...,Xn\u21d2Y) (teacher) and get the distillation result Yi with beam search:\n(6)\nStep 4: Train a new model \u03b8(X1,...,Xn+1\u21d2Y) (student) by maximizing the following objective function:\n(7)\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd where the first n items denote the loss of previous tasks produced by the teacher. The last one denotes the loss of new task. Discussion. This pseudo input distillation could alleviate extreme partial distillation problem, since the pseudo inputs Xp i contains the various tokens of previous learned language. Thus, when we utilize Xp i as the distillation input, the student model could learn from teacher that how to translate these tokens while not just unks. Meanwhile, the additional cost of this method is small, we only need to maintain a sorted vocabulary in descending order for each learned source languages.\n# 3.2.2 Reverse Teacher Distillation\nIn reverse teacher distillation, beside the old manyto-one model \u03b8(X1,...,Xn\u21d2Y), we also need a reverse one-to-many model \u03b8(Y\u21d2X1,...,Xn) at the same time. To alleviate the extreme partial distillation problem, when the training data of a new task DXn+1\u21d2Y = {(Xn+1, Y )} arrives, we treat the one-to-many model \u03b8(Y\u21d2X1,...,Xn) as a teacher, and input the target sentence Y in it. Specifically, this method contains four steps: Step 1: For each learned source languages Xi \u2208 (X1, ..., Xn), we first add the indicator <Y2Xi> into target sentence Y , and denote the target sentence with indicator by Y +<Y2Xi>. Step 2: For target sentence with indicator Y +<Y2Xi> \u2208(Y +<Y2X1>, ..., Y +<Y2Xn>), we input it into the reverse one-to-many model \u03b8(Y\u21d2X1,...,Xn) (reverse teacher) and get the distillation result Xi with beam search:\nStep 3: Train a new many-to-one model (student) \u03b8(X1,...,Xn+1\u21d2Y) by maximizing the following objective function:\n(9)\nwhere the first n items denote the loss of previous tasks produced by the reverse teacher. The last one denotes the loss of new task. Step 4: Train a new reverse one-to-many model (reverse student) \u03b8(Y\u21d2X1,...,Xn+1) by maximizing the following objective function:\n(10)\n\ufffd \ufffd\ufffd \ufffd \ufffd \ufffd\ufffd \ufffd We also need to update the reverse student, since it will be utilized as a reverse teacher when the next task Xn+2 \u21d2Y arrives. Discussion. This reverse teacher distillation could alleviate extreme partial distillation problem, since we utilize the reverse one-to-many model as a teacher and input the target into it. This idea is partially inspired by the back-translation (Sennrich et al., 2016a) and generative replay methods in lifelong learning (Shin et al., 2017; Zhai et al., 2019). Meanwhile, the additional cost of this method is acceptable. We need to maintain a revered one-tomany model when a new task arrives.\n# 4 Experimental Setting\nDataset. We test the proposed methods on 12 tasks (Table 2), where English-Italian (EN\u21d4IT), English-Dutch (EN\u21d4NL) and English-Romanian (EN\u21d4RO) come from TED dataset3. UygurChinese (UY\u21d4CH), Tibetan-Chinese (TI\u21d4CH) and Mongolian-Chinese (MO\u21d4CH) come from CCMT-19 dataset. Chinese-English (CH\u21d4EN) is LDC dataset. Japanese-English (JA\u21d4EN) is KFTT dataset4. German-English (DE\u21d4EN), FinnishEnglish (FI\u21d4EN), Latvian-English (LV\u21d4EN) and Turkish-English (TR\u21d4EN) come from WMT-17 dataset5. Training and Evaluation Details. We implement our approach based on the THUMT toolkit (Zhang et al., 2017)6. We use the \u201cbase\u201d parameters in Transformer (Vaswani et al., 2017). We use the BPE (Sennrich et al., 2016b) method to merge 30K steps. For evaluation, we use beam search with a beam size of k = 4 and length penalty. We evaluate the translation quality with BLEU (Papineni et al., 2002) for all tasks. For 3https://wit3.fbk.eu/ 4http://www.phontron.com/kftt/ 5http://data.statmt.org/wmt17/ translation-task/preprocessed/ 6https://github.com/THUNLP-MT/THUMT\nDataset\nTask\nTrain\nDev\nTest\nTED\nEN\u21d4IT\n232k\n929\n1566\nEN\u21d4NL\n237k\n1003\n1777\nEN\u21d4RO\n221k\n914\n1678\nCCMT-19\nMO\u21d4CH\n254k\n2000\n1000\nTI\u21d4CH\n155k\n2000\n1000\nUY\u21d4CH\n168k\n2000\n1000\nLDC\nCH\u21d4EN\n2.1M\n919\n6146\nKFTT\nJA\u21d4EN\n440k\n1166\n1160\nWMT-17\nDE\u21d4EN\n5.9M\n3003\n5168\nFI\u21d4EN\n2.6M\n2870\n3000\nLV\u21d4EN\n4.5M\n1000\n1003\nTR\u21d4EN\n207K\n2870\n3000\neach task, we set both source and target vocabularies by 30K. When learning a new task, the new vocabularies are the union of previous vocabularies and vocabularies of new arrival task, i.e., V(X1,...,Xn\u21d2Y) = V(X1,...,Xn\u22121\u21d2Y) \u222aV(Xn\u21d2Y). Comparing methods: We compare the proposed models against the following systems: i) Single: We train each translation task with each single model by using transformer. ii) Joint Training: We implement the joint training method (Johnson et al., 2017) as an upper bound of proposed life-long learning methods. iii) Fine-tuning: We fine-tune the old system with the new training data of a new task. iv) EWC: This is the Elastic Weight Consolidation (EWC) model (Kirkpatrick et al., 2017). The approach injects a penalty on the difference between the parameters for the old and the new tasks into the loss function to alleviate the CF. v) Multi-Distill: This is the proposed Multilingual Distillation in one-to-many scenario. MultiDistill (greedy) and Multi-Distill (beam) denote that during distillation, we utilize the greedy search and beam search, respectively. vi) Direct-Distill: In this method, we directly utilize the new source as distillation input. vii) PseudoInput-Distill: This is the proposed Pseudo Input Distillation in many-to-one scenario. viii) ReverseTeacher-Distill: This is the proposed Reverse Teacher Distillation in many-to-one scenario.\n# 5 Experimental Results\n# 5.1 Incremental One-to-many Scenario\nMain results. Table 3 shows the translation results in incremental one-to-many scenario, where line 18 report the results that the first task is EN\u21d2IT, the second one is EN\u21d2NL, and the last one is EN\u21d2RO.\n<div style=\"text-align: center;\">BLEU-task1 BLEU-task2 BLEU-task3 BLEU-avg \u25b3</div>\n#\nModel\nBLEU-task1\nBLEU-task2\nBLEU-task3\nBLEU-avg\n\u25b3\nEN\u21d2IT \u2192EN\u21d2NL \u2192EN\u21d2RO\n1\nSingle\n27.76\n28.64\n25.27\n27.22\n\u223c\n2\nJoint Training\n30.46\n31.48\n27.55\n29.83\n+2.61\n3\nFine-tuning\n1.05\n0.82\n25.82\n9.23\n\u221217.99\n4\nEWC\n10.32\n11.17\n22.31\n14.60\n\u221212.62\n5\nMulti-Distill (greedy)\n29.00\u2217\n28.97\n26.49\u2217\n28.15\n+0.93\n6\nMulti-Distill (beam)\n30.31\u2217\n30.11\u2020\n26.86\u2217\n29.09\n+1.87\n7\nMulti-Distill (2-best)\n30.10\u2217\n30.37\u2020\n27.18\u2217\n29.22\n+2.00\n8\nMulti-Distill (4-best)\n30.51\u2217\n30.52\u2217\n26.81\u2217\n29.28\n+2.06\nCH\u21d2MO \u2192CH\u21d2TI \u2192CH\u21d2UY\n9\nSingle\n29.19\n29.67\n16.20\n25.02\n\u223c\n10\nJoint Training\n28.97\n29.48\n17.51\n25.32\n+0.30\n11\nFine-tuning\n1.04\n0.93\n16.67\n6.21\n\u221218.81\n12\nEWC\n11.32\n12.43\n15.53\n13.09\n\u221211.93\n13\nMulti-Distill (greedy)\n28.88\n29.30\n16.52\n24.90\n\u22120.12\n14\nMulti-Distill (beam)\n29.11\n29.44\n16.79\u2020\n25.11\n+0.09\n15\nMulti-Distill (2-best)\n29.34\n29.39\n16.90\u2217\n25.21\n+0.19\n16\nMulti-Distill (4-best)\n29.50\u2020\n29.51\n16.98\u2217\n25.33\n+0.31\nTable 3: The translation results in incremental one-to-many scenario. BLEU-task1, BLEU-task2 and BLEUtask3 show the BLEU scores of the corresponding three tasks. BLEU-avg is the average BLEU scores. \u25b3is the improvement comparing with single method. \u201c\u2020\u201d indicates that the proposed system is statistically significant better (p < 0.05) than the single system and \u201c*\u201d indicates p < 0.01.\nLine 9-16 report the results that the first task is CH\u21d2MO, the second one is CH\u21d2TI, and the last one is CH\u21d2UY. From the results, we can reach the following conclusions: i) Fine-tuning method suffers heavily from CF problem. Compared with the single method, the average BLEU scores sharply dropped to 9.23 (line 3) and 6.21 (line 11), respectively. EWC method can partially alleviate CF, while the average BLEU scores remain below the single method by 12.62 (line 4) and 11.93 (line 12) , respectively. ii) The proposed multilingual distillation method can sharply alleviate the CF. After learning three continuous tasks, the average BLEU points are 29.09 (line 6) and 25.11 (line 14), respectively. Compared with the single method, its improvement can reach to 1.87 and 0.09 BLEU points. iii) We also investigate the k-best distillation in our method. We can find that k-best distillation (k = 2 and 4) can only slightly improve the current 1-best distillation. Meanwhile, we can also find that during distillation, the beam search distillation (line 6 and 14) can achieve better results than greedy search distillation (line 5 and 13). Results on an already existing MNMT model. We also conduct an experiment on the basis of an existing MNMT model. The existing model is trained by EN\u21d2IT, NL, RO, JA, DE, FI, LV, TR tasks, and the new task is EN\u21d2CH. Table 4 lists the results, where the initial average BLEU score of learned tasks is 23.68. After learn-\nModel\nBLEU-prev\nBLEU-new\nExisting\n23.68\n\u223c\nSingle\n23.49\n21.14\nJoint Training\n23.94\n20.88\nFine-tuning\n1.07\n21.21\nMulti-Distill\n23.80\n21.19\nTable 4: The translation results of an already existing on-to-many model. The existing model is trained by EN\u21d2IT, NL, RO, JA, DE, FI, LV, TR tasks, and the new task is EN\u21d2CH.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d7de/d7def9b4-e8f0-456d-a177-5987b121ec07.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">0 2 4 6 8 16 12 14 2 4 6 8 10 14 Learning with task 1  12 10 3 10 \uf0b4 Learning with task 2 (EN  IT) (EN  NL) 16</div>\nFigure 2: The BLEU scores during training in one-tomany scenario. x axis denotes the training epoch and y axis denotes the BLEU scores of development set.\ning EN\u21d2CH, the multilingual distillation improves the BLEU score of learned tasks to 23.80 and new task to 21.19. Compared with the joint training, it achieves comparable results (23.94 vs. 23.80 and 20.88 vs. 21.19). The results show that the proposed method can consolidate the previous knowledge when learning a new task. Results during training. We are also curious\n#\nModel\nBLEU-task1\nBLEU-task2\nBLEU-task3\nBLEU-avg\n\u25b3\nIT\u21d2EN \u2192NL\u21d2EN \u2192RO\u21d2EN\n1\nSingle\n30.15\n32.50\n30.86\n31.17\n\u223c\n2\nJoint Training\n32.04\n34.62\n33.49\n33.38\n+2.21\n3\nFine-tuning\n9.77\n14.30\n29.51\n17.86\n\u221213.31\n4\nEWC\n21.29\n24.32\n28.07\n24.56\n\u22126.61\n5\nDirect-Distill\n1.12\n1.43\n29.43\n10.66\n\u221220.51\n6\nPseudoInput-Distill (greedy)\n27.01\n29.33\n31.86\u2217\n29.40\n\u22121.77\n7\nReverseTeacher-Distill (greedy)\n31.04\u2217\n33.87\u2217\n32.75\u2217\n32.55\n+1.38\n8\nPseudoInput-Distill (beam)\n27.93\n30.01\n32.27\u2217\n30.07\n\u22121.10\n9\nReverseTeacher-Distill (beam)\n31.80\u2217\n34.32\u2217\n32.95\u2217\n33.02\n+1.85\nMO\u21d2CH \u2192TI\u21d2CH \u2192UY\u21d2CH\n10\nSingle\n40.60\n32.79\n22.62\n32.00\n\u223c\n11\nJoint Training\n42.30\n31.89\n24.10\n32.76\n+0.76\n12\nFine-tuning\n10.59\n11.05\n22.49\n14.71\n\u221217.29\n13\nEWC\n24.98\n21.38\n21.33\n22.56\n\u22129.44\n14\nDirect-Distill\n1.09\n2.11\n20.98\n8.06\n\u221223.94\n15\nPseudoInput-Distill (greedy)\n36.42\n28.77\n23.17\n29.45\n\u22122.55\n16\nReverseTeacher-Distill (greedy)\n39.52\n31.89\n23.29\u2020\n31.57\n\u22120.43\n17\nPseudoInput-Distill (beam)\n38.01\n30.19\n23.30\u2020\n30.50\n\u22121.50\n18\nReverseTeacher-Distill (beam)\n42.04\u2217\n32.14\n24.01\u2217\n32.73\n+0.73\nTable 5: The translation results in incremental many-to-one scenario. \u201c\u2020\u201d indicates that the proposed system i tatistically significant better (p < 0.05) than the single system and \u201c*\u201d indicates p < 0.01.\nabout the results at each training epoch. Thus we record the BLEU scores of development set during training, where the first task is EN\u21d2IT and the second one is EN\u21d2NL. Fig. 2 reports the results, where x axis denotes the training epoch and y axis denotes the BLEU scores of development set. The results show that when we fine-tune the learned model (\u03b8EN\u21d2IT) with EN\u21d2NL task, the BLEU score of the first one sharply drops to 0.99. The proposed multilingual distillation can alleviate this CF problem.\n# 5.2 Incremental Many-to-one Scenario\nMain results. Table 5 shows the translation results in incremental many-to-one scenario, where line 19 report the results that the first task is IT\u21d2EN, the second one is NL\u21d2EN, and the last one is RO\u21d2EN. Line 10-18 report the results that the first task is MO\u21d2CH, the second one is TI\u21d2CH, and the last one is UY\u21d2CH. From the results, we can reach the following conclusions: i) Fine-tuning method also suffers heavily from CF problem, whose average-BLEU scores seriously drop from 31.17 (line 1) to 17.86 (line 3) and from 32.00 (line 10) to 14.71 (line 12), respectively. We can also see that CF here is not serious as that in incremental one-to-many scenario (see Table 3). EWC method can partially alleviate CF, while it is still lower than the single model by 6.61 (line 4) and 9.44 (line 13) BLEU points, respectively. ii) Surprisingly, direct distillation further wors-\nModel\nBLEU-prev\nBLEU-new\nInitial\n27.34\n\u223c\nSingle\n27.01\n44.40\nJoint Training\n27.41\n44.28\nFine-tuning\n1.00\n44.53\nPseudoInput-Distill\n23.04\n43.87\nReverseTeacher-Distill\n27.27\n44.25\nTable 6: The translation results of an already existing many-to-one model. The existing model is trained by IT, NL, RO, JA, DE, FI, LV, TR\u21d2EN tasks, and the new task is CH\u21d2EN.\nens the CF due to the extreme partial distillation, whose average-BLEU scores reduce to 10.66 (line 5) and 8.06 (line 14), respectively. iii) The pseudo input distillation method can sharply alleviate the CF. The average BLEU scores can reach to 30.07 (line 8) and 30.50 (line 17), respectively. The reverse teacher distillation can exceed the single model by 1.85 (line 9) and 0.73 (line 18), respectively. Compared with the joint training, this method can achieve comparable results (33.02 vs. 33.38 and 32.73 vs. 32.76). iv) In both pseudo input distillation and reverse teacher distillation, we find that during distillation, the beam search distillation can also achieve better results than the greedy search distillation. Results on an already existing MNMT model. We also conduct an experiment on the basis of an existing many-to-one model. Here, the existing model is trained by IT, NL, RO, JA, DE, FI, LV, TR\u21d2EN tasks, and the new task\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b451/b45113c7-85e4-42a1-9a84-6a29149d4da2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Learning with task 1 Learning with task 2 (IT  EN) (NL  EN) 0 2 4 6 8 12 14 2 4 6 8 10 14 12 10 16 16 3 10 \uf0b4</div>\nFigure 3: The BLEU scores during training in many-toone scenario. x axis denotes the training epoch and y axis denotes the BLEU scores of development set.\nis CH\u21d2EN. Table 6 lists the results, where the initial BLEU score of learned tasks is 27.34. After learning CH\u21d2EN, the reverse teacher distillation can retain the BLEU score of learned tasks to 27.27. Meanwhile its BLEU score of new task is 44.25. These results are comparable with that of joint training (27.27 vs. 27.41 and 44.25 vs. 44.28). The results show that the proposed methods could also alleviate the CF in many-to-one scenario. Results during training. Fig. 3 reports the BLEU scores of development set at each training epoch, where the first task is IT\u21d2EN and the second one is NL\u21d2EN. From the results, we can see that fine-tuning also faces the CF when learning the second task. Both pseudo input distillation and reverse teacher distillation could alleviate this problem. In particular, reverse teacher distillation could further improve performance of the first task when learning the second one. Meanwhile, compared with fine-tuning method, these two methods can also improve the BLEU scores of the second task.\n# 6 Related Work\nMultilingual Neural Machine Translation. To facilitate the deployment and improve the performance, various MNMT models are proposed (Dong et al., 2015; Johnson et al., 2017; Gu et al., 2018; Wang et al., 2018; Tan et al., 2019; Aharoni et al., 2019; Wang et al., 2019; Kudugunta et al., 2019; Bapna and Firat, 2019), where Tan et al. (2019) propose a knowledge distillation method for MNMT. However, these studies are conducted in the conventional setting that bilingual pairs for all the translation tasks are available at training time. Different from these studies, we focus on incremental scenario that the training data of previous tasks is\nunavailable. Life-long Learning and Its Application in NLP. Lifelong learning aims at adapting a learned model to new tasks while retaining the previous knowledge. De Lange et al. (2019) classify these methods into three categories: i) replay-based methods (Lopez-Paz and Ranzato, 2017; Wu et al., 2018), ii) regularization-based methods (Li and Hoiem, 2017), and iii) parameter isolation-based methods (Rusu et al., 2016). Meanwhile, several studies apply these methods into NLP tasks, such as sentiment analysis (Chen et al., 2018; Xia et al., 2017), word and sentence representation learning (Xu et al., 2018; Liu et al., 2019), language modeling (Sun et al., 2019; d\u2019Autume et al., 2019), domain adaptation for NMT (Barone et al., 2017; Thompson et al., 2019) and post-editors for NMT (Turchi et al., 2017; Thompson et al., 2019). Different from these studies, we focus on the incremental MNMT scenario that each tasks arrive in a sequential manner and training data of previous tasks is unavailable. Recently, Escolano et al. (2019) propose an incremental training method for MNMT, in which they train the independent encoders and decoders for each languages. While with the increasing of learning language pair, the model parameters become larger. Different from this study, we apply the life-long learning method on a more challenging MNMT framework with one universal encoder and decoder (Johnson et al., 2017).\n# 7 Conclusion and Future Work\nIn this paper, we aim at enabling the MNMT to learn incremental translation tasks over a lifetime. To achieve this, we investigate knowledge distillation based life-long learning for MNMT. In one-tomany scenario, we propose a multilingual distillation method. In incremental many-to-one scenario, we find that direct distillation faces the extreme partial distillation problem, and propose pseudo input distillation and reverse teacher distillation to address this problem. The extensive experiments demonstrate that our method can retain the previous knowledge when learning a new task. As a novel attempt of life-long learning for MNMT, the proposed methods still have a drawback that they cost more computational overhead due to the knowledge distillation. Therefore, in the future we will study how to reduce the computational overhead of our methods and extend them into the incremental many-to-many scenario.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of catastrophic forgetting (CF) in Multilingual Neural Machine Translation (MNMT) when training tasks arrive sequentially without access to previous training data. Existing methods struggle with CF, necessitating a new approach to retain knowledge from prior tasks while learning new ones.",
        "problem": {
            "definition": "The problem is defined as the challenge of adapting an MNMT model to new translation tasks while retaining knowledge from previously learned tasks, despite the absence of their training data.",
            "key obstacle": "The core obstacle is catastrophic forgetting, where fine-tuning on new tasks leads to significant degradation in performance on previous tasks."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea stems from the observation that knowledge distillation can help retain information from previous tasks by leveraging outputs from an old model (teacher) to guide the learning of a new model (student).",
            "opinion": "The proposed idea involves using knowledge distillation to allow the student model to learn multilingual outputs from both the old model and new tasks, thus consolidating knowledge effectively.",
            "innovation": "The innovation lies in addressing the extreme partial distillation problem in many-to-one scenarios by introducing pseudo input distillation and reverse teacher distillation methods, which are not present in existing approaches."
        },
        "method": {
            "method name": "Multilingual Distillation",
            "method abbreviation": "MD",
            "method definition": "Multilingual Distillation is a method that enables a new MNMT model to learn from both the outputs of an old model and the new task's data, effectively retaining knowledge across tasks.",
            "method description": "The method allows the new model to jointly learn multilingual outputs from the old model while incorporating new tasks.",
            "method steps": [
                "Add language indicators to the source sentences.",
                "Input these modified sentences into the teacher model to obtain multilingual outputs.",
                "Train the student model by maximizing the objective function that includes losses from both previous and new tasks."
            ],
            "principle": "The method is effective because it leverages the outputs of the teacher model to provide the student model with rich multilingual knowledge, thus mitigating the effects of catastrophic forgetting."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on twelve translation tasks, including EN\u21d4IT, EN\u21d4NL, and EN\u21d4RO from the TED dataset, among others. Various baseline methods were used for comparison, including joint training, fine-tuning, and EWC.",
            "evaluation method": "The performance was assessed using BLEU scores, with comparisons made between the proposed methods and several baseline systems to evaluate improvements in translation quality."
        },
        "conclusion": "The experiments demonstrate that the proposed methods effectively retain previous knowledge while learning new tasks, significantly alleviating the issues of catastrophic forgetting in MNMT.",
        "discussion": {
            "advantage": "The key advantages include the ability to retain knowledge from previous tasks while learning new ones and improved BLEU scores compared to traditional methods.",
            "limitation": "A limitation of the proposed methods is the increased computational overhead associated with knowledge distillation.",
            "future work": "Future research will focus on reducing the computational costs of the methods and exploring their application in incremental many-to-many scenarios."
        },
        "other info": {
            "info1": "The proposed methods were evaluated on multiple language pairs and tasks to ensure robustness.",
            "info2": {
                "info2.1": "The study highlights the importance of knowledge distillation in lifelong learning for MNMT.",
                "info2.2": "The methods aim to bridge the gap between existing approaches and the challenges presented by incremental learning scenarios."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses catastrophic forgetting (CF) in Multilingual Neural Machine Translation (MNMT), highlighting the significance of retaining knowledge from prior tasks while learning new ones."
        },
        {
            "section number": "2.1",
            "key information": "Key terminologies include Multilingual Neural Machine Translation (MNMT), catastrophic forgetting, and knowledge distillation, which are foundational to understanding the paper's contributions."
        },
        {
            "section number": "5.1",
            "key information": "The proposed method, Multilingual Distillation, enables a new MNMT model to learn from both the outputs of an old model and new task data, effectively improving model performance."
        },
        {
            "section number": "5.4",
            "key information": "The paper identifies the limitation of increased computational overhead associated with knowledge distillation as a challenge in current pre-training methods."
        },
        {
            "section number": "7.1",
            "key information": "The deployment of the proposed methods faces challenges related to catastrophic forgetting and the computational costs of knowledge distillation."
        },
        {
            "section number": "7.3",
            "key information": "Future research will focus on reducing computational costs and exploring the application of the proposed methods in incremental many-to-many scenarios."
        }
    ],
    "similarity_score": 0.605233965703741,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1421_trans/papers/Life-long Learning for Multilingual Neural Machine Translation with Knowledge Distillation.json"
}