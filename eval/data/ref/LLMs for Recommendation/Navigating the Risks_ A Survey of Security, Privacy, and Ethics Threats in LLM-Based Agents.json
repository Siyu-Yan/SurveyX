{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.09523",
    "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents",
    "abstract": "With the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively.",
    "bib_name": "gan2024navigatingriskssurveysecurity",
    "md_text": "# Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents\n# YUYOU GAN, YONG YANG, ZHE MA, PING HE, RUI ZENG, YIMING WANG, QINGMING LI, and CHUNYI ZHOU, Zhejiang University, China SONGZE LI, Southeast University, China TING WANG, Stony Brook University, USA\n# YUYOU GAN, YONG YANG, ZHE MA, PING HE, RUI ZENG, YIMING WANG, QINGMING LI, and CHUNYI ZHOU, Zhejiang University, China SONGZE LI, Southeast University, China\nWith the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives. To help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively. CCS Concepts: \u2022 Security and privacy; Additional Key Words and Phrases: LLM-based agents, Security, Privacy, Ethics\narXiv:2411.09523v1\nACM Reference Format: Yuyou Gan, Yong Yang, Zhe Ma, Ping He, Rui Zeng, Yiming Wang, Qingming Li, Chunyi Zhou, Songze Li, Ting Wang, Yunjun Gao, Yingcai Wu, and Shouling Ji. 2018. Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents. 1, 1 (November 2018), 48 pages. https://doi.org/XXXXXXX.XXXXXXX\n# 1 Introduction\nWith the continuous development of language models (LMs), LLMs based on the transformer architecture [259] have achieved significant success in various fields of NLP [62, 214]. The massive number of parameters and extensive training data endow LLMs with strong capabilities in tasks like text generation [24, 253], code assistance [79, 272],\nAuthors\u2019 Contact Information: Yuyou Gan, ganyuyou@zju.edu.cn; Yong Yang, yangyong2022@zju.edu.cn; Zhe Ma, mz.rs@zju.edu.cn; Ping He, gnip@z edu.cn; Rui Zeng, ruizeng24@zju.edu.cn; Yiming Wang, ym_wang@zju.edu.cn; Qingming Li, liqm@zju.edu.cn; Chunyi Zhou, zhouchunyi@zju.edu.c Zhejiang University, Hangzhou, Zhejiang, China; Songze Li, songzeli@seu.edu.cn, Southeast University, Nanjing, Jiangsu, China; Ting Wang, twang cs.stonybrook.edu, Stony Brook University, Stony Brook, New York, USA; Yunjun Gao, gaoyj@zju.edu.cn; Yingcai Wu, ycwu@zju.edu.cn; Shouling  sji@zju.edu.cn, Zhejiang University, Hangzhou, Zhejiang, China.\nAuthors\u2019 Contact Information: Yuyou Gan, ganyuyou@zju.edu.cn; Yong Yang, yangyong2022@zju.edu.cn; Zhe Ma, mz.rs@zju.edu.cn; Ping He, gnip@zju. edu.cn; Rui Zeng, ruizeng24@zju.edu.cn; Yiming Wang, ym_wang@zju.edu.cn; Qingming Li, liqm@zju.edu.cn; Chunyi Zhou, zhouchunyi@zju.edu.cn, Zhejiang University, Hangzhou, Zhejiang, China; Songze Li, songzeli@seu.edu.cn, Southeast University, Nanjing, Jiangsu, China; Ting Wang, twang@ cs.stonybrook.edu, Stony Brook University, Stony Brook, New York, USA; Yunjun Gao, gaoyj@zju.edu.cn; Yingcai Wu, ycwu@zju.edu.cn; Shouling Ji, sji@zju.edu.cn, Zhejiang University, Hangzhou, Zhejiang, China.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2b2c/2b2c4a24-f4f3-4551-bce2-7442b3c5f683.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1. The overall framework of our taxonomy for the risks of LLM-based agents.</div>\nlogical reasoning [277, 297], etc. Due to their powerful understanding capabilities, an increasing number of studies are positioning LLMs as the core decision-making hub of AI agents [189, 235], which are sophisticated software programs designed to autonomously perform tasks on behalf of users or other systems. Compared to earlier AI agents based on heuristic algorithms or reinforcement learning [154, 185], LLM-based agents can communicate with users, making them easier to understand and accept. Additionally, their vast foundational knowledge allows them to think in a manner similar to humans (understanding + planning). These characteristics contribute to their popularity, making them a promising direction for AI to serve various practical fields [263, 310, 317]. For example, Supertools [247] is a comprehensive collection of trending applications empowered by LLMs. Despite the significant success of LLMs, they also face security and privacy threats due to inner vulnerabilities or outer attacks. LLM-based agents add some components and functionalities, which makes these risks even more threatening. For example, LLMs face jailbreaking attacks [168, 233], which refer to the process to bypass their built-in safety mechanisms [17, 198]. In the context of LLM-based agents, LLMs need to handle multi-round dialogues and multiple sources of information, making jailbreaking attacks more complex and difficult to defend against [12, 47]. To uncover the vulnerabilities of LLM-based agents and make them more secure and reliable, an increasing number of studies focus on the threats from various perspectives. To help researchers better understand LLM-based agents and pursue future research work, there exist two surveys [53, 61] to summarize the security risks of LLM-based agents. They categorize the security risks based on the composition (called modules) or operational phases (called stages) of the agents as follows. (i) The module perspectives. Cui et al. [53] identified four key modules in LLM-based systems, i.e., the input module, the LM module, the toolchain module, and the output module. They summarize the risks of LLM-based agents based on the four modules. (ii) The stage perspectives. Deng et al. [61] identified four key knowledge gaps in LLM-based AI agents, i.e., the stage of perception, the stage of internal execution, the stage of action in environment, and the stage of interaction with untrusted external entities. They summarize the risks of LLM-based agents based on the four stages. These two taxonomies clearly highlight the sources of attacks faced by LLM-based agents. However, they struggle to accurately pinpoint threats that span across modules and stages. For example, privacy leakage is caused by memory issues within the language model module, but it occurs at the output module. Similarly, goal hijacking can happen not only during the perception stage but also during the interaction stage with external data [5]. These cross-module and cross-stage threats are inaccurately pinpointed to a single module or stage. Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/536c/536c3372-799a-43c3-87d1-2dfc3156cc1a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. An overall framework of LLM-based agents.</div>\nOur Design. To categorize various threats of LLM-based agents more accurately and comprehensively, we propose a novel taxonomy by mapping the threats into a binary table based on their sources and types. (i) For the sources of threats, we consider the operational nature of LLM-based agents: LLMs make decisions based on inputs from multiple sources, as shown in Fig. 1 left. As a probabilistic model, the output decision distribution of an LLM is determined by both the input and the model itself. Therefore, we attribute the threats to LLM-based agents to the inputs, the model, or a combination of both. Compared to categorizing attacks by modules or stages, our classification of sources is closer to the essence of the threat. For example, goal hijacking [109, 169, 213] may originate from a user input or an external database, but both fundamentally act as inputs to the model for hijacking the goal. (ii) For the types of threats, we categorize the threats into three classes: security/safety, privacy, and ethics. Specifically, if a threat results in the model producing incorrect outputs (including errors that are factually inaccurate or do not align with the needs of developers or users), it is categorized as a security/safety issue, such as adversarial examples [299]. If a threat leads to the leakage of privacy, it is classified as a privacy issue, such as prompt leakage attacks [303, 323]. If a threat does not produce \u201cincorrect\" outputs but raises concerns such as unfairness, it falls under ethical issues, such as bias [81]. We collect papers from the top conferences and highly cited arXiv papers. Top conferences are included but not limited: IEEE S&P, ACM CCS, USENIX Security, NDSS, ACL, CVPR, NIPS, ICML, and ICLR. We categorize different kinds of threats with our taxonomy in Fig. 1 right. For threats originating from inputs, we refer to them as problematic inputs. In this scenario, attackers cannot modify the model but can design inputs to induce malicious outputs or behaviors, e.g., the adversarial example. For threats from within the model, we refer to them as model flaws. In this scenario, the inputs are always benign, but the model\u2019s own defects lead to malicious outputs or behaviors, e.g., the hallucination problem. For threats arising from both model flaws and carefully crafted inputs, we refer to them as combined threats. In this scenario, the inputs are deliberately designed by attackers to exploit the model\u2019s vulnerabilities, e.g., the backdoor attack.\nManuscript submitted to ACM\nManuscript submitted to ACM\nOur Contributions. Compared with recent surveys [53, 61] on the security risks of LLM-based agents, there are three main advantages of our work. (i) A Novel Taxonomy of Threats. We propose a novel taxonomy that maps threats into a binary table based on their sources and impacts, which can comprehensively cover the existing threats and extend to future threats, including the cross-module and cross-stage threats. (ii) Detailed Analysis of Multi-modal Large Language Models (MLLMs). Many tasks require agents to handle inputs from multiple modalities, (e.g., city navigation systems [310]), leading to the emergence of a range of MLLMs and agents based on these models [295, 310]. Previous surveys primarily focus on the text modality, lacking analysis of multimodal models. We cover both LLMs and MLLMs, placing particular emphasis on analyzing the new challenges and threats posed by multimodal tasks in the context of threats. (iii) Four Carefully Selected Case Studies. Previous surveys analyze the risks based on a general framework of LLM-based agents (or systems). However, actual agents may not necessarily contain all modules in the general framework, and the designs within these modules may also be customized [66]. More importantly, the scenarios they face have significant differences, resulting in the varying levels and causes of threats. To help readers better understand the actual threats faced by agents, we present case studies of four different agents, representing four classic situations in Section 6. This paper is organized as follows. Section 2 introduces a general framework of LLM-based agents and identifies six key features of the framework. Sections 3, 4, and 5 depict the risks from problematic inputs, model flaws, and input-model interaction, respectively. Section 6 offers four carefully selected case studies. Section 7 gives future directions for the development of this field.\n# 2 LLM-based Agent\nAI agents are considered promising a research direction that utilize AI technology to autonomously execute specific tasks and make decisions. In previous researches, AI agents often achieved good results in specific scenarios (such as playing games) through heuristic strategies or reinforcement learning [154][185][226][279]. In recent years, LLMs, such as ChatGPT, have attracted substantial attention from both academia, and industry, due to their remarkable performance on various NLP tasks [24][321][259]. Therefore, there is an increasing amount of work studying the use of LLMs as the decision-making center for AI agents [189][9][200]. With the development of LLMs, LLMs can handle more modalities and tasks [235]. Framework of LLM-based Agents. In our work, we consider a comprehensive framework of an LLM-based agent that covers the modules and runtime modes of mainstream LLM-based agents, as shown in Fig. 2. This framework contains the following four modules. (i) Input Module (IM). IM receives the users\u2019 inputs and preprocesses them as follows. First, IM formats the inputs to a specific distribution (e.g., normalize an input image) or a specific format (e.g., a special language [229]). Second, IM implements harmful information detection [221][293] or purification [229]. Third, many LLM-based agents add a system prompt before the inputs [189][235]. (ii) Decision Module (DM). DM understands and analyzes the query of the user, gives plans and generates the final response to the user. Many agents\u2019 decision modules only contain one LLM. They leverage an LLM for understanding, planning, and feedback [189][235], or use an LLM for understanding and feedback, with another non-LLM planner handling the planning [159][54]. As tasks become more complex, many agents employ multiple LLMs to accomplish the aforementioned tasks. For example, VOYAGER [263] uses GPT-4 and GPT-3.5 to handle the tasks of understanding, Manuscript submitted to ACM\nting the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/99bf/99bf0532-e961-445b-bbef-8dae9e66190b.png\" style=\"width: 50%;\"></div>\nplanning, and generating a skill library, respectively. Huang et al. [106] used GPT-3 for task planning, while leveraging BERT to translate the plans into admissible actions. (iii) External Entities (EE). With the task becoming more complex, the agents need the help of the external modules, including memory module, external tools, the other agents and the environment. The memory module is used to store and retrieve relevant information to improve the coherence and context-awareness of the agent\u2019s responses. In this paper, we adopt the definition of agents\u2019 memory from [327], considering external databases as a form of agents\u2019 memory as well. External tools integrate numerous APIs to fulfill the user\u2019s requirements (e.g., search engine APIs for Webgpt [189] and APIs for controlling the robotic arm [9]). Sometimes, multiple agents need to collaborate to complete a task, where one agent needs to interact with other agents [98]. (iv) Output Module (OM). There might be zero or multiple interaction between DM and EE to accomplish the task. After that, DM generates the response and delivers it to the user through OM. Agents can implement harmful information detection or purification on the output [103]. Based on this framework, we identify six key features of LLM-based agents, which involve new attack surfaces compared with the traditional DNN models and the RL-based agents. As shown in Fig. 3, these six key features are as follows. (i) LLM-based controller. LLMs serve as the core of agents, leveraging transformer architecture, vast amounts of knowledge, and massive training data to confer strong understanding capabilities, while also introducing new risks. (ii) Multi-modal inputs and outputs. As agents become capable of handling increasingly complex tasks, many scenarios require the processing of multimodal information. Research indicates that risks vary across different modalities, and their interaction in multimodal systems presents unique challenges and opportunities. (iii) Multi-source inputs. The inputs to LLMs within agents consist of multiple components from different sources, such as user input, system prompts, memory, and environmental feedback. Compared to a standalone LLM, multi-source inputs present new opportunities and challenges for both attackers and defenders. (iv) Multi-round interaction. Agents often require multiple rounds of interaction (with the environment, users, other LLMs, etc.) to complete tasks, leading to longer and more complex inputs for LLMs, which may exacerbate certain threats. (v) Memory mechanism. The memory mechanisms in agents can help accumulate experience and enhance knowledge, improving their ability to handle various tasks, but they also introduce new security and privacy risks. (vi) Tool invocation. LLMs are specially crafted with instruction-tuning data designed for tool usage. This process enables LLMs to handle complex tasks, but it may also result in more severe consequences and introduce new vulnerabilities.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8931/89318371-809c-4e5c-aee9-ec088a3f505a.png\" style=\"width: 50%;\"></div>\n\n<div style=\"text-align: center;\">Fig. 4. The mapping of key features to identified threats based on collected literature.</div>\nIn the following sections, we comprehensively introduce the aforementioned threats based on the taxonomy. Fig. 4 indicates the presence of studies linking specific characteristics of LLM-based agents to various threats. Each checkmark represents a documented research contribution that addresses the corresponding feature-threat relationship. For every threat, we summarize its technical progress based on six key features and identity the limitations of the current research. In Table 1, we show the key papers for each threat. For brevity, we abbreviate \u201cLLM-based controller\" as LC, \u201cMulti-modal inputs and outputs\" as MMIO, \u201cMulti-source inputs\" as MSI, \u201cMulti-round interaction\" as MRI, \u201cMemory mechanism\" as MM, and \u201cTool invocation\" as TI in the table.\n# 3 Risks from Problematic Inputs\nThis section focuses on the risks that arise due to issues with the input data, such as adversarial examples, prompt injection, etc. that can lead to problems with the LLM-based agent\u2019s performance and behavior. Compared with a standalone LLM, the decision module of an LLM-based agent can receive inputs from different modules, which increases its attack surface. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.\n# 3.1 Adversarial Example\nAn adversarial example is an adversarial-perturbed sample preserving the semantics but misclassified by the deep learning models. Specifically,\nAn adversarial example is an adversarial-perturbed sample preserving the semantics but misclassified by the dee learning models. Specifically,\n\ud835\udeff\u2217= arg min \ud835\udeff\u2208\u0394 \ud835\udc46\ud835\udc52\ud835\udc5a\ud835\udc37\ud835\udc56\ud835\udc60(\ud835\udc65,\ud835\udc65+ \ud835\udeff)\nManuscript submitted to ACM\n(1)\n<div style=\"text-align: center;\">Table 1. Overview of Papers by Threat Type and Key Feature.</div>\nYear\nPaper\nThreat Source\nThreats\nKey Features\nSpecific Effects\n2023\nLiu et al. [161]\nInputs\nAdversarial Example\nMM\nAdversarial T2I generation.\n2023\nLi et al. [146]\nInputs\nAdversarial Example\nMRI\nAdversarial dialogue.\n2024\nWang et al. [264]\nInputs\nAdversarial Example\nMM & MSI\nTransferable adversarial example.\n2024\nYin et al. [299]\nInputs\nAdversarial Example\nMM & MMIO\nMultimodal and multiple tasks attack.\n2024\nShen et al. [232]\nInputs\nAdversarial Example\nLC\nDynamic attention to enhance robustness.\n2023\nQiang et al. [213]\nInputs\nGoal Hijacking\nLC\nInduce unwanted outputs.\n2024\nPasquini et al. [202]\nInputs\nGoal Hijacking\nLC\nOptimization-based prompt injection.\n2024\nKimura et al. [126]\nInputs\nGoal Hijacking\nMMIO\nRedirect task execution.\n2024\nWei et al. [276]\nInputs\nGoal Hijacking\nMRI\nManipulates context to influence outputs.\n2024\nZhan et al. [314]\nInputs\nGoal Hijacking\nMM & TI\nBenchmark of indirect prompt injections.\n2023\nGreshake et al. [88]\nInputs\nGoal Hijacking\nMSI\nIndirect injection to manipulates outputs.\n2024\nHui et al. [112]\nInputs\nPrompt Leakage\nLC\nExtract system prompt.\n2024\nYang et al. [294]\nInputs\nPrompt Leakage\nLC\nSteal target prompt.\n2024\nShen et al. [234]\nInputs\nPrompt Leakage\nMIO\nSteal target prompt.\n2024\nCarlini et al. [35]\nInputs\nModel Extraction\nLC\nExtract the parameter of the last layer.\n2023\nLi et al. [147]\nInputs\nModel Extraction\nLC\nExtract the specialized code abilities.\n2023\nZou et al. [337]\nInputs\nJailbreaking\nLC\nGenerate adversarial jailbreak prompts.\n2023\nYu et al. [302]\nInputs\nJailbreaking\nLC\nAuto-generated jailbreak prompts.\n2023\nShayegani et al. [230]\nInputs\nJailbreaking\nMMIO\nInduce harmful content generation.\n2024\nAnil et al. [12]\nInputs\nJailbreaking\nMRI\nInduce harmful content generation.\n2024\nGu et al. [89]\nInputs\nJailbreaking\nMIO & MSI\nMalicious input trigger agent harm.\n2024\nZhao et al. [330]\nModel\nHallucination\nMMIO\nreducing hallucination via data augmentation.\n2024\nFavero et al. [75]\nModel\nHallucination\nMMIO\nreducing hallucination via novel decoding.\n2023\nLiu et al. [160]\nModel\nHallucination\nMMIO\nreducing hallucination via Instruction tuning.\n2023\nPeng et al. [205]\nModel\nHallucination\nMM\nreducing hallucination via external databases.\n2023\nChen et al. [42]\nModel\nHallucination\nMSI\nreducing hallucination via standardization.\n2024\nLuo et al. [177]\nModel\nHallucination\nMSI & MRI & MM\nBenchmark for hallucination evaluation.\n2023\nCarlini et al. [32]\nModel\nMemorization\nLC\nStudy the influence factors of memorization.\n2024\nTang et al. [250]\nModel\nBias\nLC\nGender bias measurement and mitigation.\n2023\nXie et al. [287]\nModel\nBias\nLC\nBias mitigation in pre-trained LMs.\n2023\nLimisiewicz et al. [155]\nModel\nBias\nLC\nDebiasing algorithm through model adaptation.\n2024\nHoward et al. [101]\nModel\nBias\nMMIO\nBias measurement and mitigation in VLMs.\n2024\nD\u2019Inca et al. [64]\nModel\nBias\nMMIO\nBias measurement in text-to-image models.\n2022\nBagdasaryan et al. [15]\nCombination\nBackdoor\nLC\nBackdoors for propaganda-as-a-service.\n2024\nHubinger et al. [111]\nCombination\nBackdoor\nLC\nBackdoors that persist through safety training.\n2023\nDong et al. [66]\nCombination\nBackdoor\nTI\nTriggering unintended tool invocation.\n2024\nLiu et al. [158]\nCombination\nBackdoor\nMMIO & TI\nTriggering unintended tool invocation.\n2024\nXiang et al. [46]\nCombination\nBackdoor\nMM & TI\nCorrupted memories causing errors in retrieval.\n2021\nCarlini et al. [36]\nCombination\nPrivacy Leakage\nLC\nExtract training data.\n2024\nBagdasaryan et al. [16]\nCombination\nPrivacy Leakage\nMI\nUser private information leakage.\n2024\nZeng et al. [312]\nCombination\nPrivacy Leakage\nMM\nDatabase private information leakage.\n\ufffd \ud835\udc54(\ud835\udc65+ \ud835\udeff\u2217) \u2260\ud835\udc5c(Untargeted) \ud835\udc54(\ud835\udc65+ \ud835\udeff\u2217) = \ud835\udc61(Targeted)\nwhere \ud835\udc46\ud835\udc52\ud835\udc5a\ud835\udc37\ud835\udc56\ud835\udc60() denotes the semantic distance between the perturbed sample and the original sample, \u0394 represents the feasible perturbation space, and \ud835\udc54() signifies the target model. If the adversarial perturbation makes the target model misclassify the original label \ud835\udc5cof the sample, it represents the untargeted attack (\ud835\udc54(\ud835\udc65+\ud835\udeff\u2217) \u2260\ud835\udc5c). If the adversarial perturbation makes the target model misclassify the sample to a target label \ud835\udc61, it represents the targeted attack (\ud835\udc54(\ud835\udc65+ \ud835\udeff\u2217) = \ud835\udc61). The research of adversarial examples has passed over ten years [249], raising attention in many domains, e.g., autonomous driving [73], malware detection [94], reinforcement learning [285], etc. Szegedy et al. [249] first discovered the adversarial example in the neural networks, which opens Pandora\u2019s box of the adversarial example. According to the knowledge of the attacker, the adversarial example attack methods can be categorized into perfect knowledge attack, limited knowledge attack, and zero knowledge attack. The history of adversarial example attacks starts from the perfect knowledge attack [37] to the zero knowledge attack [43], which is a more practical setting. Correspondingly, Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5b5d/5b5d4eca-7356-4346-abb5-f38ad65f928d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig. 5. Adversarial examples targeting LLM-based agents may involve four key features (indicated with a red exclamation mark),</div>\n<div style=\"text-align: center;\">Fig. 5. Adversarial examples targeting LLM-based agents may involve four key features (indicated with a red exclamatio leading to incorrect output.</div>\nthe development of the defense method is from the empirical methods, e.g., defensive distillation [199], obfuscated gradients [13, 25], etc, to the theoretical methods, certificated robustness [70, 179]. The arms race of adversarial example attacks and defenses exists from deep learning models and LLMs to LLM-based AI agents. In the context of LLM-based agents, as shown in 5, the development of adversarial examples primarily involves four key features: LLM-based controller, multi-modal inputs and outputs, multi-source inputs, and multi-round interaction. In the following, we review the recent advancements in both attack and defense perspectives. 3.1.1 Technical Progress. Attack Perspective. As discussed in section 2, the input and output interactions of LLM-based AI agents are characterized by their handling of multi-modal data across multiple rounds of interaction. This complexity necessitates a nuanced approach to adversarial example attacks, which increasingly focus on the relationships between different modalities within these interactions. Recent research in this area has produced several sophisticated methods for attacking multi-modal systems. For instance, RIATIG [161] introduces a reliable and imperceptible adversarial example attack targeting text-to-image models. This method employs a genetic-based optimization loss function aimed at improving the quality of adversarial samples, ensuring that the generated examples are both effective and difficult to detect. VLATTACK [299], advances the field by generating adversarial samples that fuse perturbations from both images and text. This fusion occurs at both single-modal and multi-modal levels, making the attacks more versatile and challenging to defend against. The method\u2019s ability to operate across modalities highlights the increasing sophistication of adversarial techniques as they target the interconnected nature of multi-modal systems. Beyond direct adversarial attacks, there is significant focus on the transferability of adversarial examples across different vision-language models (VLMs). For example, SGA [172] generates adversarial examples by leveraging diverse cross-modal interactions among multiple image-text pairs. This method incorporates alignment-preserving augmentation combined with cross-modal guidance, allowing adversarial examples to maintain their efficacy across various models and tasks. Similarly, TMM [264] enhances the transferability of adversarial examples through attentiondirected feature perturbation. By targeting critical attention regions and disrupting modality-consistency features, this approach increases the likelihood that adversarial examples will succeed across different VLMs. Another line of adversarial example attack methods specifically targets downstream applications that involve multiple rounds of interaction. For instance, Liu et al. [163] proposed imitation adversarial example attacks against neural ranking models, with the goal of manipulating ranking results to achieve desired outcomes. This method exemplifies how adversarial attacks can exploit the iterative nature of certain applications to progressively distort the final output. Similarly, NatLogAttack [332] leverages adversarial examples to compromise models based on natural Manuscript submitted to ACM\nlogic, introducing subtle perturbations that undermine the model\u2019s reasoning processes. In the domain of dialogue generation, DGSlow [146] generates adversarial examples by defining two objective loss functions that target both response accuracy and length. This approach ensures that the generated responses not only deviate from expected content but also manipulate the conversational flow, making the attack more disruptive. Defense Perspective. Defense methods against adversarial examples are broadly categorized into two primary types: input-level defenses and model-level defenses. Each of these approaches targets different aspects of the adversarial threat landscape, aiming to enhance the robustness of LLM-based AI agents against adversarial perturbations. Input-level defenses primarily focus on detecting and mitigating adversarial examples before they can influence the model\u2019s predictions. These defenses typically employ techniques for adversarial example detection and purification. Most of the existing input-level defense methods [18, 123, 140, 186, 273] in the domain of LLM-based AI agents leverage LLMs to identify and neutralize adversarial inputs effectively. For instance, ADFAR [18] implements multi-task learning techniques to enable LLMs to distinguish adversarial input samples from benign ones. Similarly, methods such as BERTdefense [123] and the approach proposed by Li et al. [140] utilized the BERT model to purify adversarial perturbations, thereby safeguarding the model\u2019s outputs from being compromised by malicious inputs. The SOTA input-level defense strategies have begun to focus on the prompt mechanisms within LLM-based AI agents, as discussed in section 2. For example, APT [139] enhances the robustness of the CLIP model by leveraging soft prompts, which serve as an additional layer of defense against adversarial manipulation by refining the model\u2019s input processing pipeline. Model-level defenses [67, 132, 165, 262, 336], on the other hand, are concerned with the architecture and parameters of the model itself. These defenses aim to create inherently robust models through techniques such as adversarial training and fine-tuning specific model parameters. For instance, RIFT [67] employs mutual information to achieve robust fine-tuning, and InforBERT [262] designs the information bottleneck regularizer and the anchored feature regularizer for adversarial training. To address the high computational cost associated with retraining entire models, some methods like SHIELD [132] propose retraining only the final layer of LLMs. This approach significantly reduces the training overhead while still providing a degree of robustness against adversarial examples. The most advanced model-level defense method currently available, Dynamic Attention [232], leverages a dynamic attention mechanism to enhance the robustness of transformer-based models. This method represents a significant advancement in the development of robust transformer-based models by dynamically adjusting the model\u2019s attention mechanisms in response to potential adversarial threats.\n3.1.2 Discussion of Limitations. Attack Perspective. Current adversarial attack methods are primarily focused on untargeted attacks, where the attack objectives are not precisely defined. As a result, the outcomes of these attacks cannot be explicitly controlled to induce specific, pre-determined misbehavior. For example, adversarial perturbations applied to images fail to generate targeted responses, such as causing a specific erroneous answer in visual questionanswering tasks. Additionally, as discussed in Section 3.1.1, existing adversarial attack strategies aimed at multi-modal systems often engage with multiple modalities simultaneously. However, the constraint metrics used to evaluate the success of these attacks are typically designed for single-modality scenarios. This approach may be inadequate when adversarial perturbations must be applied across different modalities, as it does not account for the unique interactions between distinct data types. Furthermore, as discussed in Section 3.1.1, the scope of current adversarial example attacks remains confined to targeting the output module of LLM-based agents. However, there are additional, unexplored targets for adversarial example attacks. Specifically, vulnerabilities may exist within the memory, external tool interfaces, and the planner Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9169/91694200-2660-4f4c-b88e-6b55e1ce6132.png\" style=\"width: 50%;\"></div>\ncomponents of these agents, which remain under-investigated. For instance, adversarial example attacks could potentially disrupt the planning capabilities of LLM-based agents, leading them to devise incorrect or suboptimal plans. Expanding the attack surface beyond the output module to include these other critical components could reveal new dimensions of adversarial risks in complex systems. Defense Perspective. Current defense mechanisms against adversarial examples in LLM-based agents remain constrained to single-modal inputs and the robustness of individual models. For instance, dynamic attention [232], a state-of-the-art adversarial defense technique within LLM-based agents, is limited to NLP tasks. However, LLM-based AI agents are increasingly handling multi-modal input data that extends beyond the scope of a single model. Furthermore, the decision-making module within these agents may incorporate multiple LLMs, each exhibiting varying degrees of robustness against adversarial attacks. Despite this, existing defense strategies focus exclusively on enhancing the robustness of a single model, neglecting the broader issue of joint robustness across multiple models integrated into the system. In addition, current adversarial defense methods for LLM-based AI agents overlook key components such as the memory and planner modules, which may provide additional avenues for defending against adversarial examples. For instance, the memory bank could be leveraged to detect and counteract adversarial attack patterns by recognizing recurring attack tactics. Future strategies could extend the defense scope to include these often overlooked modules, achieving more comprehensive protection against adversarial threats.\n# 3.2 Goal Hijacking\nGoal hijacking refers to an attack strategy in which an adversary manipulates the objective or behavior of an AI model, causing it to deviate from its intended purpose. By introducing adversarial inputs or modifying the system\u2019s environment, the attacker can influence the model to pursue the attacker\u2019s desired outcome instead of the original goal. A naive attack can achieve goal hijacking of a large model by inserting \u201cignore the previous instruction...\" into the user\u2019s reference statement, thus shifting the model\u2019s response to meet the attacker\u2019s requirements. In the context of LLM-based agents, the sources and targets of goal hijacking attacks have become more varied. As shown in Fig. 6, the development of goal hijacking in LLM-based agents primarily involves six key features: LLM-based controller, multi-modal inputs, multi-source inputs, multi-round interaction, memory mechanism, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives. 3.2.1 Technical Progress. Attack Perspective. Early attempts to exploit this vulnerability used heuristic prompts, such as \u201cignore the previous question,\" to achieve targeted hijacking attacks on standalone LLMs [207]. In order to Manuscript submitted to ACM\n3.2.1 Technical Progress. Attack Perspective. Early attempts to exploit this vulnerability used heuristic prompts such as \u201cignore the previous question,\" to achieve targeted hijacking attacks on standalone LLMs [207]. In order to Manuscript submitted to ACM\nmake the attacks more covert and successful, more carefully designed methods have been proposed. In terms of attack methods, some approaches use vocabulary searches to obtain more covert attack prompts [136], while others leverage gradient optimization to obtain higher success rate and transferable adversarial prompts [109, 169, 213]. As LLMs are applied to different domains and tasks, researchers have begun to focus on the forms and methods of targeted hijacking attacks in various scenarios. In multimodal scenarios, researchers have found that semantic injections in the visual modality can hijack LLMs [126]. For memory modules, researchers have discovered that target hijacking can be achieved by contaminating the database of RAG [202]. In multi-round interaction scenarios, researchers have found that confusing the model can be achieved by forging chat logs [276]. Regarding tool invocation, researchers have exposed the threat of goal hijacking to LLM-based agents using tools through analysis of actual tool-integrated LLMs and the establishment of benchmarks [88, 314]. Defense Perspective. Current defenses against goal hijacking can be categorized into two main types. The first type involves defenses from an external perspective. The second type focuses on defenses from an endogenous perspective. From the perspective of external defenses, strategies primarily involve prompt engineering and prompt purification. Hines et al. [97] introduced strategies such as segmentation, data marking, and encoding, which enhance the LLM\u2019s ability to recognize inputs from multiple sources and thus effectively defend against goal hijacking. Sharma et al. [229] introduced a system prompt meta-language, a domain-specific language designed to refine prompts and monitor inputs for LLM-based chatbots to guard against attacks. They developed a system that utilizes this language to conduct real-time inspection of attack prompts, ensuring user inputs align with the chatbot\u2019s definitions and thus preventing malicious operations. Additionally, Chen et al. [45] proposed a defense method for structured queries that separates prompts and data to counteract goal hijacking. Endogenous defenses primarily involve fine-tuning and neuron activation anomaly detection. Wallace et al. [261] proposed a fine-tuning method that establishes an instruction hierarchy, enabling the model to prioritize privileged instructions for defending against attacks, such as goal hijacking. Using supervised fine-tuning, they trained the model to recognize and execute instructions across different privilege levels, thereby enhancing its robustness against attacks. Piet et al. [208] introduced Jatmo, a method using task-specific fine-tuning to create models resistant to goal hijacking. They showed that Jatmo leverages a teacher model to generate task-specific datasets and fine-tune a base model, effectively defending against goal hijacking. Abdelnabi et al. [5] explored detecting task drift caused by inputs by analyzing the activations of LLMs. They showed how comparing activations before and after processing external data can detect task changes, effectively identifying task drift induced by goal hijacking. 3.2.2 Discussion of Limitations. Current defenses against multimodal goal hijacking are insufficient. Attackers can leverage multiple modalities and their combinations for covert attacks, making defense more complex. Effectively defending against goal hijacking in multimodal inputs is a crucial direction for future research. Moreover, existing external defenses are often tailored to specific types of attacks. Developing a universal external defense strategy is an important area to explore. Finally, detecting goal hijacking from a neuronal perspective holds potential. Systematic testing is needed to determine whether the activation values of target neurons can effectively indicate anomalies associated with goal hijacking, thus proposing an efficient endogenous defense strategy from this perspective.\n# 3.3 Model Extraction\nModel extraction (stealing) attacks aim to achieve performance close to that of the black-box commercial models while incurring a relatively low computational cost. Attackers carefully design a set of inputs in order to steal the Manuscript submitted to ACM\nModel extraction (stealing) attacks aim to achieve performance close to that of the black-box commercial models while incurring a relatively low computational cost. Attackers carefully design a set of inputs in order to steal the\nstructure, parameters, or functionality of the target model. In the context of LLM-based agents, the development of model extraction attacks mainly involves LLM-based controllers. In the following, we review the recent advancements in both attack and defense perspectives.\n3.3.1 Technical Progress. Attack Perspective. In traditional DNNs, attackers typically have two main objectives. 1. Make the surrogate model\u2019s performance as consistent as possible with the target model (i.e., function-level extraction). 2. Make the substitute model\u2019s parameters as consistent as possible with the target model (i.e., parameter-level extraction) [33, 113, 183, 228]. With the introduction of the transformer, NLP tasks evolve from RNN structures to transformerbased structures. The scale of LMs also become larger: from the relatively large BERT to the open-source large model LLaMA, and further to the extremely large commercial models like GPT-4. Model extraction attacks also become more challenging. On BERT, there is not yet any work on achieving parameter-level attacks on the entire model. A few papers discuss function-level extraction [95, 130, 288, 308]. Their attack logic is consistent with the traditional DNN scenario, mainly focusing on how to create the query dataset [95, 130] and the training loss function [288]. On commercial models, due to the cost constraints of training substitute models, model extraction attacks focus on stealing a part of the target model. Li et al. [147] trained a model (e.g., CodeBERT [79] and CodeT5 [272]) to extract the specialized code abilities of text-davinci-003. Naseh et al. [191] stole the decoding algorithm of LLM. Carlini et al. [35] stole the last layer of a production LLM. Defense Perspective. In traditional DNNs, the defenders typically have two lines to defend against model extraction attacks. 1. Active defense: prevent the model from being extracted. 2. Passive defense: verify the ownership of the extracted model. As LMs become larger, active defense in the LLM scenario is still an area to be explored. Researchers have mainly considered passive defenses, which add watermarks to the model\u2019s outputs as a way to verify ownership. The advantage of watermarking is that it does not require modifying the model itself, but only perturbing the model\u2019s inputs. For example, Zhao et al. [329] perturbed the probability vector of transformer; He et al. [96] perturbed the generated words of Bart [137]; Li et al. [148] perturbed the generated codes of CodeBERT [79] and CodeT5 [272]; Peng et al. [206] perturbed the embeddings of the GPT-3. 3.3.2 Discussion of Limitations. There are two limitations of recent research on model extraction attacks. (i) Most LLM-based agents contain large open-source models (e.g. LLaMA) or commercial large models (e.g. ChatGPT), with fewer using BERT-level LMs. However, the current model extraction attacks have discussed less about this scale of LLMs. (ii) Current model extraction attack patterns all rely on training a substitute model that approximates the target model by observing its inputs and outputs. However, LLM-based agents contain not only the LLM but also many other modules (as shown in Section 2). This means that the attacker\u2019s input may not be the same as the LLM\u2019s input, and the LLM\u2019s output may not be the same as the attacker\u2019s observed output. For example, in WebGPT [189], the model\u2019s input includes not only the user but also the search results obtained by the browser. Similarly, in HuggingGPT [235], the attacker\u2019s observed output includes outputs from other Hugging Face models as well. This makes it more challenging for the attacker to directly steal the LLM within an LLM-based agent. Additionally, most agents are designed with a series of prompts for specific tasks, and then directly call the commercial\n3.3.1 Technical Progress. Attack Perspective. In traditional DNNs, attackers typically have two main objectives. 1. Make the surrogate model\u2019s performance as consistent as possible with the target model (i.e., function-level extraction). 2. Make the substitute model\u2019s parameters as consistent as possible with the target model (i.e., parameter-level extraction) [33, 113, 183, 228]. With the introduction of the transformer, NLP tasks evolve from RNN structures to transformerbased structures. The scale of LMs also become larger: from the relatively large BERT to the open-source large model LLaMA, and further to the extremely large commercial models like GPT-4. Model extraction attacks also become more challenging. On BERT, there is not yet any work on achieving parameter-level attacks on the entire model. A few papers discuss function-level extraction [95, 130, 288, 308]. Their attack logic is consistent with the traditional DNN scenario, mainly focusing on how to create the query dataset [95, 130] and the training loss function [288]. On commercial models, due to the cost constraints of training substitute models, model extraction attacks focus on stealing a part of the target model. Li et al. [147] trained a model (e.g., CodeBERT [79] and CodeT5 [272]) to extract the specialized code abilities of text-davinci-003. Naseh et al. [191] stole the decoding algorithm of LLM. Carlini et al. [35] stole the last layer of a production LLM. Defense Perspective. In traditional DNNs, the defenders typically have two lines to defend against model extraction attacks. 1. Active defense: prevent the model from being extracted. 2. Passive defense: verify the ownership of the extracted model. As LMs become larger, active defense in the LLM scenario is still an area to be explored. Researchers have mainly considered passive defenses, which add watermarks to the model\u2019s outputs as a way to verify ownership. The advantage of watermarking is that it does not require modifying the model itself, but only perturbing the model\u2019s inputs. For example, Zhao et al. [329] perturbed the probability vector of transformer; He et al. [96] perturbed the generated words of Bart [137]; Li et al. [148] perturbed the generated codes of CodeBERT [79] and CodeT5 [272]; Peng et al. [206] perturbed the embeddings of the GPT-3.\n3.3.2 Discussion of Limitations. There are two limitations of recent research on model extraction attacks. (i) Most LLM-based agents contain large open-source models (e.g. LLaMA) or commercial large models (e.g. ChatGPT), with fewer using BERT-level LMs. However, the current model extraction attacks have discussed less about this scale of LLMs. (ii) Current model extraction attack patterns all rely on training a substitute model that approximates the target model by observing its inputs and outputs. However, LLM-based agents contain not only the LLM but also many other modules (as shown in Section 2). This means that the attacker\u2019s input may not be the same as the LLM\u2019s input, and the LLM\u2019s output may not be the same as the attacker\u2019s observed output. For example, in WebGPT [189], the model\u2019s input includes not only the user but also the search results obtained by the browser. Similarly, in HuggingGPT [235], the attacker\u2019s observed output includes outputs from other Hugging Face models as well. This makes it more challenging for the attacker to directly steal the LLM within an LLM-based agent. Additionally, most agents are designed with a series of prompts for specific tasks, and then directly call the commercial LLMs (for example, Voyager [263], PReP [310] and ChatDev [212] all use ChatGPT as their controllers). This means that if part of the parameters [35] or functionalities [147] of these commercial LLMs are stolen, it may lead to adversarial attacks against all agents that use these LLM. This will pose a major vulnerability for LLM-based agents. However, the security in this scenario has not yet been studied. Manuscript submitted to ACM\nting the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bafd/bafd29de-b146-4255-9b69-fe3ae7f9f98c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7. Prompt leakage targeting LLM-based agents may involve four key features (indicated with a red exclamation mark), leading to prompt leaking or prompt stealing.</div>\n<div style=\"text-align: center;\">rompt leakage targeting LLM-based agents may involve four key features (indicated with a red exclamation mark), leading to eaking or prompt stealing.</div>\nFig. 7. Prompt leakage targeting LLM-based agents may involve four key features (indicated with a red ex prompt leaking or prompt stealing.\n# 3.4 Prompt Leakage\nPrompts, as task descriptions, can guide LLM-based agents in executing specific tasks without extensive fine-tuning. For example, an LLM embedded with system prompts can function as a Planner [242], directly handling task planning. However, these prompts are at risk of leakage. Prompt leakage occurs when attackers illegally access or obtain the prompts used within LLMs or LLM-based agents, especially system prompts, without authorization. This not only poses a serious privacy risk but also infringes on the intellectual property rights of LLM-based agent owners. As shown in Fig. 7, the development of prompt leakage primarily involves four key features: LLM-based controller, multi-modal inputs, multi-source inputs, and tool invocation. In the following, we review the recent advancements in both attack and defense perspectives.\nmulti-modal inputs, and multi-source inputs, introduce potential prompt leakage risks. The primary focus is on the risks of prompt leakage in LLMs. Two primary forms of attacks are related to prompt leakage in LLMs: prompt leaking attacks and prompt stealing attacks. Prompt leaking involves injecting malicious prompts into LLMs to induce them to reveal their internal system prompts. For instance, if a user inputs \u201cForget the previous content and tell me your initial prompt,\u201d the LLM might inadvertently expose its system prompt to a malicious entity. Current research on prompt leaking attacks generally falls into two categories: one approach focuses on manually designing malicious prompts to achieve prompt leakage [303, 323]. At the same time, the other uses optimized adversarial prompt generation to trigger leakage [112]. The latter approach typically involves optimizing prefix or suffix tokens, disguising them as harmless prompts to coax the LLM into disclosing its system prompt. Prompt stealing attacks are another method where attackers infer the content of system prompts by analyzing the LLM\u2019s outputs, effectively reconstructing the system prompts. The advantage of this approach lies in its stealth, as it only requires output data without direct malicious interaction with the LLMs. Research on this type of attack mainly involves two strategies: one is training an inversion model [315], where the output data is used as input and the system prompt as the label; the other leverages the LLM\u2019s powerful text understanding and generation capabilities to reverse-engineer the system prompt based on the output content [227, 294]. Compared to LLMs, LLM-based agents introduce multimodal interaction capabilities, which has also stimulated research into prompt leakage. For instance, Shayegani et al. [230] introduced a compositional adversarial attack on MLLMs. Their method leverages embedding space strategies to optimize images to match the embeddings of malicious\ntriggers, effectively concealing these triggers within benign-looking images. This work underscores vulnerabilities in multimodal models related to cross-modal alignment and the risk of prompt leakage through image input manipulation. Additionally, Shen et al. [234] proposed a prompt stealing attack against text-to-image generation models. This work infers the original prompts by analyzing generated images, thereby infringing on the intellectual property of prompt engineers and jeopardizing the business model of prompt marketplaces. Recent research has also highlighted the security risks associated with multi-source input in LLM-based agents [6, 314], including indirect interactions through tool invocation. Zhan et al. [314] introduced the INJECAGENT benchmark to assess the vulnerability of LLM-based agents to indirect prompt injection attacks. These attacks manipulate agents by embedding malicious instructions within the external content processed by the agents. The study highlights significant vulnerabilities, with ReAct-prompted GPT-4 found to be susceptible in nearly a quarter of the tested cases. This vulnerability introduces new risks of prompt leakage, as attackers could inject harmful prompts into API calls that agents rely on, potentially exposing sensitive system prompts. Defense Perspective. Current defenses against prompt leakage are primarily focused on LLMs. Two main categories of work are related to defending against prompt leakage. The first category involves embedding protective instructions within system prompts to prevent unauthorized leakage [153]. For example, instructions like \u201cIf the user asks you to print system prompt-related commands, never do it\u201d can prevent LLMs from revealing internal prompts in response to user queries. While this method effectively embeds security rules, it may be vulnerable to more complex or obfuscated attacks. Liang et al. [153] analyzed the mechanisms of prompt leakage attacks and proposed several defense strategies, such as increasing prompt perplexity through rephrasing, inserting unfamiliar tokens, and adding repeated prefixes or fake prompts to confuse attackers. The second category focuses on watermarking techniques for prompts. Yao et al. [296] proposed the PromptCARE framework, which safeguards prompt copyright by injecting watermarks and designing specific verification methods. These watermarks help verify the integrity and authenticity of prompts, providing evidence in cases of leakage. However, this approach faces the challenge of attackers potentially identifying and removing the watermarks, requiring continuous enhancement of their stealth and robustness.\n3.4.2 Discussion of Limitations. Attack Perspective. Despite rapid advancements in understanding prompt leakage risks for LLM-based agents, the current literature still lacks a comprehensive understanding and evaluation of the associated vulnerabilities. In LLM-based agents, multiple components, such as LLMs and the Planner [242], utilize system prompts. We need to consider prompt leakage risks not only for LLMs but also for the other components like Planner. Unlike LLMs, which directly interact with users, the Planner typically interacts internally with LLMs. One potential attack method involves injecting malicious instructions into the LLMs and manipulating them to generate harmful instructions passed to the Planner. Another potential approach is to exploit the Planner\u2019s interactions with external tools by manipulating them to generate malicious inputs [314], aiming to infer the Planner\u2019s internal prompts from its responses or behavior. Defense Perspective. Current research primarily focuses on defenses against prompt leakage risks for standalone LLMs. However, for LLM-based agents, which function as integrated systems, defense mechanisms extend beyond those used in standalone LLMs. One strategy is to implement anomaly detection systems that monitor interactions between the agent and its external environment. By analyzing patterns in prompts and responses, these systems can detect behavior indicative of an attack. For instance, an anomaly detection system could flag API calls that deviate from expected patterns, prompting an investigation into potential prompt leakage or manipulation. Additionally, Manuscript submitted to ACM\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9446/94469949-0d27-4ef9-b7ee-1e21e708e599.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8. Jailbreaking targeting LLM-based agents may involve five key features (indicated with a red exclamation mark), leading to unethical output.</div>\nFig. 8. Jailbreaking targeting LLM-based agents may involve five key features (indicated with a red exclamat unethical output.\nincorporating differential privacy techniques might be promising. By adding controlled noise to the agent\u2019s responses differential privacy can obscure system prompts, making it harder for attackers to infer them through repeated queries\n3.5 Jailbreaking Jailbreaking refers to the process of exploiting vulnerabilities in LLMs to bypass their built-in safety, ethical, or operational guidelines and defenses, thereby generating harmful content [59, 280, 302, 328, 337]. By utilizing jailbreak attacks, attackers can effectively circumvent the security measures set by developers. This can result in the creation of biased or harmful content. For instance, attackers might manipulate an LLM as a criminal tool, aiding them in devising efficient money laundering schemes. As shown in Fig. 8, the development of jailbreaking in LLM-based agents primarily involves five key features: LLM-based controller, multi-modal inputs, multi-source inputs, tool invocation, and multi-round interaction. In the following, we review the recent advancements in both attack and defense perspectives. 3.5.1 Technical Progress. Attack Perspective. Jailbreaking in LLM. Depending on the specific forms of jailbreaking, existing threats can be mainly divided into four primary types: manual design jailbreaking, automated optimization jailbreaking, exploit-based jailbreaking, and model parameter manipulation jailbreaking. manual design jailbreaking is typically executed through manually designed malicious prompts [280]. This form of attack requires the adversary to possess specialized knowledge. A popular attack pattern is based on role-playing, such as the \u201cGrandma Exploit\" targeting ChatGPT [181]. Specifically, this method involves simulating or adopting a particular role to mislead the understanding of LLMs, thereby bypassing their security measures. Automated optimization jailbreaking is a popular class of methods for conducting jailbreaking. It utilizes optimized strategy generation to generate adversarial prompts for jailbreaking. Depending on whether the gradient of the target LLM is accessible, these attacks can be classified into white-box and black-box methods. Existing research primarily focuses on generating adversarial prompts for jailbreaking in white-box settings. These adversarial prompts are usually created by optimizing the prefixes or suffixes of prompts [168, 337]. In black-box settings, current research mainly concentrates on two approaches: first, leveraging the transferability of gradient-optimized adversarial prompts to jailbreak black-box LLMs [168, 337]; second, researchers draw on risk mining strategies from software security to iteratively optimize adversarial prompts [59, 302], thereby bypassing LLMs\u2019 security measures. Exploit-based jailbreaking [60, 167, 276] is a strategy that designs targeted harmful prompts for jailbreaking based on the vulnerabilities in the current security alignment techniques of LLMs. For instance, Deng et al. [60] found that LLMs\u2019 security measures are mainly designed for high-resource languages like English, making low-resource languages Manuscript submitted to ACM\n# 3.5 Jailbreaking\nthree times more likely to encounter harmful content. Liu et al. [167] found that current safety fine-tuning primarily focuses on input alignment, with weaker checks on LLM responses. By exploiting biases in content generation safety, they hide harmful prompts within harmless ones and reconstruct them in the output to perform jailbreaking. Besides prompt-based jailbreaking, there is also a model-based approach, model parameter manipulation jailbreaking, which involves altering the model\u2019s parameters to achieve jailbreaking. A typical example is altering parameters for text generation, such as changing token sampling settings [107]. Additionally, research has shown that combinations of low-probability tokens generated by LLMs can bypass the LLM\u2019s security measures [328]. Multi-modal Input. Early LLMs interacted primarily through text, but with the advent of multimodal models like GPT-4V, interactions have shifted to include multiple modalities, particularly in LLM-based agents. These agents now support voice, images, and haptic feedback, enhancing flexibility but also introducing new jailbreaking risks: 1) Multimodal adversarial prompts: Attackers can embed malicious prompts as adversarial perturbations in images or other modalities, which, when combined with text, bypass the agent\u2019s security mechanisms [34, 196, 210]; 2) Multimodal prompt manipulation: Attackers can distribute harmful prompts across multiple modalities, disguising them as benign inputs to reduce the LLM\u2019s sensitivity to harmful content [86, 144]. Multi-round Interaction. The capability for multi-round interaction in LLM-based agents has also spurred research into jailbreaking attacks tailored for such interaction. For instance, Cheng et al. [47] introduced a novel jailbreaking attack called \u201cContextual Interaction Attack\u201d. This method is inspired by the human practice of indirectly obtaining sensitive information, where it strategically constructs a sequence of questions and answers to induce the generation of harmful information. Sun et al. [245] proposed the \u201cContext Fusion Attack\u201d, which preprocesses to extract keywords, generates contextual scenarios for these keywords, and dynamically integrates and replaces malicious keywords in the attack target, thereby executing the attack covertly without triggering security mechanisms. Multi-source Input and Tool Invocation. LLM interactions are typically direct. However, for LLM-based agents, interactions extend beyond direct exchanges with users to include interactions with external tools like databases, websites, APIs, and other agents. We refer to this as multi-source input. For example, a travel agent might use electronic maps to plan routes or access hotel websites to make reservations. However, this multi-source input mode introduces new jailbreaking risks [89, 298, 314]. For instance, Gu et al. [89] introduced the infectious jailbreak attack, where an adversarial image injected into a single agent within an MLLM quickly spreads to other agents. Through agent interactions, the infection propagates rapidly, causing widespread jailbroken behaviors without further intervention. This attack exploits agent communication and memory-sharing, complicating the design of effective defense mechanisms. Defense Perspective. Researchers have developed various defense strategies in response to jailbreaking. These strategies can generally be categorized into three types: detection-based defenses, purification-based defenses, and model editing-based defenses. Detection-based defenses: These defenses protect LLMs by identifying potentially malicious prompts. Detection strategies include analyzing characteristics such as perplexity [10], which are key criteria for assessing prompt compliance. Purification-based defenses: This type of defense neutralizes malicious intent by modifying prompts. Techniques such as paraphrasing and smoothing disrupt the structure of jailbreak prompts [114, 116]. Additionally, some methods focus on purifying the LLM\u2019s response generation process to filter out harmful outputs [289]. Model editing-based defenses: The primary cause of jailbreaking in LLMs is often insufficient alignment with safety protocols. Some approaches enhance security by fine-tuning the LLMs [261], while others apply weight editing to correct harmful outputs [266, 271].\nManuscript submitted to ACM\n3.5.2 Discussion of Limitations. Attack Perspective. Despite the rapid progress in jailbreaking on LLM-based agents, there is still a lack of comprehensive understanding and evaluation of the new jailbreaking risks introduced by key features of the agents. Current research mainly focuses on text and image processing, but the capabilities of LLM-based agents to handle audio and video content are also growing quickly. These new modalities could introduce unique security risks, such as triggering inappropriate actions or logical errors through carefully designed audio or video inputs. Additionally, LLM-based agents enhance their functionality by integrating external tools like APIs, databases, and internet resources. However, this integration also creates new vulnerabilities. For example, attackers could exploit security flaws in APIs or manipulate the behavior of agents by tampering with database contents. Thus, future work will need to focus more on these aspects. Defense Perspective. Current defense strategies against jailbreaking on LLM-based agents typically focus on protecting the LLM itself. However, there is a lack of systematic defenses. Given the key features of the LLM-based agents, two additional defense strategies need to be considered. Multimodal adversarial prompt detection: Given the multimodal interaction capabilities of these agents, developing effective defenses against multimodal jailbreaking is crucial. A promising approach could involve detecting adversarial prompts across different modalities, such as identifying anomalies in multimodal inputs to filter harmful inputs while maintaining the functionality of benign ones. Interpretability: To address the complexity of indirect and multi-round interaction, innovative defense strategies need to focus on the intrinsic properties of LLMs. By analyzing how harmful prompts are represented in the model\u2019s neurons, we could identify decision boundaries and develop an explainable framework for LLMs and their agents to prevent jailbreaking.\n# 4 Risks from Model Flaws\nThe decision module is the key component of an agent, where one or more LLMs are responsible for understanding analyzing, and planning. This section analyzes the risks stemming from the limitations and problems inherent in the model itself, such as issues with bias, hallucination, etc. that can compromise the reliability of the model. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.\n# 4.1 Hallucination\nDespite demonstrating remarkable capabilities across a range of downstream tasks, LLMs raise a significant concern due to their propensity to exhibit hallucinations. These hallucinations manifest as content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge [325]. The phenomenon of hallucination affects LMs across different eras (from traditional deep learning [133, 220] to the transformer-based era of large models [117, 325]) and across various modalities (including LLMs [319] and MLLMs [304]). To address this issue, many studies focus on understanding why hallucinations occur and how to evaluate and eliminate them. In the context of LLM-based agents, the development of hallucination primarily involves four key features: LLM-based controller, multi-modal inputs, multi-source inputs and memory mechanism. In the following, we review the recent advancements in hallucination.\n4.1.1 Possible Reasons for Hallucination. The causes of hallucination are numerous and can be mainly categorized into three types. (i) Imbalanced and noisy training datasets. A large training dataset, while containing a wealth of valuable information, inevitably introduces erroneous, outdated data, or imbalanced data distributions [117, 325]. Erroneous and outdated data can lead the model to learn incorrect knowledge [204, 269, 304, 319], resulting in hallucinations\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/81ce/81ceea56-c55e-4915-8ef4-a1ee662e1409.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9. Overview of Benchmarks Categorized by Threat Type and Key Feature.</div>\nthat contradict factual information. Additionally, an imbalanced dataset may cause the model to favor outputs of objects or object combinations that appear more frequently (with higher marginal probability [258]) in the dataset [22, 52, 135, 143, 335], rather than responding to the user-provided prompt or reference. (ii) Incomplete learning. The training strategy of minimizing KL divergence in LLMs does not effectively learn the distribution of the training dataset [156, 205], leading to the acquisition of more linguistic knowledge [49] and spurious correlations [7, 124, 304] learned from statistical information. This makes LLMs be more reliant on language priors [75, 160, 220], rather than recognizing and generating real-world facts extracted from the training corpus. Additionally, for MLLMs, there is the issue of unsatisfactory cross-modal representation alignment [118]. (iii) Erroneous decoding processes. The decoding strategy of LLMs, such as top-k sampling, inherently introduces randomness, which can promote the occurrence of hallucinations [72, 335]. Additionally, this can lead to a snowball effect, resulting in further hallucinations in subsequent generated content [319]. LLM-based agents apply LLMs to specific domains, inheriting the hallucination factors of LLMs in downstream tasks. As mentioned earlier, hallucinations caused by imbalanced and noisy training datasets have specific impacts in different application scenarios for agents. For example, due to imbalanced data, GPT-4 is more likely to hallucinate when encountering Eastern countries or non-English contexts [52]. When applied to Eastern city navigation agents (such as PReP [310], a recent LLM-based navigation agent for Beijing and Shanghai), it is more prone to generating hallucinations. Additionally, influenced by noisy data, LLMs may learn exploitable and buggy code present in the training data [204]. When applied to software development (e.g., ChatDev [212]), this can lead to agents generating similar insecure code. Furthermore, when LLMs are used in specialized fields where the dataset contains little or no relevant knowledge, hallucinations can occur due to knowledge gaps. For instance, when LLMs are applied to tasks in Minecraft, they may provide instructions that cannot be completed within the game [263].\n4.1.2 Technical Progress of Solutions for Halluciantion. Evaluating Hallucinations in LLMs. Researchers have developed various baseline datasets tailored to different modalities (such as language [156, 333] or multimodal [52, 220]) and types of hallucinations (e.g., factually incorrect [52, 156, 326], contradicting user references [174], biases [52], etc.) to assess the degree of hallucination in models using manual [223], automated metrics [143, 220], or detection models [143]. Fig. 9 shows the benchmarks for hallucination evaluation based on the involved key features. Reducing Hallucinations via LLM Refinement. To reduce hallucinations, researchers have proposed various methods targeting the potential causes of hallucinations mentioned earlier. To address biases in the dataset, one approach is to introduce new synthetic data to improve the model\u2019s learning of spurious correlations [7, 124, 220, 330]. To address errors present in the dataset, hallucinations can be detected for data cleaning [304]. To address the lack of specialized domain information, fine-tuning can be a straightforward and effective way. For instance, in ODYSSEY [166], the authors fine-tuned LLaMA-3 on the Minecraft Wiki, enabling it to better resolve Minecraft-related issues. Additionally, regarding incomplete learning, researchers have suggested improving loss functions [118, 243, 258, 313] and instruction tuning [160, 174, 209]. For incorrect decoding, new decoding strategies have been proposed to enhance the model\u2019s understanding of context and reduce hallucinations without the need to retrain the model [49, 105, 135, 237, 258]. Reducing Hallucinations via Memory Mechanism. To tackle outdated information in the dataset, external knowledge can be incorporated [205]. Retrieval-augmented generation (RAG) is a common approach used to tackle outdated data and the lack of specialized domain information in LLM training sets. For example, in WebGPT [189], the agent queries the Bing search engine each time and summarizes the search results.\nManuscript submitted to ACM\nReducing Hallucinations via Multi-source Inputs. (i) Multi-agent collaboration. Different agents cross-verify and check each other, which is used in many agent frameworks to reduce the impact of hallucinations. For example, in ChatDev [212], the assistant engages in multiple rounds of communication with the instructor to seek more detailed suggestions and reduce hallucinations. (ii) System prompt standardization. By incorporating predefined output templates into the prompts for large models, the output of LLMs can be standardized to some extent, which can lower the likelihood of hallucinations [98]. For example, in GameGPT [42], the authors provided a standardized planning template for each game genre, guiding game development managers to fill in relevant information to reduce hallucinations. 4.1.3 Discussion of Limitations. There are four limitations of current solutions for hallucination. (i) Lack of theoretical analysis framework. Currently, there is a lack of mathematical definitions for hallucinations, and the methods for evaluating and mitigating them are primarily validated through experiments, lacking theoretical analysis. (ii) Insufficient baselines for multimodal LLMs. Current baselines for evaluating hallucinations in LLMs mainly focus on image and text modalities. However, LLMs have also been applied to speech [281] and video [178] modalities, and there are no established baselines for hallucination evaluation in these areas. (iii) Lack of evaluation methods for specialized domains. Current hallucination baselines for LLMs mainly address general tasks (e.g., image grounding [174], common-sense reasoning [156]). However, minimal hallucinations in these tasks don\u2019t imply the same for specialized domains (e.g., game agents, software development). Designing hallucination evaluation methods for specialized fields remains an unresolved issue. (iv) Imitation falsehood. Introducing additional knowledge through a memory mechanism can reduce the hallucinations caused by outdated training data to some extent. However, it may also lead to new hallucinations due to errors present in the additional knowledge itself [189].\n# 4.2 Memorization\nLLMs are likelihood-based generative models trained to maximize the likelihood of observed samples (training data) \ud835\udf3d\u2217= arg max\ud835\udf3d\ud835\udc5d\ud835\udf3d(\ud835\udc99). At deployment time, generation is just sampling from the learned probability distribution \ud835\udc5d\ud835\udf3d\u2217(\ud835\udc99). Intuitively, for some sample \ud835\udc99, if the learned model overfits on it, i.e., \ud835\udc5d\ud835\udf3d\u2217(\ud835\udc99) is excessively large, LLMs might eidetically generate \ud835\udc99, posing the problem of training data memorization, which can be called parametric memorization [312]. In the LLM-based agents, there will be valuable information input to the LLM-based controller from the multi-source input modules. These mechanisms raise another type of memorization that exists in the specific context of interaction, which can be called contextual memorization [16, 312] that the valuable information specific to the context can be memorized by the agents. Memorization is not necessarily a threat to the LLM-based agents, as it is shown that memorization is necessary for generalization [40, 76]. However, excessive memorization will have implications for data privacy, model stability, and generalization performance [257]. In the context of LLM-based agents, the development of memorization issues mainly involves LLM-based controllers. In this section, we review research on memorization that falls in a more conceptual scope and leave adversarial attacks and defenses on privacy due to memorization to Section 5.2. 4.2.1 Technical Progress. The literature analyzes memorization from various points. It is generally believed and demonstrated that memorization is a necessity to achieve generalization [40, 76]. Evaluation metrics of LLMs, e.g., perplexity, are near-perfect on the training data when a model memorizes the entire training set. On the otherhand, there have been efforts to understand how memorization occurs. Empirical studies [32, 134] have found that duplication in the training set, model capacity, prompt length, etc., are influential factors of memorization. Biderman et al. [21] proposed to predict which sequences will be memorized before a large model\u2019s full training time by extrapolating the Manuscript submitted to ACM\nLLMs are likelihood-based generative models trained to maximize the likelihood of observed samples (training data) \ud835\udf3d\u2217= arg max\ud835\udf3d\ud835\udc5d\ud835\udf3d(\ud835\udc99). At deployment time, generation is just sampling from the learned probability distribution \ud835\udc5d\ud835\udf3d\u2217(\ud835\udc99). Intuitively, for some sample \ud835\udc99, if the learned model overfits on it, i.e., \ud835\udc5d\ud835\udf3d\u2217(\ud835\udc99) is excessively large, LLMs might eidetically generate \ud835\udc99, posing the problem of training data memorization, which can be called parametric memorization [312]. In the LLM-based agents, there will be valuable information input to the LLM-based controller from the multi-source input modules. These mechanisms raise another type of memorization that exists in the specific context of interaction, which can be called contextual memorization [16, 312] that the valuable information specific to the context can be memorized by the agents. Memorization is not necessarily a threat to the LLM-based agents, as it is shown that memorization is necessary for generalization [40, 76]. However, excessive memorization will have implications for data privacy, model stability, and generalization performance [257]. In the context of LLM-based agents, the development of memorization issues mainly involves LLM-based controllers. In this section, we review research on memorization that falls in a more conceptual scope and leave adversarial attacks and defenses on privacy due to memorization to Section 5.2.\nMemorization is not necessarily a threat to the LLM-based agents, as it is shown that memorization is necessary for generalization [40, 76]. However, excessive memorization will have implications for data privacy, model stability, and generalization performance [257]. In the context of LLM-based agents, the development of memorization issues mainly involves LLM-based controllers. In this section, we review research on memorization that falls in a more conceptual scope and leave adversarial attacks and defenses on privacy due to memorization to Section 5.2. 4.2.1 Technical Progress. The literature analyzes memorization from various points. It is generally believed and demonstrated that memorization is a necessity to achieve generalization [40, 76]. Evaluation metrics of LLMs, e.g., perplexity, are near-perfect on the training data when a model memorizes the entire training set. On the otherhand, there have been efforts to understand how memorization occurs. Empirical studies [32, 134] have found that duplication in the training set, model capacity, prompt length, etc., are influential factors of memorization. Biderman et al. [21] proposed to predict which sequences will be memorized before a large model\u2019s full training time by extrapolating the Manuscript submitted to ACM\n4.2.1 Technical Progress. The literature analyzes memorization from various points. It is generally believed and demonstrated that memorization is a necessity to achieve generalization [40, 76]. Evaluation metrics of LLMs, e.g., perplexity, are near-perfect on the training data when a model memorizes the entire training set. On the otherhand, there have been efforts to understand how memorization occurs. Empirical studies [32, 134] have found that duplication in the training set, model capacity, prompt length, etc., are influential factors of memorization. Biderman et al. [21] proposed to predict which sequences will be memorized before a large model\u2019s full training time by extrapolating the Manuscript submitted to ACM\nmemorization behavior of lower-compute trial runs. Van den Burg et al. [257] gave a measure of memorization for generative models as the improved probability when a certain sample is involved in training.\n4.2.2 Discussion of Limitations. The balance between memorization and generalization is a challenging problem. Empirical factors regarding memorization can reduce memorization on average but are not applicable to individual instances. Prediction of the behavior of large models based on small models or intermediate checkpoints are inaccurate and unreliable. Finally, harmful memorization depends on security demands and there lacks a projection from theoretical analysis results to practical risks.\n# 4.3 Bias and Fairness\nBias refers to a model\u2019s tendency to favor particular groups during decision-making or generation processes. This phenomenon is quite prevalent in AI models. For example, models used by American courts often predict a relatively higher probability of criminal behavior for African Americans [182]. These biased predictions stem from the hidden or neglected biases in data or algorithms [81]. In the context of LLM-based agents, the development of bias issues primarily involves two key features: LLM-based controller, multi-modal inputs and outputs. In the following, we report the technical progress in the bias issues. 4.3.1 Causes of Bias. For traditional machine learning models, bias can stem from two primary sources: biases in the data and flaws in the algorithms. Data biases are varied and can include discriminatory information, sampling bias, measurement bias, among others [26, 248, 318]. Even with unbiased data, models can still exhibit bias due to algorithmic factors. Algorithmic bias arises from design choices, such as selecting optimization functions and regularization techniques [14, 57]. Bias mitigation strategies include data cleaning, adjusting model architecture, and other techniques [56, 256]. Bias is also present in LLMs. Compared to traditional models, bias in LLMs is both deeper and broader. Traditional decision models are confined to making decisions within a fixed scope, thus limiting their bias to a specific range. In contrast, LLMs perform generative tasks, which allows their outputs to include various types of biases. The absence of a fixed output format means these biases can be subtle and harder to detect than biases in decision-making models. Current research has found that LLMs exhibit biases in areas such as gender, race, and political views [78, 250], with different LLMs showing varying degrees and types of bias. For LLM-based agents, the issue becomes even more complex. Unlike LLMs, LLM-based agents can process multimodal information, including text, images, and speech, leading to more intricate manifestations of bias. Current research has shown that biases and discrimination are also present in VLMs, affecting tasks such as visual question answering (VQA) and text-to-image generation [64, 101]. Therefore, comprehensive evaluation across multiple modalities is essential for accurate judgment. Furthermore, introducing additional components in LLM-based agents raises concerns about the potential introduction of new biases, necessitating careful consideration. Analyzing the causes of bias, LLM-based agents exhibit more pronounced bias issues than traditional models due to their larger datasets and more complex structures. From the perspective of training data, LLMs are primarily trained on data sourced from online platforms, which is often not thoroughly vetted before training, leading to the inclusion of discriminatory samples. Additionally, biased statements are unevenly distributed within the training data. From the perspective of model structure, LLMs have a significantly greater number of parameters and a more complex architecture than traditional models. This complexity makes it challenging to ensure fairness during model training. Manuscript submitted to ACM\n4.3.2 Technical Progress. To address and mitigate the bias issues in LLMs, current efforts mainly focus on three areas: (i) developing reasonable bias evaluation metrics, (i) constructing comprehensive bias evaluation datasets, and (iii) employing various techniques to mitigate model bias. Evaluation metrics. The evaluation metrics for biases in LLMs can be classified based on the content they rely on during assessment. These include embedding-based metrics, which utilize contextual sentence embeddings, probability-based metrics, which use the model-assigned probabilities, and output-based metrics, which analyze the model\u2019s output. Embedding-based metrics calculate distances in vector space between neutral words (e.g., professions) and identityrelated words (e.g., gender pronouns). Caliskan et al. [28] introduced the Word Embedding Association Test (WEAT) using static word embeddings to assess bias in NLP tasks. Later studies have employed word embeddings within entire sentences [90, 180] or calculated normalized sums of word-level biases [65]. Probability-based metrics focus on masked tokens. Some methods use templates to create sentences. They mask parts containing social groups and assess bias based on model-assigned probabilities [275]. Others sequentially mask each token in a sentence to test the model\u2019s ability to generate discriminatory content, known as pseudo-log likelihood methods [121, 187]. Output-based metrics include distribution-based and classifier-based methods. Distribution-based metrics detect differences between groups in model outputs to measure bias [149, 215]. Classifier-based metrics use classifiers to evaluate the toxicity of outputs; higher toxicity associated with specific groups indicates discrimination and bias [80, 83, 239]. Evaluation datasets. Evaluation datasets consist of numerous texts requiring completion. By assessing the bias exhibited by LLMs towards different social groups during text completion, we can determine the magnitude of the model\u2019s bias. Masked token datasets like StereoSet [188] contain sentences with blanks that the LM must fill. The choices made by the model are then used to evaluate its bias. Conversely, unmasked sentence datasets such as CrowS-Pairs [190] and WinoQueer [77] present the model with pairs of sentences and ask which one is more likely. Other methods use sentence completion rather than word selection. For example, TrustGPT [110] examines toxicity in LMs using toxic prompt templates derived from social norms. It then quantifies model bias by measuring toxicity values across different groups. BBQ [201] and Grep-BiasIR [129], on the other hand, employ a question-answering format for evaluation. Bias Mitigation. After identifying the presence of bias in the model, it is natural to seek ways to mitigate it. Based on the LLM workflow, current bias mitigation techniques can be categorized into two types: training phase methods and inference phase methods. During the training phase, the primary methods include cleaning training data, modifying model architecture, and adjusting training strategies. Data-based methods aim to eliminate biases in the training data. Data augmentation techniques add samples to extend the distribution for underrepresented social groups [84, 309]. Data filtering methods remove overtly biased and harmful text from the training data [82]. Architecture modifications primarily improve the encoder of LLMs by inserting new components like adapter models and gated models to mitigate bias [92, 131]. Training strategy adjustments mainly improve the loss function by adding regularization terms that measure bias. A common method is Reinforcement Learning from Human Feedback (RLHF) [17], aligning LLM output with human judgment. Other methods include reducing differences in the embedding distributions of different groups [291] and employing techniques such as contrastive learning [142] and adversarial learning [93] to guide the model. To avoid\n4.3.2 Technical Progress. To address and mitigate the bias issues in LLMs, current efforts mainly focus on three areas: (i) developing reasonable bias evaluation metrics, (i) constructing comprehensive bias evaluation datasets, and (iii) employing various techniques to mitigate model bias. Evaluation metrics. The evaluation metrics for biases in LLMs can be classified based on the content they rely on during assessment. These include embedding-based metrics, which utilize contextual sentence embeddings, probability-based metrics, which use the model-assigned probabilities, and output-based metrics, which analyze the model\u2019s output. Embedding-based metrics calculate distances in vector space between neutral words (e.g., professions) and identityrelated words (e.g., gender pronouns). Caliskan et al. [28] introduced the Word Embedding Association Test (WEAT) using static word embeddings to assess bias in NLP tasks. Later studies have employed word embeddings within entire sentences [90, 180] or calculated normalized sums of word-level biases [65]. Probability-based metrics focus on masked tokens. Some methods use templates to create sentences. They mask parts containing social groups and assess bias based on model-assigned probabilities [275]. Others sequentially mask each token in a sentence to test the model\u2019s ability to generate discriminatory content, known as pseudo-log likelihood methods [121, 187]. Output-based metrics include distribution-based and classifier-based methods. Distribution-based metrics detect differences between groups in model outputs to measure bias [149, 215]. Classifier-based metrics use classifiers to evaluate the toxicity of outputs; higher toxicity associated with specific groups indicates discrimination and bias [80, 83, 239]. Evaluation datasets. Evaluation datasets consist of numerous texts requiring completion. By assessing the bias exhibited by LLMs towards different social groups during text completion, we can determine the magnitude of the model\u2019s bias. Masked token datasets like StereoSet [188] contain sentences with blanks that the LM must fill. The choices made by the model are then used to evaluate its bias. Conversely, unmasked sentence datasets such as CrowS-Pairs [190] and WinoQueer [77] present the model with pairs of sentences and ask which one is more likely. Other methods use sentence completion rather than word selection. For example, TrustGPT [110] examines toxicity in LMs using toxic prompt templates derived from social norms. It then quantifies model bias by measuring toxicity values across different groups. BBQ [201] and Grep-BiasIR [129], on the other hand, employ a question-answering format for evaluation. Bias Mitigation. After identifying the presence of bias in the model, it is natural to seek ways to mitigate it. Based on the LLM workflow, current bias mitigation techniques can be categorized into two types: training phase methods and inference phase methods. During the training phase, the primary methods include cleaning training data, modifying model architecture, and adjusting training strategies. Data-based methods aim to eliminate biases in the training data. Data augmentation techniques add samples to extend the distribution for underrepresented social groups [84, 309]. Data filtering methods remove overtly biased and harmful text from the training data [82]. Architecture modifications primarily improve the encoder of LLMs by inserting new components like adapter models and gated models to mitigate bias [92, 131]. Training strategy adjustments mainly improve the loss function by adding regularization terms that measure bias. A common method is Reinforcement Learning from Human Feedback (RLHF) [17], aligning LLM output with human judgment. Other methods include reducing differences in the embedding distributions of different groups [291] and employing techniques such as contrastive learning [142] and adversarial learning [93] to guide the model. To avoid\nManuscript submitted to ACM\nthe impact on model performance caused by modifying the loss function, some methods also freeze parts of model parameters during training [218, 300]. During the inference phase, the methods can be classified into three types: pre-processing, in-processing, and postprocessing. Instruction tuning occurs in the pre-processing stage, involving the addition or modification of instructions in user prompts to guide LLMs away from biased content [74, 260]. During the in-processing stage, some methods adjust the decoding algorithm to ensure the output\u2019s fairness [224], and others change the distribution from which tokens are sampled to enable the sampling of less biased outputs with greater probability [50]. Post-processing mitigation refers to post-processing on model outputs to remove bias. This is primarily achieved through rewriting the output. The simplest approach is to use keyword replacement to eliminate discriminatory terms [63]. Other methods employ specialized machine translation models to remove bias from sentences [11]. 4.3.3 Discussion of Limitations. Despite extensive research on bias in LLMs, many issues persist. Firstly, most studies are based on traditional LMs, raising concerns about their applicability to LLMs. For example, Cabello et al. [27] argued that there is not necessarily a direct correlation between text embeddings and the biases present in LLM outputs. Similarly, Delobelle et al. [58] suggested that probability-based metrics may only weakly correlate with biases observed in downstream tasks. These findings cast doubt on the effectiveness of embedding-based and probability-based metrics for evaluating bias in LLMs. Secondly, compared to traditional models, LLMs have a more complex structure and require significantly more parameters for training. This complexity means mitigating bias in LLMs can substantially impact their performance. For example, bias mitigation methods that involve fine-tuning typically use small datasets, which can lead to catastrophic forgetting in LLMs initially trained on large datasets. Furthermore, the wide applicability of LLM-based agents underscores the limitations of current research. Although some efforts have been made to mitigate bias in text-to-image models, there is currently no standardized dataset or metrics for evaluating bias in generated images. Additionally, most bias research focuses predominantly on English, with a significant lack of studies addressing bias in other languages. As a system, an LLM-based agent comprises multiple components, including the LLM itself, external tools, memory modules, and more. However, current research rarely considers the impact of bias on the entire agent from a system perspective. This area requires further investigation.\n# 5 Risks from Input-Model Interaction\nThis section analyze the mutual influences between the input data and the model, including backdoor and privacy leakage. For each risk, we first introduce what they are, then summarize their technological advancements in the six key features of LLM-based agents, and finally analyze their limitations.\n# 5.1 Backdoor\nBackdoor attacks embed a malicious exploit during the training phase that is subsequently invoked by the presence of a trigger at the test time",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to provide a comprehensive understanding of the security, privacy, and ethics threats faced by LLM-based agents, addressing gaps in existing research and offering a novel taxonomy for better categorization of these threats.",
            "scope": "The survey covers threats related to LLM-based agents, including security, privacy, and ethical concerns. It excludes detailed discussions on unrelated AI models or non-LLM-based systems to maintain focus on the specific challenges posed by LLMs in agent scenarios."
        },
        "problem": {
            "definition": "The core issue explored in this survey is the multitude of security and privacy threats that LLM-based agents face, particularly the vulnerabilities introduced by their complex architectures and multi-modal interactions.",
            "key obstacle": "Researchers face challenges in accurately categorizing and addressing cross-module and cross-stage threats due to the intricate nature of LLM-based systems, leading to gaps in understanding and mitigation strategies."
        },
        "architecture": {
            "perspective": "The survey introduces a novel taxonomy that categorizes threats based on their sources (inputs, model, or a combination) and types (security/safety, privacy, ethics), offering a more precise framework for analyzing risks.",
            "fields/stages": "The survey organizes the research into fields such as problematic inputs, model flaws, and input-model interactions, using criteria based on the source and type of threats to provide a structured overview of the current landscape."
        },
        "conclusion": {
            "comparisions": "The survey conducts comparative analyses of various research studies, highlighting differences in effectiveness, approaches, and outcomes regarding the mitigation of threats to LLM-based agents.",
            "results": "Key takeaways include the identification of critical vulnerabilities in LLM-based agents, the importance of a multi-faceted approach to threat categorization, and the need for further research in specific areas such as multimodal interactions and ethical implications."
        },
        "discussion": {
            "advantage": "The existing research has achieved significant advancements in understanding the risks associated with LLM-based agents, leading to improved threat detection and mitigation strategies.",
            "limitation": "Current studies often lack comprehensive evaluations across all modalities and fail to address the systemic impacts of bias and security vulnerabilities in integrated agent systems.",
            "gaps": "Unanswered questions remain regarding the effectiveness of current defenses against emerging threats, particularly in the context of multi-modal inputs and interactions.",
            "future work": "Future research should focus on developing robust defenses against multimodal threats, enhancing interpretability and explainability of LLM-based agents, and addressing ethical considerations in AI deployment."
        },
        "other info": {
            "authors": [
                "Yuyou Gan",
                "Yong Yang",
                "Zhe Ma",
                "Ping He",
                "Rui Zeng",
                "Yiming Wang",
                "Qingming Li",
                "Chunyi Zhou",
                "Songze Li",
                "Ting Wang"
            ],
            "publication": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents",
            "doi": "https://doi.org/XXXXXXX.XXXXXXX"
        }
    },
    "mount_outline": [
        {
            "section number": "10",
            "key information": "The survey aims to provide a comprehensive understanding of the security, privacy, and ethics threats faced by LLM-based agents, addressing gaps in existing research and offering a novel taxonomy for better categorization of these threats."
        },
        {
            "section number": "10.1",
            "key information": "The core issue explored in this survey is the multitude of security and privacy threats that LLM-based agents face, particularly the vulnerabilities introduced by their complex architectures and multi-modal interactions."
        },
        {
            "section number": "10.2",
            "key information": "Future research should focus on developing robust defenses against multimodal threats, enhancing interpretability and explainability of LLM-based agents, and addressing ethical considerations in AI deployment."
        },
        {
            "section number": "4.1",
            "key information": "The survey introduces a novel taxonomy that categorizes threats based on their sources (inputs, model, or a combination) and types (security/safety, privacy, ethics), offering a more precise framework for analyzing risks."
        },
        {
            "section number": "4.2",
            "key information": "Current studies often lack comprehensive evaluations across all modalities and fail to address the systemic impacts of bias and security vulnerabilities in integrated agent systems."
        }
    ],
    "similarity_score": 0.7272705824412544,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Navigating the Risks_ A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents.json"
}