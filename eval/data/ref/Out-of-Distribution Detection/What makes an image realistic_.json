{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2403.04493",
    "title": "What makes an image realistic?",
    "abstract": "The last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism.",
    "bib_name": "theis2024makesimagerealistic",
    "md_text": "# What makes an image realistic?\n# Abstract\nThe last decade has seen tremendous progress in our ability to generate realistic-looking data, be it images, text, audio, or video. Here, we discuss the closely related problem of quantifying realism, that is, designing functions that can reliably tell realistic data from unrealistic data. This problem turns out to be significantly harder to solve and remains poorly understood, despite its prevalence in machine learning and recent breakthroughs in generative AI. Drawing on insights from algorithmic information theory, we discuss why this problem is challenging, why a good generative model alone is insufficient to solve it, and what a good solution would look like. In particular, we introduce the notion of a universal critic, which unlike adversarial critics does not require adversarial training. While universal critics are not immediately practical, they can serve both as a North Star for guiding practical implementations and as a tool for analyzing existing attempts to capture realism.\narXiv:2403.04493v4\n# 1. Introduction\nWhat distinguishes realistic images from unrealistic images? Humans are able to detect a wide variety of flaws in images and other sensory data, yet there are no robust losses which could be used to penalize unrealistic images across a broad set of tasks in machine learning, and no widely accepted formal notion of realism exists today. In particular, we are interested in real-valued functions U producing a low value U(x) when some data x is realistic and a large value when x is unrealistic. Here, x could be a single image, a small set of images, or a video. But our discussion will also be relevant for other types of data such as text of arbitrary length or more generally any data drawn from some distribution which we will denote P.\nPotential applications of such functions are plentiful and include anomaly detection (Ruff et al., 2021), deepfake detection (Sha et al., 2023; Pondoc et al., 2023), generative model evaluation (Theis et al., 2016; Heusel et al.,\n1Google DeepMind, London, UK. Correspondence to: Lucas Theis <theis@google.com>.\n2017; Borji, 2019), model distillation (van den Oord et al., 2018; Yin et al., 2023), neural compression (Ball\u00b4e et al., 2021; Yang et al., 2023), computational photography (Fang et al., 2020), and computer graphics (Herzog et al., 2012; Reinhard et al., 2013; Poole et al., 2023). Unfortunately, their implementation is extremely challenging. Our ability to generate realistic data is rapidly improving (e.g., Dhariwal and Nichol, 2021) yet no reliable candidates or recipes for constructing U exist in machine learning today. This is not for a lack of trying. While some progress has been made in the detection of unrealistic examples, the design of functions that are robust to optimization (for tasks involving generation) has been less successful. The latter problem is significantly harder because our function now not only has to detect a limited set of artefacts but has to anticipate any unrealistic examples an optimization might run into. Weaknesses in a function\u2019s design often only make themselves known once we start optimizing (Ding et al., 2021). Complicating the matter is the fact that the optimization depends on U itself.\nTo give a more concrete example of the kind of tasks we are interested in, consider the following loss which naturally comes up in lossy compression. If x = g(z) is the output of a neural network, we may want to find a representation z such that\nR(z) + \u03b1d(x, x\u2217) + \u03b2U(x)\n(1)\nis minimal, where d measures the distance to some target image x\u2217and R is the number of bits required to encode z.\nIn this paper we will take the view that x is realistic if it appears to have come about in a particular way, which is another way of saying that x is a plausible sample of a distribution P capturing the data generating process. What is considered realistic therefore depends on P. If P is a distribution over natural images then most photos would qualify as realistic. While an MNIST image (LeCun and Cortes, 2010) would not be considered a realistic example of a natural image, we would still consider it to be realistic if P is the distibution of MNIST digits.\nIn Section 2 we will first review why common approaches to formalizing realism in terms of probability and typicality fail. This will highlight the challenges involved in defining realism and provide motivation for later sections. In Section 3 we will review much more successful notions of re-\nalism based on divergences, adversarial losses, and feature statistics, and discuss how they still fall short of our goal. In Section 4 we will make the case that randomness deficiency (Li and Vit\u00b4anyi, 1997) captures realism and introduce the concept of a universal critic. Finally, in Section 5 we will apply our newly gained understanding of realism to examples from the machine learning literature. What has been referred to as realism (e.g., Fan et al., 2018; Theis and Wagner, 2021; Careil et al., 2023) is also often referred to as perceptual quality (e.g., Blau and Michaeli, 2018; Fang et al., 2020; Salehkalaibar et al., 2023). It is therefore natural to wonder to what extent human perception should factor into its formalization. Our approach to defining realism is normative, that is, we consider how an idealized observer would judge realism. Similar to how Bayesian inference does not take inspiration from neuroscience but Bayesian decisions resemble human decisions (e.g., Knill and Pouget, 2004), we too can hope that human perception agrees with our definition of realism because it addresses a similar task as that faced by humans. In Section 4.4, we will further make the case that batched universal critics not only generalize no-reference metrics and divergences\u2014 which represent the prevalent ways of formalizing realism\u2014 but are also a better model of a human observer.\nalism based on divergences, adversarial losses, and feature statistics, and discuss how they still fall short of our goal. In Section 4 we will make the case that randomness deficiency (Li and Vit\u00b4anyi, 1997) captures realism and introduce the concept of a universal critic. Finally, in Section 5 we will apply our newly gained understanding of realism to examples from the machine learning literature.\n# 2. Probability and Typicality\nIn this section we review the two most common approaches to capture realism found in machine learning, namely those based on probability and typicality, and their failures. Similar failures of probability and typicality have been documented in the anomaly detection literature (e.g., Choi et al., 2019; Le Lan and Dinh, 2021; Osada et al., 2023) but are worth repeating as they continue to be a source of confusion.\n# 2.1. Probability\nIf x is discrete, it is natural to consider its probability under P to determine whether it is a realistic example of P. After all, if x has low probability then it seems unlikely to have come from P. This intuition is widespread in machine learning. Unsupervised anomaly detection, for instance, generally defines anomalies as those data points having low probability or density under a distribution of normal examples (Ruff et al., 2021), where the probability is often measured in some feature space (e.g., Zong et al., 2018). Probability density is also frequently maximized in an attempt to guide synthetic images towards more realistic examples (e.g., S\u00f8nderby et al., 2017; Graikos et al., 2022). To see how this approach might fail, consider the following simple example.\nExample 1 (Probability). Consider a computer program simulating a sequence of independent and nearly unbiased coin tosses, xN = (x1, . . . , xN) with P(xn = 1) = 0.5 + \u03b5 for some very small \u03b5 > 0. For reasonably large N, we would expect the program to output a number of 1s which is close to N/2 and we would suspect a bug if the program outputs a sequence of only 1s, yet this is the most probable sequence.\nExample 1 shows that maximizing P(x) can lead to unrealistic examples. It also shows that P(x) would not detect a bug which causes a program to only output 1s. If instead we count the number of 1s, k = \ufffdN n=1 xn, and measure the probability of k, this bug could be detected. Does this mean we only need to find the right set of features? By ignoring some aspects of the data, we risk not detecting unrealistic examples. We might therefore conclude that we simply need to test sufficiently many features. Unfortunately this approach also runs into trouble. Consider testing whether x has 1s in even places and 0s in odd places, x = 0101..01. The probability of this sequence is approximately 2\u2212N so that we would reject it with high confidence if we happen to observe it. However, since all sequences have roughly the same probability, we would reject every sequence as unrealistic if we tested all features identifying a specific sequence.\nUsing densities instead of probabilities introduces an additional challenge, namely that our answer now depends on the parametrization of the data. If P is an exponential distribution with rate 1, say, then values of x close to zero seem preferable over larger values if judged by their density. But if we consider y = e\u2212x instead, then all values of y would now be considered equally preferable.\n# 2.2. Weak Typicality\nMany readers will not have been surprised by the inability of probabilities to capture realism thanks to the widely known asymptotic equipartition property (AEP) of random sequences (Cover and Thomas, 2006). This property is such that if xN = (x1, . . . , xN) is a sequence of i.i.d. random variables drawn from P, then with probability 1 we have\n(2)\n# almost surely, where H[xn] is the entropy of P. The typical set is defined as (Cover and Thomas, 2006)\nalmost surely, where H[xn] is the entropy of P. The typical set is defined as (Cover and Thomas, 2006)\nAN \u03b4 = {x : | \u22121 N log P(xN) \u2212H[xn]| < \u03b4}\n(3)\nand elements from this set are considered weakly typical. While other notions of typicality exist, weak typicality is the one most commonly encountered in the machine learning literature (Nalisnick et al., 2019b; Choi et al., 2019; Dieleman, 2020). The AEP implies that as N increases, the probability that a randomly drawn sequence is contained in the typical\nset AN \u03b4 approaches 1 for any \u03b4 > 0. That is, a realistic sequence is likely to be typical. (While we have stated the AEP for sequences of independent and discrete random variables, generalizations to dependent and continuous sources exist and are well known; e.g., Algoet and Cover, 1988). The above suggests that instead of expecting the probability to be large, we should expect realistic x to have negative log-probability close to the entropy\u2014or the probability to be roughly 2\u2212H[x], especially if x is high-dimensional. It therefore appears that | \u2212log P(x) \u2212H[x]| would be a good candidate for a measure of realism (Choi et al., 2019; Nalisnick et al., 2019b). Unfortunately, also this definition fails to quantify realism as the following examples demonstrate.\nExample 2 (Typicality). Consider again a sequence of independent coin tosses. If the coin is unbiased, then the log-probability of any sequence is exactly the entropy, \u2212log2 P(xN) = N. In other words, in this case the probability of xN under P is completely uninformative and the typical set contains every sequence. Does this mean that every sequence of coin flips is realistic? Clearly, there is a sense in which the sequence 0000000000 is less realistic than 1100010100 which is not captured by weak typicality.\nExample 3 (Typicality). As another example, consider a multivariate Gaussian distribution with density p(x) \u221d exp(\u2212\u2225x\u22252). With high probability, the negative log-density of a random sample will be close to the differential entropy, which amounts to the norm \u2225x\u2225being roughly constant. While we would expect realistic examples from our distribution to look like uncorrelated noise, optimizing for typicality will only constrain the norm. If x represents an image, our optimization will merely adjust its contrast but will not decorrelate pixels as one might hope.\nWeak typicality may be a necessary criterion for realism but it is clearly not sufficient. Put differently, the typical set contains the realistic sequences we care about but also many sequences which are unrealistic, such as long sequences of fair coin flips which all come up heads. Probability and typicality both fail as a measure of realism because they address the wrong question. They tell us something about x assuming that x has distribution P. However, we cannot make this assumption, since whether or not x follows P is precisely the question we are trying to answer. That is, we are not interested in the probability (or typicality) of x given P, but in the probability of P given x. An extended discussion of typicality can be found in Appendix A.\n# 3. Divergences\nMore successful notions of realism are based on divergences between a ground-truth data distribution P and a distribution Q which we are trying to evaluate. In line with our intuitive notion of realism, if a divergence is zero, then instances of Q are indistinguishable from instances of P, that is, we have perfect realism.\nIn coding theory, formalizing realism in terms of divergences (Matsumoto, 2018; Blau and Michaeli, 2019; Chen et al., 2022) has resulted in an improved understanding of the lossy compression problem and novel methods to solve them (e.g., Theis and Agustsson, 2021). In practical applications, generative adversarial networks (GANs; Goodfellow et al., 2014) trained with adversarial losses (which approximate divergences) significantly advanced the state of the art in the perceptual quality of generated images (e.g., Denton et al., 2015; Ledig et al., 2017). For the evaluation of generated images, the Fr\u00b4echet inception distance (Heusel et al., 2017) has established itself as the method of choice and is based on a divergence between distributions over feature activations.\nIn the following, we review two approaches to approximating divergences based on samples.\n# 3.1. Adversarial Losses\nAdversarial losses provide lower bounds on divergences. For the broad class of f-divergences (R\u00b4enyi, 1961) between two distributions with densities p and q, we can write\n(4)\nwhere f is a convex function with f(1) = 0. This class of divergences includes the Jensen-Shannon divergence, the total variation distance, and the Kullback-Leibler divergences. For a real-valued function T (with an appropriately limited output range), we obtain the lower bound (Nguyen et al., 2010; Nowozin et al., 2016)\nDf[q \u2225p] \u2265Eq[T (x)] \u2212Ep[f \u2217(T (x))],\n(5)\nwhere f \u2217is the convex conjugate of f. T acts as a critic whose purpose is to produce values which are large for samples drawn from q and small for samples drawn from p. In practice, the critic may be a neural network T\u03b8 and adversarial training amounts to alternating between maximizing the lower bound with respect to its parameters, \u03b8, and minimizing the bound with respect to the parameters of q (although practical implementations often deviate from this basic recipe).\nFor the Kullback-Leibler divergence, for instance, we have\nf(u) = u log u, f \u2217(t) = exp(t \u22121)\nf(u) = u log u,\n(6)\nand the bound is tight for\nTq(x) = log q(x) \u2212log p(x) + 1.\n(7)\nNote that this optimal critic depends on the distribution q that we are trying to evaluate. In contrast, in our setting we may only have access to a single instance or a few instances drawn from q. Furthermore, the dependence of the critic on q is responsible for optimization instabilities that are known to plague adversarial training and which we would like to avoid. In Section 4 we will discuss critics which are universal in the sense that they do not depend on q and therefore do not require adversarial training.\n# 3.2. Maximum Mean Discrepancy\nMaximum mean discrepancy (MMD; Gretton et al., 2012) refers to a class of divergences which have been used for hypothesis testing as well as for generating realistic images (Li et al., 2015; Dziugaite et al., 2015). Given two sets of i.i.d. examples\u2014x1, . . . , xM and \u02dcx1, . . . , \u02dcxN\u2014estimates of MMD can be used to decide whether the two sets were drawn from the same distribution. Formally, we compute\nMMD2(xM, \u02dcxN) = \ufffd\ufffd1 M \ufffd m \u03a6(xm) \u22121 N \ufffd n \u03a6(\u02dcxn) \ufffd\ufffd2\n\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd in some potentially very high (even infinite) dimensional feature space \u03a6 to estimate a squared MMD. Notably, the estimator depends on the two distributions only through examples and unlike adversarial losses does not require optimization of any critic. This makes it worthwhile to consider as a candidate for our function U, especially in regimes where we have access to at least a small number of unrealistic examples. The basic idea is that we would fix a relatively large number of realistic examples and compare it to a small batch of examples we wish to test for realism. Support for this idea also comes from Amir and Weiss (2021) who have shown that MMD can be used to construct an effective full-reference perceptual metric1 which agrees with human judgments in determining the similarity of pairs of images. To construct the metric, each image was treated as a distribution over small patches.\nIt remains unclear how to use MMD to quantify the realism of a single data point without a reference. For an image, one might compare features averaged over image patches to the features obtained from patches of a larger dataset of images, and similar ideas have shown promise in image quality assessment (e.g., Mittal et al., 2013; Zhang et al., 2015). But the limitations of this approach are also clear as not all realistic images have statistics representative of the entire data distribution.\nA bigger concern perhaps is that the statistical power of MMD can drop quickly as the dimensionality of the problem\n1A full-reference metric takes two images as arguments where a no-reference metric only has a single input.\nincreases2 (Ramdas et al., 2015), suggesting that we might need a very large number of examples if we want to identify defects in reasonably sized images or videos.\nThe MMD estimator makes fewer assumptions than is necessary for us. In particular, it seems reasonable to assume access to P (or a good approximation) both from a conceptual and a practical point of view, given the power of today\u2019s generative models. By incorporating P into our definition of realism, we can hope to quantify realism more efficiently. MMD leaves it to us to choose \u03a6 and does not provide a clear mechanism for incorporating P.\n# 4. Universal Critics\nIn this section, we introduce an alternative notion of realism based on concepts from algorithmic information theory (AIT) (Martin-L\u00a8of, 1966; Chaitin, 1987; Li and Vit\u00b4anyi, 1997). AIT is concerned with whether a given sequence of bits is a random sequence of independent coin flips. If we can answer this question, then the answer to the more general question of whether x is an instance of P directly follows, since if we use P to (losslessly) compress x then the resulting bits should appear random. Several notions of randomness have been proposed and studied in AIT. Some have been rejected on the basis of flaws, such as von Mises randomness (Mises, 1919). Other notions survived scrutiny and turned out to be equivalent (Chaitin, 2001, Chapter 3), namely Martin-L\u00a8of randomness (Martin-L\u00a8of, 1966), Solovay randomness (Solovay, 1975), incompressibility (Li and Vit\u00b4anyi, 1997), and Chaitin randomness (Chaitin, 2001). The fact that multiple authors converged to essentially the same answer should give us hope that there is something fundamental about the concepts they discovered. Instead of reviewing the different (equivalent) definitions of randomness, we start with the conclusion relevant for us and then develop a justification for it below. In particular, AIT suggests the following measure of randomness to decide whether x was drawn from a distribution P:\n(8)\nHere, K(x) is the Kolmogorov complexity of x which is defined as the length of a shortest program (in some Turing complete programming language) which outputs x. The quantity U(x) is also known as randomness deficiency3 (Li and Vit\u00b4anyi, 1997) but for reasons that will become clear soon, we will refer to U as a universal critic. The following characterization of Kolmogorov complexity\n2This assumes that the difficulty of the estimation problem remains constant, as measured by the KL divergence between the two distributions being tested. 3A more accurate definition of randomness deficiency would be over sequences of arbitrary length but for simplicity we will work with Eq. 8.\nwill be more convenient for us,\nK(x) = \u2212log S(x), S(x) = \ufffd n \u03c0nQn(x),\n(9)\nK(x) = \u2212log S(x), S(x) = \ufffd n \u03c0nQn(x), (9) where S(x) is Solomonoff\u2019s probability (Solomonoff, 1960) and requires some explanation. Consider the set of all discrete probability distributions implementable in a programming language of your choice. Each program corresponds to a sequence of bits and we are free to interpret those bits as a natural number. In other words, the set of computable probability distributions is countable and we can assign each such distribution Qn a number n. S is a mixture of all of these. The choice of weights \u03c0n is not critical for now and we can choose \u03c0n \u221d1/n2 or \u03c0n = 2\u2212C(n) where C(n) is the number of bits assigned to n by some universal code. A similar argument holds for continuous sample spaces (Li and Vit\u00b4anyi, 1997, Chapter 4.5). That is, there is a corresponding S for continuous sample spaces which sums over measures, or lower semicomputable semimeasures4 to be precise. A measure is semicomputable if it can be approximated from below to arbitrary precision, that is, it is enough to be able to compute approximations of a measure for it to be included in the mixture S. For simplicity, we will focus on discrete spaces even though continuous spaces are relevant in practice if we want to optimize for realism. For a more thorough treatment of these concepts, see the excellent introduction to Kolmogorov complexity by Li and Vit\u00b4anyi (1997). Here we will try to not get hung up on technical details since we are ultimately interested in practical applications and\u2014as some readers may already rightfully object\u2014Kolmogorov complexity and S are uncomputable. Nevertheless, we will argue that universal critics as defined in Eq. 8 correctly formalize realism, and that it is useful to understand practical approaches as (good or bad) approximations of it\u2014similar to how deriving Bayesian posteriors is useful even when they are intractable since they can guide us towards better approximations. As a first step, note that if P is computable (or just lower semi-computable), then there exists an m with Qm = P. If \u03c0n is our prior belief that x was generated by Qn, then\n \ufffd where S(x) is Solomonoff\u2019s probability (Solomonoff, 1960) and requires some explanation. Consider the set of all discrete probability distributions implementable in a programming language of your choice. Each program corresponds to a sequence of bits and we are free to interpret those bits as a natural number. In other words, the set of computable probability distributions is countable and we can assign each such distribution Qn a number n. S is a mixture of all of these. The choice of weights \u03c0n is not critical for now and we can choose \u03c0n \u221d1/n2 or \u03c0n = 2\u2212C(n) where C(n) is the number of bits assigned to n by some universal code.\nAs a first step, note that if P is computable (or just lower semi-computable), then there exists an m with Qm = P. If \u03c0n is our prior belief that x was generated by Qn, then\n(10)\n(10) (11)\n(11) (12)\n(12)\ncan be seen as the log-posterior probability of P given x up to a constant, consistent with our earlier notion of realism.\n4A semimeasure integrates to a value less or equal 1. S itself is an example of a semimeasure with \ufffd x S(x) < 1. This is due to the halting problem causing some unknowable set of indices n to correspond to programs which never stop running. For these n, we set Qn(x) = 0.\n# 4.1. Batched Universal Critics\nHow does our new notion of realism compare to existing notions of realism? U is a particular instance of a no-reference metric since it can be applied to a single instance x. But it turns out that we can also use it to approximate divergences by taking averages, as we will demonstrate. Consider evaluating the distribution Q based on its average realism score as assigned by U. We have\n(13) (14) (15)\nwhere Eq. 14 is due to Q minimizing cross-entropy when the data is distributed according to Q. On the other hand, if Q is computable (or just lower semicomputable), we have\n(16) (17) (18)\nsince we must have Qm = Q for some m. For ease of notation, we also write \u03c0Q to refer to \u03c0m. The inequality follows because the terms we dropped from the sum are all nonnegative. What this sandwich bound implies is that our universal critic works well as a replacement for the optimal critic TQ (Eq. 7) if the complexity of Q, log(1/\u03c0Q), is low relative to the KL divergence between Q and P. This agrees with our intuition for realism. In particular, we are more likely to accept an alternative explanation of the data if the explanation is simple, that is, if it can be described in a few words (or bits). A sequence of zeros (Examples 1 and 2) is easy to detect because it is cheap to describe (\u201calways output 0\u201d). While the critic U depends on P, it is universal in the sense that it does not depend on Q.\nExample 4 (Low Complexity). Consider a distribution over natural images P and a distribution Q0 which assigns all its mass to a single flat image, Q0(x = 0) = 1. Based on our bounds above, we should expect U to detect Q0 as unrealistic since it is cheap to describe, that is, log(1/\u03c0Q0) is small for any reasonable coding scheme. In contrast, using \u2212log P(x) instead of U(x) would fail to detect Q0 since natural image distributions generally assign high probability to flat images. Similarly, images of Gaussian white noise would be detected since their distribution is cheap to describe as independent copies of a simple distribution.\nNote from Example 4 that low-complexity distributions can have both low or high entropy, that is, the complexity (or coding cost) log(1/\u03c0Q) of a distribution Q is different from its entropy.\nExample 5 (High Complexity). As another example, consider a distribution which has memorized a training set of natural images, QD(x) \u221d\ufffd x\u2032\u2208D \u03b4x\u2032(x). This distribution will remain undetected since its complexity is high. To describe QD, we would have to encode every image in the training set D. On the one hand, this means that U may perform poorly as an approximation of the KL divergence between QD and P (due to the loose lower bound, Eq. 17). On the other hand, this behavior is in line with our intuitive notion of realism since we would also fail to tell a single example generated by P from a single example selected from the training set. Like the universal critic, we consider training set images to be realistic5.\nAs a side note, a tighter bound can be obtained by choosing m which maximizes \u03c0mQm(x) instead of choosing m with Qm = Q as in Eq. 17. This would correspond to the minimum description length (MDL) principle of selecting models based on the total cost of describing the data and the model (Rissanen, 1978). That is, where adversarial training uses objectives such as maximum likelihood to select a critic, the universal critic can be viewed as selecting a critic based on MDL.\nWe can further improve the critic\u2019s odds of detecting Q by feeding it multiple independent examples. We define a batched universal critic as a critic of the form\nU B(xB) = log \ufffd n \u03c0n \ufffd b Qn(xb) \u2212log \ufffd b P(xb), (19\n(19)\nwhere xB = (x1, . . . , xB). In the following, let QB indicate the product measure, that is, a distribution over B independent samples from Q. Then\n(20) (21)\n(20)\n(22) (23)\nfor some m where Qm = Q. Compared to Eq. 17, we now obtain a tighter bound, which agrees with our intuition that upon observing multiple examples we should be able to do a better job of discriminating Q from P. In the limit of large B we recover the KL divergence. In this sense our notion of realism generalizes prior notions of realism based on noreference metrics or divergences, and allows us to interpolate between the two.\n5More concretely, we can say that an average training set image would be considered realistic in the sense that ED[EQD[U(x)]] = EP [U(x)] \u22640 (Eq. 14).\n5More concretely, we can say that an average training set image would be considered realistic in the sense that ED[EQD[U(x)]] = EP [U(x)] \u22640 (Eq. 14).\n# 4.2. Universal Tests\nDeciding whether x is realistic or not means deciding between two hypotheses. The null hypothesis is that x is realistic, by which we mean that x came about in a particular way, modelled by x being drawn from the distribution P. Our alternative hypothesis is that x is unrealistic, or that it came about by some other process Q. For example, P may be a distribution over photos but an alternative explanation could involve heavy compression with JPEG, corresponding to a distribution over images with blocking artefacts. If there are multiple ways in which x can fail to be realistic, Qn, then it is natural to assign probabilities \u03c0n to these events and to consider a mixture distribution as our alternative hypothesis. We end up with S as our alternative hypothesis if the only assumption we are willing to make is that x was generated by some computable process. By the well-known NeymanPearson lemma (Neyman et al., 1933), the most powerful test is then a likelihood ratio test of the form\n(24)\nwhere \u03b7 is a parameter which controls the trade-off between false positives and false negatives. Note that the left-hand side is our universal critic. If we accept the Neyman-Pearson lemma then it is easy to accept that our measure of realism should take the form of a likelihood ratio instead of just P(x). However, this does not yet explain why our choice of alternative hypothesis should be S.\nWe can provide the following additional justification for the universal critic. Assume that instead of S we decide to use another alternative hypothesis correspondingto a computable (or just lower semicomputable) measure Q. Then it is not difficult to see that\n(25)\nfor all xB and all B (following the same reasoning as in Eqs. 20-23). That is, U B additively dominates any computable likelihood ratio test and the constant log(1/\u03c0Q) becomes negligible for sufficiently large B. Asymptotically, the universal critic is as sensitive to unrealistic examples as any other test based on an alternative hypothesis Q.6\nWhen optimizing data for realism it is natural to look to Markov chain Monte Carlo (MCMC) methods for solutions. In MCMC, the data is stochastically perturbed until it converges to a sample from our target distribution P (at which point it would appear realistic). For example, for a continuous distribution with differentiable density p, a simple\n6Li and Vit\u00b4anyi (1997, Chapter 4.3) proved the stronger result that randomness deficiency additively dominates any so-called sumP test.\nMCMC strategy based on Langevin diffusion uses updates of the form\n(26)\nwhere \u03b7t \u223cN(0, I) is independent Gaussian noise. For infinitesimal \u03b5, the sequence of xt converges to the distribution P. For a fixed \u03b5 > 0 the stationary distribution will only approximate P, but this can be addressed by performing additional Metropolis-Hastings accept/reject steps (Besag, 1994; Welling and Teh, 2011). While MCMC produces realistic examples, it is not directly applicable to problems of the form of Eq. 1, since it is unclear how to translate an MCMC algorithm into a loss function U. If we naively interpreted Eq. 26 as a noisy gradient update, then this would correspond to using p as a measure of realism and is bound to fail (Section 2.1). In a second attempt to make MCMC work for us, consider the sequence of distributions generated by Eq. 26. Let q0 be the density used to initialize x0. Then each update produces a new density qt which approaches p as t goes to infinity. Maoutsa et al. (2020) and Song et al. (2021) showed that the deterministic updates\nwhere \u03b7t \u223cN(0, I) is independent Gaussian noise. For infinitesimal \u03b5, the sequence of xt converges to the distribution P. For a fixed \u03b5 > 0 the stationary distribution will only approximate P, but this can be addressed by performing additional Metropolis-Hastings accept/reject steps (Besag, 1994; Welling and Teh, 2011).\nWhile MCMC produces realistic examples, it is not directly applicable to problems of the form of Eq. 1, since it is unclear how to translate an MCMC algorithm into a loss function U. If we naively interpreted Eq. 26 as a noisy gradient update, then this would correspond to using p as a measure of realism and is bound to fail (Section 2.1).\nIn a second attempt to make MCMC work for us, consider the sequence of distributions generated by Eq. 26. Let q0 be the density used to initialize x0. Then each update produces a new density qt which approaches p as t goes to infinity. Maoutsa et al. (2020) and Song et al. (2021) showed that the deterministic updates\nxt+\u03b5 = xt + \u03b5 \ufffd\u2207log p(xt) \u2212\u2207log qt(xt)\ufffd\n \ufffd\ufffd follow the same sequence of distributions qt (for infinitesimal \u03b5, or approximately for \u03b5 > 0). Eq. 27 suggests moving xt towards high-density regions of p but away from high-density regions of its current distribution qt. When optimizing for realism, we do not know qt. But assuming an underlying qt exists, a Bayesian approach would be to estimate the missing gradient in Eq. 27 by assigning prior probabilities \u03c0n to candidate densities qn and then to form the posterior expectation\n(28)\n\ufffd  \ufffd where P(n | xt) \u221d\u03c0nqn(xt) (Appendix B). Note the resemblance of the right-hand side to Solomonoff\u2019s probability. If we restrict the universal critic to distributions with differentiable densities, then gradient descent on its density can be viewed as a Bayesian\u2019s attempt to simulate Eq. 27.\n# 4.4. Limited-Memory Observer\nWe demonstrated useful statistical properties of universal critics and discussed connections to adversarial critics, significance testing, and MCMC. However, did we capture anything about how humans perceive inputs? In this section we will argue that batched universal critics not only generalize no-reference metrics and divergences, but also represent a more realistic model of human observers.\nNo-reference metrics are motivated by the idea that humans can look at a single image and decide whether it is realistic or not. It should therefore be possible to design a function which performs this task similarly well. However, in\npractice, even human observers often have access to not just a single image but a number of images. When evaluating the quality of image codecs or generative models, for example, human raters typically receive a stream of images and are asked to rate them. Mean opinion score tests ask raters to assign a score between 1 and 5 to each image while an alternative approach asks raters to classify between real and generated images (Denton et al., 2015). A generative model which always produces the same output would easily be identified by humans in such a task, even when the image appears realistic when viewed in isolation. While humans would be able to better detect a faulty generative model over time, no-reference metrics continue to produce the same output no matter how many examples they receive. That is, a no-reference metric is memoryless. While it may have been obtained through training on a set of realistic and unrealistic examples, it is unable to adapt to the method(s) currently under evaluation once it has been fixed. Divergences represent the other extreme as they have access to the entire distribution. This corresponds to a human observer who has received an infinite stream of examples of either real or generated data. The total variation distance, for example, measures the probability of an optimal observer correctly classifying real from generated data (Nguyen et al., 2009; Blau and Michaeli, 2018),\n(29)\nthat is, an observer who has had access to infinitely many training examples. Other divergences can be similarly interpreted as classifiers which are optimal but with respect to different losses (Nguyen et al., 2009). Like other no-reference metrics and human observers, universal critics provide a score for individual examples. Like divergences they can also be viewed as the score of a classifier deciding between two hypotheses, but unlike divergences they only have access to a finite set of training examples. This limitation means that prior assumptions become more important. Alternatively, universal critics can be viewed as measuring the performance of an ideal observer with limited memory (Appendix C). In this sense, batched universal critics are a better model of human observers than either no-reference metrics (memoryless) or divergences (infinite memory). Universal critics as defined in Eq. 8 depend on an uncomputable Kolmogorov complexity and therefore could be implemented neither by humans nor computers. Given sufficient evidence, it will detect any failures a human observer might detect (Section 4.2) but will also detect any unrealistic properties that would be missed by us. In this sense, universal critics provide a sufficient but not necessary criterion for high perceptual quality (unlike typicality, which is necessary but not sufficient). The limitations of human observers can be incorporated naturally into universal critics by limiting S\nthat is, an observer who has had access to infinitely many training examples. Other divergences can be similarly interpreted as classifiers which are optimal but with respect to different losses (Nguyen et al., 2009).\nto a mixture over fewer components. However, characterizing the limitations and abilities of human observers is beyond the scope of this paper. We refer to Griffiths and Tenenbaum (2003), who studied the ability of humans to detect randomness in binary sequences, and compared it to algorithmic notions of randomness.\n# 5. Related Work\nGiven the wide range of related fields and the vast amount of work in them (Section 1), it is impossible to review any meaningful fraction of related work here. Instead, we will focus on two successful examples with interesting connections to universal critics.\n# 5.1. Input Complexity\nSeveral papers on outlier detection made the puzzling observation that generative models trained on one dataset of images can assign higher probability to other datasets (Choi et al., 2019; Nalisnick et al., 2019a; Hendrycks et al., 2019). Serr`a et al. (2020) found that the issue virtually disappears if instead of measuring log-probabilities, the negative log-probability under the model is compared with the coding cost of a lossless image compression method such as PNG,\n\u2212log P(x) \u2212C(x),\n(30)\nwhere C(x) is the coding cost obtained via compression. The authors found that this signal performed significantly better for outlier detection, providing support for our definition of realism (Eq. 8) by viewing C(x) as an approximation to Kolmogorov complexity. It is further enouraging that a simple but flexible compression scheme can provide a useful signal. An interesting question for future research is what a differentiable analogue of C would look like, and whether it can be made robust enough for optimization.\nWe note that input complexity has also been considered in statistics for its applications in hypothesis testing, including as an approximation to universal tests (Ryabko et al., 2006).\n# 5.2. Score Distillation Sampling\nScore distillation sampling (SDS; Poole et al., 2023) is a technique which has gained a lot of popularity for training 3D generative models. Training 3D generative models is challenging due to the high cost associated with collecting 3D data. SDS tries to overcome these limitations by leveraging diffusion models (Sohl-Dickstein et al., 2015) trained on large amounts of 2D images to guide text-to-3D models towards realistic outputs. Briefly, diffusion models define latent variables zt = \u03b1tx + \u03c3t\u01eb where \u01eb \u223cN(0, I) and a function \u02c6\u01ebt(zt) is trained to predict \u01eb. For a conditional diffusion model whose outputs depend on text y, we have the\nimportant relationship (Robbins, 1956)\n(31)\nwhere pt is the distribution of zt so that \u02c6\u01ebt can also be used to estimate the gradient of these log-densities. Simplifying a bit, Poole et al. (SDS; 2023) propose the following gradient,\n(32)\nwhere w(t) are hyperparameters assigning weights to the different noise levels. Is LSDS a good candidate for U? We can see that SDS tries to find x such that zt is near modes of pt. Note that pt is essentially the density of x smoothed via convolution with a Gaussian kernel, and so SDS appears fundamentally similar to using p as a measure of realism (Section 2.1) and susceptible to similar failures. Indeed, if the data distribution was Gaussian, then pt would also be Gaussian and the optimal x would be the mean, which tends to be unrealistic. This raises the question of why SDS performs well in practice. The key to its success lies in classifier-free guidance (CFG; Ho and Salimans, 2021). Instead of using \u02c6\u01ebt directly, this common trick is to use\n(33)\n\u02c6\u01ebv t (zt; y) = (1 + v)\u02c6\u01ebt(zt; y) \u2212v\u02c6\u01ebt(zt),\nwhere \u02c6\u01ebt(zt) is an unconditional prediction of \u01eb and the guidance weight v \u22650 is a hyperparemeter. This corresponds to a gradient signal proportional to\nv\u2207zt log pt(zt) \u2212(1 + v)\u2207zt log pt(zt | y). (\n(34)\nImplicit in the marginal density pt(zt) is a large mixture over all possible texts y,\n# Implicit in the marginal density pt(zt) is a large mixture over all possible texts y,\npt(zt) = \ufffd y p(y)pt(zt | y).\n(35)\n \ufffd Note the resemblance of Eq. 34 to our universal critic. For large v, the constant 1 becomes negligible and we are left with a density ratio between the target distribution and a large mixture distribution over alternative explanations. Indeed, Poole et al. (2023) found that SDS without CFG produced blurry 3D scenes and very large guidance weights worked best.\nWe therefore submit that the reason SDS works well is not explained by its ability to find modes in densities or its connections to model distillation techniques, but by its ability to approximate universal critics. Reinterpreting SDS in this way suggests new ways of overcoming its weaknesses (e.g., its tendency to produce oversaturated images), such as a more intentional design of the mixture of alternatives, or batched losses analogous to Eq. 19.\n# 6. Discussion\nIn this position paper we have argued that the question of realism is equivalent to the question of randomness, that is, whether observations originated from a particular distribution. This allowed us to draw on insights from algorithmic information theory and to propose universal critics, or randomness deficiency (Li and Vit\u00b4anyi, 1997), as a rational answer. Perceptual quality can be viewed as the result of a (necessarily) imperfect approximation of universal critics. However, despite the relevance of these concepts to problems in machine learning, discussions of randomness deficiency are notably absent from its literature. Instead, dominant notions of realism continue to be based on probability (e.g., Ruff et al., 2021; Poole et al., 2023), typicality (e.g., Nalisnick et al., 2019b) or divergences (e.g., Blau and Michaeli, 2018; Theis and Wagner, 2021). A divergence of zero is a sufficient condition for perfect realism but corresponds to an ideal observer with access to an infinite stream of examples. As such, it is stronger than required for most practical applications where observers only have access to one or a few examples. At the other end of the spectrum, weak typicality is an example of a criterion which only considers a necessary criterion, while most no-reference metrics correspond to neither a necessary nor a sufficient criterion (e.g., high probability in some feature space). Universal critics enable principled relaxations of divergence-based constraints. While weaker than divergences (in the desired way), they are still strong in the sense that they are as strong as other likelihood-ratio tests for realism, up to a constant which depends on the complexity of the competing test (Section 4.2). Many interesting practical and theoretical questions remain. For example, what is the impact of different choices of \u03c0n on the sample efficiency of universal critics? What are the implications of using universal critics in rate-distortion-realism trade-offs? Most importantly, what do practical approximations to universal critics (Eqs. 8 or 19) look like that can serve as optimization targets? Acknowledgements\nMany interesting practical and theoretical questions remain. For example, what is the impact of different choices of \u03c0n on the sample efficiency of universal critics? What are the implications of using universal critics in rate-distortion-realism trade-offs? Most importantly, what do practical approximations to universal critics (Eqs. 8 or 19) look like that can serve as optimization targets?\n# Acknowledgements\nI would like to thank Aaron B. Wagner and Johannes Ball\u00b4e for many helpful discussions shaping the arguments presented in this paper, Andriy Mnih, Matthias Bauer, J\u00a8org Bornschein, Iryna Korshunova, Emilien Dupont, Eirikur Agustsson, and Alexandre Galashov for valuable feedback on the manuscript, and Daniel Severo for exploring various ideas to make universal critics practical.\n# References\nPaul H. Algoet and Thomas M. Cover. A Sandwich Proof of the Shannon-McMillan-Breiman Theorem. The Annals of Probability, 16:899\u2014-909, 1988. Dan Amir and Yair Weiss. Understanding and simplifying perceptual distances. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12226\u201312235, June 2021. Johannes Ball\u00b4e, Philip A. Chou, David Minnen, Saurabh Singh, Nick Johnston, Eirikur Agustsson, Sung Jin Hwang, and George Toderici. Nonlinear transform coding. IEEE Journal of Selected Topics in Signal Processing, 15(2): 339\u2013353, 2021. doi: 10.1109/JSTSP.2020.3034501. J. Besag. Comments on \u2018Representations of knowledge in complex systems\u2019 by U. Grenander and M. I. Miller. Journal of the Royal Statistical Society, Series B, 56:591\u2013592, 1994. Y. Blau and T. Michaeli. The perception-distortion tradeoff. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6228\u20136237, 2018. Y. Blau and T. Michaeli. Rethinking lossy compression: The rate-distortion-perception tradeoff. In International Conference on Machine Learning, 2019. A. Borji. Pros and Cons of GAN Evaluation Measures. Computer Vision and Image Understanding, 179:41\u2013\u201365, 2019. Marl`ene Careil, Matthew J Muckley, Jakob Verbeek, and St\u00b4ephane Lathuili`ere. Towards image compression with perfect realism at ultra-low bitrates. arXiv preprint arXiv:2310.10325, 2023. Gregory. J. Chaitin. Algorithmic Information Theory. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press, 1987. Gregory J. Chaitin. Exploring RANDOMNESS. Discrete Mathematics and Theoretical Computer Science. Springer London, 2001. ISBN 9781852334178. Jun Chen, Lei Yu, Jia Wang, Wuxian Shi, Yiqun Ge, and Wen Tong. On the rate-distortion-perception function. IEEE Journal on Selected Areas in Information Theory, 3(4): 664\u2013673, 2022. doi: 10.1109/JSAIT.2022.3231820. Hyunsun Choi, Eric Jang, and Alexander A. Alemi. WAIC, but Why? Generative Ensembles for Robust Anomaly Detection, 2019. T. M. Cover and J. A. Thomas. Elements of Information Theory, volume 2. John Wiley & Sons, 2006.\nEmily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\nEmily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021.\n# Sander Dieleman. Musings on typicality, 2020.\nK. Ding, K. Ma, S. Wang, and E. P. Simoncelli. Comparison of Full-Reference Image Quality Models for Optimization of Image Processing Systems. International Journal of Computer Vision, (129):1258\u20131281, 2021. Gintare Karolina Dziugaite, Daniel M. Roy, and Zoubin Ghahramani. Training generative neural networks via maximum mean discrepancy optimization. In Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI\u201915, page 258\u2013267, Arlington, Virginia, USA, 2015. AUAI Press. ISBN 9780996643108. Shaojing Fan, Tian-Tsong Ng, Bryan Lee Koenig, Jonathan Samuel Herberg, Ming Jiang, Zhiqi Shen, and Qi Zhao. Image visual realism: From human perception to machine computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(9): 2180\u20132193, 2018. doi: 10.1109/TPAMI.2017.2747150. Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, and Zhou Wang. Perceptual quality assessment of smartphone photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3677\u2013 3686, 2020. Pierre Glaser, Michael Arbel, and Arthur Gretton. KALE flow: A relaxed KL gradient flow for probabilities with disjoint support. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, 2021. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. WardeFarley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27, 2014. Alexandros Graikos, Nikolay Malkin, Nebojsa Jojic, and Dimitris Samaras. Diffusion models as plug-and-play priors. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\nK. Ding, K. Ma, S. Wang, and E. P. Simoncelli. Comparison of Full-Reference Image Quality Models for Optimization of Image Processing Systems. International Journal of Computer Vision, (129):1258\u20131281, 2021.\nArthur Gretton, Karsten M. Borgwardt, Malte J. Rasch, Bernhard Sch\u00a8olkopf, and Alexander Smola. A kernel twosample test. Journal of Machine Learning Research, 13 (25):723\u2013773, 2012. Thomas Griffiths and Joshua Tenenbaum. From algorithmic to subjective randomness. In S. Thrun, L. Saul, and B. Sch\u00a8olkopf, editors, Advances in Neural Information Processing Systems, volume 16. MIT Press, 2003. Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019. Robert Herzog, Martin \u02c7Cad\u00b4\u0131k, Tunc\u00b8 O. Ayd\u0131n, Kwawng In Kim, Karol Myszkowski, and Hans-Peter Seidel. NoRM: no-reference image quality metric for realistic image synthesis. Computer Graphics Forum, 31(2):545\u2013554, 2012. doi: 10.1111/j.1467-8659.2012.03055.x. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems, volume 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. David C. Knill and Alexandre Pouget. The Bayesian brain: the role of uncertainty in neural coding and computation. Trends in Neurosciences, 27(12):712\u2013719, 2004. ISSN 0166-2236. doi: https://doi.org/10.1016/j.tins.2004. 10.007. Charline Le Lan and Laurent Dinh. Perfect density models cannot guarantee anomaly detection. Entropy, 23(12), 2021. ISSN 1099-4300. doi: 10.3390/e23121690. Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. C. Ledig, L. Theis, F. Husz\u00b4ar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR, 2017. Ming Li and Paul Vit\u00b4anyi. An Introduction to Kolmogorov Complexity and Its Applications. Springer, 1997. Yujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1718\u20131727, Lille, France, 07\u201309 Jul 2015. PMLR.\nDan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2019.\nRobert Herzog, Martin \u02c7Cad\u00b4\u0131k, Tunc\u00b8 O. Ayd\u0131n, Kwawng In Kim, Karol Myszkowski, and Hans-Peter Seidel. NoRM: no-reference image quality metric for realistic image synthesis. Computer Graphics Forum, 31(2):545\u2013554, 2012. doi: 10.1111/j.1467-8659.2012.03055.x.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems, volume 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\nDavid C. Knill and Alexandre Pouget. The Bayesian brain: the role of uncertainty in neural coding and computation. Trends in Neurosciences, 27(12):712\u2013719, 2004. ISSN 0166-2236. doi: https://doi.org/10.1016/j.tins.2004. 10.007.\nC. Ledig, L. Theis, F. Husz\u00b4ar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. In CVPR, 2017. Ming Li and Paul Vit\u00b4anyi. An Introduction to Kolmogorov Complexity and Its Applications. Springer, 1997.\nYujia Li, Kevin Swersky, and Rich Zemel. Generative moment matching networks. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 1718\u20131727, Lille, France, 07\u201309 Jul 2015. PMLR.\nDimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting Particle Solutions of Fokker\u2013Planck Equations Through Gradient\u2013Log\u2013Density Estimation. Entropy, 22 (8), 2020. ISSN 1099-4300. doi: 10.3390/e22080802. Per Martin-L\u00a8of. The definition of random sequences. Information and Control, 9(6):602\u2013619, 1966. ISSN 0019-9958. doi: https://doi.org/10.1016/S0019-9958(66) 80018-9. Ryutaroh Matsumoto. Introducing the perception-distortion tradeoff into the rate-distortion theory of general information sources. IEICE Communications Express, advpub:2018XBL0109, 2018. doi: 10.1587/comex. 2018XBL0109. R. v. Mises. Grundlagen der Wahrscheinlichkeitsrechnung. Mathematische Zeitschrift, 5(1):52\u201399, 1919. doi: 10. 1007/BF01203155. A. Mittal, R. Soundararajan, and A. C. Bovik. Making a \u201cCompletely Blind\u201d Image Quality Analyzer. IEEE Signal Processing Letters, 20(3):209\u2013212, March 2013. doi: 10. 1109/LSP.2012.2227726. Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do deep generative models know what they don\u2019t know? In International Conference on Learning Representations, 2019a. Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-distribution inputs to deep generative models using typicality, 2019b. Jerzy Neyman, Egon Sharpe Pearson, and Karl Pearson. IX. On the problem of the most efficient tests of statistical hypotheses. Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character, 231(694-706):289\u2013337, 1933. doi: 10.1098/rsta.1933.0009. X. L. Nguyen, M. J. Wainwright, and M. I. Jordan. On surrogate loss functions and f-divergences. The Annals of Statistics, 37(2):876 \u2013 904, 2009. doi: 10.1214/08-AOS595. XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010. doi: 10.1109/TIT.2010.2068870. S. Nowozin, B. Cseke, and R. Tomioka. f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization. In Advances in Neural Information Processing Systems, volume 29, 2016. G. Osada, T. Takahashi, B. Ahsan, and T. Nishide. Outof-distribution detection with reconstruction error and\nXuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory, 56(11):5847\u20135861, 2010. doi: 10.1109/TIT.2010.2068870.\nG. Osada, T. Takahashi, B. Ahsan, and T. Nishide. Outof-distribution detection with reconstruction error and\ntypicality-based penalty. In 2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 5540\u20135552, Los Alamitos, CA, USA, jan 2023. IEEE Computer Society. doi: 10.1109/WACV56688.2023. 00551.\nIEEE Computer Society. doi: 10.1109/WACV56688.2023. 00551. Christopher Pondoc, Joseph C O\u2019Brien, and Joseph Guman. Seeing through the facade: Understanding the realism, expressivity, and limitations of diffusion models. 2023. Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In The Eleventh International Conference on Learning Representations, 2023. Aaditya Ramdas, Sashank J. Reddi, Barnab\u00b4as P\u00b4oczos, Aarti Singh, and Larry Wasserman. On the decreasing power of kernel and distance based nonparametric hypothesis tests in high dimensions. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI\u201915, page 3571\u20133577. AAAI Press, 2015. ISBN 0262511290. Erik Reinhard, Alexei A. Efros, Jan Kautz, and Hans-Peter Seidel. On visual realism of synthesized imagery. Proceedings of the IEEE, 101(9):1998\u20132007, 2013. doi: 10.1109/JPROC.2013.2260711. Alfr\u00b4ed R\u00b4enyi. On Measures of Entropy and Information. In The 4th Berkeley Symposium on Mathematics, Statistics and Probability, page 547\u2013561. University of California Press, 1961. J. Rissanen. Modeling by shortest data description. Automatica, 14(5):465\u2013471, 1978. ISSN 0005-1098. doi: https://doi.org/10.1016/0005-1098(78)90005-5. H. Robbins. An empirical Bayes approach to statistics. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 157\u2013 163, 1956. Lukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr\u00b4egoire Montavon, Wojciech Samek, Marius Kloft, Thomas G. Dietterich, and Klaus-Robert M\u00a8uller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021. doi: 10. 1109/JPROC.2021.3052449. Boris Ryabko, Jaakko Astola, and Alex Gammerman. Application of kolmogorov complexity and universal codes to identity testing and nonparametric testing of serial independence for time series. Theoretical Computer Science, 359(1):440\u2013448, 2006. ISSN 03043975. doi: https://doi.org/10.1016/j.tcs.2006.06.004. URL https://www.sciencedirect.com/science/a\nChristopher Pondoc, Joseph C O\u2019Brien, and Joseph Guman. Seeing through the facade: Understanding the realism, expressivity, and limitations of diffusion models. 2023.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. DreamFusion: Text-to-3D using 2D Diffusion. In The Eleventh International Conference on Learning Representations, 2023.\nH. Robbins. An empirical Bayes approach to statistics. In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, volume 1, pages 157\u2013 163, 1956.\nLukas Ruff, Jacob R. Kauffmann, Robert A. Vandermeulen, Gr\u00b4egoire Montavon, Wojciech Samek, Marius Kloft, Thomas G. Dietterich, and Klaus-Robert M\u00a8uller. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 109(5):756\u2013795, 2021. doi: 10. 1109/JPROC.2021.3052449.\nBoris Ryabko, Jaakko Astola, and Alex Gammerman Application of kolmogorov complexity and universal codes to identity testing and nonparametric testing of serial independence for time series. Theoretical Computer Science, 359(1):440\u2013448, 2006. ISSN 03043975. doi: https://doi.org/10.1016/j.tcs.2006.06.004. URL https://www.sciencedirect.com/science/a\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR.\nRay J. Solomonoff. A preliminary report on a general theory of inductive inference. Technical report, Cambridge, MA, 1960.\nRay J. Solomonoff. A preliminary report on a general theory of inductive inference. Technical report, Cambridge, MA,\n# R. M. Solovay. Lecture notes. 1975.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021. C. S\u00f8nderby, J. Caballero, L. Theis, W. Shi, and F. Husz\u00b4ar. Amortised map inference for image super-resolution. In International Conference on Learning Representations, 2017. L. Theis and E. Agustsson. On the advantages of stochastic encoders. In Neural Compression Workshop at ICLR, 2021. L. Theis and A. B. Wagner. A coding theorem for the ratedistortion-perception function. In Neural Compression Workshop at ICLR, 2021. L. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In International Conference on Learning Representations, Apr 2016.\nL. Theis, A. van den Oord, and M. Bethge. A note on the evaluation of generative models. In International Conference on Learning Representations, Apr 2016.\nAaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury, Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen King, Tom Walters,\nDan Belov, and Demis Hassabis. Parallel WaveNet: Fast high-fidelity speech synthesis. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 3918\u2013 3926. PMLR, 10\u201315 Jul 2018. Max Welling and Yee Whye Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML\u201911, page 681\u2013688, Madison, WI, USA, 2011. Omnipress. ISBN 9781450306195. Y. Yang, S. Mandt, and L. Theis. An introduction to neural data compression. Foundations and Trends in Computer Graphics and Vision, 15(2):113\u2013200, 2023. Tianwei Yin, Micha\u00a8el Gharbi, Richard Zhang, Eli Shechtman, Fr\u00b4edo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. Lin Zhang, Lei Zhang, and Alan C. Bovik. A featureenriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):2579\u20132591, 2015. doi: 10.1109/TIP.2015.2426416. Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection. In International Conference on Learning Representations, 2018.\nLin Zhang, Lei Zhang, and Alan C. Bovik. A featureenriched completely blind image quality evaluator. IEEE Transactions on Image Processing, 24(8):2579\u20132591, 2015. doi: 10.1109/TIP.2015.2426416.\nA. Typicality\nIn Appendix A.1 we extend our discussion of weak typicality. In particular, we elaborate on how a majority of examples in the typical set can be unrealistic. In Appendix A.2 we additionally consider strong typicality.\nA.1. Bounded size of weakly typical sets\nAs discussed in the main text, the typical set contains sequences xN \u223cP N with high probability, that is, AN \u03b4 is large enough that P N(AN \u03b4 ) approaches 1 as N goes to infinity. On the other hand, the typical set is small in the sense that the number of elements is bounded by (Cover and Thomas, 2006)\nThis fact is exploited in information theory to build simple but efficient codes for data compression. Using\nbits, we can address each element in the typical set. Normalized by the number of elements, this becomes\nwe can address each element in the typical set. Normalized by the number\nt in the typical set. Normalized by the number of elements, this becomes\napproaching the entropy of P as N increases and \u03b4 decreases. Counter-intuitively, this suggests that the typical set cannot contain too many unrealistic sequences, or else our compression scheme would be inefficient. However, note that while the coding rate overhead is only (\u03b4 + 1/N) above H[xn] (Eq. 38), the number of elements in the typical set already exceeds 2H[x] by a factor of up to 2N\u03b4 (Eq. 36). If we relax the threshold \u03b4 so that N\u03b4 increases by 1, then this would increase the total coding cost of a sequence by only 1 bit, yet the number of elements in the typical set increases by a factor of up to 2.\n# A.2. Strong typicality\nHere we consider strong typicality. A closely related notion (P-typicality) was considered by Chen et al. (2022) to quantify the realism of a batch of examples. Strong typicality is also similar in spirit to maximum mean discrepancy (MMD; Gretton et al., 2012), which was discussed in Section 3.2. Let X be a finite set and let #(x, xN) be the number of occurrences of x in a sequence xN = (x1, . . . , xN), that is, a histogram. The set of strongly typical sequences is defined as (Cover and Thomas, 2006)\n\ufffd\ufffd \ufffd\ufffd As for weakly typical sets AN \u03b4 , the probability that a randomly drawn sequence xN \u223cP N is strongly typical approaches 1 for any \u03b4 > 0 as N increases. Strong typicality requires the empirical distribution of elements in a sequence to be close to the distribution of interest, P. For large N, a randomly selected element of a strongly typical sequence will appear like a sample from P, that is, it will appear realistic. However, the main challenge we are trying to overcome is to define realism for short sequences and individual x. If we naively apply strong typicality to a single element (N = 1), we obtain\n\ufffd x\u2208X |#(x, (x1)) \u2212P(x)| = |1 \u2212P(x1)| + \ufffd x\u0338=x1 |0 \u2212P(x)|\nthat is, we are effectively back to measuring the probability of x1. It is therefore unclear how strong typicality could be used to evaluate objects as high-dimensional as images. One might consider dividing an image into patches of lower dimensionality and treating the image as a sequence of these. However, this would ignore dependencies between patches and we would further have to assume that the statistics of each realistic image is representative of the entire distribution (i.e., ergodicity), which may not be the case.\n(36)\n(37)\n(38)\n(39)\n(40) (41) (42) (43)\n# B. Expected gradient of log-density\n P(n | x) \u221d\u03c0nqn(x) be the posterior probability that x was drawn from qn. Then the expected gradient of the log-density\nLet P(n | x) \u221d\u03c0nqn(x) be the posterior probability that x was drawn from qn. Then the expected gradient of the lo is:\nLet P(n | x) \u221d\u03c0nqn(x) be the posterior probability that x was drawn from qn. Then the expected gradient of the log-density is:\n# C. Limited-memory observer\nHere we elaborate on the relationship between the batched universal critic and an ideal observer in a sequential predicti Assume an observer assigns a value T (x) to an image x. Further assume we ask the observer to\nthat is, the observer receives a reward of T (x) if x \u223cQ and a penalty of exp(T (x)) if x \u223cP. The optimal output is then given by (Glaser et al., 2021) TQ(x) = log Q(x) \u2212log P(x). (49)\nNote that\nEP [exp(TQ(x))] = EP [Q(x)/P(x)] = \ufffd Q(x) = 1.\n\ufffd TQ remains the optimal solution if we solve the following closely related constrained optimization problem maximize T EQ[T (x)] subject to EP [exp(T (x))] \u22641,\nand so we can use EQ[T (x)] to evaluate T if we fix P and restrict the class of allowed T in this way. In other words, an equivalent task presents raters only with examples from Q, but applies restrictions to the scores that can be assigned. Unlike typical classification problems where both P and Q are unknown and must be learned, we can assume P to be known to the observer through prior experience while Q still needs to be learned. A rational observer who expects x to be distributed according Qn with probability \u03c0n would maximize the expected reward by using U(x) = log P(x) \u2212log S(x), where S(x) = \ufffd n \u03c0nQn(x). (52) After receiving B examples from Q, xB = (x1, . . . , xB), a rational observer would update those beliefs to \u03c0(n | xB) \u221d\u03c0QB  (xB) = \u03c0 \ufffdB  Q(x) (53)\n \ufffd After receiving B examples from Q, xB = (x1, . . . , xB), a rational observer would update those beliefs to \u03c0(n | xB) \u221d\u03c0nQB n (xB) = \u03c0n \ufffdB b=1 Qn(xb)\nand receive a reward of\nU(x | xB) = log P(x) \u2212log S(x | xB), where S(x | xB) = \ufffd n \u03c0(n | xB)Qn(x)\n \ufffd for a subsequent example x from the unknown Q. This score is slightly different from the batched universal cr more readily interpreted as the combined value assigned to an entire batch of examples. However, the following holds:\n   \u2212 \ufffd = log P B(xB) \u2212log \ufffd n \u03c0nQB\u22121 n (xB\u22121) \ufffd m \u03c0mQB\u22121 m (xB\u22121)Qn(xB) \u2212log \ufffd m \u03c0mQB\u22121 m (xB\u22121) = log P(xB) \u2212log \ufffd n \u03c0(n | xB\u22121)Qn(xB) + log P B\u22121(xB\u22121) \u2212log \ufffd m \u03c0mQB\u22121 m (xB\u22121) = U(xB | xB\u22121) + U(xB\u22121) = \ufffdB b=1 U(xb | xb\u22121)\n(44) (45) (46) (47)\n(47)\n(48)\n(49)\n(50)\n(51)\n(52)\n(53)\n(54)\n(55) (56) (57) (58) (59) (60)\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of quantifying realism in generated data, particularly images, by designing functions that can reliably differentiate realistic data from unrealistic data. Despite advancements in generative AI, the challenge of defining and measuring realism remains poorly understood.",
        "problem": {
            "definition": "The problem is to create a function U that outputs a low value for realistic data samples and a high value for unrealistic samples, applicable across various data types including images and text.",
            "key obstacle": "The main difficulty lies in the lack of robust loss functions that can generalize across diverse tasks and the inherent challenges in detecting unrealistic examples during the optimization process."
        },
        "idea": {
            "intuition": "The idea is inspired by the insights from algorithmic information theory, particularly the concept of randomness and how it relates to the notion of realism.",
            "opinion": "The authors propose that a good generative model alone is insufficient to capture realism; instead, a new approach, termed the universal critic, should be developed.",
            "innovation": "The primary improvement over previous methods is the introduction of the universal critic, which does not require adversarial training and can serve as a guiding principle for practical implementations."
        },
        "Theory": {
            "perspective": "The theoretical perspective is grounded in algorithmic information theory, which provides a framework for understanding the randomness of sequences and their relation to data distributions.",
            "opinion": "The authors argue that realism should be defined in terms of randomness deficiency, where a sequence is deemed realistic if it can be compressed without loss of information.",
            "proof": "The paper discusses the Kolmogorov complexity and its implications for defining realism, asserting that a sequence's complexity can provide a measure of its realism."
        },
        "experiments": {
            "evaluation setting": "The evaluation involves comparing the performance of the universal critic against traditional methods in various settings, including datasets of generated images and real images.",
            "evaluation method": "The evaluation steps include applying the universal critic to assess realism and comparing its outputs against established metrics like Fr\u00e9chet Inception Distance and Maximum Mean Discrepancy."
        },
        "conclusion": "The paper concludes that universal critics offer a promising approach to quantifying realism, bridging gaps left by traditional methods and providing a more nuanced understanding of perceptual quality.",
        "discussion": {
            "advantage": "The universal critic framework allows for a more flexible and robust assessment of realism, potentially leading to better outcomes in generative modeling and image synthesis.",
            "limitation": "The main limitation is that universal critics are not immediately practical and require further development to be effectively implemented in real-world applications.",
            "future work": "Future research should focus on developing practical approximations of universal critics that can be used as optimization targets in generative models."
        },
        "other info": [
            {
                "info1": "The paper emphasizes the normative approach to defining realism based on an idealized observer's judgments."
            },
            {
                "info2": {
                    "info2.1": "Insights from human perception of realism could inform the design of universal critics.",
                    "info2.2": "The authors highlight the need for more empirical studies to validate their theoretical claims."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of quantifying realism in generated data, particularly images, highlighting the importance of developing reliable differentiation between realistic and unrealistic data."
        },
        {
            "section number": "2.1",
            "key information": "The problem is to create a function U that outputs a low value for realistic data samples and a high value for unrealistic samples, applicable across various data types including images and text."
        },
        {
            "section number": "2.5",
            "key information": "The authors argue that realism should be defined in terms of randomness deficiency, where a sequence is deemed realistic if it can be compressed without loss of information."
        },
        {
            "section number": "3.5",
            "key information": "The primary improvement over previous methods is the introduction of the universal critic, which does not require adversarial training and can serve as a guiding principle for practical implementations."
        },
        {
            "section number": "6.1",
            "key information": "The evaluation involves comparing the performance of the universal critic against traditional methods in various settings, including datasets of generated images and real images."
        },
        {
            "section number": "7.1",
            "key information": "The main difficulty lies in the lack of robust loss functions that can generalize across diverse tasks and the inherent challenges in detecting unrealistic examples during the optimization process."
        },
        {
            "section number": "7.2",
            "key information": "Future research should focus on developing practical approximations of universal critics that can be used as optimization targets in generative models."
        }
    ],
    "similarity_score": 0.6232711625393369,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/What makes an image realistic_.json"
}