{
    "from": "google",
    "scholar_id": "ofIWcXZ7RawJ",
    "detail_id": null,
    "title": "Physics-informed machine learning",
    "abstract": "\nAbstract | Despite great progress in simulating multiphysics problems using the numerical  discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy  data into existing algorithms, mesh generation remains complex, and high-\u200bdimensional problems  governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with  hidden physics is often prohibitively expensive and requires different formulations and elaborate  computer codes. Machine learning has emerged as a promising alternative, but training deep neura networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at  random points in the continuous space-\u200btime domain). Such physics-\u200binformed learning integrates  (noisy) data and mathematical models, and implements them through neural networks or other  kernel-\u200bbased regression networks. Moreover, it may be possible to design specialized network  architectures that automatically satisfy some of the physical invariants for better accuracy, faster  training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-\u200binformed learning both for forward and inverse problems, including discovering hidden physics and tackling high-\u200bdimensional problems.\n1Division of Applied \nMathematics, Brown \nUniversity Providence, RI, USA.\n2School of Engineering, Brown \nUniversity Providence, RI, USA.\n3Department of Chemical and \nBiomolecular Engineering, \nJohns Hopkins University, \nBaltimore, MD, USA.\n4Department of Applied \nMathematics and Statistics, \nJohns Hopkins University, \nBaltimore, MD, USA.\n5Department of Mathematics, \nMassachusetts Institute of \nTechnology, Cambridge,  \nMA, USA.\n6Department of Mechanical \nEngineering an",
    "bib_name": "karniadakis2021physics",
    "md_text": "# Physics-informed machine learning\n# Physics-\u200binformed machine learning\nGeorge Em Karniadakis  1,2 \u2709, Ioannis G. Kevrekidis3,4, Lu Lu  5, Paris Perdikaris6, Sifan Wang7 and Liu Yang  1\nAbstract | Despite great progress in simulating multiphysics problems using the numerical  discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy  data into existing algorithms, mesh generation remains complex, and high-\u200bdimensional problems  governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with  hidden physics is often prohibitively expensive and requires different formulations and elaborate  computer codes. Machine learning has emerged as a promising alternative, but training deep neura networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at  random points in the continuous space-\u200btime domain). Such physics-\u200binformed learning integrates  (noisy) data and mathematical models, and implements them through neural networks or other  kernel-\u200bbased regression networks. Moreover, it may be possible to design specialized network  architectures that automatically satisfy some of the physical invariants for better accuracy, faster  training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-\u200binformed learning both for forward and inverse problems, including discovering hidden physics and tackling high-\u200bdimensional problems.\n1Division of Applied \nMathematics, Brown \nUniversity Providence, RI, USA.\n2School of Engineering, Brown \nUniversity Providence, RI, USA.\n3Department of Chemical and \nBiomolecular Engineering, \nJohns Hopkins University, \nBaltimore, MD, USA.\n4Department of Applied \nMathematics and Statistics, \nJohns Hopkins University, \nBaltimore, MD, USA.\n5Department of Mathematics, \nMassachusetts Institute of \nTechnology, Cambridge,  \nMA, USA.\n6Department of Mechanical \nEngineering and Applied \nMechanics, University of \nPennsylvania, Philadelphia, \nPA, USA.\n7Graduate Group in  \nApplied Mathematics and \nComputational Science, \nUniversity of Pennsylvania, \nPhiladelphia, PA, USA.\n\u2709e-\u200bmail: george_karniadakis@ \nbrown.edu\nhttps://doi.org/10.1038/ \ns42254-021-00314-5\nModelling and forecasting the dynamics of multiphysics and multiscale systems remains an open scientific problem. Take for instance the Earth system, a uniquely complex system whose dynamics are intricately governed by the interaction of physical, chemical and biological processes taking place on spatiotemporal scales that span 17 orders of magnitude1. In the past 50 years, there has been tremendous progress in understanding multiscale physics in diverse applications, from geophysics to biophysics, by numerically solving partial differential equations (PDEs) using finite differences, finite elements, spectral and even meshless methods. Despite relentless progress, modelling and predicting the evolution of nonlinear multiscale systems with inhomogeneous cascades-\u200bof-\u200bscales by using classical analytical or computational tools inevitably faces severe challenges and introduces prohibitive cost and multiple sources of uncertainty. Moreover, solving inverse problems (for inferring material properties in functional materials or discovering missing physics in reactive transport, for example) is often prohibitively expensive and requires complex formulations, new algorithms and elaborate computer codes. Most importantly, solving real-\u200blife physical problems with missing, gappy or noisy boundary conditions through traditional approaches is currently impossible. This is where and why observational data play a crucial role. With the prospect of more than a trillion sensors\n# REVIEWS\n\nin the next decade, including airborne, seaborne and satellite remote sensing, a wealth of multi-\u200bfidelity observations is ready to be explored through data-\u200bdriven methods. However, despite the volume, velocity and variety of available (collected or generated) data streams, in many real cases it is still not possible to seamlessly incorporate such multi-\u200bfidelity data into existing physical models. Mathematical (and practical) data-\u200bassimilation efforts have been blossoming; yet the wealth and the spatiotemporal heterogeneity of available data, along with the lack of universally acceptable models, underscores the need for a transformative approach. This is where machine learning (ML) has come into play. It can explore massive design spaces, identify multi-\u200bdimensional correlations and manage ill-\u200bposed problems. It can, for instance, help to detect climate extremes or statistically predict dynamic variables such as precipitation or vegetation productivity2,3. Deep learning approaches, in particular, naturally provide tools for automatically extracting features from massive amounts of multi-\u200bfidelity observational data that are currently available and characterized by unprecedented spatial and temporal coverage4. They can also help to link these features with existing approximate models and exploit them in building new predictive tools. Even for biophysical and biomedical modelling, this synergistic integration between ML tools and multiscale and multiphysics models has been recently advocated5.\n# Reviews\n# Key points\nA common current theme across scientific domains is that the ability to collect and create observational data far outpaces the ability to assimilate it sensibly, let alone understand it4 (Box 1). Despite their towering empirical promise and some preliminary success6, most ML approaches currently are unable to extract interpretable information and knowledge from this data deluge. Moreover, purely data-\u200bdriven models may fit observations very well, but predictions may be physically inconsistent or implausible, owing to extrapolation or observational biases that may lead to poor generalization performance. Therefore, there is a pressing need for integrating fundamental physical laws and domain knowledge by \u2018teaching\u2019 ML models about governing physical rules, which can, in turn, provide \u2018informative priors\u2019 \u2014 that is, strong theoretical constraints and inductive biases on top of the observational ones. To this end, physics-\u200binformed learning is needed, hereby defined as the process by which prior knowledge stemming from our observational, empirical, physical or mathematical understanding of the world can be leveraged to improve the performance of a learning algorithm. A recent example reflecting this new learning philosophy is the family\nMulti-\u200bfidelity data Data of variable accuracy\nThe figure below schematically illustrates three possible categories of physical problems and associated available data. In the small data regime, it is assumed that one knows all  the physics, and data are provided for the initial and boundary conditions as well as the  coefficients of a partial differential equation. The ubiquitous regime in applications is   the middle one, where one knows some data and some physics, possibly missing some  parameter values or even an entire term in the partial differential equation, for example, reactions in an advection\u2013diffusion\u2013reaction system. Finally, there is the regime with   big data, where one may not know any of the physics, and where a data-\u200bdriven approach may be most effective, for example, using operator regression methods to discover   new physics. Physics-\u200binformed machine learning can seamlessly integrate data and the  governing physical laws, including models with partially missing physics, in a unified way This can be expressed compactly using automatic differentiation and neural networks7  that are designed to produce predictions that respect the underlying physical principles\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d5ce/d5ce0f88-42cc-4bdd-b6de-6a58fba76419.png\" style=\"width: 50%;\"></div>\nof \u2018physics-\u200binformed neural networks\u2019 (PINNs)7. This is a class of deep learning algorithms that can seamlessly integrate data and abstract mathematical operators, including PDEs with or without missing physics (Boxes 2,3). The leading motivation for developing these algorithms is that such prior knowledge or constraints can yield more interpretable ML methods that remain robust in the presence of imperfect data (such as missing or noisy values, outliers and so on) and can provide accurate and physically consistent predictions, even for extrapolatory/generalization tasks. Despite numerous public databases, the volume of useful experimental data for complex physical systems is limited. The specific data-\u200bdriven approach to the predictive modelling of such systems depends crucially on the amount of data available and on the complexity of the system itself, as illustrated in Box 1. The classical paradigm is shown on the left side of the figure in Box 1, where it is assumed that the only data available are the boundary conditions and initial conditions whereas the specific governing PDEs and associated parameters are precisely known. On the other extreme (on the right side of the figure), a lot of data may be available, for instance, in the form of time series, but the governing physical law (the underlying PDE) may not be known at the continuum level7\u20139. For the majority of real applications, the most interesting category is sketched in the centre of the figure, where it is assumed that the physics is partially known (that is, the conservation law, but not the constitutive relationship) but several scattered measurements (of a primary or auxiliary state) are available that can be used to infer parameters and even missing functional terms in the PDE while simultaneously recovering the solution. It is clear that this middle category is the most general case, and in fact it is representative of the other two categories, if the measurements are too few or too many. This \u2018mixed\u2019 case may lead to much more complex scenarios, where the solution of the PDEs is a stochastic process due to stochastic excitation or an uncertain material property. Hence, stochastic PDEs can be used to represent these stochastic solutions and uncertainties. Finally, there are many problems involving long-\u200brange spatiotemporal interactions, such as turbulence, visco-\u200belasto-\u200bplastic materials or other anomalous transport processes, where non-\u200blocal or fractional calculus and fractional PDEs may be the appropriate mathematical language to adequately describe such pheno\u00admena as they exhibit a rich expressivity not unlike that of deep neural networks (DNNs). Over the past two decades, efforts to account for uncertainty quantification in computer simulations have led to highly parameterized formulations that may include hundreds of uncertain parameters for complex problems, often rendering such computations infeasible in practice. Typically, computer codes at the national labs and even open-\u200bsource programs such as OpenFOAM10 or LAMMPS11 have more than 100,000 lines of code, making it almost impossible to maintain and update them from one generation to the next. We believe that it is possible to overcome these fundamental and practical problems using physics-\u200binformed learning, seamlessly integrating data and mathematical models, and implementing\nwww.nature.com/natrevphys\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5069/5069a231-f37c-4930-a578-b57d876b6501.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5727/57274aad-71f6-4e93-93ac-29f7cd30a30f.png\" style=\"width: 50%;\"></div>\nthem using PINNs or other nonlinear regression-\u200bbased physics-\u200binformed networks (PINs) (Box 2). In this Review, we first describe how to embed physics in ML and how different physics can provide guidance to developing new neural network (NN) architectures. We then present some of the new capabilities of physics-\u200binformed learning machines and highlight relevant applications. This is a very fast moving field, so at the end we provide an outlook, including some thoughts on current limitations. A taxonomy of several existing physics-\u200bbased methods integrated with ML can also be found in ref.12.\n# How to embed physics in ML\nNo predictive models can be constructed without assumptions, and, as a consequence, no generalization performance can be expected by ML models without\nappropriate biases. Specific to physics-\u200binformed learning, there are currently three pathways that can be followed separately or in tandem to accelerate training and enhance generalization of ML models by embedding physics in them (Box 2).\n# Observational biases\nObservational data are perhaps the foundation of the recent success of ML. They are also conceptually the simplest mode of introducing biases in ML. Given sufficient data to cover the input domain of a learning task, ML methods have demonstrated remarkable power in achieving accurate interpolation between the dots, even for high-\u200bdimensional tasks. For physical systems in particular, thanks to the rapid development of sensor networks, it is now possible to exploit a wealth of variable fidelity observations and monitor the evolution of complex phenomena across several spatial and temporal scales. These observational data ought to reflect the underlying physical principles that dictate their generation, and, in principle, can be used as a weak mechanism for embedding these principles into an ML model during its training phase. Examples include NNs proposed in refs13\u201316. However, especially for over-\u200bparameterized deep learning models, a large volume of data is typically necessary to reinforce these biases and generate predictions that respect certain symmetries and conservation laws. In this case, an immediate difficulty relates to the cost of data acquisition, which for many applications in the physical and engineering sciences could be prohibitively large, as observational data may be generated via expensive experiments or large-\u200bscale computational models.\n# Inductive biases\nInductive biases Another school of thought pertains to efforts focused on designing specialized NN architectures that implicitly embed any prior knowledge and inductive biases associated with a given predictive task. Without a doubt, the most celebrated example in this category are convolutional NNs17, which have revolutionized the field of computer vision by craftily respecting invariance along the groups of symmetries and distributed pattern representations found in natural images18. Additional representative examples include graph neural networks (GNNs)19, equivariant networks20, kernel methods such as Gaussian processes21\u201326, and more general PINs27, with kernels that are directly induced by the physical principles that govern a given task. Convolutional networks can be generalized to respect more symmetry groups, including rotations, reflections and more general gauge symmetry transformations19,20. This enables the development of a very general class of NN architectures on manifolds that depend only on the intrinsic geometry, leading to very effective models for computer vision tasks involving medical images28, climate pattern segmentation20 and others. Translation-\u200binvariant representations can also be constructed via wavelet-\u200bbased scattering transforms, which are stable to deformations and preserve high-\u200bfrequency information29. Another example includes covariant NNs30, tailored to conform with the rotation and translation invariances present\n# Reviews\nin many-\u200bbody systems (Fig. 1a). A similar example is  the equivariant transformer networks31, a family of   differentiable mappings that improve the robustness  of models for predefined continuous transformation \nLax\u2013Oleinik formula A representation formula   for the solution of the  Hamilton\u2013Jacobi equation\n# Box 3 | Physics-\u200binformed neural networks\nPhysics-\u200binformed neural networks (PINNs)7 seamlessly integrate the information from  both the measurements and partial differential equations (PDEs) by embedding the PDEs into the loss function of a neural network using automatic differentiation. The PDEs could be integer-\u200border PDEs7, integro-\u200bdifferential equations154, fractional PDEs103 or stochastic  PDEs42,102. Here, we present the PINN algorithm for solving forward problems using the example   of the viscous Burgers\u2019 equation\nwith a suitable initial condition and Dirichlet boundary conditions. In the figure, the left (physics-\u200buninformed) network represents the surrogate of the PDE solution u(x, t), while the right (physics-\u200binformed) network describes the PDE residual  \u03bd + \u2212 \u2202 \u2202 \u2202 \u2202 \u2202 \u2202 u u t u x u x 2 2. The los function includes a supervised loss of data measurements of u from the initial and  boundary conditions and an unsupervised loss of PDE: = + L L L w w , ( data data PDE PDE\nHere {(xi, ti)} and {(xj, tj)} are two sets of points sampled at the initial/boundary locations and in the entire domain, respectively, and ui are values of u at (xi, ti); wdata and wPDE are th weights used to balance the interplay between the two loss terms. These weights can   be user-\u200bdefined or tuned automatically, and play an important role in improving the  trainability of PINNs76,173. The network is trained by minimizing the loss via gradient-\u200bbased optimizers, such as  Adam196 and L-\u200bBFGS206, until the loss is smaller than a threshold \u03b5. The PINN algorithm   is shown below, and more details about PINNs and a recommended Python library  DeepXDE can be found in ref.154.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0cfc/0cfc4ab5-1101-49b9-a2c4-7829b588aea0.png\" style=\"width: 50%;\"></div>\ngroups. Despite their remarkable effectiveness, such approaches are currently limited to tasks that are characterized by relatively simple and well-\u200bdefined physics or symmetry groups, and often require craftsmanship and elaborate implementations. Moreover, their extension to more complex tasks is challenging, as the underlying invariances or conservation laws that characterize many physical systems are often poorly understood or hard to implicitly encode in a neural architecture. Generalized convolutions are not the only building blocks for designing architectures with strong implicit biases. For example, anti-\u200bsymmetry under the exchange of input variables can be obtained in NNs by using the determinant of a matrix-\u200bvalued function32. Reference33 proposed to combine a physics-\u200bbased model of bond-\u200border potential with an NN and divide structural parameters into local and global parts to predict interatomic potential energy surface in large-\u200bscale atomistic modelling. In another work34, an invariant tensor basis was used to embedded Galilean invariance into the network architecture, which significantly improved the NN prediction accuracy in turbulence modelling. For the problem of identifying Hamiltonian systems, networks are designed to preserve the symplectic structure of the underlying Hamiltonian system35 For example, ref.36 modified an auto-\u200bencoder to represent a Koopman operator for identifying coordinate transformations that recast nonlinear dynamics into approximately linear ones. Specifically for solving differential equations using NNs, architectures can be modified to satisfy exactly the required initial conditions37, Dirichlet boundary conditions37,38, Neumann boundary conditions39,40, Robin boundary conditions41, periodic boundary conditions42,43 and interface conditions41. In addition, if some features of the PDE solutions are known a priori, it is also possible to encode them in network architectures, for example, multiscale features44,45, even/odd symmetries and energy conservation46, high frequencies47 and so on. For a specific example, we refer to the recent work in ref.48, which proposed new connections between NN architectures and viscosity solutions to certain Hamilton\u2013Jacobi PDEs (HJ-\u200bPDEs). The two-\u200blayer architecture depicted in Fig. 1b defines  R R \u2192 f : \u00d7 [0,+ \u221e) n as follows\n(1)\nwhich is reminiscent of the celebrated Lax\u2013Oleinik formula. Here, x and t are the spatial and temporal variables, L is a convex and Lipschitz activation function,  R \u2208 ai  and R \u2208 ui n are the NN parameters, and m is the number of neurons. It is shown in ref.48 that f is the viscosity solution to the following HJ-\u200bPDE\n(2)\nwhere both the Hamiltonian H and the initial data J are explicitly obtained by the parameters and the activation\nwww.nature.com/natrevphys\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b253/b253eb43-9b3b-4d49-94d9-fa9058629c9f.png\" style=\"width: 50%;\"></div>\nFig. 1 | Physics-inspired neural network architectures. a | Predicting molecular properties with covariant compositional networks204. The architecture is based on graph neural networks19 and is constructed by decomposing into a hierarchy   of sub-\u200bgraphs (middle) and forming a neural network in which each \u2018neuron\u2019 corresponds to one of the sub-\u200bgraphs and  receives inputs from other neurons that correspond to smaller sub-\u200bgraphs (right). The middle panel shows how this can  equivalently be thought of as an algorithm in which each vertex receives and aggregates messages from its neighbours.  Also depicted on the left are the molecular graphs for C18H9N3OSSe and C22H15NSeSi from the Harvard Clean Energy  Project (HCEP) data set205 with their corresponding adjacency matrices. b | A neural network with the Lax\u2013Oleinik  formula represented in the architecture. f is the solution of the Hamilton\u2013Jacobi partial differential equations, x and t are  the spatial and temporal variables, L is a convex and Lipschitz activation function, \u2208R ai  and \u2208R ui n are the neural network  parameters, and m is the number of neurons. Panel a is adapted with permission from ref.204, AIP Publishing. Panel b image  courtesy of J. Darbon and T. Meng, Brown University.\nfunctions of the networks. The Hamiltonian H must be convex, but the initial data J are not. Note that the results of ref.48 do not rely on universal approximation theorems established for NNs. Rather, the NNs in ref.48 show that the physics contained in certain classes of HJ-\u200bPDEs can be naturally encoded by specific NN architectures without any numerical approximation in high dimensions.\n# Learning bias\nYet another school of thought approaches the problem of endowing an NN with prior knowledge from a different angle. Instead of designing a specialized architecture that implicitly enforces this knowledge, current efforts aim to impose such constraints in a soft manner by appropriately penalizing the loss function of conventional NN approximations. This approach can be viewed as a specific use-\u200bcase of multi-\u200btask learning,\nDeep Galerkin method A physics-\u200binformed neural network-\u200blike method with  random sampling.\nin which a learning algorithm is simultaneously constrained to fit the observed data, and to yield predictions that approximately satisfy a given set of physical constraints (for example, conservation of mass, momentum, monotonicity and so on). Representative examples include the deep Galerkin method49 and PINNs and their variants7,37,50\u201352. The framework of PINNs is further explained in Box 3, as it accurately reflects the key advantages and limitations of enforcing physics via soft penalty constraints. The flexibility of soft penalty constraints allows one to incorporate more general instantiations of domain-\u200bspecific knowledge into ML models. For example, ref.53 presented a statistically constrained generative adversarial network (GAN) by enforcing constraints of covariance from the training data, which results in an improved ML-\u200bbased emulator to capture the statistics of the training data generated by solving fully\n# Reviews\nresolved PDEs. Other examples include models tailored to learn contact-\u200binduced discontinuities in robotics54, physics-\u200binformed auto-\u200bencoders55, which use an additional soft constraint to preserve the Lyapunov stability, and InvNet56, which is capable of encoding invariances by soft constraints in the loss function. Further extensions include convolutional and recurrent architectures, and probabilistic formulations51,52,57. For example, ref.52 includes a Bayesian framework that allows for uncertainty quantification of the predicted quantities of interest in complex PDE dynamical systems. Note that solutions obtained via optimization with such soft penalty constraints and regularization can be viewed as equivalent to the maximum a-\u200bposteriori estimate of a Bayesian formulation stemming from physics-\u200bbased likelihood assumptions. Alternatively, a fully Bayesian treatment using Markov chain Monte Carlo methods or variational inference approximations can be used to quantify the uncertainty arising from noisy and gappy data, as discussed below.\nGappy data Sets with regions of missing data.\n# Hybrid approaches\nThe aforementioned principles of physics-\u200binformed ML have their own advantages and limitations. Hence, it would be ideal to use these different principles together, and indeed different hybrid approaches have been proposed. For example, non-\u200bdimensionalization can recover characteristic properties of a system, and thus it is beneficial to introduce physics bias via appropriate non-\u200bdimensional parameters, such as Reynolds, Froude or Mach numbers. Several methods have been proposed to learn operators that describe physical phenomena13,15,58,59. For example, DeepONets13 have been demonstrated as a powerful tool to learn nonlinear operators in a supervised data-\u200bdriven manner. What is more exciting is that by combining DeepONets with physics encoded by PINNs, it is possible to accomplish real-\u200btime accurate predictions with extrapolation in multiphysics applications such as electro-\u200bconvection60 and hypersonics61. However, when a low-\u200bfidelity model is available, a multi-\u200bfidelity strategy62 can be developed to facilitate the learning of a complex system. For example, ref.63 combines observational and learning biases through the use of large-\u200beddy simulation data and constrained NN training methods to construct closures for lower-\u200bfidelity Reynolds-\u200baveraged Navier\u2013Stokes models of turbulent fluid flow. Additional representative use-\u200bcases include the multi-\u200bfidelity NN used in ref.64 to extract material properties from instrumented indentation data, the PINs in ref.65 used to discover constitutive laws of non-\u200bNewtonian fluids from rheological data, and the coarse-\u200bgraining strategies proposed in ref.66. Even if it is not possible to encode the low-\u200bfidelity model into the learning directly, the low-\u200bfidelity model can be used through data augmentation \u2014 that is, generating a large amount of low-\u200bfidelity data via inexpensive low-\u200bfidelity models, which could be simplified mathematical models or existing computer codes, such as ref.64. Other representative examples include FermiNets32 and graph neural operator methods58. It is also possible to enforce the physics to an NN by embedding a network\ninto a traditional numerical method (such as finite element). This approach was applied to solve problems in many different fields, including nonlinear dynamical systems67, computational mechanics to model constitutive relations68,69, subsurface mechanics70\u201372, stochastic inversion73 and more74,75.\n# Connections to kernel methods\nMany of the presented NN-\u200bbased techniques have a close asymptotic connection to kernel methods, which can be exploited to produce new insight and understanding. For example, as demonstrated in refs76,77, the training dynamics of PINNs can be understood as a kernel regression method as the width of the network goes to infinity. More generally, NN methods can be rigorously interpreted as kernel methods in which the underlying warping kernel is also learned from data78,79. Warping kernels are a special kind of kernels that were initially introduced to model non-\u200bstationary spatial structures in geostatistics80 and have been also used to interpret residual NN models27,80. Furthermore, PINNs can be viewed as solving PDEs in a reproducing kernel Hilbert space spanned by a feature map (parametrized by the initial layers of the network), where the latter is also learned from data. Further connections can be made by studying the intimate connection between statistical inference techniques and numerical approximation. Existing works have explored these connections in the context of solving PDEs and inverse problems81, optimal recovery82 and Bayesian numerical analysis83\u201388. Connections between kernel methods and NNs can be established even for large and complicated architectures, such as attention-\u200bbased transformers89, whereas operator-\u200bvalued kernel methods90 could offer a viable path of analysing and interpreting deep learning tools for learning nonlinear operators. In summary, analysing NN models through the lens of kernel methods could have considerable benefits, as kernel methods are often interpretable and have strong theoretical foundations, which can subsequently help us to understand when and why deep learning methods may fail or succeed.\n# Connections to classical numerical methods\nClassical numerical algorithms, such as Runge\u2013Kutta methods and finite-\u200belement methods, have been the main workhorses for studying and simulating physical systems in silico. Interestingly, many modern deep learning models can be viewed and analysed by observing an obvious correspondence and specific connections to many of these classical algorithms. In particular, several architectures that have had tremendous success in practice are analogous to established strategies in numerical analysis. Convolutional NNs, for example, are analogous to finite different stencils in translationally equivariant PDE discretizations91,92 and share the same structures as the multigrid method93; residual NNs (ResNets, networks with skip connections)94 are analogous to the basic forward Euler discretization of autonomous ordinary differential equations95\u201398; inspection of simple Runge\u2013Kutta schemes (such as an RK4) immediately brings forth the analogy with recurrent NN architectures (and even with Krylov-\u200btype matrix-\u200bfree\nwww.nature.com/natrevphys\nlinear algebra methods such as the generalized minimal residual method)95,99. Moreover, the representation of DNNs with the ReLU activation function is equivalent to the continuous piecewise linear functions from the linear finite-\u200belement method100. Such analogies can provide insights and guidance for cross-\u200bfertilization, and pave the way for new \u2018mathematics-\u200binformed\u2019 meta-\u200blearning architectures. For example, ref.7 proposed a discrete-\u200btime NN method for solving PDEs that is inspired by an implicit Runge\u2013Kutta integrator: using up to 500 latent stages, this NN method can allow very large time-\u200bsteps and lead to solutions of high accuracy.\n# Merits of physics-\u200binformed learning\nThere are already many publications on physicsinformed ML across different disciplines for specific applications. For example, different extensions of PINNs cover conservation laws101 as well as stochastic and fractional PDEs for random phenomena and for anomalous transport102,103. Combining domain decomposition with PINNs provides more flexibility in multiscale problems, while the formulations are relatively simple to implement in parallel since each subdomain may be represented by a different NN, assigned to a different GPU with very small communication cost101,104,105. Collectively, the results from these works demonstrate that PINNs are particularly effective in solving ill-\u200bposed and inverse problems, whereas for forward, well-\u200bposed problems that do not require any data assimilation the existing numerical grid-\u200bbased solvers currently outperform PINNs. In the following, we discuss in more detail for which scenarios the use of PINNs may be advantageous and highlight these advantages in some prototypical applications.\n# Incomplete models and imperfect data\nAs shown in Box 1, physics-\u200binformed learning can easily combine both information from physics and scattered noisy data, even when both are imperfect. Recent research106 demonstrated that it is possible to find meaningful solutions even when, because of smoothness or regularity inherent in the PINN formulation, the problem is not perfectly well posed. Examples include forward and inverse problems, where no initial or boundary conditions are specified or where some of the parameters in the PDEs are unknown \u2014 scenarios in which classical numerical methods may fail. When dealing with imperfect models and data, it is beneficial to integrate the Bayesian approach with physics-\u200binformed learning for uncertainty quantification, such as Bayesian PINNs (B-\u200bPINNs)107. Moreover, compared with the traditional numerical methods, physics-\u200binformed learning is mesh-\u200bfree, without computationally expensive mesh generation, and thus can easily handle irregular and moving-\u200bdomain problems108. Lastly, the code is also easier to implement by using existing open-\u200bsource deep learning frameworks such as TensorFlow and PyTorch. Strong generalization in small data regime Deep learning usually requires a large amount of data for training, and in many physical problems it is difficult to obtain the necessary data at high accuracy. In these\nGenerative stochastic artificial  neural networks that can learn a probability distribution over  their set of inputs.\nDeep learning usually requires a large amount of data for training, and in many physical problems it is difficult to obtain the necessary data at high accuracy. In these\nsituations, physics-\u200binformed learning has the advantage of strong generalization in the small data regime. By enforcing or embedding physics, deep learning models are effectively constrained on a lower-\u200bdimensional manifold, and thus can be trained with a small amount of data. To enforce the physics, one can embed the physical principles into the network architecture, use physics as soft penalty constraints or use data augmentation as discussed previously. In addition, physics-\u200binformed learning is capable of extrapolation, not only interpolation: that is, it can perform spatial extrapolation in boundary-\u200bvalue problems107.\n# Understanding deep learning\nUnderstanding deep learning In addition to enhancing the trainability and generalization of ML models, physical principles are also being used to provide theoretical insight and elucidate the inner mechanisms behind the surprising effectiveness of deep learning. For example, in refs109\u2013112, the authors use the jamming transition of granular media to understand the double-\u200bdescent phenomenon of deep learning in the over-\u200bparameterized regime. Shallow NNs can also be viewed as interacting particle systems and hence can be analysed in the probability measure space with mean-\u200bfield theory, instead of the high-\u200bdimensional parameter space113. Another work114 rigorously constructed an exact mapping from the variational renormalization group to deep learning architectures based on restricted Boltzmann machines. Inspired by the successful density matrix renormalization group algorithm developed in physics, ref.115 proposed a framework for applying quantum-\u200binspired tensor networks to multi-\u200bclass supervised learning tasks, which introduces considerable savings in computational cost. Reference116 studied the landscape of deep networks from a statistical physics viewpoint, establishing an intuitive connection between NNs and the spin-\u200bglass models. In parallel, information propagation in wide DNNs has been studied based on dynamical systems theory117,118, providing an analysis of how network initialization determines the propagation of an input signal through the network, hence identifying a set of hyper-\u200bparameters and activation functions known as the \u2018edge of chaos\u2019 that ensure information propagation in deep networks.\n# Tackling high dimensionality\nDeep learning has been very successful in solving high-\u200bdimensional problems, such as image classification with fine resolution, language modelling, and high-\u200bdimensional PDEs. One reason for this success is that DNNs can break the curse of dimensionality under the condition that the target function is a hierarchical composition of local functions119,120. For example, in ref.121 the authors reformulated general high-\u200bdimensional parabolic PDEs using backward stochastic differential equations, approximating the gradient of the solution with DNNs, and then designing the loss based on the discretized stochastic integral and the given terminal condition. In practice, this approach was used to solve high-\u200bdimensional Black\u2013Scholes, Hamilton\u2013Jacobi\u2013Bellman and Allen\u2013Cahn equations.\nGANs122 have also proven to be fairly successful in generating samples from high-\u200bdimensional distributions in tasks such as image or text generation123\u2013125. As for their application to physical problems, in ref.102 the authors used GANs to quantify parametric uncertainty in high-\u200bdimensional stochastic differential equations, and in ref.126 GANs were used to learn parameters in high-\u200bdimensional stochastic dynamics. These examples show the capability of GANs in modelling high-\u200bdimensional probability distributions in physical problems. Finally, in refs127,128 it was demonstrated that even for operator regression and applications to PDEs, deep operator networks (DeepONets) can tackle the curse of dimensionality associated with the input space.\n# Uncertainty quantification\nUncertainty quantification Forecasting reliably the evolution of multiscale and multiphysics systems requires uncertainty quantification. This important issue has received a lot of attention in the past 20 years, augmenting traditional computational methods with stochastic formulations to tackle uncertainty due to the boundary conditions or material properties129\u2013131. For physics-\u200binformed learning models, there are at least three sources of uncertainty: uncertainty due to the physics, uncertainty due to the data, and uncertainty due to the learning models. The first source of uncertainty refers to stochastic physical systems, which are usually described by stochastic PDEs (SPDEs) or stochastic ordinary differential equations (SODEs). The parametric uncertainty arising from the randomness of parameters lies in this category. In ref.132 the authors demonstrate the use of NNs as a projection function of the input that can recover a low-\u200bdimensional nonlinear manifold, and present results for a problem on uncertainty propagation in an SPDE with uncertain diffusion coefficient. In the same spirit, in ref.133 the authors use a physics-\u200binformed loss function \u2014 that is, the expectation of the energy functional of the PDE over the stochastic variables \u2014 to train an NN parameterizing the solution of an elliptic SPDE. In ref.51, a conditional convolutional generative model is used to predict the density of a solution, with a physics-\u200binformed probabilistic loss function so that no labels are required in the training data. Notably, as a model designed to learn distributions, GANs offer a powerful approach to solving stochastic PDEs in high dimensions. The physics-\u200binformed GANs in refS102,134 represent the first such attempts. Leveraging data collected from simultaneous reads at a limited number of sensors for the multiple stochastic processes, physics-\u200binformed GANs are able to solve a wide range of problems ranging from forward to inverse problems using the same framework. Also, the results so far show the capability of GANs, if pro\u00ad perly formulated, to tackle the curse of dimensionality for problems with high stochastic dimensionality. The second source of uncertainty, in general, refers to aleatoric uncertainty arising from the noise in data and epistemic uncertainty arising from the gaps in data. Such uncertainty can be well tackled in the Bayesian framework. If the physics-\u200binformed learning model is based on Gaussian process regression, then it is straightforward to quantify uncertainty and exploit it for active\nAleatoric uncertainty\nUncertainty due to the inherent \nrandomness of data.\nEpistemic uncertainty\nUncertainty due to limited data \nand knowledge.\nArbitrary polynomial chaos\nA type of generalized \npolynomial chaos with \nmeasures defined by data.\nBoussinesq approximation\nAn approximation used in \ngravity-\u200bdriven flows, which \nignores density differences \nexcept in the gravity term.\nlearning and resolution refinement studies in PDEs23,135, or even design better experiments136. Another approach was proposed in ref.107 using B-\u200bPINNs. The authors of ref.107 showed that B-\u200bPINNs can provide reasonable uncertainty bounds, which are of the same order as the error and increase as the size of noise in data increases, but how to set the prior for B-\u200bPINNs in a systematic way is still an open question. The third source of uncertainty refers to the limitation of the learning models \u2014 for example, the approximation, training and generalization errors of NNs \u2014 and is usually hard to rigorously quantify. In ref.137, a convolutional encoder\u2013decoder NN is used to map the source term and the domain geometry of a PDE to the solution as well as the uncertainty, trained by a probabilistic supervised learning procedure with training data coming from finite-\u200belement methods. Notably, a first attempt to quantify the combined uncertainty from learning was given in ref.138, using the dropout method of ref.139 and, due to physical randomness, using arbitrary polynomial chaos. An extension to time-\u200bdependent systems and long-\u200btime integration was reported in ref.42: it tackled the parametric uncertainty using dynamic and bi-\u200borthogonal modal decomposition of the stochastic PDE, which are effective methods for long-\u200bterm integration of stochastic systems.\n# Applications highlights\nIn this section, we discuss some of the capabilities of physics-\u200binformed learning through diverse applications. Our emphasis is on inverse and ill-\u200bposed problems, which are either difficult or impossible to solve with conventional approaches. We also present several ongoing efforts on developing open-\u200bsource software for scientific ML.\n# Some examples\nFlow over an espresso cup. In the first example, we discuss how to extract quantitative information on the 3D velocity and pressure fields above an espresso coffee cup140. The input data is based on a video of temperature gradient (Fig. 2). This is an example of the \u2018hidden fluid mechanics\u2019 introduced in ref.106. It is an ill-\u200bposed inverse problem as no boundary conditions or any other information are provided. Specifically, 3D visualizations obtained using tomographic background-\u200boriented Schlieren (Tomo-\u200bBOS) imaging that measures density or temperature are used as input to a PINN, which seamlessly integrates the visualization data and the flow and passive scalar governing equations, to infer the latent quantities. Here, the physical assumption is that of the Boussinesq approximation, which is valid if the density variation is relatively small. The PINN uses the space and time coordinates as inputs and infers the velocity and pressure fields; it is trained by minimizing a loss function including a data mismatch of temperature and the residuals of the conservation laws (mass, momentum and energy). Independent experimental results from particle image velocimetry have verified that the Tomo-\u200bBOS/PINN approach is able to provide continuous, high-\u200bresolution and accurate 3D flow fields.\nwww.nature.com/natrevphys\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/875f/875f4214-19d6-465b-b9c8-a3a29fbc873d.png\" style=\"width: 50%;\"></div>\nFig. 2 | inferring the 3D flow over an espresso cup based using the Tomo-BOs imaging system and physics-informed  neural networks (PiNNs). a | Six cameras are aligned around an espresso cup, recording the distortion of the dot-\u200bpatterns  in the panels placed in the background, where the distortion is caused by the density variation of the airflow above the  espresso cup. The image data are acquired and processed with LaVision\u2019s Tomographic BOS software (DaVis 10.1.1).   b | 3D temperature field derived from the refractive index field and reconstructed based on the 2D images from all six  cameras. c | Physics-\u200binformed neural network (PINN) inference of the 3D velocity field (left) and pressure field (right) from  the temperature data. The Tomo-\u200bBOS experiment was performed by F. Fuest, Y. J. Jeon and C. Gray from LaVision. The PINN  inference and visualization were performed by S. Cai and C. Li at Brown University. Image courtesy of S. Cai and C.   Li, Brown University.\nPhysics-\u200binformed deep learning for 4D-\u200bflow MRI. Next, we discuss the use of PINNs in biophysics using real magnetic resonance imaging (MRI) data. Because it is non-\u200binvasive and proves a range of structural and physiological contrasts, MRI has become an indispensable tool for quantitative in-\u200bvivo assessment of blood flow and vascular function in clinical scenarios involving patients with cardiac and vascular disease. However, MRI measurements are often limited by the very coarse reso\u00ad lution and may be heavily corrupted by noise, leading to tedious and empirical workflows for reconstructing vascular topologies and associated flow conditions. Recent developments on physics-\u200binformed deep learning can greatly enhance the resolution and information content of current MRI technologies, with a focus on 4D-\u200bflow MRI. Specifically, it is possible to construct DNNs that are constrained by the Navier\u2013Stokes equations in order to effectively de-\u200bnoise MRI data and yield physically consistent reconstructions of the underlying velocity and pressure fields that ensure conservation of mass and momentum at an arbitrarily high spatial and temporal resolution. Moreover, the filtered velocity fields can be used to identify regions of no-\u200bslip\nflow, from which one can reconstruct the location and motion of the arterial wall and infer important quantities of interest such as wall shear stresses, kinetic energy and dissipation (Fig. 3). Taken together, these methods can considerably advance the capabilities of MRI technologies in research and clinical scenarios. However, there are potential pitfalls related to the robustness of PINNs, especially in the presence of high signal-\u200bto-\u200bnoise ratio in the MRI measurements and complex patterns in the underlying flow (for example, due to boundary layers, high-\u200bvorticity regions, transient turbulent bursts through a stenosis, tortuous branched vessels and so on). That said, under physiological conditions, blood flow is laminar, a regime under which current PINN models usually remain effective.\nUncovering edge plasma dynamics via deep learning from partial observations. Predicting turbulent transport on the edge of magnetic confinement fusion devices is a longstanding goal spanning several decades, currently presenting significant uncertainties in the particle and energy confinement of fusion power plants. In ref.141 it was demonstrated that PINNs can accurately\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e07/5e072e4c-b54b-444f-8f22-cbb908de2baf.png\" style=\"width: 50%;\"></div>\n# Reviews\nlearn turbulent field dynamics consistent with the twofluid theory from just partial observations of a synthetic plasma, for plasma diagnosis and model validation in challenging thermonuclear environments. Figure 4 displays the turbulent radial electric field learned by PINNs from partial observations of a 3D synthetic plasma\u2019s electron density and temperature141.\nCommittor function A function used to study  transitions between metastable  states in stochastic systems.\nAllen\u2013Cahn type system A type of system with both  reaction and diffusion.\nStudying transitions between metastable states of a distribution. Next, we discuss how physics-\u200binformed learning can be creatively used to tackle high-\u200bdimensional  problems. In ref.142, the authors proposed to use physics-\u200b informed learning to study transitions between two  metastable states of a high-\u200bdimensional probability  distribution. In particular, an NN was used to represent  the committor function, trained with a physics-\u200binformed  loss function defined as the variational formula for the  committor function combined with a soft penalty on   the boundary conditions. Moreover, adaptive importance sampling was used to sample rare events that  dominate the loss function, which reduces the asymptotic variance of the solution and improves generalization. Results for a probability distribution in a  144-\u200bdimensional Allen\u2013Cahn type system are illustrated  in Fig. 5. Although these computational results suggest  that this approach is effective for high-\u200bdimensional  problems, the application of the method to more complicated systems and the selection of the NN architecture  in adapting it to a given system remain challenging. Thermodynamically consistent PINNs. The physics  regularization generally pursued in PINNs admits an  interpretation as a least-\u200bsquares residual of point evaluations using an NN basis. For hyperbolic problems  involving shocks, where point evaluation of the solution is ill-\u200bdefined, it is natural to consider alternative  physics stabilization requiring reduced regularity. The  control volume PINN (cvPINN) pursued by ref.143  generalizes traditional finite-\u200bvolume schemes to deep  learning settings. In addition to offering increased \n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a870/a870cac0-089c-43a4-9bce-43ace4580ee0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d75/1d75924f-84a3-4c7d-af88-1fdf1f388480.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3193/31935d4e-1f3c-4f79-8ac7-108d9f157802.png\" style=\"width: 50%;\"></div>\nFig. 3 | Physics-informed filtering of in-vivo 4D-flow magnetic resonance imaging data of blood flow in a porcine descending aorta. Physics-\u200binformed neural network (PINN) models can be used to de-\u200bnoise and reconstruct clinical magnetic resonance imaging (MRI) data of blood velocity, while constraining this reconstruction to respect the underlying physical laws of momentum and mass conservation, as described by the incompressible Navier\u2013Stokes equations. Moreover, a trained PINN model has the potential to aid the automatic segmentation of the arterial wall\naccuracy due to reduced regularity requirements, connections to traditional finite-\u200bvolume schemes allow natural adaptation of total variation diminishing limiters and recovery of entropy solutions. This framework has allowed the estimation of black-\u200bbox equations of state for shock hydrodynamics models appropriate for materials such as metals. For scenarios such as phase transitions at extreme pressures and temperatures, DNNs provide an ideal means of addressing unknown model form, whereas the finite-\u200bvolume structure provided by cvPINNs allows enforcement of thermodynamic consistency.\nApplication to quantum chemistry. In some other applications, researchers have also used the physics to design specific new architectures together with the principles of physics-\u200binformed learning. For example, in ref.32, a fermionic NN (FermiNet) was proposed for the ab initio calculation of the solution of the many-\u200belectron Schr\u00f6dinger equation. FermiNet is a hybrid approach for embedding physics. First, to parameterize the wavefunction, the NN has a specialized architecture that obeys Fermi\u2013Dirac statistics: that is, it is anti-\u200bsymmetric under the exchange of input electron states and the boundary conditions (decay at infinity). Second, the training of FermiNet is also physics-\u200binformed: that is, the loss function is set as the variational form of the energy expectation value, with the gradient estimated by the Monte Carlo method. Although the application of NNs leads to eliminating the basis-\u200bset extrapolation, which is a common source of error in computational quantum chemistry, the performance of NNs, in general, depends on many factors, including the architectures and optimization algorithms, which require further systematic investigation.\nApplication to material sciences. In applications to materials, from the characterization of the material properties to the non-\u200bdestructive evaluation of their strength, physics-\u200binformed learning can play an important role\ngeometry and to infer important biomarkers such as blood pressure and wall shear stresses. a | Snapshot of in-\u200bvivo 4D-\u200bflow MRI measurements. b\u2013d | A PINN reconstruction of the velocity field (panel b), pressure (panel c), arterial wall surface geometry and wall shear stresses (panel d). The 4D-\u200bflow MRI data were acquired by E. Hwuang and W. Witschey at the University of Pennsylvania. The PINN inference and visualization were performed by S. Wang, G. Kissas and P. Perdikaris at the University of Pennsylvania.\nwww.nature.com/natrevphys\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a38f/a38f57e5-7a67-4de1-a0ff-0a337cb0d2eb.png\" style=\"width: 50%;\"></div>\nas the underlying problems are typically ill-\u200bposed and  of inverse type. In ref.144, the authors introduced an  optimized PINN trained to identify and precisely characterize a surface breaking crack in a metal plate. The  PINN was supervised with realistic ultrasonic surface  acoustic wave data acquired at a frequency of 5 MHz  and physically informed by the acoustic wave equation,  with the unknown wave speed function represented as  an NN. A key element in training was the use of adaptive  activation functions, which introduced new trainable  hyper-\u200bparameters and substantially accelerated convergence even in the presence of significant noise in the  data. An alternative approach to introducing physics into  ML is through a multi-\u200bfidelity framework as in ref.64 for  extracting mechanical properties of 3D-\u200bprinted materials via instrumented indentation. By solving the inverse  problem of depth-\u200bsensing indentation, the authors could  determine the elastoplastic properties of 3D-\u200bprinted titanium and nickel alloys. In this framework, a composite  NN consisting of two ResNets was used. One is a low-\u200b fidelity ResNet that uses synthetic data (a lot of finite-\u200b element simulations) and the other is a high-\u200bfidelity  ResNet that uses as input the sparse experimental data  and the output of the low-\u200bfidelity data. The objective was  to discover the nonlinear correlation function between  the low- and high-\u200bfidelity data, and subsequently   predict the modulus of elasticity and yield stress at high  fidelity. The results reported in ref.64 show impressive  performance of the multi-\u200bfidelity framework, reducing  the inference error for the yield stress from over 100%  with existing techniques to lower than 5% with the  multi-\u200bfidelity framework.\nApplication to molecular simulations. In ref.145, an NN  architecture was proposed to represent the potential  energy surfaces for molecular dynamics simulations,  where the translational, rotational and permutational \n# Reviews\nsymmetry of the molecular system is preserved with proper pre-\u200bprocessing. Such an NN representation could be further improved in deep potential molecular dynamics (DeePMD)146. With traditional artificially designed potential energy functions replaced by the NN trained with data from ab initio simulations, DeePMD achieves an ab initio level of accuracy at a cost that scales linearly with the system size. In ref.147, the limit of molecular dynamics simulations was pushed with ab initio accuracy to simulating more than 1-\u200bns-\u200blong trajectories of over 100 million atoms per day, using a highly optimized code for DeePMD on the Summit supercomputer. Before this work, molecular dynamics simulations with ab initio accuracy were performed in systems with up to 1 million atoms147,148.\nApplication to geophysics. Physics-\u200binformed learning has also been applied to various geophysical inverse problems. The work in ref.71 estimates subsurface properties, such as rock permeability and porosity, from seismic data by coupling NNs with full-\u200bwaveform inversion, subsurface flow processes and rock physics models. Furthermore, in ref.149, it was demonstrated that by combining DNNs and numerical PDE solvers as we discussed in the section on hybrid approaches, physics-\u200binformed learning is capable of solving a wide class of seismic inversion problems, such as velocity estimation, fault rupture imaging, earthquake location and source\u2013time function retrieval.\nTo implement PINNs efficiently, it is advantageous to build new algorithms based on the current ML libraries, such as TensorFlow150, PyTorch151, Keras152 and JAX153. Several software libraries specifically designed for physics-informed ML have been developed and are con\u00ad tributing to the rapid development of the field (Table 1).\nTo implement PINNs efficiently, it is advantageous to build new algorithms based on the current ML libraries, such as TensorFlow150, PyTorch151, Keras152 and JAX153. Several software libraries specifically designed for physics-informed ML have been developed and are con\u00ad tributing to the rapid development of the field (Table 1).\n# Reviews\nFig. 5 | Transitions between metastable states. Results obtained from studying transitions between metastable states   of a distribution in a 144-\u200bdimensional Allen\u2013Cahn type system. The top part of the figure shows the two metastable states.  The lower part of the figure shows, from left to right, a learned sample path with the characteristic nucleation pathway   for a transition between the two metastable states. Here, q is the committor function. Figure courtesy of G. M. Rotskoff,  Stanford University, and E. Vanden-\u200bEijnden, Courant Institute.\nAt the present time, some of the actively developed libraries include DeepXDE154, SimNet155, PyDEns156, NeuroDiffEq157, NeuralPDE158, SciANN159 and ADCME160. Because Python is the dominant programming language for ML, it is more convenient to use Python for physics-\u200binformed ML, and thus most of these libraries are written in Python, except the NeuralPDE158 and ADCME160, which are written in Julia. All these libraries use the automatic differentiation mechanism provided in other softwares such as TensorFlow150. Some of these libraries (such as DeepXDE154 and SimNet155) can be used as a solver, that is, users only need to define the problem and then the solver will deal with all the underlying details and solve the problem, whereas some (such as SciANN159 and ADCME160) only work as a wrapper, meaning they wrap low-\u200blevel functions of other libraries (such as TensorFlow) into relatively high-\u200blevel functions for easier implementation of physics-\u200binformed learning and users still need to implement all the steps to solve the problem. Software packages such as GPyTorch161 and Neural Tangents162 also enable the study of NNs and PINNs through the lens of kernel methods. This viewpoint has produced new understanding of the training dynamics of PINNs, subsequently motivating the design of new effective architectures and training algorithms76,77. DeepXDE not only solves integer-\u200border ODEs and PDEs, but it can also solve integro-\u200bdifferential equations and fractional PDEs. DeepXDE supports complex domain geometries via the technique of constructive solid geometry, and enables the user code to stay compact, resembling closely the mathematical formulation. DeepXDE is also well-\u200bstructured and highly configurable, since all its components are loosely coupled. We note that in addition to being used as a research tool for solving problems in computational science and engineering, DeepXDE can also be used as an educational tool in diverse courses. Although DeepXDE is suitable for education and research, SimNet155 developed by Nvidia is specifically optimized for Nvidia GPUs for large-\u200bscale engineering problems. In PINNs (Box 3), one needs to compute the derivatives of the network outputs with respect to the network\ninputs. One can compute the derivatives using automatic differentiation provided by ML packages such as TensorFlow150. For example, U t \u2202 \u2202 can be computed using TensorFlow as tf.gradients(U, t), and secondorder derivatives can be computed by applying tf. gradients twice. DeepXDE provides a more convenient way to compute higher-\u200border derivatives, for example using dde.grad.hessian to compute the Hessian matrix. Moreover, there are two extra advantages to using dde.grad.hessian: first, it is lazy evaluation, meaning it will only compute an element in the Hessian matrix until that element is needed, rather than computing the whole Hessian matrix. Second, it memorizes all the gradients that have already been computed to avoid duplicate computation, even if the user calls the function multiple times in different parts of the code. These two features could speed up the computation in problems where one needs to compute the gradients many times, for example in a system of coupled PDEs. Most of these libraries (such as DeepXDE and SimNet) use physics as the soft penalty constraints (Box 3), and ADCME embeds DNNs in standard scientific numerical schemes (such as Runge\u2013Kutta methods for ODEs, and the finite-\u200bdifference, finite-\u200belement and finite-\u200bvolume methods for PDEs) to solve inverse problems. ADCME was recently extended to support implicit schemes and nonlinear constraints163,164. To enable truly large-\u200bscale scientific computations on large meshes, support for MPI-\u200bbased domain decomposition methods is also available and was demonstrated to scale very well on complex problems165.\n# Which model, framework, algorithm to use?\nWith a growing collection of methodologies and software tools, a series of questions naturally arises: given a physical system and/or governing law and some observational data, which ML framework should one use? Which training algorithm to choose? How many training samples to consider? Although at present there are no rule-\u200bof-\u200bthumb strategies for answering these questions, and some degree of experience is required to set up a physics-\u200binformed ML model properly, meta-\u200blearning techniques166\u2013168 could automate this process in the\nwww.nature.com/natrevphys\nfuture. The choices intimately depend on the specific task that needs to be tackled. In terms of providing a high-\u200blevel taxonomy, we note that PINNs are typically used to infer a deterministic function that is compatible with an underlying physical law when a limited number of observations is available (either initial/boundary conditions or other measurements). The underlying architecture of a PINNs model is determined by the nature of a given problem: multi-\u200blayer perceptron architectures are generally applicable but do not encode any specialized inductive biases, convolutional NN architectures are suitable for gridded 2D domains, Fourier feature networks are suitable for PDEs whose solution exhibits high frequencies or periodic boundaries, and recurrent architectures are suitable for non-\u200bMarkovian and time-\u200bdiscrete problems. Moreover, probabilistic variants of PINNs can also be used to infer stochastic processes that can allow capturing epistemic/model uncertainty (via Bayesian inference or frequentist ensembles) or aleatoric uncertainty (via generative models such as variational auto-\u200bencoders and GANs). However, the DeepONet framework can be used to infer an operator (instead of a function). In DeepONet, the choice of the underlying architecture can also vary depending on the nature of available data, such as scattered sensor measurements (multi-\u200blayer perceptron), images (convolutional NNs) or time series (recurrent NNs). In all the aforementioned cases, the required sample complexity is typically not known a priori and is generally determined by: the strength of inductive biases used in the architecture; the compatibility between the observed data, and the underlying physical law used as regularization; and the complexity of the underlying function or operator to be approximated.\nhp-\u200brefinement Dual refinement of the   mesh by increasing either   the number of subdomains   or the approximations degree.\nDespite the recent success of physics-\u200binformed learning across a range of applications, multiscale and multi\u00ad physics problems require further developments. For example, fully connected NNs have difficulty learning  high-\u200bfrequency functions, a phenomenon referred to in the literature as the \u2018F-\u200bprinciple\u2019169 or \u2018spectral bias\u2019170. Additional work171,172 rigorously proved the existence of frequency bias in DNNs and derived convergence rates\n<div style=\"text-align: center;\">Table 1 | Major software libraries specifically designed for physics-\u200binformed  machine learning</div>\n<div style=\"text-align: center;\">Table 1 | Major software libraries specifically designed for physics-\u200binformed  machine learning</div>\nsoftware name\nUsage\nLanguage\nBackend\nRef.\nDeepXDE\nSolver\nPython\nTensorFlow\n154\nSimNet\nSolver\nPython\nTensorFlow\n155\nPyDEns\nSolver\nPython\nTensorFlow\n156\nNeuroDiffEq\nSolver\nPython\nPyTorch\n157\nNeuralPDE\nSolver\nJulia\nJulia\n158\nSciANN\nWrapper\nPython\nTensorFlow\n159\nADCME\nWrapper\nJulia\nTensorFlow\n160\nGPyTorch\nWrapper\nPython\nPyTorch\n161\nNeural Tangents\nWrapper\nPython\nJAX\n162\nof training as a function of target frequency. Moreover, high-\u200bfrequency features in the target solution generally result in steep gradients, and thus PINN models often struggle to penalize accurately the PDE residuals45. As a consequence, for multiscale problems, the networks struggle to learn high-\u200bfrequency components and often may fail to train76,173. To address the challenge of learning high-\u200bfrequency components, one needs to develop new techniques to aid the network learning, such as domain decomposition105, Fourier features174 and multiscale DNN45,175. However, learning multiphysics simultaneously could be computationally expensive. To address this issue, one may first learn each physics separately and then couple them together. In the method of DeepM&M for the problems of electro-\u200bconvection60 and hypersonics61, several DeepONets were first trained for each field separately and subsequently learned the coupled solutions through either a parallel or a serial DeepM&M architecture using supervised learning based on additional data for a specific multiphysics problem. It is also possible to learn the physics at a coarse scale by using the fine-\u200bscale simulation data only in small domains176. Currently in NN-\u200bbased ML methods, the physicsinformed loss functions are mainly defined in a pointwise way. Although NNs with such loss functions can be successful in some high-\u200bdimensional problems, they may also fail in some special low-\u200bdimensional cases, such as the diffusion equation with non-\u200bsmooth conductivity/permeability177.\n# New algorithms and computational frameworks\nNew algorithms and computational frameworks Physics-\u200binformed ML models often involve training large-\u200bscale NNs with complicated loss functions, which generally consist of multiple terms and thus are highly non-\u200bconvex optimization problems178. The terms in the loss function may compete with each other during training. Consequently, the training process may not be robust and sufficiently stable, and thus convergence to the global minimum cannot be guaranteed179. To resolve this issue, one needs to develop more robust NN architectures and training algorithms for diverse applications. For example, refs76,77,173 have identified two fundamental weaknesses of PINNs, relating spectral bias170 to a discrepancy in the convergence rate of different components in a PINN loss function. The latter is manifested by training instabilities leading to vanishing back-\u200bpropagated gradients. As discussed in these refs76,77,173, these pathologies can be mitigated by designing appropriate model architectures and new training algorithms for PINNs. Also, ref.104 used the weak form of the PDE and hp-\u200brefinement via decomposition to enhance the approximation capability of networks. Other examples include adaptively modifying the activation functions180 or sampling the data points and the residual evaluation points during training181, which accelerate convergence and improve the performance of physics-\u200binformed models. Moreover, the design of effective NN architectures is currently done empirically by users, which could be very time-\u200bconsuming. However, emerging meta-\u200blearning techniques can be used to automate this search166\u2013168. What is interesting here is that\nthe architecture may be changing as the bifurcation parameters of the system (such as the Reynolds number) increase. The training and optimization of deep learning models is expensive, and it is crucial to speed up the learning, for instance through transfer learning via DeepONets as in the example of crack propagation reported in ref.182. In addition, scalable and parallel training algorithms should be developed by using hardware like GPUs and tensor processing units, using both data-\u200bparallel and model-\u200bparallel algorithms. Unlike classic classification or regression tasks, where the first-\u200border derivative is required for gradient descent, physics-\u200binformed ML usually involves higher-\u200border derivatives. Currently, their efficient evaluation is not well supported in popular software frameworks such as TensorFlow and PyTorch. An ML software library that is more efficient for computing high-\u200border derivatives (for example, via Taylor-\u200bmode automatic differentiation)183,184 could greatly reduce the computational cost and boost the application of physics-\u200binformed ML across different disciplines. In addition to integer-\u200border derivatives, other operators such as integral operators and even fractional-\u200border derivatives103 are very useful in physics-\u200binformed learning.\n# Data generation and benchmarks\nIn the ML community dealing with imaging, speech  and natural language processing problems, the use of  standard benchmarks is very common in order to assess algorithm improvement, reproducibility of results, and expected computational cost. The UCI Machine Learning Repository185, which was created over three decades ago,  is a collection of databases and data generators that are often used to compare the relative performance of new algorithms. Currently, they also include experimental  data sets in the physical sciences, for example noise generated by an aerofoil, ocean temperature and current measurements related to El Ni\u00f1o, and hydrodynamic resistance related to different yacht designs. These data sets are useful and are intended for data-\u200bdriven modelling in ML, but in principle they can also be used for benchmarking physics-\u200binformed ML methods, assuming that proper parameterized physical models can be explicitly included in the databases. However, in many different applications in physics and chemistry, full-\u200bfield data are required, which cannot be obtained experimentally (for example in density-\u200bfunctional theory and molecular dynamics simulation or in direct numerical simulations of turbulence), and which tax computational resources  heavily both in terms of time and memory. Hence, careful consideration should be given to how to make these data publicly available, how to curate such valuable data, and how to include the physical models and all parameters required for the generation of these databases. In addition, it will take a concerted effort by researchers to design meaningful benchmarks that test accuracy and speed-\u200bup of the new proposed physics-\u200binformed algorithms, which is a non-\u200btrivial task. Indeed, even for the aforementioned imaging and other established ML  applications, there are still new developments on refining  existing benchmarks and metrics, especially if software and hardware considerations are also factored in such\nH\u00f6lder regularization A regularization term  associated with H\u00f6lder  constants of differential  equations that controls the  derivatives of neural networks. Rademacher complexity A quantity that measures  richness of a class of  real-\u200bvalued functions with  respect to a probability  distribution.\nevaluations (for example, an in-\u200bdepth analysis for image recognition)186. In physical systems, these difficulties are exacerbated by the fact that the aim is to predict dynamics, and it will be complicated, for example, to determine how to capture or identify bifurcations in dynamical systems and chaotic states. However, new metrics such as the valid-\u200btimeprediction introduced in ref.187 may be appropriate and offer a promising direction to follow.\nNew mathematics Despite the empirical success of physics-\u200binformed learning models, little is known about the theoretical foundation of such constrained NNs. A new theory is required to rigorously analyse the capabilities and limitations of physics-\u200binformed learning (for example, the learning capacity of NNs). More specifically, a fundamental question is: can a network find solutions to PDE via gradient-\u200bbased optimization? To answer this question, one should analyse the total error in deep learning, which can be decomposed into three types of errors: approximation error (can a network approximate a solution to PDE with any accuracy?), optimization error (can one attain zero or very small training loss?) and generalization error (does smaller training error mean more accurate predicted solution?). It is important to analyse the well-\u200bposedness of the problem and the stability and convergence in terms of these errors. In particular, if the operator to be solved is (possibly partially) learned by the data themselves, establishing how well-\u200bposed any problem involving this operator is becomes an exciting mathematical challenge. The challenge is exacerbated when the initial/boundary/internal conditions are provided themselves as (possibly uncertain) data. This well-\u200bposedness issue must be analysed mathematically, aided by ML computational exploration. The first mathematical analysis for PINNs in solving forward problems appeared in ref.188, where the H\u00f6lder regularization was introduced to control generalization error. Specifically, ref.188 analysed the second-\u200border linear elliptic and parabolic type PDEs and proved the consistency of results. References189,190 used quadrature points in the formulation of the loss and provided an abstract error estimate for both forward and inverse problems. However, no convergence results were reported, as the use of quadrature points does not quantify the generalization error. In subsequent work, ref.191 studied linear PDEs and proposed an abstract error estimates framework for analysing both PINNs7 and variational PINNs104,192. Based on the compactness assumptions and the norm equivalence relations, sufficient conditions for convergence to the underlying PDE solution were obtained. The generalization error was handled by the Rademacher complexity. For the continuous loss formulation, refs49,193\u2013195 derived some error estimates based on the continuous loss formulations of PINNs. Although known error bounds involved with continuous norms (from PDE literature) may serve as error bounds for (continuous) PINNs, data samples have to be taken into account to quantify the generalization error. In general, NNs are trained by gradient-\u200bbased optimization methods, and a new theory should be developed\nwww.nature.com/natrevphys\nto better understand their training dynamics (gradient descent, stochastic gradient descent, Adam196 and so on). In ref.197, over-\u200bparameterized two-\u200blayer networks were analysed, and it was proved that the convergence of gradient descent for second-\u200border linear PDEs, but the boundary conditions were not included in the analysis. In ref.76, the neural tangent kernel theory198 was extended to PINNs, and it was shown that the training dynamics of PINNs sometimes can be regarded as a kernel regression as the width of network goes to infinity. It is also helpful to understand the training process of networks by visualizing the landscape of loss function of different formulations (strong form, weak form and so on). Furthermore, more methods are being rapidly developed nowadays, and thus it is also important to understand the equivalence between models and the equivalence between different loss functions with different norms. Analysing the physics-\u200binformed ML models based on rigorous theory calls for a fruitful synergy between deep learning, optimization, numerical analysis and PDE theory that not only has the potential to lead to more robust and effective training algorithms, but also to build a solid foundation for this new generation of computational methods.\n# Outlook\nPhysics-\u200binformed learning integrates data and mathematical models seamlessly even in noisy and highdimensional contexts, and can solve general inverse problems very effectively. Here, we have summarized some of the key concepts in Boxes 1\u20133 and provided references to frameworks and open-\u200bsource software for the interested reader to have a head start in exploring physics-\u200binformed learning. We also discussed current capabilities and limitations and highlighted diverse applications from fluid dynamics to biophysics, plasma physics, transition between metastable states and other applications in materials. Next, we present possible new directions for applications of physics-\u200binformed learning machines as well as research directions that will contri\u00adbute to their faster training, more accurate predictions, and better interpretability for diverse physics applications and beyond. Although there have been tools like TensorBoard to visualize the model graph, track the variables and metrics, and so on, for physical problems, extended requirements may include incorporating multiple physics and complicated geometry domain into the learning algorithm, visualizing the solution field (even high-\u200bdimensional ones), as in traditional computing platforms such as FEniCS199, OpenFOAM10 and others. A user-\u200bfriendly, graph-\u200bbased ML development environ\u00ad ment that can address the above issues could help more practitioners to develop physics-\u200binformed ML algorithms for applications to a wide range of diverse physical problems.\n# Future directions\nDigital twins. \u2018Digital twins\u2019, a concept first put forth  by General Electric to describe the digital copy of  an engine manufactured in their factories, are now \nbecoming a reality in a number of industries. By assimilating real measurements to calibrate computational models, a digital twin aims to replicate the behaviour of a living or non-\u200bliving physical entity in silico. Before these emerging technologies can be translated into practice, a series of fundamental questions need to be addressed. First, observational data can be scarce and noisy, are often characterized by vastly hetero\u00adgeneous data modalities (images, time series, lab tests, histo\u00ad rical data, clinical records and so on), and may not be directly available for certain quantities of interest. Second, physics-\u200bbased computational models heavily rely on tedious pre-\u200bprocessing and calibration procedures (such as mesh generation or calibration of initial and boundary conditions) that typically have a considerable cost, hampering their use in real-\u200btime decisionmaking settings. Moreover, physical models of many complex natural systems are, at best, \u2018partially\u2019 known as conservation laws, and do not provide a closed system of equations unless appropriate constitutive laws are postulated. Thanks to its natural capability of blending physical models and data as well as the use of automatic differentiation that removes the need for mesh generation, physics-\u200binformed learning is well placed to become an enabling catalyst in the emerging era of digital twins.\nData and model transformations, fusion and interpretability. As the interactions between physics-\u200bbased modelling and ML intensify, one will encounter \u2014 with increasing frequency \u2014 situations in which different researchers arrive at different data-\u200bdriven models of the same phenomenon, even if they use the same training data (or equally informative data, observed through different sensors). For example, two research groups using the same or equivalent alternative data may end up having differently trained networks (differently learned latent spaces, differently learned operators) even though their predictions are practically indistinguishable on the training set. Recognizing that there is often no unique physical interpretation of an observed phenomenon, here we foresee the importance of building ML-\u200bbased transformations between predictive models, models at different fidelities, and theories, that are one-\u200bto-\u200bone (transformable, \u2018dual\u2019, calibratable) to each other in a verifiable manner. Researchers are increasingly discovering such transformations (for example from nonlinear dynamics to the corresponding Koopman model; from a Poisson system to the corresponding Hamiltonian one; from Nesterov iterations to their corresponding ODEs) in a data-\u200bdriven manner. Such transformations will allow data and models to be systematically fused. Transformations between the ML latent space features and the physically interpretable observables, or ML-\u200blearned operators and closed-\u200bform equations, will obviously bolster the interpretability of the ML model. Ultimately, one needs to test how far these transformations generalize: for what range of observations an ML model can be mapped to a different ML model, or to a physical model, and what the generalization limit is, beyond which they cannot be transformed or calibrated to each other.\n# Reviews\nSearching for intrinsic variables and emergent, useful representations. Most of the current physics-\u200binformed ML methods follow this paradigm: first define a set of (humanly interpretable) observables/variables; then collect data; formulate the physics completely or incompletely using a \u2018reasonable\u2019 dictionary of operators based on the chosen observables; and finally apply the learning algorithm of choice. An emerging paradigm fuelled by advances in ML is to use observations and learning methods to automatically determine good/ intrinsic variables and to also find useful or informative physical model formulations. Stepping beyond principal component analysis, manifold learning techniques (from ISOMAP to t-\u200bSNE and diffusion maps) and their deep learning counterparts of generative models and (possibly variational) auto-\u200bencoders are used to embed raw observations in reduced, mathematically useful latent spaces, in which evolution rules can be learned. Remarkably, these useful representations can go beyond embedding the relevant features, the dependent vari\u00adables in a PDE. For spatiotemporally disordered data, one can also create ML-\u200bdriven emergent spaces200,201 in terms of ML-\u200blearned independent variables: emergent \u2018spacetimes\u2019 in which the model operators will be learned. The DARPA Shredder Challenge202 of 2011 recreated space by effectively solving puzzles: documents shredded using a variety of paper shredding techniques. Today, disorganized spatiotemporal observations can be embedded in informative \u2018independent variable\u2019 emergent spaces. For example, evolution operators in the form of PDEs or\nISOMAP A nonlinear dimensionality  reduction technique for  embedding intrinsically  low-\u200bdimensional data   from high-\u200bdimensional  representations to  lower-\u200bdimensional spaces.\n# t-\u200bSNE\nt-\u200bdistributed stochastic  neighbour embedding.   A nonlinear dimensionality  reduction technique for  embedding intrinsically  low-\u200bdimensional data   from high-\u200bdimensional  representations to  lower-\u200bdimensional spaces. Diffusion maps A nonlinear dimensionality  reduction technique for  embedding intrinsically  low-\u200bdimensional data   from high-\u200bdimensional  representations to  lower-\u200bdimensional spaces.\n14. Kashefi, A., Rempe, D. & Guibas, L. J. A point-\u200bcloud  deep learning framework for prediction of fluid flow  fields on irregular geometries. Phys. Fluids 33,  027104 (2021). 15. Li, Z. et al. Fourier neural operator for parametric  partial differential equations. in Int. Conf. Learn.  Represent. (2021). 16. Yang, Y. & Perdikaris, P. Conditional deep surrogate  models for stochastic, high-\u200bdimensional, and   multi-\u200bfidelity systems. Comput. Mech. 64, 417\u2013434  (2019). 17. LeCun, Y. & Bengio, Y. et al. Convolutional networks  for images, speech, and time series. Handb. Brain  Theory Neural Netw. 3361, 1995 (1995). 18. Mallat, S. Understanding deep convolutional networks. Phil. Trans. R. Soc. A 374, 20150203 (2016). 19. Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A.   & Vandergheynst, P. Geometric deep learning: going  beyond Euclidean data. IEEE Signal Process. Mag. 34, 18\u201342 (2017). 20. Cohen, T., Weiler, M., Kicanaoglu, B. & Welling, M.  Gauge equivariant convolutional networks and the  icosahedral CNN. Proc. Machine Learn. Res. 97,  1321\u20131330 (2019). 21. Owhadi, H. Multigrid with rough coefficients   and multiresolution operator decomposition from  hierarchical information games. SIAM Rev. 59,  99\u2013149 (2017). 22. Raissi, M., Perdikaris, P. & Karniadakis, G. E.   Inferring solutions of differential equations using noisy multi-\u200bfidelity data. J. Comput. Phys. 335, 736\u2013746  (2017). 23. Raissi, M., Perdikaris, P. & Karniadakis, G. E.  Numerical Gaussian processes for time-\u200bdependent   and nonlinear partial differential equations. SIAM J.  Sci. Comput. 40, A172\u2013A198 (2018). 24. Owhadi, H. Bayesian numerical homogenization.  Multiscale Model. Simul. 13, 812\u2013828 (2015). 25. Hamzi, B. & Owhadi, H. Learning dynamical systems  from data: a simple cross-\u200bvalidation perspective, part I: parametric kernel flows. Physica D 421, 132817  (2021). 26. Reisert, M. & Burkhardt, H. Learning equivariant  functions with matrix valued kernels. J. Mach.   Learn. Res. 8, 385\u2013408 (2007).\n1.  Hart, J. K. & Martinez, K. Environmental sensor  networks: a revolution in the earth system science?  Earth Sci. Rev. 78, 177\u2013191 (2006). 2.  Kurth, T. et al. Exascale deep learning for climate  analytics (IEEE, 2018). 3.  Reddy, D. S. & Prasad, P. R. C. Prediction of  vegetation dynamics using NDVI time series data   and LSTM. Model. Earth Syst. Environ. 4, 409\u2013419  (2018). 4.  Reichstein, M. et al. Deep learning and process  understanding for data-\u200bdriven earth system science.  Nature 566, 195\u2013204 (2019). 5.  Alber, M. et al. Integrating machine learning and  multiscale modeling \u2014 perspectives, challenges,   and opportunities in the biological, biomedical, and  behavioral sciences. NPJ Digit. Med. 2, 1\u201311 (2019). 6.  Iten, R., Metger, T., Wilming, H., Del Rio, L. &   Renner, R. Discovering physical concepts with neural  networks. Phys. Rev. Lett. 124, 010508 (2020). 7.  Raissi, M., Perdikaris, P. & Karniadakis, G. E.   Physics-\u200binformed neural networks: a deep learning  framework for solving forward and inverse problems  involving nonlinear partial differential equations.   J. Comput. Phys. 378, 686\u2013707 (2019). 8.  Schmidt, M. & Lipson, H. Distilling free-\u200bform natural  laws from experimental data. Science 324, 81\u201385  (2009). 9.  Brunton, S. L., Proctor, J. L. & Kutz, J. N. Discovering  governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl Acad.   Sci. USA 113, 3932\u20133937 (2016). 10. Jasak, H. et al. OpenFOAM: A C++ library for complex physics simulations. Int. Workshop Coupled Methods  Numer. Dyn. 1000, 1\u201320 (2007). 11. Plimpton, S. Fast parallel algorithms for short-\u200brange  molecular dynamics. J. Comput. Phys. 117, 1\u201319  (1995). 12. Jia, X. et al. Physics-\u200bguided machine learning for  scientific discovery: an application in simulating lake  temperature profiles. Preprint at arXiv https://arxiv.org abs/2001.11086 (2020). 13. Lu, L., Jin, P., Pang, G., Zhang, Z. & Karniadakis, G. E. Learning nonlinear operators via DeepONet based   on the universal approximation theorem of operators. Nat. Mach. Intell. 3, 218\u2013229 (2021).\nSODEs will then be learned in terms of these new, emergent space \u2014 even possibly time \u2014 independent vari\u00ad ables; there is a direct analogy here with the discussion  of emergent space-\u200btime in modern physics203. Such new paradigms could play a critical role in  design optimization or in building a digital twin for  complicated systems, even systems of systems, where  humans can hardly write down a neat physical formulation in closed form. Moreover, instead of collecting  data from experiments first and then performing the  learning algorithm, it becomes important to integrate  both in an active learning framework. In this way,   a judicious selection of new and informative data can be  aided by exploiting the geometry of latent space of the  learning algorithms, while the algorithms can gradually  improve the choice of latent space descriptors, as well as  the mathematical formulation governing the physics, so  as to yield realistic predictions as the experiments go on. Ultimately, the main element we see changing is what  we mean by \u2018understanding\u2019. Up to now, understanding  meant that, say, each term in a PDE had a physical or  mechanistic interpretation operated on some physically  meaningful observables (dependent variables) and also  operated in terms of some physically meaningful space/ time (independent) variables. Now, it becomes possible to make accurate predictions without this type of  mechanistic understanding \u2014 and \u2018understanding\u2019 is  something that may be redefined in the process.\n# Published online xx xx xxxx\n27. Owhadi, H. & Yoo, G. R. Kernel flows: from learning  kernels from data into the abyss. J. Comput. Phys.  389, 22\u201347 (2019). 28. Winkens, J., Linmans, J., Veeling, B. S., Cohen, T. S.   & Welling, M. Improved semantic segmentation   for histopathology using rotation equivariant  convolutional networks. in Conf. Med. Imaging   Deep Learn. (2018). 29. Bruna, J. & Mallat, S. Invariant scattering convolution networks. IEEE Trans. Pattern Anal. Mach. Intell. 35,  1872\u20131886 (2013). 30. Kondor, R., Son, H. T., Pan, H., Anderson, B. &   Trivedi, S. Covariant compositional networks for  learning graphs. Preprint at arXiv https://arxiv.org/ abs/1801.02144 (2018). 31. Tai, K. S., Bailis, P. & Valiant, G. Equivariant  transformer networks. Proc. Int. Conf. Mach. Learn.  97, 6086\u20136095 (2019). 32. Pfau, D., Spencer, J. S., Matthews, A. G. &   Foulkes, W. M. C. Ab initio solution of the many-\u200b electron Schr\u00f6dinger equation with deep neural  networks. Phys. Rev. Res. 2, 033429 (2020). 33. Pun, G. P., Batra, R., Ramprasad, R. & Mishin, Y.  Physically informed artificial neural networks for  atomistic modeling of materials. Nat. Commun. 10,  1\u201310 (2019). 34. Ling, J., Kurzawski, A. & Templeton, J. Reynolds  averaged turbulence modelling using deep neural  networks with embedded invariance. J. Fluid Mech.  807, 155\u2013166 (2016). 35. Jin, P., Zhang, Z., Zhu, A., Tang, Y. & Karniadakis, G. E. SympNets: intrinsic structure-\u200bpreserving symplectic  networks for identifying Hamiltonian systems.   Neural Netw. 132, 166\u2013179 (2020). 36. Lusch, B., Kutz, J. N. & Brunton, S. L. Deep learning  for universal linear embeddings of nonlinear dynamics Nat. Commun. 9, 4950 (2018). 37. Lagaris, I. E., Likas, A. & Fotiadis, D. I. Artificial neural networks for solving ordinary and partial differential  equations. IEEE Trans. Neural Netw. 9, 987\u20131000  (1998). 38. Sheng, H. & Yang, C. PFNN: A penalty-\u200bfree neural  network method for solving a class of second-\u200border  boundary-\u200bvalue problems on complex geometries.   J. Comput. Phys. 428, 110085 (2021).\npredicting the coupled flow and finite-\u200brate chemistry  behind a normal shock using neural-\u200bnetwork  approximation of operators. Preprint at arXiv   https://arxiv.org/abs/2011.03349 (2020). 62. Meng, X. & Karniadakis, G. E. A composite neural  network that learns from multi-\u200bfidelity data:  application to function approximation and inverse   PDE problems. J. Comput. Phys. 401, 109020  (2020). 63. Sirignano, J., MacArt, J. F. & Freund, J. B. DPM:   a deep learning PDE augmentation method with  application to large-\u200beddy simulation. J. Comput. Phys. 423, 109811 (2020). 64. Lu, L. et al. Extraction of mechanical properties of  materials through deep learning from instrumented  indentation. Proc. Natl Acad. Sci. USA 117,   7052\u20137062 (2020). 65. Reyes, B., Howard, A. A., Perdikaris, P. &   Tartakovsky, A. M. Learning unknown physics of   non-\u200bNewtonian fluids. Preprint at arXiv https://arxiv.org abs/2009.01658 (2020). 66. Wang, W. & G\u00f3mez-\u200bBombarelli, R. Coarse-\u200bgraining  auto-\u200bencoders for molecular dynamics. NPJ Comput.  Mater. 5, 1\u20139 (2019). 67. Rico-\u200bMartinez, R., Anderson, J. & Kevrekidis, I.  Continuous-\u200btime nonlinear signal processing: a neural network based approach for gray box identification  (IEEE, 1994). 68. Xu, K., Huang, D. Z. & Darve, E. Learning constitutive  relations using symmetric positive definite neural  networks. Preprint at arXiv https://arxiv.org/abs/  2004.00265 (2020). 69. Huang, D. Z., Xu, K., Farhat, C. & Darve, E. Predictive  modeling with learned constitutive laws from indirect  observations. Preprint at arXiv https://arxiv.org/abs/  1905.12530 (2019). 70. Xu, K., Tartakovsky, A. M., Burghardt, J. & Darve, E.  Inverse modeling of viscoelasticity materials using  physics constrained learning. Preprint at arXiv   https://arxiv.org/abs/2005.04384 (2020). 71. Li, D., Xu, K., Harris, J. M. & Darve, E. Coupled   time-\u200blapse full-\u200bwaveform inversion for subsurface flow  problems using intrusive automatic differentiation.  Water Resour. Res. 56, e2019WR027032 (2020). 72. Tartakovsky, A., Marrero, C. O., Perdikaris, P.,  Tartakovsky, G. & Barajas-\u200bSolano, D. Physics-\u200binformed deep neural networks for learning parameters and  constitutive relationships in subsurface flow problems. Water Resour. Res. 56, e2019WR026731 (2020). 73. Xu, K. & Darve, E. Adversarial numerical analysis for  inverse problems. Preprint at arXiv https://arxiv.org/ abs/1910.06936 (2019). 74. Yang, Y., Bhouri, M. A. & Perdikaris, P. Bayesian  differential programming for robust systems  identification under uncertainty. Proc. R. Soc. A 476,  20200290 (2020). 75. Rackauckas, C. et al. Universal differential equations  for scientific machine learning. Preprint at arXiv  https://arxiv.org/abs/2001.04385 (2020). 76. Wang, S., Yu, X. & Perdikaris, P. When and why PINNs fail to train: a neural tangent kernel perspective.  Preprint at arXiv https://arxiv.org/abs/2007.14527  (2020). 77. Wang, S., Wang, H. & Perdikaris, P. On the eigenvecto bias of Fourier feature networks: from regression to  solving multi-\u200bscale PDEs with physics-\u200binformed neural  networks. Preprint at arXiv https://arxiv.org/abs/  2012.10047 (2020). 78. Pang, G., Yang,",
    "paper_type": "survey",
    "attri": {
        "background": {
            "purpose": "This survey aims to address the integration of physics into machine learning, particularly in the context of solving complex multiphysics and multiscale problems where traditional numerical methods face significant challenges.",
            "scope": "The survey includes topics such as physics-informed neural networks (PINNs), the incorporation of observational data into machine learning models, and applications in various scientific fields. It excludes discussions on purely data-driven machine learning approaches that do not integrate physical laws."
        },
        "problem": {
            "definition": "The core issue explored is how to effectively incorporate physical laws into machine learning models to enhance their predictive capabilities for complex physical systems.",
            "key obstacle": "Primary challenges include the difficulty of incorporating noisy and incomplete data, the complexity of mesh generation, and the high dimensionality of problems governed by parameterized partial differential equations (PDEs)."
        },
        "architecture": {
            "perspective": "The survey introduces a novel framework of physics-informed learning, which categorizes existing research into methods that integrate physics with machine learning, focusing on how physical laws can guide the design of neural network architectures.",
            "fields/stages": "The survey organizes current methods into categories such as small data regimes where physics is known, regimes with some known physics and missing parameters, and big data regimes where physics is largely unknown."
        },
        "conclusion": {
            "comparisions": "The survey compares various physics-informed methods, highlighting their effectiveness in solving inverse and ill-posed problems compared to traditional numerical methods.",
            "results": "Key takeaways include the effectiveness of physics-informed learning in integrating data and physical models, leading to improved generalization and interpretability in machine learning applications."
        },
        "discussion": {
            "advantage": "Strengths of existing research include the ability to tackle complex problems with high-dimensional data and the capacity to leverage physical laws for improved model accuracy.",
            "limitation": "Limitations involve challenges in extending physics-informed methods to more complex systems and the need for better understanding of underlying physical invariances.",
            "gaps": "Gaps in current research include unanswered questions about the integration of diverse physical phenomena and the scalability of existing methods to larger, more complex systems.",
            "future work": "Future directions suggest focusing on digital twins, improving interpretability through data and model transformations, and automating the selection of suitable machine learning frameworks for specific physical problems."
        },
        "other info": {
            "additional_details": {
                "software_libraries": [
                    {
                        "name": "DeepXDE",
                        "usage": "Solver",
                        "language": "Python",
                        "backend": "TensorFlow"
                    },
                    {
                        "name": "SimNet",
                        "usage": "Solver",
                        "language": "Python",
                        "backend": "TensorFlow"
                    },
                    {
                        "name": "NeuralPDE",
                        "usage": "Solver",
                        "language": "Julia",
                        "backend": "Julia"
                    }
                ]
            }
        }
    },
    "mount_outline": [
        {
            "section number": "2. Background and Definitions",
            "key information": "The survey aims to address the integration of physics into machine learning, particularly in the context of solving complex multiphysics and multiscale problems."
        },
        {
            "section number": "2.1",
            "key information": "Defines the core issue as how to effectively incorporate physical laws into machine learning models to enhance their predictive capabilities for complex physical systems."
        },
        {
            "section number": "2.2",
            "key information": "Discusses the historical context of physics-informed neural networks (PINNs) and their evolution in the integration of observational data into machine learning models."
        },
        {
            "section number": "3. Algorithmic Bias",
            "key information": "Highlights the complexity of incorporating noisy and incomplete data as a key obstacle in the integration of physical laws into machine learning."
        },
        {
            "section number": "5. Model Interpretability",
            "key information": "Key takeaways include the effectiveness of physics-informed learning in integrating data and physical models, leading to improved generalization and interpretability in machine learning applications."
        },
        {
            "section number": "8. Future Directions",
            "key information": "Future directions suggest focusing on digital twins, improving interpretability through data and model transformations, and automating the selection of suitable machine learning frameworks for specific physical problems."
        }
    ],
    "similarity_score": 0.5373968756565461,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d5ce/d5ce0f88-42cc-4bdd-b6de-6a58fba76419.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5069/5069a231-f37c-4930-a578-b57d876b6501.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5727/57274aad-71f6-4e93-93ac-29f7cd30a30f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0cfc/0cfc4ab5-1101-49b9-a2c4-7829b588aea0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b253/b253eb43-9b3b-4d49-94d9-fa9058629c9f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/875f/875f4214-19d6-465b-b9c8-a3a29fbc873d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e07/5e072e4c-b54b-444f-8f22-cbb908de2baf.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a870/a870cac0-089c-43a4-9bce-43ace4580ee0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d75/1d75924f-84a3-4c7d-af88-1fdf1f388480.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3193/31935d4e-1f3c-4f79-8ac7-108d9f157802.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a38f/a38f57e5-7a67-4de1-a0ff-0a337cb0d2eb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/462c/462c4be6-162b-4cfc-9ae0-058c2e8ebea0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/89b6/89b6215d-7a13-452b-b13e-73589e37eceb.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/17c6/17c653d3-644c-44cc-88f6-b274a7de5598.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Physics-informed machine learning.json"
}