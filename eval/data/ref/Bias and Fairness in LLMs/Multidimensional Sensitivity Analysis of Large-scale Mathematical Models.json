{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1701.05494",
    "title": "Multidimensional Sensitivity Analysis of Large-scale Mathematical Models",
    "abstract": "Sensitivity analysis (SA) is a procedure for studying how sensitive are the output results of large-scale mathematical models to some uncertainties of the input data. The models are described as a system of partial differential equations. Often such systems contain a large number of input parameters. Obviously, it is important to know how sensitive is the solution to some uncontrolled variations or uncertainties in the input parameters of the model. Algorithms based on analysis of variances technique (ANOVA) for calculating numerical indicators of sensitivity and computationally efficient Monte Carlo integration techniques have recently been developed by the authors. They have been successfully applied to sensitivity studies of air pollution levels calculated by the Unified Danish Eulerian Model (UNI-DEM) with respect to several important input parameters. In this paper a comprehensive theoretical and experimental study of the Monte Carlo algorithm based on \\textit{symmetrised shaking} of Sobol sequences has been done. It has been proven that this algorithm has an optimal rate of convergence for functions with continuous and bounded second derivatives in terms of probability and mean square error. Extensive numerical experiments with Monte Carlo, quasi-Monte Carlo (QMC) and scrambled quasi-Monte Carlo algorithms based on Sobol sequences are performed to support the theoretical studies and to analyze applicability of the algorithms to various classes of problems. The numerical tests show that the Monte Carlo algorithm based on \\textit{symmetrised shaking} of Sobol sequences gives reliable results for multidimensional integration problems under consideration.",
    "bib_name": "dimov2017multidimensionalsensitivityanalysislargescale",
    "md_text": "# Multidimensional Sensitivity Analysis of Large-scale Mathematical Models\nIvan Dimov and Rayna Georgieva\n19 Jan 2017\nAbstract Sensitivity analysis (SA) is a procedure for studying how sensitive are the output results of large-scale mathematical models to some uncertainties of the input data. The models are described as a system of partial differential equations. Often such systems contain a large number of input parameters. Obviously, it is important to know how sensitive is the solution to some uncontrolled variations or uncertainties in the input parameters of the model. Algorithms based on analysis of variances technique (ANOVA) for calculating numerical indicators of sensitivity and computationally efficient Monte Carlo integration techniques have recently been developed by the authors. They have been successfully applied to sensitivity studies of air pollution levels calculated by the Unified Danish Eulerian Model (UNI-DEM) with respect to several important input parameters. In this paper a comprehensive theoretical and experimental study of the Monte Carlo algorithm based on symmetrised shaking of Sobol sequences has been done. It has been proven that this algorithm has an optimal rate of convergence for functions with continuous and bounded second derivatives in terms of probability and mean square error. Extensive numerical experiments with Monte Carlo, quasi-Monte Carlo (QMC) and scrambled quasiMonte Carlo algorithms based on Sobol sequences are performed to support the theoretical studies and to analyze applicability of the algorithms to various classes of problems. The numerical tests show that the Monte Carlo algorithm based on symmetrised shaking of Sobol sequences gives reliable results for multidimensional integration problems under consideration.\nIvan Dimov and Rayna Georgieva Department of Parallel Algorithms, IICT, Bulgarian Academy of Sciences, Acad. G. Bonchev 25 A, 1113 Sofia, Bulgaria, e-mail: ivdimov@bas.bg,rayna@parallel.bas.bg\nIvan Dimov and Rayna Georgieva Department of Parallel Algorithms, IICT, Bulgarian Academy of Sciences, Acad. G. Bonchev 25 A, 1113 Sofia, Bulgaria, e-mail: ivdimov@bas.bg,rayna@parallel.bas.bg\n# 1 Introduction\nMost existing methods for providing SA rely on special assumptions connected to the behavior of the model (such as linearity, monotonicity and additivity of the relationship between model input and model output) [22]. Such assumptions are often applicable to a large range of mathematical models. At the same time there are models that include significant nonlinearities and/or stiffness. For such models assumptions about linearity and additivity are not applicable. This is especially true when one deals with non-linear systems of partial differential equations. The numerical study and results reported in this paper have been done by using a large-scale mathematical model called Unified Danish Eulerian Model (UNI-DEM) [33, 34]. The model enables us to study the transport of air pollutants and other species over a large geographical region. The system of partial differential equations describes the main physical processes, such as advection, diffusion, deposition, as well as chemical and photochemical processes between the studied species. The emissions, and the quickly changing meteorological conditions are also described. The nonlinearity of the equations are mainly introduced when modeling chemical reactions [33]. If the model results are sensitive to a given process, one can describe it mathematically in a more adequate way, or more precisely. Thus, the goal of our study is to increase the reliability of the results produced by the model, and to identify processes that must be studied more carefully, as well as to find input parameters that need to be measured with a higher precision. A careful sensitivity analysis is needed in order to decide where and how simplifications of the model can be made. That\u2019s why it is important to develop and study more adequate and reliable methods for sensitivity analysis. A good candidate for reliable sensitivity analysis of models containing nonlinearity is the variance based method [22]. The idea of this approach is to estimate how the variation of an input parameter or a group of inputs contributes into the variance of the model output. As a measure of this analysis we use the total sensitivity indices (TSI) (see, Section 2) described as multidimensional integrals:\nwhere g(x) is a square integrable function in \u2126and p(x) \u22650 is a probability density function, such that \ufffd \u2126p(x)dx = 1. That\u2019s why it is important to deal with efficient numerical methods for highdimensional integration. The progress in the area of sensitivity analysis is closely connected to the progress in reliable algorithms for multidimensional integration.\n(1)\n# 2 Problem Setting\n# 2.1 Modeling and Sensitivity\nAssume that the mathematical model can be presented as a function\nu = f(x), where x = (x1,x2,...,xd) \u2208Ud \u2261[0;1]d\nis the vector of input parameters with a joint probability density function (p.d.f.) p(x) = p(x1,...,xd). Assume also that the input variables are independent (noncorrelated) and the density function p(x) is known, even if xi are not actually random variables (r.v.). The total sensitivity index [10] provides a measure of the total effect of a given parameter, including all the possible coupling terms between that parameter and all the others. The total sensitivity index (TSI) of an input parameter xi,i\u2208{1,...,d} is defined in the following way [10, 26]:\nwhere Si is called the main effect (first-order sensitivity index) of xi and Sil1...lj\u22121 is the j-th order sensitivity index. The higher-order terms describe the interaction effects between the unknown input parameters xi1,...,xi\u03bd ,\u03bd \u2208{2,...,d} on the output variance. The method of global SA used in this work is based on a decomposition of an integrable model function f in the d-dimensional factor space into terms of increasing dimensionality [26]:\nwhere f0 is a constant. The representation (4) is referred to as the ANOVArepresentation of the model function f(x) if each term is chosen to satisfy the following condition [26]:\n\ufffd1 0 fl1...l\u03bd(xl1,xl2,...,xl\u03bd)dxlk = 0, 1 \u2264k \u2264\u03bd, \u03bd = 1,...,d.\nLet us mention the fact that if the whole presentation (4) of the right-hand site is used, then it doesn\u2019t simplify the problem. The hope is that a truncated sequence f0 +\u2211dtr \u03bd=1 \u2211l1<...<l\u03bd fl1...l\u03bd (xl1,xl2,...,xl\u03bd ), where dtr < d (or even dtr << d), can be considered as a good approximation to the model function f. The quantities\nD = \ufffd Ud f 2(x)dx \u2212f 2 0, Dl1 ... l\u03bd = \ufffd f 2 l1 ... l\u03bddxl1 ...dxl\u03bd\nD = \ufffd Ud f 2(x)dx \u2212f 2 0, Dl1 ... l\u03bd = \ufffd f 2 l1 ... l\u03bddxl1 ...dxl\u03bd\n(2)\n(3)\n(4)\n(5)\nare the so-called total and partial variances respectively and are obtained after squaring and integrating over Ud the equality (4) on the assumption that f(x) is a square integrable function (thus all terms in (4) are also square integrable functions). Therefore, the total variance of the model output is split into partial variances in the analogous way as the model function, that is the unique ANOVA-decomposition: D = \u2211d \u03bd=1 \u2211l1<...<l\u03bd Dl1...l\u03bd . The use of probability theory concepts is based on the assumption that the input parameters are random variables distributed in Ud that defines fl1 ... l\u03bd(xl1,xl2,...,xl\u03bd ) also as random variables with variances (5). For example fl1 is presented by a conditional expectation: fl1(xl1) = E(u|xl1) \u2212f0 and respectively Dl1 = D[fl1(xl1)] = D[E(u|xl1)]. Based on these assumptions about the model function and the output variance, the following quantities\nare referred to as the global sensitivity indices [26]. Based on the formulas (5)-(6) it is clear that the mathematical treatment of the problem of providing global sensitivity analysis consists in evaluating total sensitivity indices (3) of corresponding order that, in turn, leads to computing multidimensional integrals of the form (1). It means that to obtain Stot i in general, one needs to compute 2d integrals of type (5). As we discussed earlier the basic assumption underlying representation (4) is that the basic features of the model functions (2) describing typical real-life problems can be presented by low-order subsets of input variables, containing terms of the order up to dtr, where dtr < d (or even dtr << d). Therefore, based on this assumption, one can assume that the dimension of the initial problem can be reduced. The procedure for computing global sensitivity indices (see [26]) is based on the following representation of the variance\nwhere y = (xk1,...,xkm), 1 \u2264k1 < ... < km \u2264d, is an arbitrary set of m variables (1 \u2264m \u2264d \u22121) and z is the set of d \u2212m complementary variables, i.e. x = (y,z). The equality (7) enables the construction of a Monte Carlo algorithm for evaluating f0,D and Dy:\nwhere \u03be = (\u03b7,\u03b6) is a random sample and \u03b7 corresponds to the input subset denoted by y. Instead of randomized (Monte Carlo) algorithms for computing the above sensitivity parameters one can use deterministic quasi-Monte Carlo algorithms, or randomized quasi-Monte Carlo [13, 14]. Randomized (Monte Carlo) algorithms have\nwhere \u03be = (\u03b7,\u03b6) is a random sample and \u03b7 corresponds to the input subset denoted by y.\n(6)\n(7)\nproven to be very efficient in solving multidimensional integrals in composite domains [3, 23]. At the same time the QMC based on well-distributed Sobol sequences can be considered as a good alternative to Monte Carlo algorithms, especially for smooth integrands and not very high effective dimensions (up to d = 15) [12]. Sobol \u039b\u03a0\u03c4 are good candidates for efficient QMC algorithms. Algorithms based on \u039b\u03a0\u03c4 sequences while being deterministic, mimic the pseudo-random sequences used in Monte Carlo integration. One of the problems with \u039b\u03a0\u03c4 sequences is that they may have bad two-dimensional projection. In this context bad means that the distribution of the points is far away from the uniformity. If such projections are used in a certain computational problem, then the lack of uniformity may provoke a substantial lost of accuracy. To overcome this problem randomized QMC can be used. There are several ways of randomization and scrambling is one of them. The original motivation of scrambling [11, 19] aims toward obtaining more uniformity for quasirandom sequences in high dimensions, which can be checked via two-dimensional projections. Another way of randomisation is to shake the quasi-random points according to some procedure. Actually, the scrambled algorithms obtained by shaking the quasi-random points can be considered as Monte Carlo algorithms with a special choice of the density function. It\u2019s a matter of definition. Thus, there is a reason to be able to compare two classes of algorithms: deterministic and randomized.\n# 3 Complexity in Classes of Algorithms\nOne may pose the task to consider and compare two classes of algorithms: deterministic algorithms and randomized (Monte Carlo) algorithms. Let I be the desired value of the integral. Assume for a given r.v. \u03b8 one can prove that the mathematical expectation satisfies E\u03b8 = I. Suppose that the mean value of n values of \u03b8: \u03b8 (i), i = 1,...,n is considered as a Monte Carlo approximation to the solution: \u00af\u03b8n = 1/n\u2211n i=1 \u03b8 (i) \u2248I, where \u03b8 (i)(i = 1,2,...,n) correspond to values (realizations) of a r.v. \u03b8. In general, a certain randomized algorithm can produce the result with a given probability error. So, dealing with randomized algorithms one has to accept that the result of the computation can be true only with a certain (although high) probability. In most practical computations it is reasonable to accept an error estimate with a probability smaller than 1. Consider the following integration problem:\nwhere x \u2261(x1,...,xd) \u2208Ud \u2282Rd and f \u2208C(Ud) is an integrable function on Ud. The computational problem can be considered as a mapping of function f : {[0,1]d \u2192R} to R: S(f) : f \u2192R, where S(f) = \ufffd Ud f(x)dx and f \u2208F0 \u2282C(Ud). We refer to S as the solution operator. The elements of F0 are the data, for which the problem has to be solved; and for f \u2208F0, S(f) is the exact solution. For a given f, we want to compute exactly or approximately S(f). One may be interested in cases\n(8)\nwhen the integrand f has a higher regularity. It is because in many cases of practical computations f is smooth and has high order bounded derivatives. If this is the case, then is it reasonable to try to exploit such a smoothness. To be able to do that we need to define the functional class F0 \u2261Wk(\u2225f\u2225;Ud) in the following way: Definition 3.1 Let d and k be integers, d,k \u22651. We consider the class Wk(\u2225f\u2225;Ud) (sometimes abbreviated to Wk) of real functions f defined over the unit cube Ud = [0,1)d, possessing all the partial derivatives \u2202r f(x) \u2202x\u03b11 1 ...\u2202x\u03b1d d , \u03b11+...+\u03b1d = r \u2264k, which are continuous when r < k and bounded in sup norm when r = k. The seminorm \u2225\u00b7\u2225on Wk is defined as\nwhen the integrand f has a higher regularity. It is because in many cases of practical computations f is smooth and has high order bounded derivatives. If this is the case, then is it reasonable to try to exploit such a smoothness. To be able to do that we need to define the functional class F0 \u2261Wk(\u2225f\u2225;Ud) in the following way:\nDefinition 3.1 Let d and k be integers, d,k \u22651. We consider the class Wk(\u2225f\u2225;Ud) (sometimes abbreviated to Wk) of real functions f defined over the unit cube Ud = [0,1)d, possessing all the partial derivatives \u2202r f(x) \u2202x\u03b11 1 ...\u2202x\u03b1d d , \u03b11+...+\u03b1d = r \u2264k, which are continuous when r < k and bounded in sup norm when r = k. The seminorm \u2225\u00b7\u2225on Wk is defined as\n\u2225f\u2225= sup \ufffd\ufffd\ufffd\ufffd\ufffd\ufffd \u2202k f(x) \u2202x\u03b11 1 ...\u2202x\u03b1d d \ufffd\ufffd\ufffd\ufffd\ufffd, \u03b11 + ...+ \u03b1d = k, x \u2261(x1,...,xd) \u2208Ud \ufffd .\n\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd We call a quadrature formula any expression of the form\nwhich approximates the value of the integral S(f). The real numbers ci \u2208R are called weights and the d dimensional points x(i) \u2208Ud are called nodes. It is clear that for fixed weights ci and nodes x(i) \u2261(xi,1,...,xi,d) the quadrature formula AD(f,n) may be used to define an algorithm with an integration error err(f,AD) \u2261 \ufffd Ud f(x)dx \u2212AD(f,n). We call a randomized quadrature formula any formula of the following kind: AR(f,n) = \u2211n i=1 \u03c3i f(\u03be (i)), where \u03c3i and \u03be (i) are random weights and nodes respectively. The algorithm AR(f,n) belongs to the class of randomized (Monte Carlo) denoted by A R.\nDefinition 3.2 Given a randomized (Monte Carlo) integration formula for the fun tions from the space Wk we define the integration error\n# Definition 3.2 Given a randomized (Monte Carlo) integration formula for the functions from the space Wk we define the integration error\nerr(f,AR) \u2261 \ufffd Ud f(x)dx \u2212AR(f,n)\nby the probability error \u03b5P(f) in the sense that \u03b5P(f) is the least possible real number, such that Pr \ufffd\ufffd\ufffderr(f,AR) \ufffd\ufffd< \u03b5P(f) \ufffd \u2265P,\n# by the probability error \u03b5P(f) in the sense that \u03b5P(f) is the least possible real number, such that Pr \ufffd\ufffd\ufffderr(f,AR) \ufffd\ufffd< \u03b5P(f) \ufffd \u2265P,\nand the mean square error\nr(f) = \ufffd E \ufffd err2(f,AR) \ufffd\ufffd1/2 .\n\ufffd \ufffd \ufffd\ufffd We assume that it suffices to obtain an \u03b5P(f)-approximation to the solution with a probability 0 < P < 1. If we allow equality, i.e., 0 < P \u22641 in Definition 3.2, then \u03b5P(f) can be used as an accuracy measure for both randomized and deterministic algorithms. In such a way it is consistent to consider a wider class A of algorithms that contains both classes: randomized and deterministic algorithms.\nDefinition 3.3 Consider the set A of algorithms A: A = {A : Pr(|err(f,A)| \u2264\u03b5) \u2265c}, A \u2208{AD,AR}, 0 < c < 1 that solve a given problem with an integration error err(f,A). In such a setting it is correct to compare randomized algorithms with algorithms based on low discrepancy sequences like Sobol \u039b\u03a0\u03c4 sequences.\n# 4 The Algorithms\nThe algorithms we study are based on Sobol \u039b\u03a0\u03c4 sequences.\n4.1 \u039b\u03a0\u03c4 Sobol Sequences\n# 4.1 \u039b\u03a0\u03c4 Sobol Sequences\n\u039b\u03a0\u03c4 sequences are uniformly distributed sequences (u.d.s.) The term u.d.s. was introduced by Hermann Weyl in 1916 [30]. For practical purposes an u.d.s. must be found that satisfied three requirements [23, 25]: (i) the best asymptote as n \u2192 \u221e, (ii) well distributed points for small n, and (iii) a computationally inexpensive algorithm. All \u039b\u03a0\u03c4-sequences given in [25] satisfy the first requirement. Suitable distributions such as \u039b\u03a0\u03c4 sequences are also called (t,m,s)-nets and (t,s)-sequences in base b \u22652. To introduce them, define first an elementary s-interval in base b as a subset of Us of the form E = \u220fs j=1 \ufffdaj bdj , aj+1 bdj \ufffd , where a j,d j \u22650 are integers and a j < bdj for all j \u2208{1,...,s}. Given two integers 0 \u2264t \u2264m, a (t,m,s)-net in base b is a sequence x(i) of bm points of Us such that Card E \u2229{x(1),...,x(bm)} = bt for any elementary interval E in base b of hypervolume \u03bb(E) = bt\u2212m. Given a non-negative integer t, a (t,s)-sequence in base b is an infinite sequence of points x(i) such that for all integers k \u22650,m \u2265t, the sequence {x(kbm),...,x((k+1)bm\u22121)} is a (t,m,s)-net in base b. I. M. Sobol [23] defines his \u03a0\u03c4-meshes and \u039b\u03a0\u03c4 sequences, which are (t,m,s)nets and (t,s)-sequences in base 2 respectively. The terms (t,m,s)-nets and (t,s)sequences in base b (also called Niederreiter sequences) were introduced in 1988 by H. Niederreiter [18]. To generate the j-th component of the points in a Sobol sequence, we need to choose a primitive polynomial of some degree sj over the Galois field of two elements GF(2) Pj = xsj + a1,jxsj\u22121 + a2,jxsj\u22122 + ... + asj\u22121,jx + 1, where the coefficients a1,j,...,asj\u22121,j are either 0 or 1. A sequence of positive integers {m1,j,m2,j,...} are defined by the recurrence relation\nwhere \u2295is the bit-by-bit exclusive-or operator. The values m1,j,...,msj,j can be chosen freely provided that each mk,j,1 \u2264k \u2264sj, is odd and less than 2k. Therefore, it is possible to construct different Sobol sequences for the fixed dimension s. In practice, these numbers must be chosen very carefully to obtain really efficient Sobol sequence generators [27]. The so-called direction numbers {v1,j,v2,j,...} are defined by vk,j = mk,j 2k . Then the j-th component of the i-th point in a Sobol sequence, is given by xi,j = i1v1,j \u2295i2v2,j \u2295..., where ik is the k-th binary digit of i = (...i3i2i1)2. Subroutines to compute these points can be found in [2, 24]. The work [15] contains more details.\n# 4.2 The Monte Carlo Algorithms based on Modified Sobol Sequences - MCA-MSS\nOne of the algorithms based on a procedure of shaking was proposed recently in [5]. The idea is that we take a Sobol \u039b\u03a0\u03c4 point (vector) x of dimension d. Then x is considered as a centrum of a sphere with a radius \u03c1. A random point \u03be \u2208Ud uniformly distributed on the sphere is taken. Consider a random variable \u03b8 defined as a value of the integrand at that random point, i.e., \u03b8 = f(\u03be). Consider random points \u03be (i)(\u03c1) \u2208Ud,i = 1,...,n. Assume \u03be (i)(\u03c1) = x(i) + \u03c1\u03c9(i), where \u03c9(i) is a unique uniformly distributed vector in Ud. The radius \u03c1 is relatively small \u03c1 << 1 2dj , such that \u03be (i)(\u03c1) is still in the same elementary ith interval Ed i = \u220fd j=1 \ufffd a(i) j 2dj , a(i) j +1 2dj \ufffd , where the pattern \u039b\u03a0\u03c4 point x(i) is. We use a subscript i in Ed i to indicate that the i-th \u039b\u03a0\u03c4 point x(i) is in it. So, we assume that if x(i) \u2208Ed i , then \u03be (i)(\u03c1) \u2208Ed i too. It was proven in [5] that the mathematical expectation of the random variable \u03b8 = f(\u03be) is equal to the value of the integral (8), that is E\u03b8 = S(f) = \ufffd Ud f(x)dx. This result allows for defining a randomized algorithm. One can take the Sobol \u039b\u03a0\u03c4 point x(i) and shake it somewhat. Shaking means to define random points \u03be (i)(\u03c1) = x(i)+\u03c1\u03c9(i) according to the procedure described above. For simplicity the algorithm described above is called MCA-MSS-1. The probability error of the algorithm MCA-MSS-1 was analysed in [6]. It was proved that for integrands with continuous and bounded first derivatives, i.e. f \u2208 F0 \u2261W1(L;Ud), where L = \u2225f\u2225, it holds\nwhere the constants c \u2032 d and c \u2032\u2032 d do not depend on n. In this work a modification of algorithm MCA-MSS-1 is proposed and analysed. The new algorithm will be called MCA-MSS-2. It is assumed that n = md, m \u22651. The unit cubeUd is divided into md disjoint subdomains, such that they coincide with the elementary d-dimensional subintervals\ndefined in Subsection 4.1 Ud = \ufffdmd j=1Kj, where Kj = \u220fd i=1[a(j) i ,b(j) i ), with b(j) i \u2212 a(j) i = 1 m for all i = 1,...,d. In such a way in each d-dimensional sub-domain Kj there is exactly one \u039b\u03a0\u03c4 point x(j). Assuming that after shaking, the random point stays inside Kj, i.e., \u03be (j)(\u03c1) = x(j) + \u03c1\u03c9(j) \u2208Kj one may try to exploit the smoothness of the integrand in case if the integrand has second continuators and bounded derivatives, i.e., f \u2208F0 \u2261W2(L;Ud). Then, if p(x) is a probability density function, such that \ufffd Ud p(x)dx = 1, then\nwhere c(j) 1 are constants. If d j is the diameter of Kj, then\nwhere c(j) 2 are another constants. In the particular case when the subintervals are with edge 1/m for all constants we have: c(j) 1 = 1 and c(j) 2 = \u221a d. In each sub-domain Kj the central point is denoted by s(j), where s(j) = (s(j) 1 ,s(j) 2 ,...,s(j) d ). Suppose two random points \u03be (j) and \u03be (j)\u2032 are chosen, such that \u03be (j) is selected during our procedure used in MCA-MSS-1. The second point \u03be (j)\u2032 is chosen to be symmetric to \u03be (j) according to the central point s(j) in each cube Kj. In such away the number of random points is 2md. One may calculate all function values f(\u03be (j)) and f(\u03be (j)\u2032), for j = 1,...,md and approximate the value of the integral in the following way:\nThis estimate corresponds to MCA-MSS-2. Later on it will be proven that this algorithm has an optimal rate of convergence for functions with second bounded derivatives, i.e., for functions f \u2208F0 \u2261W2(L;Ud), while the algorithm MCA-MSS1 has an optimal rate of convergence for functions with first bounded derivatives: f \u2208F0 \u2261W1(L;Ud). One can prove the following\nTheorem 1. The quadrature formula (9) constructed above for integrands f fro W2(L;Ud) satisfies\nand\n(9)\n\ufffd   where the constants \ufffdc \u2032 d and \ufffdc \u2032\u2032 d do not depend on n.\n \ufffd Proof. One can see that\nFor the fixed \u039b\u03a0\u03c4 point x(j) \u2208Kj one can use the d-dimensional Taylor formula to present the function f(x(j)) in Kj around the central point s(j). For simplicity the superscript of the argument (j) will be omitted assuming that the formulas are written for the jth cube Kj:\nNow, one can write this formula at previously defined random points \u03be and \u03be \u2032 both belonging to Kj. In such a way we have:\nwhere Df(s) is the gradient of f evaluated at x = s and D2 f(s) is the Hessian matrix e.,\nwhere Df(s) is the gradient of f evaluated at x = s and D2 f(s) is the Hessian matr i.e., \uf8ee 2  2  2  \uf8f9\n\uf8f0 Summarising (10) and (11) one can get\n\ufffd \ufffd Because of the symmetry there is no member depending on the gradient Df(s) in the previous formula. If we consider the variance D[f(\u03be) + f(\u03be \u2032)] taking into account that the variance of the constant 2 f(s) is zero, then we will get\nIvan Dimov and Rayna Georgieva\n(10)\n(11)\n\ufffd \ufffd \ufffd \ufffd Now we return back to the notation with superscript taking into account that the above consideration is just for an arbitrary sub-domain Kj. Since f \u2208W2(L;Ud), \u2225f\u2225\u2264Lj and L = \u2225f\u2225is the majorant for all Lj, i.e., Lj \u2264L for j = 1,...,n. Obviously, there is a point (d-dimensional vector) \u03b7 \u2208Kj, such that\n\ufffd and the variance can be estimated from above in the following way:\nD[f(\u03be)+ f(\u03be \u2032)] \u2264L2 j sup x(j) 1 ,x(j) 2 \ufffd\ufffd\ufffdx(j) 1 \u2212x(j) 2 \ufffd\ufffd\ufffd 4 \u2264L2 j(c(j) 2 )4n\u22124/d.\n\ufffd \ufffd Now the variance of \u03b8n = \u2211n j=1\u03b8 (j) can be estimated:\n\ufffd \ufffd Therefore r(f,d) \u2264\ufffdc \u2032\u2032 d \u2225f\u2225n \u22121 2 \u22122 d . The application of the Tchebychev\u2019s inequality to the variance (12) yields\n\ufffd   for the probable error \u03b5, where \ufffdc \u2032 d = \u221a 2d, which concludes the proof.\n \ufffd One can see that the Monte Carlo algorithm MCA-MSS-2 has an optimal rate of convergence for functions with continuous and bounded second derivative [3]. This means that the rate of convergence (n\u22121 2 \u22122 d ) can not be improved for the functional class W2 in the class of the randomized algorithms A R. Note that both MCA-MSS-1 and MCA-MSS-2 have one control parameter, that is the radius \u03c1 of the sphere of shaking. At the same time, to be able to efficiently use this control parameter one should increase the computational complexity. The problem is that after shaking the random point may leave the multidimensional subdomain. That\u2019s why after each such a procedure one should be checking if the random point is still in the same sub-domain. It is clear that the procedure of checking if a random point is inside the given domain is a computationally expensive procedure when one has a large number of points. A small modification of MCA-MSS-2 algorithm allows to overcome this difficulty. If we just generate a random point \u03be (j) \u2208Kj uniformly distributed inside Kj and after that take the symmetric point \u03be (j)\u2032 according to the central point s(j), then this procedure will simulate the algo-\n(12)\nrithm MCA-MSS-2. Such a completely randomized approach simulates algorithm MCA-MSS-2, but the shaking is with different radiuses \u03c1 in each sub-domain. We will call this algorithm MCA-MSS-2-S, because this approach looks like the stratified symmetrised Monte Carlo. Obviously, MCA-MSS-2-S is less expensive than MCA-MSS-2, but there is not such a control parameter like the radius \u03c1, which can be considered as a parameter randomly chosen in each sub-domain Kj. It is important to notice that all three algorithms MCA-MSS-1, MCA-MSS-2 and MCA-MSS-2-S have optimal (unimprovable) rate of convergence for the corresponding functional classes, that is MCA-MSS-1 is optimal in F0 \u2261W1(L;Ud) and both MCA-MSS-2 and MCA-MSS-2-S are optimal in F0 \u2261W2(L;Ud). We also will be considering the known Owen Nested Scrambling Algorithm [19] for which it is proved that the rate of convergence is n\u22123/2(log n)(d\u22121)/2, which is very good but still not optimal even for integrands in F0 \u2261W1(L;Ud). One can see that if the logarithmic function from the estimate can be omitted, then the rate will became optimal. Let us mention that it is still not proven that the above estimate is exact, that is, we do not know if the logarithm can be omitted. It should be mentioned that the proved convergence rate for the Owen Nested Scrambling Algorithm improves significantly the rate for the unscrambled nets, which is n\u22121(log n)d\u22121. That\u2019s why it is important to compare numerically our algorithms MCA-MSS with the Owen Nested Scrambling. The idea of Owen nested scrambling is based on randomization of a single digit at each iteration. Let x(i) = (xi,1,xi,2,...,xi,s), i = 1,...,n be quasi-random numbers in [0,1)s, and let z(i) = (zi,1,zi,2,...,zi,s) be the scrambled version of the point x(i). Suppose that each xi,j can be represented in base b as xi,j = (0.xi1,j xi2,j ...xiK,j ...)b with K being the number of digits to be scrambled. Then nested scrambling proposed by Owen [19, 20] can be defined as follows: zi1,j = \u03c0\u2022(xi1,j), and zil,j = \u03c0\u2022xi1,jxi2,j...xil\u22121,j(xil,j), with independent permutations \u03c0\u2022xi1,jxi2,j...xil\u22121,j for l \u22652. Of course, (t,m,s)-net remains (t,m,s)-net under nested scrambling. However, nested scrambling requires bl\u22121 permutations to scramble the l-th digit. Owen scrambling (nested scrambling), which can be applied to all (t,s)-sequences, is powerful; however, from the implementation point-of-view, nested scrambling or so-called path dependent permutations requires a considerable amount of bookkeeping, and leads to more problematic implementation. There are various versions of scrambling methods based on digital permutation, and the differences among those methods are based on the definitions of the \u03c0l\u2019s. These include Owen nested scrambling [19, 20], Tezuka\u2019s generalized Faure sequences [29], and Matousek\u2019s linear scrambling [17].\n# 5 Case-study: Variance-based Sensitivity Analysis of the Unified Danish Eulerian Model\nThe input data for the sensitivity analysis performed in this paper has been obtained during runs of a large-scale mathematical model for remote transport of air pollu-\ntants (Unified Danish Eulerian Model, UNI-DEM, [33]). The model enables us to study concentration variations in time of a high number of air pollutants and other species over a large geographical region (4800 \u00d7 4800 km), covering the whole of Europe, the Mediterranean and some parts of Asia and Africa. Such studies are important for environmental protection, agriculture, health care. The model presented as a system of partial differential equations describes the main processes in the atmosphere including photochemical processes between the studied species, the emissions, the quickly changing meteorological conditions. Both non-linearity and stiffness of the equations are mainly introduced when modeling chemical reactions [33]. The chemical scheme used in the model is the well-known condensed CBMIV (Carbon Bond Mechanism). Thus, the motivation to choose UNI-DEM is that it is one of the models of atmospheric chemistry, where the chemical processes are taken into account in a very accurate way. This large and complex task is not suitable for direct numerical treatment. For the purpose of numerical solution it is split into submodels, which represent the main physical and chemical processes. The sequential splitting [16] is used in the production version of the model, although other splitting methods have also been considered and implemented in some experimental versions [4, 7]. Spatial and time discretization makes each of the above submodels a huge computational task, challenging for the most powerful supercomputers available nowadays. That is why parallelization has always been a key point in the computer implementation of DEM since its very early stages. Our main aim here is to study the sensitivity of the ozone concentration according to the rate variation of some chemical reactions. We consider the chemical rates to be the input parameters and the concentrations of pollutants to be the output parameters.\n# 6 Numerical Results and Discussion\nSome numerical experiments are performed to study experimentally various properties of the algorithms. The expectations based on theoretical results are that for non-smooth functions MCA-MSS algorithms based on the shaking procedures outperform the QMC even for relatively low dimensions. It is also interesting to observe how behave the randomized QMC based on scrambled Sobol sequences. For our numerical tests we use the following non-smooth integrand:\nfor which even the first derivative does not exist. Such kind of applications appear also in some important problems in financial mathematics. The referent value of the integral S(f1) is approximately equal to 7.22261. To make a comparison we also consider an integral with a smooth integrand:\n(13)\n<div style=\"text-align: center;\">Table 1 Relative error and computational time for numerical integration of a smooth function (S( f2) \u22480.10897).</div>\n2 \u2248\nn\nSFMT\nSobol QMCA\nOwen scrambling\nMCA-MSS-1\nRel.\nTime\nRel.\nTime\nRel.\nTime\n\u03c1\nRel.\nTime\nerror\n(s)\nerror\n(s)\nerror\n(s)\n\u00d7103\nerror\n(s)\n102\n0.0562\n0.002\n0.0365\n< 0.001\n0.0280\n0.001\n3.9\n0.0363\n0.001\n13\n0.0036\n0.001\n103\n0.0244\n0.004\n0.0023\n0.001\n0.0016\n0.001\n1.9\n0.0038\n0.010\n6.4\n0.0019\n0.010\n104\n0.0097\n0.019\n0.0009\n0.002\n0.0003\n0.003\n0.8\n0.0007\n0.070\n2.8\n0.0006\n0.065\nThe second integrand (14) is an infinitely smooth function with a referent value of the integral S(f2) approximately equal to 0.10897. The integration domain in both cases is U4 = [0,1]4. Some results from the numerical integration tests with a smooth (14) and a nonsmooth (13) integrand are presented in Tables 1 and 2 respectively. As a measure of the efficiency of the algorithms both the relative error (defined as the absolute error divided by the referent value) and computational time are shown. For generating Sobol quasi-random sequences the algorithm with Gray code implementation [1] and sets of direction numbers proposed by Joe and Kuo [9] are used. The MCAMSS-1 algorithm [5] involves generating random points uniformly distributed on a sphere with radius \u03c1. One of the best available random number generators, SIMDoriented Fast Mersenne Twister (SFMT) [21, 32] 128-bit pseudo-random number generator of period 219937\u22121 has been used to generate the required random points. SFMT algorithm is a very efficient implementation of the Plain Monte Carlo method [23]. The radius \u03c1 depends on the integration domain, number of samples and minimal distance between Sobol deterministic points \u03b4. We observed experimentally that the behaviour of the relative error of numerical integration is significantly influenced by the fixed radius of spheres. That is why the values of the radius \u03c1 are presented according to the number of samples n used in our experiments, as well as to a fixed coefficient, radius coefficient \u03ba = \u03c1/\u03b4. The latter parameter gives the ratio of the radius to the minimal distance between Sobol points. The code of scrambled quasi-random sequences used in our studies is taken from the collection of NAG C Library [31]. This implementation of scrambled quasi-random sequences is based on TOMS Algorithm 823 [11]. In the implementation of the scrambling there is a possibility to make a choice of three methods of scrambling: the first is a restricted form of Owen scrambling [19], the second based on the method of Faure and Tezuka [8], and the last method combines the first two (it is referred to as a combined approach). Random points for the MCA-MSS-1 algorithm have been generated using the original Sobol sequences and modeling a random direction in d-dimensional space. The computational time of the calculations with pseudo-random numbers generated\n(14)\n<div style=\"text-align: center;\">Table 2 Relative error and computational time for numerical integration of a non-smooth function (S( f1) \u22487.22261).</div>\n(1 \u2248722261).\nn\nSFMT\nSobol QMCA\nOwen scrambling\nMCA-MSS-1\nRel.\nTime\nRel.\nTime\nRel.\nTime\n\u03c1\nRel.\nTime\nerror\n(s)\nerror\n(s)\nerror\n(s)\n\u00d7103\nerror\n(s)\n103\n0.0010\n0.011\n0.0027\n0.001\n0.0021\n0.002\n1.9\n0.0024\n0.020\n6.4\n0.0004\n0.025\n7.103 0.0009\n0.072\n0.0013\n0.009\n0.0003\n0.011\n1.0\n0.0004\n0.110\n3.4\n0.0005\n0.114\n3.104 0.0005\n0.304\n0.0003\n0.032\n0.0003\n0.041\n0.6\n0.0001\n0.440\n1.9\n0.0002\n0.480\n5.104 0.0007\n0.513\n0.0002\n0.053\n2e-05\n0.066\n0.4\n7e-05\n0.775\n1.4\n0.0001\n0.788\nby SFMT (see columns labeled as SFMT and MCA-MSS in Tables 1 and 2 has been estimated for all 10 algorithm runs. Comparing the results in Tables 1 and 2 one observes that \u2022 all algorithms under consideration are efficient and converge with the expected rate of convergence; \u2022 in the case of smooth functions, the Sobol algorithm is better than SFMT (the relative error is up to 10 times smaller than for SFMT); \u2022 the scrambled QMC and MCA-MSS-1 are much better than the classical Sobol algorithm; in many cases even the simplest shaking algorithm MCA-MSS-1 gives a higher accuracy than the scrambled algorithm. \u2022 in case of non-smooth functions SFMT algorithm implementing the plain Monte Carlo method is better than the Sobol algorithm for relatively small samples (n); \u2022 in the case of non-smooth functions our Monte Carlo shaking algorithm MCAMSS-1 gives similar results as the scrambled QMC; for several values of n we observe advantages for MCA-MSS-1 in terms of accuracy; \u2022 both MCA-MSS-1 and scrambled QMC are better than SFMT and Sobol quasi MC algorithm in the case of non-smooth functions. Another observation is that for the chosen integrands the scrambling algorithm does not outperform the algorithm with the original Sobol points, but the scrambled algorithm and Monte Carlo algorithm MCA-MSS-1 are more stable with respect to relative errors for relatively small values of n. The facts we observed that some further improvements of implementation of more refined shaking algorithms MCA-MSS-2 and MCA-MSS-2-S may be expected for relatively smooth integrands. That\u2019s why we compare Sobol QMCA with MCA-MSS-2 and MCA-MSS-2-S, as well as with simplest shaking algorithm MCA-MSS-1 (see Table 3). The results show that the simplest shaking algorithm MCA-MSS-1 gives relative errors similar to errors of the Sobol QMCA, which is expected since the \u039b\u03a0\u03c4 Sobol sequences are already quite well distributed. That\u2019s why one should not expect improvement for a very smooth integrand. But the symmetrised shaking algorithm MCA-MSS-2 improves the relative error. The effect\nAnother observation is that for the chosen integrands the scrambling algorithm does not outperform the algorithm with the original Sobol points, but the scrambled algorithm and Monte Carlo algorithm MCA-MSS-1 are more stable with respect to relative errors for relatively small values of n. The facts we observed that some further improvements of implementation of more refined shaking algorithms MCA-MSS-2 and MCA-MSS-2-S may be expected for relatively smooth integrands. That\u2019s why we compare Sobol QMCA with MCA-MSS-2 and MCA-MSS-2-S, as well as with simplest shaking algorithm MCA-MSS-1 (see Table 3). The results show that the simplest shaking algorithm MCA-MSS-1 gives relative errors similar to errors of the Sobol QMCA, which is expected since the \u039b\u03a0\u03c4 Sobol sequences are already quite well distributed. That\u2019s why one should not expect improvement for a very smooth integrand. But the symmetrised shaking algorithm MCA-MSS-2 improves the relative error. The effect\n<div style=\"text-align: center;\">Table 3 Relative error and computational time for numerical integration of a smooth functio (S( f ) \u22480.10897).</div>\n \u2248\n# of points n\nSobol QMCA\nMCA-MSS-1\nMCA-SMS-2\nMCA-SMS-2-S\n(# of double Rel.\nTime\n\u03c1\nRel.\nTime\nRel.\nTime\nRel.\nTime\npoints 2n)\nerror\n(s)\n\u00d7103\nerror\n(s)\nerror\n(s)\nerror\n(s)\n29\n0.0059\n< 0.001 2.1\n0.0064\n0.009\n0.0033\n0.010\n0.0016\n0.005\n(2\u00d729)\n6.4\n0.0061\n0.010\n0.0032\n0.010\n210\n0.0035\n0.002\n1.9\n0.0037\n0.010\n9e-05\n0.020\n0.0002\n0.007\n(2\u00d7210)\n6.4\n0.0048\n0.010\n0.0002\n0.020\n216\n2e-05\n0.027\n0.4\n3e-05\n1.580\n7e-06\n1.340\n9e-06\n0.494\n(2\u00d7216)\n1.2\n0.0001\n1.630\n5e-06\n1.380\n<div style=\"text-align: center;\">Table 4 Relative error and computational time for numerical integration of a smooth function (S( f ) \u22480.10897).</div>\n \u2248\nn\nSobol QMCA\nMCA-MSS-1\nMCA-MSS-2-S\nRel.\nTime\n\u03c1\nRel.\nTime\nRel.\nTime\n\u00d7103\nerror\n(s)\nerror\n(s)\nerror\n(s)\n2\u00d744\n0.0076\n< 0.001\n2.1\n0.0079\n< 0.001\n0.0016\n0.005\n(512)\n6.4\n0.0048\n< 0.001\n2\u00d764\n0.0028\n0.001\n1.2\n0.0046\n0.030\n0.0004\n0.009\n(2592)\n4.1\n0.0046\n0.030\n2\u00d784\n0.0004\n0.004\n0.9\n0.0008\n0.090\n0.0002\n0.025\n(8192)\n2.9\n0.0024\n0.090\n2\u00d7104\n0.0002\n0.008\n0.6\n0.0001\n0.220\n5e-05\n0.070\n(20000)\n2.0\n0.0013\n0.230\n2\u00d7134\n0.0001\n0.022\n0.4\n0.0001\n0.630\n4e-06\n0.178\n(57122)\n1.2\n0.0007\n0.640\n2\u00d7144\n5e-06\n0.029\n0.4\n1e-05\n0.860\n1e-05\n0.237\n(76832)\n1.2\n0.0005\n0.880\n2\u00d7154\n8e-06\n0.036\n0.4\n0.0001\n1.220\n9e-07\n0.313\n(101250)\n1.2\n0.0005\n1.250\nof this improvement is based on the fact that the second derivatives of the integrand exists, they are bounded and the construction of the MCA-MSS-2 algorithm gives a better convergence rate of order O(n\u22121/2\u22122/d). The same convergence rate has the algorithm MCA-MSS-2-S, but the latter one does not allow to control the value of the radius of shaking. As expected MCA-MSS-2-S gives better results than MCA-MSS-1. The relative error obtained by MCA-MSS-2 and MCA-MSS-2-S are of the same magnitude (see Table 3). The advantage of MCA-MSS-2-S is that its computational complexity is much smaller. A comparison of the relative error and computational complexity for different values of n is presented in Table 4. To have a fair comparison we have to consider again a smooth function (14). The observation is that MCA-MSS-2-S algorithm outperforms the simplest shaking algorithm MCA-MSS-1 in terms of relative error and complexity.\nTable 5 Relative error (in absolute value) and computational time for estimation of sensitivity indices of input parameters using various Monte Carlo and quasi-Monte Carlo approaches (n = 6600,c \u22480.51365,\u03b4 \u22480.08).\n<div style=\"text-align: center;\">Table 5 Relative error (in absolute value) and computational time for estimation of sensitivity indices of input parameters using various Monte Carlo and quasi-Monte Carlo approaches (n = 6600,c \u22480.51365,\u03b4 \u22480.08).</div>\nTable 5 Relative error (in absolute value) and computational time for estimation of sensitivity indices of input parameters using various Monte Carlo and quasi-Monte Carlo approaches (n = 6600051365\u03b4008).\n \u2248 \u2248\nEstimated\nSobol QMCA\nOwen scrambling\nMCA-MSS-1\nquantity\n\u03c1\nRel. error\ng0\n1e-05\n0.0001\n0.0007\n0.0001\n0.007\n6e-05\nD\n0.0007\n0.0013\n0.0007\n0.0003\n0.007\n0.0140\nStot\n1\n0.0036\n0.0006\n0.0007\n0.0009\n0.007\n0.0013\nStot\n2\n0.0049\n6e-05\n0.0007\n2e-05\n0.007\n0.0034\nStot\n3\n0.0259\n0.0102\n0.0007\n0.0099\n0.007\n0.0211\nAfter testing the algorithms under consideration on the smooth and non-smooth functions we studied the efficiency of the algorithms on real-life functions obtained after running UNI-DEM. Polynomials of 4-th degree with 35 unknown coefficients are used to approximate the mesh functions containing the model outputs. We use various values of the number of points that corresponds to situations when one needs to compute the sensitivity measures with different accuracy. We have computed results for g0 (g0 is the integral over the integrand g(x) = f(x)\u2212c, f(x) is the approximate model function of UNI-DEM, and c is a constant obtained as a Monte Carlo estimate of f0, [28]), the total variance D, as well as total sensitivity indices Stot i ,i = 1,2,3. The above mentioned parameters are presented in Table 5. Table 5 presents the results obtained for a relatively low sample size n = 6600. One can notice that for most of the sensitivity parameters the simplest shaking algorithm MCA-MSS-1 outperforms the scrambled Sobol sequences, as well as the algorithm based on the \u039b\u03a0\u03c4 Sobol sequences in terms of accuracy. For higher values of sample sizes this effect is even stronger. One can clearly observe that the simplest shaking algorithm MCA-MSS-1 based on modified Sobol sequences improves the error estimates for non-smooth integrands. For smooth functions modified algorithms MCA-MSS-2 and MCA-MSS2-S give better results than MCA-MSS-1. Even for relatively large radiuses \u03c1 the results are good in terms of accuracy. The reason is that centers of spheres are very well uniformly distributed by definition. So that, even for large values of radiuses of shaking the generated random points continue to be well distributed. We should stress on the fact that for relatively low number of points (< 1000) the algorithm based on modified Sobol sequences gives results with a high accuracy.\n# 7 Conclusion\nA comprehensive theoretical and experimental study of the Monte Carlo algorithm MCA-MSS-2 based on symmetrised shaking of Sobol sequences has been done. The algorithm combines properties of two of the best available approaches - Sobol quasi-Monte Carlo integration and a high quality SFMT pseudo-random number generator. It has been proven that this algorithm has an optimal rate of convergence for functions with continuous and bounded second derivatives in terms of probability and mean square error. A comparison with the scrambling approach, as well as with the Sobol quasiMonte Carlo algorithm and the algorithm using SFMT generator has been provided for numerical integration of smooth and non-smooth integrands. The algorithms mentioned above are tested numerically also for computing sensitivity measures for UNI-DEM model to study sensitivity of ozone concentration according to variation of chemical rates. All algorithms under consideration are efficient and converge with the expected rate of convergence. It is important to notice that the Monte Carlo algorithm MCA-MSS-2 based on modified Sobol sequences when symmetrised shaking is used has a unimprovable rate of convergence and gives reliable numerical results.\n# Acknowledgment\nThe research reported in this paper is partly supported by the Bulgarian NSF Grants DTK 02/44/2009 and DMU 03/61/2011.\n# References\n1. I. Antonov, V. Saleev, An economic method of computing LP\u03c4-sequences, USSR Comput. Math. Phy. 19 (1979) 252-256. 2. P. Bradley, B. Fox, Algorithm 659: Implementing Sobol\u2019s quasi random sequence generator, ACM Trans. Math. Software 14(1) (1988) 88-100. 3. I.T. Dimov, Monte Carlo methods for applied scientists, World Scientific, London, Singapore, 2008. 4. I.T. Dimov, I. Farago, A. Havasi, Z. Zlatev, Operator splitting and commutativity analysis in the Danish Eulerian Model, Math. Comp. Sim. 67 (2004) 217-233. 5. I.T. Dimov, R. Georgieva, Monte Carlo method for numerical integration based on Sobol\u2019 sequences, in: LNCS 6046, Springer, 2011, 50-59. 6. I. T. Dimov, R. Georgieva, Tz. Ostromsky, Z. Zlatev, Advanced algorithms for multidimensional sensitivity studies of large-scale air pollution models based on Sobol sequences, Comput Math Appl. Elsevier (in press). ISSN: 0898-1221. Doi: 10.1016/j.camwa.2012.07.005. 7. I.T. Dimov, Tz. Ostromsky, Z. Zlatev, Challenges in using splitting techniques for large-scale environmental modeling, in: Advances in Air Pollution Modeling for Environmental Security (Farago, I., Georgiev, K., Havasi, A. - eds.) NATO Science Series 54, 2005, Springer, 115132.\n8. H. Faure, S. Tezuka, Another random scrambling of digital (t,s)-sequences Monte Carlo and Quasi-Monte Carlo methods, Springer-Verlag, Berlin, Germany (K. Fang, F. Hickernell, H. Niederreiter, eds.), 2000. 9. S. Joe, F. Kuo, Constructing Sobol\u2019 sequences with better two-dimensional projections, SIAM J. Sci. Comput. 30 (2008), 2635-2654. 10. T. Homma, A. Saltelli, Importance measures in global sensitivity analysis of nonlinear models, Reliability Engineering and System Safety 52 (1996) 1-17. 11. H. Hong, F. Hickernell, Algorithm 823: Implementing scrambled digital sequences, ACM Trans. Math. Software 29(2) (2003) 95-109. 12. S. Kucherenko, B. Feil, N. Shah, W. Mauntz, The identification of model effective dimensions using global sensitivity analysis, Reliability Engineering and System Safety 96 (2011) 440449. 13. P. L\u2019Ecuyer, C. Lecot, B. Tuffin, A randomized quasi-Monte Carlo simulation method for Markov chains. Operations Research 56(4) (2008) 958-975. 14. P. L\u2019Ecuyer, C. Lemieux, Recent advances in randomized quasi-Monte Carlo methods, in: Dror, M., L\u2019Ecuyer, P., Szidarovszki, F. (eds.), Modeling Uncertainty: An Examination of Stochastic Theory, Methods, and Applications, Kluwer Academic Publishers, Boston, 2002, 419-474. 15. Y. Levitan, N. Markovich, S. Rozin, I. Sobol, On quasi-random sequences for numerical computations, USSR Comput. Math. and Math. Phys. 28(5) (1988) 755-759. 16. G.I. Marchuk, Mathematical modeling for the problem of the environment, Studies in Mathematics and Applications, No. 16, North-Holland, Amsterdam, 1985. 17. J. Matousek, On the L2-discrepancy for anchored boxes, Journal of Complexity 14 (1998) 527-556. 18. H. Niederreiter, Low-discrepancy and low-dispersion sequences, Journal of Number Theory 30 (1988) 51-70. 19. A. Owen, Randomly permuted (t,m,s)-nets and (t,s)-sequences, Monte Carlo and QuasiMonte Carlo Methods in Scientific Computing, 106, Lecture Notes in Statistics, 299-317, 1995. 20. A. Owen, Variance and Discrepancy with Alternative Scramblings, ACM Trans. on Computational Logic., V (2002) 1-16. 21. M. Saito, M. Matsumoto, SIMD-oriented fast Mersenne Twister: a 128-bit pseudorandom number generator, in: Keller, A., Heinrich, S., Niederreiter, H. (eds.) Monte Carlo and QuasiMonte Carlo Methods 2006, Springer (2008) 607-622. 22. A. Saltelli, S. Tarantola, F. Campolongo, M. Ratto, Sensitivity analysis in practice: A guide to assessing scientific models, Halsted Press, New York, 2004. 23. I.M. Sobol, Monte Carlo numerical methods, Nauka, Moscow, 1973 (in Russian). 24. I.M. Sobol, On the systematic search in a hypercube, SIAM J. Numerical Analysis 16 (1979) 790-793. 25. I.M. Sobol, On quadratic formulas for functions of several variables satisfying a general Lipschitz condition, USSR Comput. Math. and Math. Phys. 29(6) (1989) 936-941. 26. I.M. Sobol, Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates, Mathematics and Computers in Simulation, 55(1-3) (2001) 271-280. 27. I. Sobol, D. Asotsky, A. Kreinin, S. Kucherenko, Construction and comparison of highdimensional Sobol\u2019 generators, Wilmott Journal (2011) 67-79. 28. I. Sobol, E. Myshetskaya, Monte Carlo estimators for small sensitivity indices, Monte Carlo Methods and Applications 13(5-6) (2007) 455-465. 29. S. Tezuka, Uniform Random Numbers, Theory and Practice. Kluwer Academic Publishers, IBM Japan, 1995. 30. H. Weyl, Ueber die Gleichverteilung von Zahlen mod Eins. Math. Ann. 77(3) (1916) 313-352. 31. www.nag.co.uk/numeric/CL/CLdescription.asp. 32. www.math.sci.hiroshima-u.ac.jp/\u223cm-mat/MT/SFMT/index.html. 33. Z. Zlatev, I. T. Dimov, Computational and numerical challenges in environmental modelling, Elsevier, Amsterdam, 2006. 34. Z. Zlatev, I.T. Dimov, K. Georgiev, Three-dimensional version of the Danish Eulerian model, Zeitschrift f\u00a8ur Angewandte Mathematik und Mechanik, 76(S4) (1996) 473-476.\n",
    "paper_type": "method",
    "attri": {
        "background": "Sensitivity analysis (SA) is crucial for understanding how variations in input parameters affect the output of large-scale mathematical models, particularly those governed by complex systems of partial differential equations. Previous methods often relied on assumptions of linearity and additivity, which do not hold for many nonlinear models, necessitating the development of more reliable sensitivity analysis techniques.",
        "problem": {
            "definition": "The problem addressed in this paper is the need for effective sensitivity analysis methods for large-scale mathematical models, particularly those with nonlinear dynamics, such as the Unified Danish Eulerian Model (UNI-DEM).",
            "key obstacle": "Existing methods fail to accurately assess the sensitivity of models with significant nonlinearities and stiffness, as they often rely on simplifying assumptions that do not apply to these complex systems."
        },
        "idea": {
            "intuition": "The development of a new Monte Carlo algorithm based on symmetrised shaking of Sobol sequences was motivated by the need for a more effective approach to sensitivity analysis in the presence of nonlinearities.",
            "opinion": "The proposed method involves a Monte Carlo approach that utilizes Sobol sequences to improve the reliability and efficiency of sensitivity analysis for high-dimensional integrals.",
            "innovation": "This method differs from traditional approaches by combining the strengths of quasi-Monte Carlo integration and advanced random number generation techniques, resulting in improved convergence rates and accuracy."
        },
        "method": {
            "method name": "Monte Carlo Algorithm based on Modified Sobol Sequences",
            "method abbreviation": "MCA-MSS",
            "method definition": "MCA-MSS is a Monte Carlo algorithm designed for efficient numerical integration and sensitivity analysis, employing symmetrised shaking of Sobol sequences to enhance convergence.",
            "method description": "The method leverages Sobol sequences and a shaking procedure to generate random points for integration, ensuring uniform distribution and reducing variance in estimates.",
            "method steps": [
                "Generate Sobol sequences for the input parameters.",
                "Apply a shaking procedure to create random points around each Sobol point.",
                "Evaluate the integrand at these random points.",
                "Compute the integral and sensitivity indices based on the sampled values."
            ],
            "principle": "The effectiveness of this method lies in its ability to exploit the properties of Sobol sequences and the randomization introduced by shaking, which collectively enhance the accuracy of the numerical integration and sensitivity analysis."
        },
        "experiments": {
            "evaluation setting": "The experimental setup involved applying the MCA-MSS method to the UNI-DEM model, with sensitivity indices computed for various chemical reaction rates affecting ozone concentration across a large geographical region.",
            "evaluation method": "Performance was assessed by comparing the relative errors and computational times of the MCA-MSS method against other Monte Carlo and quasi-Monte Carlo methods, using both smooth and non-smooth integrands."
        },
        "conclusion": "The study demonstrates that the MCA-MSS algorithm achieves an optimal rate of convergence for functions with continuous and bounded second derivatives, providing reliable results for sensitivity analysis in complex mathematical models.",
        "discussion": {
            "advantage": "The MCA-MSS method outperforms traditional Monte Carlo and quasi-Monte Carlo methods, particularly for non-smooth functions, by providing more accurate estimates with lower computational costs.",
            "limitation": "While the method shows significant improvements in accuracy, its performance may still be affected by the choice of parameters, such as the radius of shaking, and may require careful tuning for optimal results.",
            "future work": "Future research could focus on refining the shaking procedures and exploring additional randomization techniques to further enhance the method's performance across a wider range of applications."
        },
        "other info": {
            "acknowledgment": "The research was supported by the Bulgarian NSF Grants DTK 02/44/2009 and DMU 03/61/2011."
        }
    },
    "mount_outline": [
        {
            "section number": "2. Background and Definitions",
            "key information": "Sensitivity analysis (SA) is crucial for understanding how variations in input parameters affect the output of large-scale mathematical models, particularly those governed by complex systems of partial differential equations."
        },
        {
            "section number": "2.1",
            "key information": "The problem addressed in this paper is the need for effective sensitivity analysis methods for large-scale mathematical models, particularly those with nonlinear dynamics, such as the Unified Danish Eulerian Model (UNI-DEM)."
        },
        {
            "section number": "3. Algorithmic Bias",
            "key information": "Existing methods fail to accurately assess the sensitivity of models with significant nonlinearities and stiffness, as they often rely on simplifying assumptions that do not apply to these complex systems."
        },
        {
            "section number": "4. Ethical AI",
            "key information": "The proposed method involves a Monte Carlo approach that utilizes Sobol sequences to improve the reliability and efficiency of sensitivity analysis for high-dimensional integrals."
        },
        {
            "section number": "5. Model Interpretability",
            "key information": "The MCA-MSS method leverages Sobol sequences and a shaking procedure to generate random points for integration, ensuring uniform distribution and reducing variance in estimates."
        },
        {
            "section number": "6. Responsible AI",
            "key information": "The study demonstrates that the MCA-MSS algorithm achieves an optimal rate of convergence for functions with continuous and bounded second derivatives, providing reliable results for sensitivity analysis in complex mathematical models."
        },
        {
            "section number": "8. Future Directions",
            "key information": "Future research could focus on refining the shaking procedures and exploring additional randomization techniques to further enhance the method's performance across a wider range of applications."
        }
    ],
    "similarity_score": 0.5350876764702027,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0337_algor/papers/Multidimensional Sensitivity Analysis of Large-scale Mathematical Models.json"
}