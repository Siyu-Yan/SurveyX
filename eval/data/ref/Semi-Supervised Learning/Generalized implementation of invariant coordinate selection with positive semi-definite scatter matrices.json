{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.02258",
    "title": "Generalized implementation of invariant coordinate selection with positive semi-definite scatter matrices",
    "abstract": "Invariant coordinate selection (ICS) is an unsupervised multivariate data transformation useful in many contexts such as outlier detection or clustering. It is based on the simultaneous diagonalization of two affine equivariant and positive definite scatter matrices. Its classical implementation relies on a non-symmetric eigenvalue problem (EVP) by diagonalizing one scatter relatively to the other. In case of collinearity, at least one of the scatter matrices is singular and the problem cannot be solved. To address this limitation, three approaches are proposed based on: a Moore-Penrose pseudo inverse (GINV), a dimension reduction (DR), and a generalized singular value decomposition (GSVD). Their properties are investigated theoretically and in different empirical applications. Overall, the extension based on GSVD seems the most promising even if it restricts the choice of scatter matrices that can be expressed as cross-products. In practice, some of the approaches also look suitable in the context of data in high dimension low sample size (HDLSS).",
    "bib_name": "archimbaud2024generalizedimplementationinvariantcoordinate",
    "md_text": "# Generalized implementation of invariant coordinate selection with positive semi-definite scatter matrices\nAurore Archimbauda\naTBS Business School, 1 Place Alphonse Jourdain, 31000 Toulouse, France\nAbstract\nInvariant coordinate selection (ICS) is an unsupervised multivariate data transformation useful in many contexts such as outlier detection or clustering. It is based on the simultaneous diagonalization of two affine equivariant and positive definite scatter matrices. Its classical implementation relies on a non-symmetric eigenvalue problem (EVP) by diagonalizing one scatter relatively to the other. In case of collinearity, at least one of the scatter matrices is singular and the problem cannot be solved. To address this limitation, three approaches are proposed based on: a Moore-Penrose pseudo inverse (GINV), a dimension reduction (DR), and a generalized singular value decomposition (GSVD). Their properties are investigated theoretically and in different empirical applications. Overall, the extension based on GSVD seems the most promising even if it restricts the choice of scatter matrices that can be expressed as cross-products. In practice, some of the approaches also look suitable in the context of data in high dimension low sample size (HDLSS). Keywords: Dimension reduction, Generalized Eigenvalue Problem, High-dimension, Pseudo-inverse, Singular scatters, Singular value decomposition 2020 MSC: Primary 62H99, Secondary 62-08, Tertiary 65F99\n3 Sep 2024\n 3 Sep 2\n3 Sep\n[stat.ME]\n# 1. Introduction\nInvariant Coordinate Selection (ICS) is a powerful unsupervised multivariate method designed to identify the structure of multivariate datasets on a subspace. It relies on the joint diagonalization of two affine equivariant and positive definite scatter matrices V1 and V2 and is particularly relevant as a dimension reduction tool prior to clustering [1] or outlier detection [5]. It goes beyond the well-known Principal Components Analysis (PCA) method by not maximizing the inertia but optimizing a generalized kurtosis. More precisely, some theoretical results [43] proved that under some elliptical mixture models, the subspace spanned by the first and/or last components carries the information regarding the multivariate structure and recovers the Fisher discriminant subspace, whatever the choice of scatter matrices is. The goal of ICS is to find the p \u00d7 p matrix B = (b1, . . . , bp)\u22a4of eigenvectors, which simultaneously diagonalizes two scatter matrices V1 \u2208Pp and V2 \u2208Pp, with Pp be the set of all symmetric positive definite matrices of order p and where \u22a4denotes the transpose operation. Typically, this simultaneous diagonalization corresponds to a generalized eigenvalue problem (GEP) or solving a linear matrix pencil, which can be simplified to a non-symmetric eigenvalue problem (EVP) by diagonalizing one scatter relatively to the other. To fix the order of the components and their normalization, we follow the standard definition of [43] by diagonalizing V2 relatively to V1:\n# BV1B\u22a4= Ip and BV2B\u22a4= D,\nBV1B\u22a4= Ip and BV2B\u22a4= D,\nwhere D is a diagonal matrix with decreasing diagonal elements \u03c11 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03c1p > 0, which correspond to t eigenvalues of V\u22121 1 V2 and B contains the corresponding eigenvectors as its rows. This problem can be re-written as:\nwhere D is a diagonal matrix with decreasing diagonal elements \u03c11 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03c1p > 0, which correspond to the igenvalues of V\u22121 1 V2 and B contains the corresponding eigenvectors as its rows.\nV2bi = \u03c1iV1bi \u21d4V\u22121 1 V2bi = \u03c1ibi, for i \u2208{1, . . . , p}.\n\u2217Corresponding author. Email address: a.archimbaud@tbs-education.fr Preprint submitted to Journal of Multivariate Analysis\n\u2217Corresponding author. Email address: a.archimbaud@tbs-education.fr\nPreprint submitted to Journal of Multivariate Analysis\nPreprint submitted to Journal of Multivariate Analysis\nPreprint submitted to Journal of Multivariate Analysis\n(1)\n(2)\nwith the following normalization: \u2022 b\u22a4 i V1bj = 0 for i \ufffdj and b\u22a4 j V1bj = 1 for i = j, with i, j \u2208{1, . . . , p}, \u2022 b\u22a4 i V2bj = 0 for i \ufffdj and b\u22a4 j V2bj = \u03c1j for i = j, with i, j \u2208{1, . . . , p}. Equivalently, as stated in [43], the eigenvalues \u03c1i, for i \u2208{1, . . . , p} and the eigenvectors b1, . . . , bp can also be sequentially defined by solving the successive maximization or minimization problems of the ratio:\ni  j  \u2208{} Equivalently, as stated in [43], the eigenvalues \u03c1i, for i \u2208{1, . . . , p} and the eigenvectors b1, . . . , bp can also be sequentially defined by solving the successive maximization or minimization problems of the ratio:\nwhere \u03c11 is the maximal possible value of K(b) over b \u2208Rp which is achieved in the direction of the eigenvector b1. The so-called invariant coordinates or components are then obtained as: Zn = (Xn \u22121nT(Xn)\u22a4)B(Xn)\u22a4, where Xn = (x1, . . . , xn)\u22a4\u2208Rn\u00d7p is a p-variate sample with n observations, 1n denotes an n-variate vector full of ones, and T(Xn) denotes a location estimator, usually the one that goes along with V1. In practice, the requirement of positive definiteness for the two scatter matrices is limiting and we focus on the case where at least one of these scatter matrices is singular. This is a common case since the variance-covariance matrix can be semi-definite positive as soon as some variables are collinear or in a high dimension low sample size context (HDLSS), i.e. if the number of variables exceeds the number of observations. Nowadays, more and more data are easily collected and collinearity issues arrive more frequently even when the number of observations is still higher than the number of observations. In this case, performing ICS is very challenging as we might not be able to compute: (i) one or the two scatter matrices and (ii) the inverse of V1 as required to solve the GEP 2. A simple idea is to perform a variable selection, if many variables are known to be non-relevant. However, this procedure can induce the deletion of a substantial number of variables to obtain a convenient number of dimensions on which both scatter estimators can be defined. And so, it can also lead to a potential loss of information. The collinearity is a long-standing issue in multivariate analysis since quite a lot of methods rely on the simultaneous diagonalization of two or more scatter matrices[29], for which the non-singularity of a scatter matrix is required. One of the most well-known methods that solves a GEP of two scatter matrices is the classical Fisher Linear Discriminant Analysis (LDA) which maximizes the separation ratio of the between and the within-group covariance matrices \u03a3B and \u03a3W. In the case of collinearity, the rank of the between-class covariance matrix \u03a3B is equal to the number of groups, leading to a singular matrix. So, the maximization problem cannot be performed by solving the \u03a3\u22121 W \u03a3B eigenvalue problem anymore. To overcome such an issue, [19] and [39] review some solutions even in case the within-group covariance matrix \u03a3W is also singular. Among others, the proposed approaches exploit the MoorePenrose pseudo-inverse or the Generalized Singular Value Decomposition (GSVD). Another well-used approach is the dimension reduction through a singular value decomposition like in [22] or [12]. This rank reduction is also used for the LDA method in the HDLSS context, as explained by [19] and as a pre-processing step of ICS in [13]. The objective of this paper is to adapt some of those approaches to generalizing ICS to the singularity issue, to investigate theoretically and practically their properties and to provide R [33] implementations as well. The structure of this paper is as follows. In Section 2, we introduce a definition of ICS with semi-definite positive scatter matrices and we present the challenges associated with such a context. In Section 3 we propose three approaches to adapting ICS to the case of semi-definite positive scatter estimates based on the Moore-Penrose pseudo inverse (GINV), the dimension reduction (DR) and the generalized singular value decomposition (GSVD). We also investigate theoretically their properties in terms of: (i) the criterion to optimize, (ii) the affine invariance of the scores and (iii) the symmetry of the roles of the two scatter estimates V1 and V2. Section 4 focuses on different empirical applications to infirm or confirm the theoretical aspects. Finally, Section 5 concludes the paper and discusses further perspectives.\n# 2. ICS with semi-definite positive scatter matrices\nIn Subsection 2.1, some scatter matrices are detailed and a more general definition is given in case it is semidefinite positive. Subsection 2.2 generalizes ICS to positive semi-definite scatter matrices, while Subsection 2.3 presents the main challenges associated with it.\n(3)\n# 2.1. Scatter matrices\nLet Pp be the set of all symmetric positive definite matrices of order p, SPp be the set of all symmetric positive semi-definite matrices of order p. Generally, a scatter matrix is defined as a p \u00d7 p scatter matrix V(Xn) \u2208Pp which is affine equivariant in the sense that: V(XnA + 1n\u03b3\u22a4) = A\u22a4V(Xn)A, where A is a full rank p \u00d7 p matrix, \u03b3 a p-vector and 1n an n-vector full of ones. Among the most common ones there are the regular covariance matrix:\nLet Pp be the set of all symmetric positive definite matrices of order p, SPp be the set of all symmetric positive semi-definite matrices of order p. Generally, a scatter matrix is defined as a p \u00d7 p scatter matrix V(Xn) \u2208Pp which is affine equivariant in the sense that: V(XA + 1\u03b3\u22a4) = A\u22a4V(X)A,\nwhere A is a full rank p \u00d7 p matrix, \u03b3 a p-vector and 1n an n-vector full of ones. Among the most common ones there are the regular covariance matrix:\nCOV4(Xn) = 1 (p + 2)n n \ufffd i=1 r2 i (xi \u2212\u00afx)(xi \u2212\u00afx)\u22a4,\nwhere r2 i = (xi \u2212\u00afx)\u22a4COV(Xn)\u22121(xi \u2212\u00afx) is the classical squared Mahalanobis distance. It is well known that those two scatter matrices are not robust against non-normality and the presence of outliers. One of the most widely-used robust alternatives is the minimum covariance determinant estimator (MCD) [34]. For a tuning parameter \u03b1 \u2208[0.5, 1], the MCD selects out of the n observations those n\u03b1 = \u2308\u03b1n\u2309observations xi1, . . . , xin\u03b1 for which the sample covariance matrix has the smallest determinant:\nMCD\u03b1(Xn) = c\u03b1 1 n\u03b1 n\u03b1 \ufffd j=1 (xi j \u2212\u00afx\u03b1,n)(xi j \u2212\u00afx\u03b1,n)\u22a4,\nwhere \u00afx\u03b1,n is the sample mean of the selected set of observations and c\u03b1 is a consistency factor. To increase efficiency  is often combined with a reweighting step, see for example Hubert et al. [21] for more details.\nTheoretically, we can consider scatter matrices V(Xn) which are only symmetric positive semi-definite. If Xn is not of full rank, then COV(Xn) \u2208SPp. For robust scatter matrices based on a subset of observations lying on a subspace of lower dimension than the entire space, V(Xn) belongs to SPp. This arises for example in the presence of outliers in the orthogonal complement subspace (OC outliers). In this context, we can extend the definition of a scatter matrix V(Xn) \u2208SPp which is said to be affine equivariant in the sense that:\nV(XnA + 1n\u03b3\u22a4) = A\u22a4V(Xn)A,\n# V(XnA + 1n\u03b3\u22a4) = A\u22a4V(Xn)A,\nfor any A and \u03b3 as previously defined. Tyler [42, Lemma 2] proves that if Xn lies in some r \u2264p-dimensional hyperplane, then affine equivariant location and scatter statistics are essentially affine equivariant statistics defined on this hyperplane. For a p \u00d7 (p \u2212r) matrix M of rank p \u2212r and m \u2208Rp\u2212r, let us defined the hyperplane H(M, m) = {x \u2208 Rp | M\u22a4x = m}, then for any affine equivariant scatter matrix V(Xn): M\u22a4V(Xn)M = 0(p\u2212r)\u00d7(p\u2212r), where 0 j\u00d7k denote the j \u00d7 k matrix of all zeroes. In addition the lemma states that if L is any p \u00d7 r matrix such that A = \ufffd L M \ufffd is non singular then:\nwhere V(r)(Y) is a scatter matrix for n \u00d7 r data matrices Y. Tyler [42] also note that if the data is in general position then all affine equivariant scatter statistics, symmetric in the observations2, are proportional to the variance-covarianc\n1Data is in general position if there is no subset of k observations lying on a subspace of dimension k \u22122, with k \u2264p + 1 and p denotes the number of variables. 2i.e. V(QXn) = V(Xn) for any permutation matrix Q of order n and Xn \u2208Rn\u00d7p, the initial data containing n observations, characterized by p variables.\n1Data is in general position if there is no subset of k observations lying on a subspace of dimension k \u22122, with k \u2264p + 1 and p denotes the number of variables. 2i.e. V(QXn) = V(Xn) for any permutation matrix Q of order n and Xn \u2208Rn\u00d7p, the initial data containing n observations, characterized by p\n(4)\nmatrix. In practice, if the data is perfectly multicollinear but with n > p then the data is not in general position. If n \u2264p, the situation depends on the data themselves but there are examples, mainly in the automotive field [2, Chapter 5], where the data is not in general position. Another challenge is that many of the well-known robust affine equivariant scatter statistics such as the Mestimators [24] or the MCD are not well-defined when the data contains collinear variables. Usually, we can only define the variance-covariance matrix or the projection-based estimators [10, 25, 41] such as the Stahel-Donoho estimator [38]. To overcome this issue, regularized estimators of scatter matrices have been proposed: the regularized M-estimators [30] or the minimum regularized covariance determinant estimator [9] (MRCD) among others. The scatter matrix based on the fourth moments COV4 can also be defined in a more general context if the inverse of COV is replaced by its pseudo-inverse COV+:\n# GCOV4(Xn) = 1 (p + 2)n n \ufffd i=1 r2 i (xi \u2212\u00afx)(xi \u2212\u00afx)\u22a4,\nwhere r2 i = (xi \u2212\u00afx)\u22a4COV+(Xn)(xi \u2212\u00afx) is the classical squared Mahalanobis distance. All those p \u00d7 p scatter matrices belong to SPp if Xn is not full rank and are no longer necessary affine equivariant. This is a well-known consequence and it is common to relax the affine invariance of the multivariate methods in case of singularity. For example, [36] propose to focus on some \u201cweak invariance\u201d based on the relative ranking of the outlierness measures of the observations instead of requiring the same score value after an affine transformation. For convenience, for the rest of the paper, the dependence on Xn is dropped from the different scatter matrices V(Xn) when the context is obvious.\n# 2.2. ICS as a generalized eigenvalue problem\nWith the common definition of the ICS method, the two scatter matrices V1 and V2 should be definite positive to find a finite and nonzero eigenvalue \u03c1 to the eigenproblem (2): V2b = \u03c1V1b \u21d4V\u22121 1 V2b = \u03c1b. The positive definiteness of V1 is required to compute its inverse whereas the positive definiteness of V2 ensures nonzero eigenvalues. If V1 is singular and not proportional to V2, then the equivalence (2) is not true anymore since V1 is not invertible. The problem can not be simplified to a non-symmetric eigenvalue problem (EVP) anymore and so, we have to solve the initial Generalized Eigenvalue Problem (GEP):\n# V2bi = \u03c1iV1bi for i \u2208{1, . . . , p},\nwith V1 \u2208SPp and V2 \u2208SPp which are not necessarily of full ranks. If V1 and/or V2 is singular, that means that the null spaces of the two scatter matrices are not empty and they do not necessarily span the same subspace. Concretely, in this context, solving the GEP of V1 and V2, leads to consider the following cases: \u2022 if b \u2208range(V1) \u2229range(V2) then \u03c1 \u2208R+\u2217, \u2022 if b \u2208null(V2) \u2212null(V1) then \u03c1 = 0, \u2022 if b \u2208null(V1) \u2212null(V2) then \u03c1 = \u221e, \u2022 if b \u2208null(V1) \u2229null(V2) then any \u03c1 \u2208R is a solution of the GEP. The corresponding eigenvectors are not well-defined and might cause stability issues for some algorithms. However, the structure of the data is not associated with those directions and so, we do not need to consider them further. So, contrary to the classical ICS, the directions b associated with infinite or zero eigenvalues should also be analyzed as they might highlight some of the structure of the data. Let us illustrate the challenges of considering semi-definite scatter matrices for ICS on one artificial data.\n \u2208 \u2212 \u221e \u2022 if b \u2208null(V1) \u2229null(V2) then any \u03c1 \u2208R is a solution of the GEP. The corresponding eigenvectors are not well-defined and might cause stability issues for some algorithms. However, the structure of the data is not associated with those directions and so, we do not need to consider them further. o, contrary to the classical ICS, the directions b associated with infinite or zero eigenvalues should also be analyzed  they might highlight some of the structure of the data. Let us illustrate the challenges of considering semi-definite atter matrices for ICS on one artificial data.\n2.3. Challenges with singular scatter matrices: an illustrative example Let X = (X1, . . . , Xp)\u22a4be a p-multivariate real random vector and assume the distribution of X is a mixture of two Gaussian distributions with different covariance matrices:\nLet X = (X1, . . . , Xp)\u22a4be a p-multivariate real random vector and assume the distribution of X is a mixture of tw Gaussian distributions with different covariance matrices:\nX \u223c(1 \u2212\u03f5) N \ufffd 0p, \ufffdW1 0 0 0 \ufffd\ufffd + \u03f5 N \ufffd 0p, \ufffdW1 0 0 W2 \ufffd\ufffd ,\n# X \u223c(1 \u2212\u03f5) N \ufffd 0p, \ufffdW1 0 0 0 \ufffd\ufffd + \u03f5 N \ufffd 0p, \ufffdW1 0 0 W2 \ufffd\ufffd ,\n(5)\nwith \u03f5 < 1/2, W1 \u2208Pr1 and W2 \u2208SPp\u2212r1 with rank(W2) = r2 \u2264p \u2212r1. Such a distribution illustrates a model containing two clusters: the majority of the data and a group that can be identified as outlying observations. The first cluster follows a Gaussian distribution such that the majority of the data is contained in a r1-dimensional subspace spanned by the range of W1. The observations from the second cluster behave the same as previously on the r1-dimensional subspace but they are also present in r2 directions not spanned by the majority of the data. The goal of the ICS method is to find this r2-dimensional subspace where the observations of the second cluster are outlying. Here this subspace is spanned by the range of W2 which is the orthogonal complement of the range of W1, i.e. the null space of W1. Let us try to recover this subspace using the ICS method with a theoretical \u201cperfectly robust\u201d scatter functional V1 = \ufffdW1 0 0 0 \ufffd and a theoretical \u201cnon-robust\u201d scatter functional, the covariance of X, V2 = \ufffdW1 0 0 \u03f5W2 \ufffd . We have V1 \u2208SPp and V2 \u2208SPp, with rank(V1) = r1 < rank(V2) \u2264p. In addition, range(V1) = range(W1) and range(V2) = range(W1) \u2295range(W2). Several of the aforementioned cases arise on this example. First, the intersection of the spaces spanned by the two scatter functionals V1 and V2 corresponds to the r1-dimensional subspace spanned by W1, so r1 nonzero eigenvalues should be found. Then, since null(V1) \u2212null(V2) \ufffd{0}, a new direction associated to an \u221eeigenvalue should also be analyzed. In fact, this is the one that reveals the outliers. Finally, if rank(V2) < p, then the two scatter functionals share a part of their null subspaces. This subspace is not important since it contains no structure. However, we consider this phenomenon in our analysis because it is common in practice that the data is not of full rank. In addition, this feature could also make some algorithms unstable. Practically, to illustrate the model (2.3), we generate 1000 observations with exactly 20 outliers, W1 = I2 and W2 = diag(2, 0). On the left scatterplot matrix of the Figure 1, we can see that the outliers represented by some blue triangles behave differently than the majority of the data only on the third variable. The subspace spanned by this third variable is the only one of interest to identify these observations as outliers. This example can be seen as tricky since the outliers are well-identified on the third variable and no observations lie on the fourth one. So, we apply an affine transformation based on a non-singular p \u00d7 p particular Toeplitz matrix A:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2092/20924db4-0657-4d54-a37f-b4a91fba7422.png\" style=\"width: 50%;\"></div>\n\uf8ec\uf8ec\uf8ec\uf8ed \uf8f7\uf8f7\uf8f7\uf8f8 to transform the initial data X to X\u2217= AX. We can notice on the right scatterplot matrix of Figure 1, that the outlier are no longer as well separated on the third transformed variable as they were initially. In addition, we are no longe able to see that the observations lie in a three-dimensional subspace. However, the structure of outlierness of the dat is still contained in one dimension only. The challenge is to be able to recover the direction spanned by the outlier with ICS.\n# . Implementation of ICS for semi-definite positive scatter matrices\nSeveral methods exist to solve a Generalized Eigenvalue Problem. Among others, it exists the well-known QZalgorithm introduced by [26] or the procedure described by [35]. However, in practice, solving a GEP of two scatter matrices with a common null space from a numerical point of view is particularly challenging. Indeed, the presence of this null space makes procedures like the well-known QZ-algorithm very unstable. So far, the available algorithms are not satisfactory as they might lead to complex and negative eigenvalues. For that reason, GEP is not directly solved in general and surrogate approaches are used in [39]. In this section, we investigate three theoretical and practical implementations of ICS based on Moore-Penrose pseudo-inverse (GSVD) in Subsection 3.1, dimension reduction (DR) in Subsection 3.2 and generalized singular value decomposition (GSVD) in Subsection 3.3. We focus on collinearity issue with n > p.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8143/81436c51-90c2-4af3-beca-c974883ea646.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6ead/6ead1679-bff2-4971-b48a-b02f6a207dcb.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 1: Left: Scatterplot matrix of the simulated observations with 1000 observations, 20 outliers, W1 = I2 and W2 = diag(2, 0). Right: Scatterpl matrix of the simulated observations transformed by the non-singular matrix A.</div>\nIf we assume that V1 and V2 can be defined and computed on Xn but that V1 \u2208SPp with rank(V1) = r1 < p, then it is not possible to solve V\u22121 1 V2 since V1 is not invertible. Instead, we can replace the inverse of V1 with its Moore-Penrose pseudo-inverse V+ 1 and solve:\nwhere V+ 1 = P\u039b+P\u22a4= P1\u039b\u22121 r1 P\u22a4 1 , with \u039b\u22121 r1 containing only the inverse of the r1 nonzero eigenvalues of V1, P  an orthogonal matrix containing the eigenvectors of V1. P can be partitioned as P = [P1 P2], with P1, the p\u00d7r1 matr containing the first r1 eigenvectors associated to the r1 nonzero eigenvalues of V1, which is an orthonormal basis fo the range space of V1. Similarly, the p \u00d7 p \u2212r1 matrix P2 spans the null space of V1. P1 and P2 are semi-orthogon matrices such that: P\u22a4 1 P1 = Ir1 and P\u22a4 2 P2 = Ip\u2212r1.\nwhere V+ 1 = P\u039b+P\u22a4= P1\u039b\u22121 r1 P\u22a4 1 , with \u039b\u22121 r1 containing only the inverse of the r1 nonzero eigenvalues of V1, P is an orthogonal matrix containing the eigenvectors of V1. P can be partitioned as P = [P1 P2], with P1, the p\u00d7r1 matrix containing the first r1 eigenvectors associated to the r1 nonzero eigenvalues of V1, which is an orthonormal basis for the range space of V1. Similarly, the p \u00d7 p \u2212r1 matrix P2 spans the null space of V1. P1 and P2 are semi-orthogonal matrices such that: P\u22a4 1 P1 = Ir1 and P\u22a4 2 P2 = Ip\u2212r1. Property 1. Solving V+ 1V2 restricts the direction b associated to the largest eigenvalue \u03c11 to b = v1 + v0 with v1 \u2208range(V1), v0 \u2208null(V1) only onto the subspace spanned by V1 and expressed by v1:\nv1 = argmax b\u2208Rp,b\ufffd0 b\u22a4ProjV1V2ProjV1b b\u22a4P1\u039br1P\u22a4 1 b ,\n# v1 = argmax b\u2208Rp,b\ufffd0 b\u22a4ProjV1V2ProjV1b b\u22a4P1\u039br1P\u22a4 1 b\nwhere ProjV1 = P1P\u22a4 1 . The roles of V1 and V2 are not exchangeable anymore. Proof: Instead of solving the non-symmetric EVP V+ 1V2, we transform it to V+1/2 1 V2V+1/2 1 , which is symmetric: V+ 1V2b = \u03c1b \u21d4V+1/2 1 V2V+1/2 1 b\u2217= \u03c1b\u2217\u21d4(\u039b\u22121/2 r1 P\u22a4 1 V2P1\u039b\u22121/2 r1 \u2212\u03c1Ir1)b\u2217= 0,\nwhere ProjV1 = P1P\u22a4 1 . The roles of V1 and V2 are not exchangeable anymore. Proof: Instead of solving the non-symmetric EVP V+ 1V2, we transform it to V+1/2 1 V2V+1/2 1 , which is symmetric: V+ 1V2b = \u03c1b \u21d4V+1/2 1 V2V+1/2 1 b\u2217= \u03c1b\u2217\u21d4(\u039b\u22121/2 r1 P\u22a4 1 V2P1\u039b\u22121/2 r1 \u2212\u03c1Ir1)b\u2217= 0, ( with b = P1\u039b\u22121/2 r1 b\u2217. By multiplying by P1\u039b1/2 r1 , and because P1 is only semi-orthogonal, the equation (7) can b rewritten as: (P1P\u22a4 1 V2P1P\u22a4 1 \u2212\u03c1P1\u039br1P\u22a4 1 )b = 0, which leads to the following modified ICS criterion for the eigenvector associated with the largest eigenvalue: max b\u2208Rp,b\ufffd0 b\u22a4P1P\u22a4 1 V2P1P\u22a4 1 b b\u22a4P1\u039br1P\u22a4 1 b , ( 6\nV 1V2b = \u03c1b \u21d4V 1 V2V 1 b\u2217= \u03c1b\u2217\u21d4(\u039b\u2212 r1 P\u22a4 1 V2P1\u039b\u2212 r1 \u2212\u03c1Ir1)b\u2217= 0, (7 with b = P1\u039b\u22121/2 r1 b\u2217. By multiplying by P1\u039b1/2 r1 , and because P1 is only semi-orthogonal, the equation (7) can b rewritten as: (P1P\u22a4 1 V2P1P\u22a4 1 \u2212\u03c1P1\u039br1P\u22a4 1 )b = 0, which leads to the following modified ICS criterion for the eigenvector associated with the largest eigenvalue: max b\u2208Rp,b\ufffd0 b\u22a4P1P\u22a4 1 V2P1P\u22a4 1 b b\u22a4P1\u039br1P\u22a4 1 b , (8 6\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/239b/239bc2e2-0641-4815-8d42-6ec83b9e8831.png\" style=\"width: 50%;\"></div>\n(7)\nwith P1P\u22a4 1 = ProjV1, an orthogonal projection matrix onto the range(V1), as P1 is an orthonormal basis for range(V1). So range(P1P\u22a4 1 V2P1P\u22a4 1 ) \u2286range(V1) and range(P1\u039br1P\u22a4 1 ) = range(V1). In addition, Rp can be decomposed such that: Rp = range(V1) \u2295null(V1), thus the solution b of the criterion (8) can be expressed as: b = v+ v with v \u2208range(V) and v \u2208null(V),\nwith P1P\u22a4 1 = ProjV1, an orthogonal projection matrix onto the range(V1), as P1 is an orthonormal basis for range(V1) So range(P1P\u22a4 1 V2P1P\u22a4 1 ) \u2286range(V1) and range(P1\u039br1P\u22a4 1 ) = range(V1). In addition, Rp can be decomposed such that: Rp = range(V1) \u2295null(V1), thus the solution b of the criterion (8) can be expressed as:\n# b = v1 + v0 with v1 \u2208range(V1) and v0 \u2208null(V1)\nwith v1 = argmax b\u2208Rp,b\ufffd0 b\u22a4ProjV1V2ProjV1b b\u22a4P1\u039br1P\u22a4 1 b .\nAs null(V1) \u2286null(ProjV1V2ProjV1), optimizing the new criterion (8) restricts the solutions to directions b only on the subspace spanned by V1 and expressed by V1.\nAs null(V1) \u2286null(ProjV1V2ProjV1), optimizing the new criterion (8) restricts the solutions to directions b only onto the subspace spanned by V1 and expressed by V1. Remark 1. If the structure of the data is only visible onto the subspace spanned by V2, in the null space of V1, then it is not possible to highlight it and recover the outlying observations: if b \u2208null(V1) \u2212null(V2), then \u03c1 = \u221e. The roles of V1 and V2 are not exchangeable anymore. Indeed, the directions found only span the range of the inverted scatter matrix. So, the ranks of the null spaces of V1 and V2 are now important. The results remain if V2 is singular or not. Remark 2. Equivalence with the classical ICS. If V1 \u2208Pp then solving the V\u22121 1 V2 eigenvalue problem or using the Moore-Penroe pseudo-inverse of V1 is equivalent because V+ 1 = V\u22121 1 . Remark 3. If V2(Xn) is not defined, the standard ICS algorithm is not applicable but computing V2 on the whitened data might be possible: V2(XnV+1/2 1 ). Example 1. Going back to our artificial example 2.3, using the Moore-Penrose pseudo-inverse of V1 leads to optimize the following criterion:\nClearly, in this case, any b \u2208Rp is a solution of the maximization which implies that the structure of outlierness contained in W2 cannot be highlighted. We obtain two eigenvalues equal to one since V1 is two-dimensional and two others equal to zero. The projection of the data onto the eigenvectors space is illustrated in Figure 2. Definitely, the outliers cannot be identified because the eigenspace is restricted to the subspace spanned by V1 which does not contain the structure of outlierness defined by W2. So, the pseudo-inverse of V1 does not always give the correct solution to the singularity issue of the scatter matrices. Property 2. If the roots \u03c11, . . . , \u03c1p are all distinct, then for the orthogonal transformation X\u2217 n = XnA + 1n\u03b3\u22a4, with A being non-singular and \u03b3 \u2208Rp, the coordinates Z\u2217 n = (X\u2217 n \u22121nT(X\u2217 n)\u22a4)B(X\u2217 n)\u22a4and Zn = (Xn \u22121nT(Xn)\u22a4)B(Xn)\u22a4,\nClearly, in this case, any b \u2208Rp is a solution of the maximization which implies that the structure of outlierness contained in W2 cannot be highlighted. We obtain two eigenvalues equal to one since V1 is two-dimensional and two others equal to zero. The projection of the data onto the eigenvectors space is illustrated in Figure 2. Definitely, the outliers cannot be identified because the eigenspace is restricted to the subspace spanned by V1 which does not contain the structure of outlierness defined by W2. So, the pseudo-inverse of V1 does not always give the correct solution to the singularity issue of the scatter matrices.\nProperty 2. If the roots \u03c11, . . . , \u03c1p are all distinct, then for the orthogonal transformation X\u2217 n = XnA + 1n\u03b3\u22a4, with being non-singular and \u03b3 \u2208Rp, the coordinates Z\u2217 n = (X\u2217 n \u22121nT(X\u2217 n)\u22a4)B(X\u2217 n)\u22a4and Zn = (Xn \u22121nT(Xn)\u22a4)B(Xn) then\nwhere J is a p \u00d7 p diagonal matrix with diagonal elements \u00b11, which means the coordinates Z\u2217 n and Zn are invarian up to their signs through an orthogonal transformation.\nProof: Let X\u2217 n = XnA+1n\u03b3\u22a4, with A being non-singular and \u03b3 \u2208Rp and V1(Xn) \u2208SPp with rank(V1) < p. By definition of a scatter matrix: V1(X\u2217 n) = A\u22a4V1(Xn)A and if A is an orthogonal matrix, then: V+ 1(X\u2217 n) = A\u22121V+ 1(Xn)(A\u22a4)\u22121 = A\u22a4V+ 1(Xn)A. Following the computations detailed in the proof of Property 2:\nCompared to the criterion (8), the eigenvectors are rotated by A\u22a4and so projecting the transformed data X\u2217 n onto BA or projecting Xn onto B leads to the same coordinates Z\u2217 n and Zn up to their signs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/095e/095e6033-0d14-4c98-a3ce-82938145cac4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e3af/e3af1a0a-1ff6-4f50-a1e7-a641d6479d58.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2: Scatterplot matrix of the IC resulting of ICS using the generalized inverse of V1 on Xn (left panel) and on XnA (right panel).</div>\n \u2208SP assumption of orthogonality is required in the proof (see the next counter-example 2). Example 2. We consider the simulated data transformed by the non-singular matrix A (2.3). In this case, the structure of outlierness of the data is still contained only in one dimension. So, if the two scatter matrices V1 and V2 are of full ranks then, doing ICS on the initial data Xn or on the transformed X\u2217 n = XnA should lead to the same coordinates. However, if rank(V1) < p and if we use the pseudo-inverse V+ 1 then we lose this affine invariance property of the Invariant Components (IC). Indeed, in the simulated example, we obtain two different eigenvalues, \u03c11 = 1.1237 and \u03c12 = 1 instead of the two equal to one, as illustrated on the right panel on Figure 2. Obviously, projecting the data onto the eigenvectors\u2019 space leads to new scores, and the affine invariance of the coordinates is lost. To conclude, using the generalized inverse shows some differences. First, the initial ICS criterion (3) may be modified to the criterion (8), which leads to finding directions only on the subspace spanned by V1 and so the structure contained in the space spanned by null(V1) \u2212null(V2) cannot be highlighted. Second, if we use a generalized inverse, the coordinates are invariant up to an orthogonal transformation as for PCA but no longer to an affine transformation. This is unfortunate since an additional choice is required: standardize the data or not. Finally, the two scatter functionals V1 and V2 are not exchangeable anymore. Indeed, the directions found only span the range of the inverted scatter matrix. The results remain if V2 is singular or not.\n# 3.2. ICS with a dimension reduction as pre-processing\nAnother well-known approach consists of getting rid of the singularity issues by doing a reduction of dimension (DR) first, hoping that no information about the data structure will be lost. The idea is to perform a Singular Value Decomposition (SVD) of the initial data and to project it onto the right-singular vectors associated with the nonzero singular values. Among others, [22] or [12] use this pre-processing step before applying their outlier detection algorithms based on PCA or Mahalanobis distances. This rank reduction is also used for the LDA method in the HDLSS context, as explained by [19]. However, the performance of the preprocessing for the LDA method relies on the rank of the covariance matrix which has to fall into a specific range to ensure that the new within-covariance becomes non-singular. Another pitfall is noted by [37] who advised against using an SVD before a robust PCA in the presence of OC outliers.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/03bf/03bfb40d-da9c-4ff3-b265-b1260c6ce26e.png\" style=\"width: 50%;\"></div>\n# Considering the reduced data X\u2217 n = XnP1 \u2208Rn\u00d7rXn of rank rXn, we have to solve the GEP of V1(X\u2217 n) and V2(X\u2217 n\n# Considering the reduced data X\u2217 n = XnP1 \u2208Rn\u00d7rXn of rank rXn, we have to solve the GEP of V1(X\u2217 n) and V2(X\u2217 n) V2(X\u2217 n)b = \u03c1V1(X\u2217 n)b, (1\nConsidering the reduced data X\u2217 n = XnP1 \u2208Rn\u00d7rXn of rank rXn, we have to solve the GEP of V1(X\u2217 n) and V2(X\u2217 n): V2(X\u2217 n)b = \u03c1V1(X\u2217 n)b, (10\nV2(X\u2217 n)b = \u03c1V1(X\u2217 n)b,\n\ufffd \u2212\u00d7 \u2212\u00d7\u2212 \ufffd The elements of the diagonal matrix \u22061/2 are the square roots of the positive eigenvalues of XnX\u22a4 n and X\u22a4 n Xn. The columns of P are also the eigenvectors of X\u22a4 n Xn and the columns of U are the eigenvectors of XnX\u22a4 n . U and P can be partitioned as U = [U1 U2] where U1 is n \u00d7 rXn, U2 is n \u00d7 (n \u2212rXn) and P = [P1 P2] where P1 is p \u00d7 rXn and P2 is p \u00d7 (p \u2212rXn). U1 and P1 are both semi-orthogonal matrices and U1 (resp. P1) are orthonormal basis for the column space (resp. the row space) of Xn.\nRemark 5. If Xn is of full rank. If Xn is of full rank then performing an SVD as a preprocessing step before ICS leads to the same components as if we directly compute the invariant coordinates (IC) from the initial data Xn. Indeed, in this case, rank(Xn) = r = p, the data is transformed by a P non-singular orthogonal p \u00d7 p matrix and it is known that the invariant coordinates are invariant by an orthogonal transformation. However, from a computational point of view, some numerical discrepancies can arise due to the additional step.\nProperty 3. If V1 = COV and V2 is any scatter matrix as defined in (4), then performing ICS with the Moore-Penrose pseudoinverse of V1 or running ICS on the reduced data leads to the same coordinates up to their signs. Proof: Let us start with the initial EVP in 2: V2(Xn)b = \u03c1V1(Xn)b. By multiplying by P\u22a4and with b = P\u02dcb the equation can be rewritten as:\n# P\u22a4V2(Xn)P\u02dcb = \u03c1P\u22a4V1(Xn)P\u02dcb \u21d4V2(XnP)\u02dcb = \u03c1V1(XnP)\u02dcb,\nwith V(XnP) = \ufffd V(rXn)(XnP1) 0rXn\u00d7(p\u2212rXn) 0(p\u2212rXn)\u00d7rXn 0(p\u2212rXn)\u00d7(p\u2212rXn) \ufffd and V(rXn)(XnP1) = V(X\u2217 n). Using the notations introduced in Subsection 3.1 for the Moore-Penrose pseudo-inverse: V1(X\u2217 n) = \u039br1. This is because the right eigenvectors from the singular value decomposition of Xn are the ones of X\u22a4 n Xn = V1(Xn) = P1\u039br1P\u22a4 1 and V1(X\u2217 n) = P\u22a4 1 V1(Xn)P1 with P1 being a p \u00d7 rXn semi-orthognal matrix and so rXnik < p.\nwith \u02dcb = v1 + v0 with v1 \u2208range(Xn) and v0 \u2208null(Xn), and so if we restrict the solutions only onto the subsp spanned by range(X\u2217 n):\nwith \u02dcb = v1 + v0 with v1 \u2208range(Xn) and v0 \u2208null(Xn), and so if we restrict the solutions only onto the subspace spanned by range(X\u2217 ):\n(10)\nwith a = a1+a0 with a1 \u2208range(V1(Xn)) = range(Xn) and a0 \u2208null(V1(Xn)). So, after projecting, the new coordinates are the same up to their signs. Remark 8. So in this case, doing the pre-processing leads to an additional step to the method which is not needed and which implies the same drawbacks as doing ICS with a generalized inverse. Remark 9. From a practical point of view, estimating the rank of Xn might be very challenging as illustrated in Subsection 4.2 and lead to a loss of information regarding the structure of the data. To conclude, the preprocessing step of dimension reduction does not fulfill all its promises. First, it cannot guarantee that it solves the singularity issues of the scatter matrices. Then, even if it does, if we choose V1 as the variance-covariance matrix, we recover exactly the same modified criterion to solve as when we use the generalized inverse and so the same drawbacks. Finally, if we choose V2 as the variance-covariance matrix, we might be unable to recover the structure of the data if it is only contained on the subspace spanned by V1.\nwith a = a1+a0 with a1 \u2208range(V1(Xn)) = range(Xn) and a0 \u2208null(V1(Xn)). So, after projecting, the new coordina are the same up to their signs.\nRemark 8. So in this case, doing the pre-processing leads to an additional step to the method which is not neede and which implies the same drawbacks as doing ICS with a generalized inverse.\nRemark 9. From a practical point of view, estimating the rank of Xn might be very challenging as illustrated i Subsection 4.2 and lead to a loss of information regarding the structure of the data.\nTo conclude, the preprocessing step of dimension reduction does not fulfill all its promises. First, it canno guarantee that it solves the singularity issues of the scatter matrices. Then, even if it does, if we choose V1 as the variance-covariance matrix, we recover exactly the same modified criterion to solve as when we use the generalized inverse and so the same drawbacks. Finally, if we choose V2 as the variance-covariance matrix, we might be unable to recover the structure of the data if it is only contained on the subspace spanned by V1.\n# 3.3. ICS with a generalized singular value decomposition\nIn this section, we focus on an implementation based on a Generalized Singular Value Decomposition (GSVD) as proposed by [18\u201320] and [23] in the LDA context. More specifically, they use a GSVD for computing eigenvectors to define the Fisher\u2019s discriminant subspace, when the between \u03a3B and the within-group \u03a3W covariance matrices, are susceptible to be singular. The only requirement with this method is to express \u03a3B and \u03a3W as cross-product matrices, which is easily obtained by their definition. This procedure, which uses a GSVD to solve a GEP, can be applied to other scatter matrices which can be expressed as crossproducts. However, defining a stable algorithm for the GSVD is very challenging and a lot of research was done regarding this topic, such as [6\u20138, 15, 31, 32] among others. In this section, we present the GSVD procedure as it is given in [17, Section 75-11], restricted to the case of real matrices. We retain this definition since it is already implemented into LAPACK and can be used directly in R through the geigen [16] package. Let us define XV1 \u2208Rn\u00d7p s.t. X\u22a4 V1XV1 = V1 = V1(Xn) and XV2 \u2208Rn\u00d7p s.t. X\u22a4 V2XV2 = V2 = V2(Xn). V1, V2 \u2208SPp with rank(XV1) = rank(V1) = r1 \u2264p and rank(XV2) = rank(V2) = r2 \u2264p. The GSVD of XV1 and XV2 allows us to define the generalized eigenvalues and eigenvectors of the pencil X\u22a4 V2 XV2 \u2212\u03c1X\u22a4 V1XV1:\nBX\u22a4 V1XV1B\u22a4= BV1(Xn)B\u22a4= \ufffd0 0 0 D\u22a4 1 D1 \ufffd and BX\u22a4 V2XV2B\u22a4= BV2(Xn)B\u22a4= \ufffd0 0 0 D\u22a4 2 D2 \ufffd ,\n# BX\u22a4 V1XV1B\u22a4= BV1(Xn)B\u22a4= \ufffd0 0 0 D\u22a4 1 D1 \ufffd and BX\u22a4 V2XV2B\u22a4= BV2(Xn)B\u22a4= \ufffd0 0 D\nwhere XV1 = UD1[0 R]Q\u22a4, XV2 = VD2[0 R]Q\u22a4, U and V are n \u00d7 n, Q is p \u00d7 p, U, V and Q are orthogonal. R is r \u00d7 r, upper triangular and nonsingular with r = rank([X\u22a4 V1, X\u22a4 V2]\u22a4). [0 R] is r \u00d7 p (in other words, the 0 is an r \u00d7 (p \u2212r) zero matrix). D1 and D2 are n \u00d7 r. Both are real, nonnegative, and diagonal, satisfying D\u22a4 1 D1 + D\u22a4 2 D2 = Ir. Write D\u22a4 1 D1 = diag(\u03b12 1, . . . , \u03b12 r) and D\u22a4 2 D2 = diag(\u03b22 1, . . . , \u03b22 r), the ratios \u03b1 j/\u03b2 j for j = 1, . . . , r are called the generalized singular values. B\u22a4= Q \ufffdIn\u2212r 0 0 R\u22121 \ufffd . Note that the normalization is not the same as the one presented in the standard definition 2 but it can easily be adapted. Vectorially, it is equivalent to solving a modified version of the GEP (5): V2bi = \u03c1iV1bi, for i = 1, . . . , p:\n\u03b22 i V2bi = \u03b12 i V1bi \u21d4V2bi = \u03c1iV1bi, for i = 1, . . . , p.\nwhere \u03c1i = \u03b12 i /\u03b22 i is real, nonnegative and possibly infinite. The rows of B are the eigenvectors of X\u22a4 V2XV2 \u2212\u03c1X\u22a4 V1XV1 or equivalently of V2 \u2212\u03c1V1, and the \u201cnontrivial\u201d eigenvalues are the squares of the generalized singular values: \u03c1i = \u03b12 i /\u03b22 i , for i = p \u2212r + 1, . . . , p. The \u201ctrivial\u201d eigenvalues are those corresponding to the leading p \u2212r rows of B, which span the common null space of X\u22a4 V1XV1 and X\u22a4 V2XV2. These eigenvalues are not well defined and are not of interest. All the cases of interest are summarized in Table 1.\n(11)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/012a/012a4120-9ef2-4df2-b025-cd6339d6d6f2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">able 1: Summary of the different possible directions bi depending on the values of \u03b1i an</div>\nRe-writting the GEP (5) as in (11) presents some advantages compared to the other two methods. First, it allows to find all the directions which can reveal some structure of the data in the general case where V1 \u2208SPp and V2 \u2208SPp as summarized in the Table 1. Second, it is clear that V1 and V2 play a symmetric role. This is important since the other methods can miss the structure of the data if it is contained into the subspace spanned by V2 and in the nul space of V1 in particular. Third, this formulation is still equivalent to the classical EVP (2) if the two scatter matrices are of full ranks with \u03c1i = \u03b12 i /\u03b22 i . In addition to these nice characteristics, the invariant coordinates remain invarian by affine transformation.\nProperty 4. Affine invariance property. For two affine equivariant scatter matrices V1 and V2, and using the eigenvectors defined by (11), the inva coordinates are invariant by an affine transformation.\nExample 3. Let us take the same example as previously from Subsection 2.3. Using the GSVD of XV1 and XV2 to solve the GEP (11) leads to investigate four different cases for the direction b as illustrated in the left panel of Figure 3: \u2022 if b \u2208range(V1) \u2229range(V2) = range(W1), then the direction b is restricted to the subspace spans by W1 as when we use the Moore-Penrose pseudo-inverse and we obtain two eigenvalues equal to one, \u2022 if b \u2208null(V2) \u2212null(V1) = {0}, then no direction b exists, \u2022 if b \u2208null(V1) \u2212null(V2) = range(W2), then \u03c1 = \u221ebecause \u03b22 = 0 and so the direction b can highlight the structure of outlierness contained into the range(W2), which is not the case when we use the Moore-Penrose pseudo-inverse, \u2022 if b \u2208null(V1) \u2229null(V2) = null(V2), then \u03c1 is a \u201ctrivial\u201d eigenvalue and any direction b \u2208Rp is a solution. However, only the \u201cnon-trivial\u201d eigenvalues, corresponding to the first three cases, are interesting for highlighting the structure of the data. More precisely, in this example, only the eigenvector b \u2208null(V1) \u2212null(V2) = range(W2) associated with the infinite eigenvalue, contains the structure of outlierness of the data. This is clearly visible in Figure 3 which illustrates the projection of our simulated data onto the eigenvectors space. So, using the GSVD outperforms the use of a Moore-Penrose pseudo-inverse because it recovers the structure of outlierness of the data. In addition, in the right panel of Figure 3, we can see we obtain the same coordinates and eigenvalues if we transform the initial data by the non-singular matrix A. To conclude, solving the GEP of V1 and V2 through the GSVD of XV1 and XV2 presents three major advantages. First, it solves the possible singularity issues of V1 and/or V2 by searching in all directions, and remains equivalent to the EVP of V\u22121 1 V2 if the scatter matrices are of full ranks. In addition, it leads that V1 and V2 play a symmetric role since the symmetry of the problem is kept. The affine invariance property of the scores continues to be valid in the general case of semi-definite positive scatter matrices. Finally, it is interesting to note that this GSVD procedure is already implemented into the geigen R package. However, in practice, it is difficult to define another affine equivariant scatter estimator than the variance-covariance matrix. Overall, all three approaches present some advantages and limits as summarized in Table 2. The next section investigates if in practice those properties are kept.\n# 4. Empirical applications\nIn this section, we illustrate the characteristics of the different approaches on different empirical applications. We restrict our analysis to ICS with a generalized inverse (GINV), pre-processed by a dimension reduction (DR) through\n<div style=\"text-align: center;\">Optimization of the ICS ratio (3)</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4aa8/4aa8b3d5-9c77-4d91-921e-9bae6a920f49.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e4c/5e4c8c88-0343-4d0f-8c81-9869ac39e89e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3: Scatterplot matrix of the IC resulting of ICS using the GSVD of XV1 and XV2 on Xn (right panel) and on transformed data XnA (right panel).</div>\nMethods\nInvariance\nExchangeability\nof V1 and V2\nDirections found\nStability\nLimits\nVariable\nselection\n?\n?\n\u2286range(Xselected\nn\n)\nStable: \u223c\nChallenging to find the sub-\nspace\nof\ninterest\nof\nnon-\ncollinear variables.\nRegularized\nscatters\nNo\n?\n\u2286range(\n\u02dc\nV1(Xn))\nStable: \u223c\nMight be challenging to com-\npute with additional tuning pa-\nrameters. Not a scatter strictly\ncensus.\nGeneralized\ninverse\nOrtho.\nNo\n\u2286range(V1(Xn))\nStable: \u223c\nRank estimation is challenging.\nDo not solve the issue of com-\nputing V2(Xn).\nDimension\nreduction\nOrtho.\nNo\n\u2286range(V(rXn)(X\u2217\nn))\nQR[4]:\nstable\nRank estimation is challenging.\nQR[4] estimation is limited to\none-step M-scatter matrices. Do\nnot ensure that V1(X\u2217\nn) is not\nsingular.\nGSVD\nAffine\nYes\nAll\nStable\nOnly for scatter matrices which\ncan be expressed as crossprod-\nucts.\nGEP\nAffine\nYes\nAll\nUnstable\nCan lead to negative and com-\nplex eigenvalues.\nTable 2: Summary of the main characteristics of the different approaches for solving ICS when at least one scatter is singular. Ortho. stands fo orthogonal invariance. ? indicates that it depends on the situation and \u223cthat it depends on the algorithm.\na singular value decomposition and using the generalized value decomposition (GSVD). We exclude the direct GEP approach since it might result in complex and negative eigenvalues. First, Subsection 4.1 analyses the consequences\na singular value decomposition and using the generalized value decomposition (GSVD). We exclude the direct GEP approach since it might result in complex and negative eigenvalues. First, Subsection 4.1 analyses the consequences\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41ca/41ca25a0-165f-4f07-b89a-bae9d8e62205.png\" style=\"width: 50%;\"></div>\nof exchanging the role of V1 and V2 on a simulated correlated mixture of Gaussian distributions. Subsection 4.2 focuses on two examples for which estimating the rank of the data is challenging. Subsection 4.3 evaluates the impact of transforming the data through an affine transformation on a collinear industrial data set. Finally, Subsection 4.4 investigates if those approaches are applicable also in case of high dimension low sample size (HDLSS) with n < p.\nX \u223c\u03f51 N(\u00b51, Id) + \u03f52 N(\u00b52, Id),\n   with \u03f51 + \u03f52 = 1, \u00b51 = 0d, \u00b52 = (\u03b4, 0, . . . , 0)\u22a4where \u03b4 = 10. We generate n = 1000 observations on d = 3 variables for two balanced groups with \u03f51 = \u03f52 = 0.5. Two collinear variables are added: X4 = X2 \u22123X3 and X5 = X3 + 5X4.\n# 4.1.1. COV-COV4\n4.1.1. COV-COV4 First of all, we compare the different ICS methods for the scatter pair COV \u2212COV4. For GSVD, we consider GCOV4 since it is not possible to compute COV4(Xn) as Xn is not full rank and so COV(Xn) is singular and it cannot be inverted. Figure 4 shows the scatterplots matrix of the IC resulting of ICS with the generalized inverse of COV (GINV on 1st column), after a dimension reduction (DR on 2nd column) and with a generalized singular value decomposition (GSVD on 3rd column). The second row illustrates the same results when we exchange V1 and V2. It is interesting to note that depending on the method we do not obtain the same number of components: 5 with GINV and 3 for the other two. Indeed, with a DR the rank of the data is estimated to be 3 and so only three dimensions are kept. For GSVD, three non-trivial eigenvalues are also detected. For GINV, 5 components are illustrated but the last two are associated with almost zero eigenvalues: 1.5e\u221215 and \u22122.5e\u221218. Clearly, this indicates numerical issues and the last two components should be disregarded. Now, if we focus on the first three, we can notice that all the methods allow us to easily identify the two clusters on IC3. In addition, the components are the same between GINV and DR as mentioned in Property 3. Finally, if we exchange V1 and V2 then the clustering structure is shown on the first component for the three methods. So here, on an example of simple collinearity it appears that the three methods lead to similar results. The only point of attention is with GINV, where some trivial eigenvalues are estimated and should be put away.\n# 4.1.2. MCD0.5-COV\nFocusing on a different scatter pair based on a more robust scatter matrix such as the MCD0.5 raises several issues. Indeed, it is not possible to compute the MCD0.5 on our data because \u201cMore than half of the observations lie on a hyperplane\u201d, so GINV and GSVD are not applicable. Instead, we can perform ICS with the MCD on the reduced data or use its regularized version MRCD0.5 as shown in Figure 5. As previously, with the DR approach, only three dimensions are kept and the clusters are identifiable on the third IC. This is also true with the MRCD but five eigenvalues are estimated for MRCD0.5 \u2212COV or COV \u2212MRCD0.5. In addition, two of those eigenvalues are really small or high: 3.5e\u221215, \u22125.3e\u221215 and 6.7e+14, 1.9e+14. So in each case with MRCD0.5 some eigenvalues need to be disregarded and it is an additional step to take into account.\n4.2. Challenging estimation of the rank\n# 4.2. Challenging estimation of the rank\nThis section investigates the difficulty of correctly estimating the rank of two empirical applications: a nearl singular industrial data in Subsection 4.2.1 and some simulated data with OC outliers in Subsection 4.2.2.\n4.2.1. HTP3: nearly singular industrial data We consider the HTP3 data set, analyzed by [4] and available in the R package ICSOutlier[28]. It describes n = 371 high-tech parts designed for consumer products and characterized by p = 33 tests. The part 32 showed defects in use and is considered as an outlier. Here, the data set contains tests in different units which lead to nearly singularity and so the classical ICS algorithm returns an error. Using the ICSQR implementation presented in [4] solves the issue for a combination of scatter based on a one-step M-scatter matrix and COV. In Figure 6, we compute the so-called squared ICS distances [5], denoted ICSD2, of the k selected components for different approaches to perform ICS. On the first plot, we use ICSQR with only the first\n(12)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1ac/b1ac0ecd-936c-4d9f-9e88-a3c0e2db4f0f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/44ed/44ed86d4-6510-4a10-bb2e-a9a2a557aedf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a79a/a79abf8c-d452-43f2-81dc-0fc2f1cf3e21.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: Scatterplots matrix of the IC resulting of ICS of V1 and V2 using the generalized inverse of COV (GINV on 1st column), after a dimension reduction (DR on 2nd column) and with a generalized singular value decomposition (GSVD on 3rd column). The second row illustrates the same results when we exchange V1 and V2.</div>\n<div style=\"text-align: center;\">: Scatterplots matrix of the IC resulting of ICS of V1 and V2 using the generalized inverse of COV (GINV on 1st column), after a dimension ion (DR on 2nd column) and with a generalized singular value decomposition (GSVD on 3rd column). The second row illustrates the same s when we exchange V and V.</div>\n<div style=\"text-align: center;\">Fig. 4: Scatterplots matrix of the IC resulting of ICS of V1 and V2 using the generalized inverse of COV (GIN reduction (DR on 2nd column) and with a generalized singular value decomposition (GSVD on 3rd column). results when we exchange V1 and V2.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1daa/1daa0fc2-6696-476c-9d09-0e97c4c96535.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e73e/e73e96f6-69c9-4f19-b852-5286a5690b56.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: Scatterplots matrix of the IC resulting of ICS of MCD0.5 \u2212COV after a dimension reduction (DR on 1st column) and of MRCD0.5 \u2212CO (on 2nd column) and of COV \u2212MRCD0.5 (on 3rd column).</div>\ncomponent and the defective part (in orange) is clearly identified as having a high distance and so being an outlier compared to the other observations. We obtain similar results if we use the GSVD approach with COV \u2212COV4 or COV4 \u2212COV as illustrated in Appendix 10. However, GINV is not working in this context as we can never compute COV4. For the DR approach, a new challenge arises regarding the estimation of the rank of the SVD. As mentioned in [4], it is common practice to use a relative rule to estimate the rank based on the first eigenvalue \u03bbi with i = 1, . . . , p such as: (i) \u03bbi/\u03bb1 < \u221a(\u03bd) with the epsilon machine \u03bd = 2.2\u221216, (ii) \u03bbi/\u03bb1 < max(n, p)\u03bd or (iii) (\ufffdl i=1 \u03bb2 i )/(\ufffdp i=1 \u03bb2 i ) < 0.99, to explain at least 99% of the inertia as with PCA for example. Here, we obtain respectively a rank of 23, 33 or 3 based on the different criteria, meaning that we do not reduce the dimension in the second case. For the others, as we can see in the second column of Figure 6, the defective part is identified with rank = 23 only if we take two components, and it is not detectable in case of rank = 3. Considering different scatter pairs as the MCD0.5 is tricky because the scatter matrix cannot be computed on the reduced data. In this case, it is necessary to consider COV \u2212MCD0.5 and not MCD0.5 \u2212COV, but the results are not improved as visible in the\nAppendix 10. So on real datasets, the estimation of the rank for the DR approach can be very challenging and not the best approach.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3212/32126f70-c56d-488c-8c9a-390b3ec1c9d8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7ec/a7ec6a13-7f2f-4efb-8d39-807efa7e323b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: HTP3 data set: ICS distances, ICSD2, computed with k components for ICS with QR algorithm (1st panel), after DR with rank(Xn) =  (2nd panel) and with rank(Xn) = 3 (3rd panel). The defective part is in orange.</div>\n# 4.2.2. OC outliers\nThis issue for estimating the rank is even more problematic in case of the presence of OC outliers. We generate n = 100 observations following the projected mean-shift outlier model presented by [37] without noise: Xn = UDV\u22a4+ (1\u00b5\u22a4+ S )V\u22a4with random orthogonal U, V, p = 5, r = 3, D = diag{1000, 400, 200}, \u00b5 = 0, and the row outlier matrix S has first O rows as L \u2217[1, . . . , 1] and 0 otherwise, O = 4 and L = 3.5. In this context, the true rank of the data set is equal to 4 and so the classical ICS returns an error. With a DR step first, if we estimate the rank to 4 then ICS with COV \u2212COV4 identifies the outliers on IC1 as illustrated in Figure 7. However, it is not possible to compute a more robust scatter matrix like the MCD0.5. In this context, we can estimate the rank based on 95% of explained variance, leading to two dimensions but the information about the OC outliers is lost no matter the scatter pair. However, using a GSVD or GINV approach directly works fine as visible in the second and third plots of Figure 11 in Appendix 5. With MRCD \u2212COV the situation is a bit tricky because the outliers are found on IC2 as illustrated in Figure 7.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dee1/dee182b9-78a1-40d8-a2e5-74810c622a46.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fe9d/fe9def15-6f2f-4b67-b554-2033a3d13075.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 7: OC outliers: scatterplots matrix of the IC resulting of ICS of COV4 \u2212COV after a dimension reduction with rank = 4 (on 1st column rank = 2 (on 2nd column) and of MRCD0.5 \u2212COV (on 3rd column).</div>\n4.3. Impact of affine transformation: HTP2 - collinear industrial data We consider another industrial data set also analyzed by [4] and available in the R package ICSOutlier [28 called HTP2 to evaluate the impact of an affine transformation on the data such as the classical standardization. I contains n = 149 tests for p = 457 high-tech parts with a defective part at number 28. This data set is ill-conditioned and so the classical ICS algorithm returns an error as well as the ICSQR implementation and the GINV approach.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1e80/1e804c30-11a6-496f-97e8-5f1c8041d6dc.png\" style=\"width: 50%;\"></div>\nThe rank estimation for a DR is quite challenging and unstable. It is estimated to 138 with the first criterion mentioned in Subsection 4.2, 141 with the second and even to 1 with the third one based on the inertia. In addition, if the data is standardized then the two criteria lead to a rank of 141 and 51 for the third one. If we focus on the case of a rank of 138, then we cannot compute a robust scatter matrix like the MCD0.5. In Figure 8, we compute the ICSD2 based on IC1 and COV \u2212COV4 and the defective part is weirdly detectable as the observation having the smallest distance instead of the highest. However, doing it on the standardized data with 141 dimensions allows identifying it (see Figure 12 in Appendix 5). This behavior shows that the DR approach is sensible to the estimation of the rank and the standardization of the data. On the contrary, if we perform ICS of COV \u2212GCOV4 with GSVD on the initial data (2nd plot) or on the standardized one (3rd plot), then the outlier is revealed and its ICSD2 is stable between the two cases. It is noteworthy that the GSVD estimates 141 non-trivial eigenvalues. Finally, the regularized approach based on the MRCD does not identify the outlier (see Figure 12 in Appendix 5).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f97a/f97afdfe-dc40-48d1-a357-9efd9b0b9d3a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8: HTP2 data set: ICS distances, ICSD2, computed with k components for ICS of COVCOV4 after DR with rank(Xn) = 138 on initial data (1st panel), of COV \u2212GCOV4 with GSVD on initial data (2nd column) and on standardized data (3rd panel). The defective part is in orange.</div>\n<div style=\"text-align: center;\">Fig. 8: HTP2 data set: ICS distances, ICSD2, computed with k components for ICS of COVCOV4 after DR with rank(Xn) = 138 on initial data (1st panel), of COV \u2212GCOV4 with GSVD on initial data (2nd column) and on standardized data (3rd panel). The defective part is in orange.</div>\n# 4.4. High Dimension Low Sample Size (HDLSS) case\nTo go further, we generate some data in an HDLSS context with more variables p than observations n but not lying in general position. Following [37], we generate n = 50 observations on p = 100 variables such as in Subsection 4.2.2. In this context, we retrieve similar results as when n > p: the classical ICS and GINV are returning an error, the results depend on the estimated rank and GSVD is working fine as we can see in Figure 9 for COV\u2212COV4 and COV\u2212GCOV4. For the robust scatter matrix MCD0.5 it is not possible to compute it on reduced data of 4 components and the outliers are not visible if we kept only two dimensions (see Figure 13 in Appendix 5). The last plot shows that MRCD \u2212COV identifies the outliers, but on IC2 which is not what is expected.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/742d/742d3a36-620f-42d0-b3d0-283367f87b65.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1939/1939f4c9-acda-44c1-a5a5-6a44dc9bff09.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 9: OC outliers in HDLSS: scatterplots matrix of the IC resulting of ICS of COV4 \u2212COV after a dimension reduction with rank = 4 (on 1st column), of COV \u2212GCOV4 with GSVD (on 2nd column) and of MRCD \u2212COV (on 3rd column).</div>\n# 5. Conclusion\nFollowing previous ideas mainly used in LDA context, we proposed three new ways of generalizing ICS to the case of positive semi-definite scatter matrices based on generalized inverse, dimension reduction or GSVD. We also investigated their theoretical properties (summarized in Table 2) and provided implementations in R. Theoretically, the approach based on GSVD looks the most appealing by keeping all the nice properties of the classical ICS. In practice, this method can only deal with a specific type of scatter matrices that has to be expressed as crossproducts and being affine equivariant. Relaxing this affine invariance property of ICS, empirical results showed interesting and stable results for different situations for GSVD with COV \u2212GCOV4, even on standardized data. For DR, estimating the rank of the data appears to be the main challenge with quite some sensibility on the results. In addition, the method is only orthogonal invariant and does not ensure that some robust scatter matrices can be computed on the reduced data. Finally, GINV seems the least suitable as it requires that V2 can be computed on the initial data. Overall, those methods allow to generalize ICS to the context of positive semi-definite scatter matrices and might be also helpful in the HDLSS case as long as the data are not in general position. Depending on the approach, it is important to think about which scatter should be the first one and if some OC outliers are present. In practice, it might be useful to perform ICS multiple times, exchanging V1 and V2 and trying the different implementations to compare the results or performing localized projection pursuit after ICS as suggested by [11]. In the future, another idea could be to penalize or regularize ICS, as [14] or [44] do for LDA for example.\n# Computational details\nAll computations are performed with R version 4.3.3 [33] and uses the R packages ICS[27] for ICS, ICSClust[3] ICSOutlier[28], rrcov[40] for the MCD scatter matrix and geigen[16] for computing GSVD. Replication files ar available upon request.\n# Acknowledgements\nThis work is a generalization of the research conducted during my PhD under the supervision of Professor Anne Ruiz-Gazen whom I deeply thank for her guidance and insightful remarks on this topic.\n# Appendix A. Calculation details of the Moore-Penrose pseudo-inverse for Subsection 3.1\nV(X\u2217 n)+ has to satisfy the four conditions to be a Moore-Penrose pseudo-inverse: \u2022 Condition 1: V(X\u2217 n)V(X\u2217 n)+V(X\u2217 n) = V(X\u2217 n). \u2022 Condition 2: V(X\u2217 n)+V(X\u2217 n)V(X\u2217 n)+ = V(X\u2217 n)+. \u2022 Condition 3: (V(X\u2217 n)V(X\u2217 n)+)\u22a4= V(X\u2217 n)V(X\u2217 n)+. \u2022 Condition 4: (V(X\u2217 n)+V(X\u2217 n))\u22a4= V(X\u2217 n)+V(X\u2217 n). The proof of conditions 1 and 2 can be generalized to any matrix A but conditions 3 and 4 rely on the assumption of orthogonality of A. Indeed, for condition 3, we have (V(X\u2217 n)V(X\u2217 n)+)\u22a4= (A\u22a4)\u22121V1(Xn)V1(Xn)+A\u22a4. Since A is orthogonal (A\u22a4)\u22121 = A and V1(Xn)+ = AV1(Xn)+A\u22a4, so we obtained the desired equality:\nThe proof is similar for the condition 4.\n# Appendix B. Affine invariance for ICS with a Generalized Singular Value Decomposition for Subsection 3.3\nAppendix B. Affine invariance for ICS with a Generalized Singular Value Decomposition for Subse Proof: (i) Adaptation of the proof from [43], appendix A.1, for distinct roots. Let X\u2217 n = XnA + 1n\u03b3\u22a4, with \u03b3 \u2208Rp. Then V1(X\u2217 n) = A\u22a4V1(Xn)A and V2(X\u2217 n) = A\u22a4V2(Xn)A.\nProof: (i) Adaptation of the proof from [43], appendix A.1, for distinct roots. Let X\u2217 n = XnA + 1n\u03b3\u22a4, with \u03b3 \u2208Rp. Then V1(X\u2217 n) = A\u22a4V1(Xn)A and V2(X\u2217 n) = A\u22a4V2(Xn)A.\n\u03b12 i (Xn)V2(Xn)bi(Xn) = \u03b22 i (Xn)V1(Xn)bi(Xn). )\u22a4x\u2217= (A\u22121bi(Xn))\u22a4A\u22a4x = bi(Xn)\u22a4x = zi.\n\u03b12 i (Xn)V2(Xn)bi(Xn) = \u03b22 i (Xn)V1(Xn)bi(Xn). ojection onto bi(X\u2217 n): z\u2217 i = bi(X\u2217 n)\u22a4x\u2217= (A\u22121bi(Xn))\u22a4A\u22a4x = bi(Xn)\u22a4x = zi.\n(ii) Proof from [43], appendix A.1, for multiple roots. In case of a multiple root of multiplicity pl, the eigenvectors are not uniquely defined and can be chosen as any linearly independent vectors spanning the corresponding pl-dimensional eigenspace. However the roots are still the same and so the subspace spanned by the corresponding pl-dimensional eigenspace is still the same.\nIn case of a multiple root of multiplicity pl, the eigenvectors are not uniquely defined and can be chosen as any linearly independent vectors spanning the corresponding pl-dimensional eigenspace. However the roots are still the same and so the subspace spanned by the corresponding pl-dimensional eigenspace is still the same. The case of multiple roots may appear when V1 \u2208SPp and/or V2 \u2208SPp as it means than null(V1) \ufffd{0} and/or null(V2) \ufffd{0}. For example, if we only focus on the cases where B \u2208null(V2) \u2212null(V1) or B \u2208null(V1) \u2212null(V2), if dim(null(V2) \u2212null(V1)) > 1 and/or dim(null(V1) \u2212null(V2)) > 1 then 0 and/or \u221eare multiple roots.\nThe case of multiple roots may appear when V1 \u2208SPp and/or V2 \u2208SPp as it means than null(V1) \ufffd{0} and/or null(V2) \ufffd{0}. For example, if we only focus on the cases where B \u2208null(V2) \u2212null(V1) or B \u2208null(V1) \u2212null(V2), if dim(null(V2) \u2212null(V1)) > 1 and/or dim(null(V1) \u2212null(V2)) > 1 then 0 and/or \u221eare multiple roots.\n# pendix C. Additionnal results for the empirical applications in Sec\nWe display additional results regarding Subsection 4.2.1 in Figure 10, Subsection 4.2.2 in Figure 11, Subsection 4.3 in Figure 12 and Subsection 4.4 in Figure 13.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/63db/63db34d6-280b-4494-877b-a5b9b6a77e0c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 10: HTP3 dataset: ICS distances, ICSD2, computed with k components for ICS with GSVD of COV \u2212GCOV4 (1st panel), GCOV4 \u2212COV (2nd panel) and with COV \u2212MCD0.5 after DR (3rd panel). The defective part is in orange.</div>\n# References\n[1] A. Alfons, A. Archimbaud, K. Nordhausen, A. Ruiz-Gazen, Tandem clustering with invariant coordinate selection, Econometrics and Statistics In press (2024). [2] A. Archimbaud, M\u00b4ethodes statistiques de d\u00b4etection d\u2019observations atypiques pour des donn\u00b4ees en grande dimension, Th`ese, Universit\u00b4e Toulouse 1 Capitole, 2018. [3] A. Archimbaud, A. Alfons, K. Nordhausen, A. Ruiz-Gazen, ICSClust: Tandem Clustering with Invariant Coordinate Selection, 2023. R package version 0.1.0. [4] A. Archimbaud, Z. Drma\u02c7c, K. Nordhausen, U. Radoji\u02c7ci\u00b4c, A. Ruiz-Gazen, Numerical Considerations and a new implementation for invariant coordinate selection, SIAM Journal on Mathematics of Data Science 5 (2023) 97\u2013121. [5] A. Archimbaud, K. Nordhausen, A. Ruiz-Gazen, ICS for multivariate outlier detection with application to quality control, Computational Statistics and Data Analysis 128 (2018) 184\u2013199. [6] Z. Bai, The CSD, GSVD, Their Applications and Computations, IMA Preprint Series (1992). Publisher: Minneapolis, MN, USA: University of Minnesota.\n[1] A. Alfons, A. Archimbaud, K. Nordhausen, A. Ruiz-Gazen, Tandem clustering with invariant coordinate selection, Econometrics and Statistics In press (2024). [2] A. Archimbaud, M\u00b4ethodes statistiques de d\u00b4etection d\u2019observations atypiques pour des donn\u00b4ees en grande dimension, Th`ese, Universit\u00b4e Toulouse 1 Capitole, 2018. [3] A. Archimbaud, A. Alfons, K. Nordhausen, A. Ruiz-Gazen, ICSClust: Tandem Clustering with Invariant Coordinate Selection, 2023. R package version 0.1.0. [4] A. Archimbaud, Z. Drma\u02c7c, K. Nordhausen, U. Radoji\u02c7ci\u00b4c, A. Ruiz-Gazen, Numerical Considerations and a new implementation for invariant coordinate selection, SIAM Journal on Mathematics of Data Science 5 (2023) 97\u2013121. [5] A. Archimbaud, K. Nordhausen, A. Ruiz-Gazen, ICS for multivariate outlier detection with application to quality control, Computational Statistics and Data Analysis 128 (2018) 184\u2013199. [6] Z. Bai, The CSD, GSVD, Their Applications and Computations, IMA Preprint Series (1992). Publisher: Minneapolis, MN, USA: University of Minnesota.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b6d7/b6d74b42-0e14-44c0-a4e5-65f618af65b7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9f0/e9f0135f-8df3-4c5a-a299-bb4b238e4d01.png\" style=\"width: 50%;\"></div>\n",
    "paper_type": "method",
    "attri": {
        "background": "Invariant Coordinate Selection (ICS) is a powerful unsupervised multivariate method designed to identify the structure of multivariate datasets on a subspace. It relies on the joint diagonalization of two affine equivariant and positive definite scatter matrices V1 and V2. However, its classical implementation faces limitations when at least one of the scatter matrices is singular, particularly in cases of collinearity or high dimension low sample size contexts. This paper proposes three approaches to generalize ICS to address these limitations.",
        "problem": {
            "definition": "The problem addressed is the challenge of performing ICS when at least one of the scatter matrices is singular, which prevents the computation of the generalized eigenvalue problem (GEP). This situation is common in multivariate analysis, especially with high-dimensional data.",
            "key obstacle": "The core obstacle is the non-invertibility of scatter matrices, which restricts the application of traditional methods that rely on the positive definiteness of these matrices."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to adapt existing methods to handle singular scatter matrices, which are increasingly common in contemporary data analysis due to collinearity and high-dimensional datasets.",
            "opinion": "The proposed idea involves three new methods: using the Moore-Penrose pseudo-inverse, dimension reduction techniques, and generalized singular value decomposition (GSVD) to extend ICS's applicability.",
            "innovation": "The primary innovation lies in the ability to perform ICS in the presence of singular scatter matrices, which is a significant advancement over traditional methods that require positive definiteness."
        },
        "method": {
            "method name": "Generalized Invariant Coordinate Selection",
            "method abbreviation": "GICS",
            "method definition": "GICS refers to the adaptation of the ICS method to allow for the use of positive semi-definite scatter matrices through three distinct approaches.",
            "method description": "The method involves the application of three techniques: Moore-Penrose pseudo-inverse, dimension reduction, and generalized singular value decomposition (GSVD) to generalize ICS.",
            "method steps": [
                "Identify scatter matrices V1 and V2.",
                "Determine if either scatter matrix is singular.",
                "Apply the appropriate approach (Moore-Penrose pseudo-inverse, dimension reduction, GSVD) based on the properties of the scatter matrices."
            ],
            "principle": "The method is effective because it allows for the extraction of meaningful structure from data that would otherwise be unusable due to singularity issues in the scatter matrices."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using various datasets, including those with known collinearity and high-dimensional data, to evaluate the performance of the proposed methods against traditional ICS.",
            "evaluation method": "The performance was assessed by comparing the results of the new methods with traditional ICS, focusing on the ability to recover meaningful structures in the data despite the presence of singular scatter matrices."
        },
        "conclusion": "The proposed methods for generalizing ICS to accommodate positive semi-definite scatter matrices demonstrate promising results, particularly the GSVD approach, which maintains desirable properties of the classical ICS while effectively addressing singularity issues.",
        "discussion": {
            "advantage": "The main advantage of the proposed methods is their ability to effectively handle singular scatter matrices, allowing for robust analysis of high-dimensional datasets.",
            "limitation": "A limitation of the methods is that they may require careful consideration of the choice of scatter matrices, and the performance can be sensitive to the specifics of the data.",
            "future work": "Future research could explore the regularization of the ICS methods, the development of more robust scatter estimators, and the application of these methods in various fields of data science."
        },
        "other info": {
            "computational details": "All computations were performed using R version 4.3.3 with relevant packages for ICS and GSVD implementations.",
            "acknowledgements": "The research was conducted under the supervision of Professor Anne Ruiz-Gazen, who provided guidance and insights throughout the study."
        }
    },
    "mount_outline": [
        {
            "section number": "2",
            "key information": "Invariant Coordinate Selection (ICS) is a powerful unsupervised multivariate method designed to identify the structure of multivariate datasets on a subspace."
        },
        {
            "section number": "2.1",
            "key information": "Key terms include Invariant Coordinate Selection (ICS), multivariate datasets, scatter matrices, and positive definiteness."
        },
        {
            "section number": "3",
            "key information": "The proposed methods for generalizing ICS involve using the Moore-Penrose pseudo-inverse, dimension reduction techniques, and generalized singular value decomposition (GSVD)."
        },
        {
            "section number": "3.1",
            "key information": "The method Generalized Invariant Coordinate Selection (GICS) adapts ICS to allow for the use of positive semi-definite scatter matrices."
        },
        {
            "section number": "4.1",
            "key information": "The main advantage of the proposed methods is their ability to effectively handle singular scatter matrices, allowing for robust analysis of high-dimensional datasets."
        },
        {
            "section number": "7",
            "key information": "Future research could explore the regularization of the ICS methods, the development of more robust scatter estimators, and the application of these methods in various fields of data science."
        },
        {
            "section number": "8",
            "key information": "The proposed methods demonstrate promising results, particularly the GSVD approach, which maintains desirable properties of the classical ICS while effectively addressing singularity issues."
        }
    ],
    "similarity_score": 0.5857719061779165,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-2055_semi-/papers/Generalized implementation of invariant coordinate selection with positive semi-definite scatter matrices.json"
}