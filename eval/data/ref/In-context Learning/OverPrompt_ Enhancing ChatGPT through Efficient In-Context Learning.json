{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.14973",
    "title": "OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning",
    "abstract": "The remarkable performance of pre-trained large language models has revolutionised various natural language processing applications. Due to huge parameter sizes and extensive running costs, companies or organisations tend to transfer the models to the target task by zero-shot prompting techniques. However, the prohibitive costs of tokens and time have hindered their adoption in applications. We propose OverPrompt, leveraging the in-context learning capability of LLMs to handle multiple task inputs, thereby reducing token and time costs. This approach could potentially improve task performance during API queries due to better conditional distribution mapping. Evaluated across diverse classification datasets, our experiments show that OverPrompt can achieve cost-efficient zero-shot classification without causing significant detriment to task performance, and in some cases, even improving it. An ablation study conducted on various LLMs, along with an investigation into the robustness of our prompting strategy to different input ordering, offers valuable insights into the broader applicability of our method across diverse tasks. These findings also suggest a more seamless integration of our method with LLMs through an API.",
    "bib_name": "li2023overpromptenhancingchatgptefficient",
    "md_text": "# OverPrompt: Enhancing ChatGPT through Efficient In-Context Learning\n# Jiazheng Li*1, Runcong Zhao*1, Yongxin Yang3, Yulan He1,2, and Lin Gui1\n1Department of Informatics, King\u2019s College London, UK 2The Alan Turing Institute, UK 3Queen Mary University of London, UK\n# Abstract\nThe remarkable performance of pre-trained large language models has revolutionised various natural language processing applications. Due to huge parameter sizes and extensive running costs, companies or organisations tend to transfer the models to the target task by zero-shot prompting techniques. However, the prohibitive costs of tokens and time have hindered their adoption in applications. We propose OverPrompt, leveraging the in-context learning capability of LLMs to handle multiple task inputs, thereby reducing token and time costs. This approach could potentially improve task performance during API queries due to better conditional distribution mapping. Evaluated across diverse classification datasets, our experiments show that OverPrompt can achieve cost-efficient zero-shot classification without causing significant detriment to task performance, and in some cases, even improving it. An ablation study conducted on various LLMs, along with an investigation into the robustness of our prompting strategy to different input ordering, offers valuable insights into the broader applicability of our method across diverse tasks. These findings also suggest a more seamless integration of our method with LLMs through an API.\narXiv:2305.14973v2\n# 1 Introduction\nLarge Language Models (LLMs), such as ChatGPT (Ouyang et al., 2022), have shown emergent abilities on various Natural Language Processing (NLP) tasks (Wei et al., 2022). In-context learning (ICL) approaches, including zero-shot or few-shot prompting strategies (Kojima et al., 2022; Brown et al., 2020; Kaplan et al., 2020), offering computational efficiency and considerable performance gains without the need of fine-tuning LLMs. Various research has been carried out to improve the performance of zero/few-shot learning, such as searching for better few-shot examples (Zhang et al., 2023; Wang et al., 2023b) or finding appropriate prompts (Wang et al., 2023c; White et al., 2023). However, cost-efficient prompting strategies are under-explored. Given the immense parameter size of LLMs, deploying them locally for certain industrial applications becomes impractical. Consequently, the substantial time and token costs associated with accessing these models through APIs present a significant challenge when adopting these models in production environments. While few-shot prompting entails the inclusion of demonstration examples, thereby raising the token cost of API queries, in most zero-shot prompting cases (Mialon et al., 2023; White et al., 2023), the bulk of the input content is allocated to the task description, leaving only a portion for the task input. The repetition of the task description can result in a substantial cumulative cost for each individual query. Hence, it becomes imperative to reduce the token and time costs associated with utilising these LLMs.\n*Equal contribution. Email: {jiazheng.li, runcong.zhao}@kcl.ac.uk\nR0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023.\nR0-FoMo: Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models at NeurIPS 2023\nTo address this issue, we propose OverPrompt, a zero-shot prompting strategy designed to process multiple instances simultaneously in a single query to enhance efficiency. Leveraging the emergent capability of LLMs, known as ICL, we analyse our prompting strategy within a Bayesian inference framework. Theoretically, our prompting strategy ensures better approximation of input task distributions by incorporating additional data and mitigating format errors. We also empirically show that our designed sampling and formatting framework enhances performance. In order to understand the overall impact of OverPrompt on query efficiency, we evaluate OverPrompt across ten different text classification datasets. Our experiments reveal that OverPrompt reduces both token and time costs, while leveraging the in-context learning capabilities of LLMs to produce improved conditional distributions for tasks when additional instances are provided. We also modify the output formatting to address performance degradation and reduce errors. Performance enhancements are observed when contextual information supplements the model\u2019s decision-making process. This is particularly useful in tasks such as fact-checking, where extra evidence or logical deductions can be provided, and sentiment analysis, where well-defined category boundaries can be established through comparison. Nevertheless, tasks like sentence entailment may not gain any advantage from such context input1.\n# 2 OverPrompt\nIn this section, we introduce OverPrompt, a zero-shot classification strategy that utilizes ChatGPT\u2019s ICL ability for more efficient zero-shot classification, reducing token consumption and time costs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3e46/3e4625e2-974c-40e2-98f4-15c64ee33baa.png\" style=\"width: 50%;\"></div>\nFigure 1: This illustration highlights the difference between traditional zero-shot classification prompting strategy and our OverPrompt. Deploying LLMs requires significant computational resources, so abandoning API queries is not practical. Our OverPrompt strategy prioritizes cost efficiency while maintaining task performance. It achieves this by reusing task descriptions and batch-processing task inputs, which reduces token usage and the number of API calls. In a text classification setup, let X = {xi}N i=1 be the text input, and Y = {yi}N i=1 be the associated label set for the given input. The label yi belongs to a category set C = {ci}m i=1. For each prompt, the dataset element xi is incorporated into the task-related description dt, which introduces the target category set for label prediction. For instance, in the SST-2 dataset, the task description is \"Sentiment Analysis\", and the label set C comprises \"positive\" and \"negative\". We use \u02c6yi to denote the predicted label for a given task input xi and highlight the prediction in blue. Figure 1 shows a traditional zero-shot classification prompt template on the left-hand side (Wang et al., 2023a). However, this template has two main limitations when it is applied: Firstly, it requires time-consuming iterations to process each task input, which can be expensive considering the delay of internet connections. Secondly, the current zero-shot classification paradigm only considers the task description dt while ignoring connections between the task inputs. Previous research has not explored how multiple unlabelled task inputs might aid LLMs in using their ICL capability to determine a suitable task-related batch grouping, which might lead to a more accurate output generation Building on the work of Xie et al. (2022), who used Bayesian inference to interpret the ICL capability of LLMs, we assume an LLM can perfectly fit the pre-training distribution p\u03b8 with sufficient data, i.e., pLLM = p\u03b8. The crux is to extract the hidden concept \u03b8\u2217from the given prompt (dt, Xj) and use it to derive the conditional distribution p\u03b8\u2217(yi|xi), where xi \u2208Xj. Our aim is to investigate whether argmaxyip\u03b8(yi|dt, Xj \\ xi, xi) \u2192argmaxyip\u03b8\u2217(yi|xi) as n increase. In other words, we 1Our code is avaliable at https://github.com/lijiazheng99/OverPrompt.\n1Our code is avaliable at https://github.com/lijiazheng99/OverPrompt\nwant to understand whether the LLM becomes more effective at making a more accurate predictions on the testing examples when provided with a larger set of samples Xj to aid in the inference of the prompt concept \u03b8\u2217. This convergence holds under a distinguishability condition wherein \u03b8\u2217is distinguishable:\nwhere the k is number of task inputs, \u03f5\u03b8 start and \u03f5\u03b8 delim represent errors of mismatches between the prompt and pre-training distributions and the delimiter token for each task input, which is bounded by the KL-divergence of corresponding difference of prompt and pre-trained distributions. Obviously, the right-hand-side of Eq.1 increases with a larger number of examples k, improving distinguishability (Xie et al., 2022). In other words, the context of task inputs, beyond just the input-output mapping, can be valuable for ICL. Therefore, we introduce OverPrompt, a zero-shot classification prompt strategy that utilises LLMs\u2019 emergent ICL capability by increasing the number of task inputs included in the prompt to n, as shown on the right-hand-side of the Figure 1. Our proposed strategy involves finding a partition of input text set X where \ufffd j=1 Xj = X and \ufffd j=1 Xj = \u03d5. The predicted labels can be obtained by LLM( \u02c6Yj|dt, Xj), where \u02c6Yj represents the predicted labels corresponding with input texts Xj. This strategy enables LLMs to handle multiple inputs simultaneously. Our experiments in \u00a73.1.1 demonstrate that this approach significantly reduces query and lag time by reducing the number of API requests. Additionally, OverPrompt reduces the number of input tokens due to the shared prompt information base (e.g., task description dt, the label set C), resulting in lower token usage costs. Besides, comprehensively, prompt grouping can provide task-specific hints. This is because taskspecific tokens usually appear more often in the inputs than general corpora. Our proposed strategy amplifies these informative words by grouping together semantically similar instances (grp), which is able to help concentrate p(\u03b8|dt, Xj) on the prompt concept with more examples. This, in turn, facilitates \u201clocating\u201d the concept \u03b8\u2217. We also propose additional grouping strategies for ablation studies: (a) mixing these topics with random samples (mix), and (b) filtering mix to keep only topic-specific instances (fil). We provide detailed comparisons in \u00a73.1.2. OverPrompt can improve the performance of semantic meaning focused classification tasks like sentiment analysis, or fact-checking. However, as demonstrated in section A.2, adding more training instances does not always lead to better results for inferencing-related tasks. The i.i.d nature of training examples can cause unnatural transitions when randomly concatenated, which introduces noise and mismatches between the pre-training and prompt distributions. This can have a negative impact on performance, as observed in natural language inference tasks.\nOutput Formatting: Mitigating Performance Degradation and Errors As the number of outputs increases, we may encounter issues with inconsistencies in output formatting, resulting in a mismatch error. While most inconsistencies can be resolved using rule-based post-processing methods, mismatches where the number of outputs does not match the number of inputs cannot be fixed this way. In order to avoid confusion and provide a clearer delineation, we use input indices and JSON formatting. For example, instead of using a prompt like \u201cGive me the labels only\u201d, we use \u201cReturn in JSON format, such as: {\"1\": \"c1\", \"2\":\"c2\"}\u201d. Here, c1 and c2 are arbitrary labels from the set C. We avoid specifying the full format (e.g., {\"1\": \"c1\", \"2\":\"c2\", ..., \"n\":\"cn\"}) to reduce time and token consumption. This succinct prompt allows for correct output formatting without compromising predictive performance.\n# 3 Experiment\nWe provide detailed experimental setup: datasets, parameters setting and evaluation metrics in \u00a7A.1.\nWe provide detailed experimental setup: datasets, parameters setting and evaluation metrics in \u00a7A.1.\n# 3.1 Overall Analysis\nIn order to evaluate the effectiveness and cost of the OverPrompt strategy, we conducted experiments on three different classification datasets: Fever, Vitamin C, and HoVer. To measure efficiency, we\n(1)\ncalculated the average time required per instance, denoted as ctime = t N , and the average token cost per query, denoted as ctoken = #token N , under two different settings: traditional zero-shot prompting (one instance per query), and OverPrompt (multiple instances per query). Here, t represents the total time taken to run the entire dataset, and N represents the number of data points in the dataset. We increased the number of instances requested per query, with settings at n=1 (traditional zero-shot setting), n=10, and n=20.\nDataset\nTime\nToken\nn=1\nn=10\nn=20\nn=1\nn=10\nn=20\nFever\n1.3751\n0.5010\n0.3579\n100.51\n63.07\n60.79\nVitaminC\n1.0753\n0.3950\n0.3298\n110.15\n69.65\n67.40\nHoVer\n1.7366\n0.4997\n0.4639\n65.03\n38.93\n37.48\non of average time cost (in seconds) and average token\n# 3.1.1 Efficiency and Cost Comparison\nThe efficiency of our OverPrompt strategy is demonstrated in Table 1, which shows that as the number of prompts increases, the average time requirement generally decreases, regardless of the dataset. This is because the latency time for processing longer input by the language model is shorter than the time for API requests. Therefore, OverPrompt becomes more time efficient as the number of inputs increases, since the model only needs to process the task description in the prompt message once for each batch of n inputs. Similarly, the token cost per request decreases as the number of prompts increases across all three datasets. This reduction can be attributed to the token cost of the task description in the prompt being averaged across an increasing number of instances. Therefore, compared to the traditional zero-shot prompting strategy, each OverPrompt request with n inputs can omit n \u22121 task descriptions.\nDataset\nAccuracy\nMacro-F1\nn=1\nn=10\nn=20\nn=1\nn=10\nn=20\nFever\n0.6830\n0.7413\n0.7843\n0.4321\n0.4913\n0.5226\nVitaminC\n0.5235\n0.5440\n0.5465\n0.3883\n0.4945\n0.4969\nHoVer\n0.5452\n0.5347\n0.5385\n0.3305\n0.5106\n0.3364\nmparison of classification accuracy and Macro-F1 under differe\n# 3.1.2 Performance Evaluation Results\nTable 2 shows that the OverPrompt strategy may improve the task performance as the number of instances increases. For instance, in the Fever and Vitamin C datasets, OverPrompt achieves the highest accuracy when n=20, with values of 78.43% and 54.65%, respectively. However, in the HoVer dataset, the n=1 (traditional zero-shot prompting) setting outperforms the others, reaching an accuracy of 54.52%. Additionally, the Fever and Vitamin C datasets reached their peak Macro-F1 scores at n=20, with scores of 52.26 and 49.69, respectively. On the other hand, in the HoVer dataset, n=10 yields the highest Macro-F1 score (51.06), differing from the observed accuracy trend where the zero-shot setting was superior. We found that certain claims in all three fact-checking datasets may based on related content. For instance, in the HoVer dataset, \u201cSkagen Painter, who painted the 1893 painting Roses, favored naturalism. Theodor Esbern Philipsen and the artist that Ossian Elgstr\u00f6m studied with in 1907 also favored naturalism.\u201d and \u201cSkagen Painter Peder Severin Kr\u00f8yer favored naturalism along with Theodor Esbern Philipsen and Kristian Zahrtmann.\u201d are related. Grouping these similar claims can help LLMs use their ICL abilities to improve performance. The number of similar cases varies in different datasets, which is the potential reason that the optimal n varies for different datasets. Other Text Classification Tasks We have evaluated the performance of the OverPrompt strategy on three distinct text classification tasks: sentiment analysis, natural language inference, and opinion analysis. The strategy showed a significant increase in efficiency and cost reduction across multiple datasets. OverPrompt was able to constantly reduce time and token costs due to its batch processing ability. Moreover, we observed a steady improvement in performance when we enlarged the number\nOther Text Classification Tasks We have evaluated the performance of the OverPrompt strategy on three distinct text classification tasks: sentiment analysis, natural language inference, and opinion analysis. The strategy showed a significant increase in efficiency and cost reduction across multiple datasets. OverPrompt was able to constantly reduce time and token costs due to its batch processing ability. Moreover, we observed a steady improvement in performance when we enlarged the number\nDataset\nTask\nAccuracy\nMacro-F1\nn=1\nn=10\nn=20\nn=1\nn=10\nn=20\nSST-2\nSentiment Analysis\n0.9197\n0.9461\n0.9495\n0.9197\n0.9455\n0.9495\nRTE\nNatural Language Inference\n0.7365\n0.7870\n0.8231\n0.7358\n0.7848\n0.8204\nMPQA\nOpinion Analysis\n0.5931\n0.6164\n0.6223\n0.4020\n0.4118\n0.4139\nTable 3: Comparison of applied OverPrompt across other classification tasks. Similar to our observation on fact-checking datasets, OverPrompt can achieve better task performance on both accuracy and f1 score while reducing time and token costs.\nDataset\nTime\nToken\nn=1\nn=10\nn=20\nn=1\nn=10\nn=20\nSST-2\n0.9777\n0.2740\n0.1278\n52.52\n30.43\n29.07\nRTE\n2.3654\n0.3480\n0.3010\n110.88\n81.84\n79.35\nMPQA\n0.9080\n0.2782\n0.2438\n68.43\n38.95\n37.76\nComparison of average time cost (in seconds) and average token costs per task i\nDataset\nTime\nToken\nn=1\nn=10\nn=20\nn=1\nn=10\nn=20\nSST-2\n0.9777\n0.2740\n0.1278\n52.52\n30.43\n29.07\nRTE\n2.3654\n0.3480\n0.3010\n110.88\n81.84\n79.35\nMPQA\n0.9080\n0.2782\n0.2438\n68.43\n38.95\n37.76\n of average time cost (in seconds) and average token\nof instances in each prompt, as shown in Table 3. These results highlight the trend that increasing the number of prompts may enhance the task performance of ChatGPT. Our method exhibits potential performance improvements (Table 4), and this pattern extends to various text classification tasks. We believe that this enhanced effectiveness is due to LLM\u2019s ICL ability, where more task inputs may help the models distinguish between classification instances more easily.\n# 3.2 Case Studies\nIn this section, we offer case studies to interpret the phenomena behind the potential performance improvement of the OverPrompt strategy.\nRefute \nNot Enough Info \nNot Enough Info \nSupport\n VitaminC Dataset\n[Claim]: Dragon Con had less than 1000 guests. \n[Evidence]: Among the more than 512 guests and \nmusical performers at the 2009 convention were such \nnotables as Patrick Stewart, William \u2026\n[Claim]: Dragon Con had less than 1000 guests ., \n[Evidence]: \u2026 more than 6000 guests \u2026\n[Claim]: Dragon Con had  over 5000 guests ., \n[Evidence]: \u2026 more than 512 guests \u2026\n[Claim]: Dragon Con had  over 5000 guests ., \n[Evidence]: \u2026 more than 6000 guests \u2026\nClassical Zero-shot \nPlease read through this pair of claim and \nevidence:  \n[Claim]: Dragon Con\u2026 \n[Evidence]: Among the more than\u2026 \nand determine whether the evidence \n\"support\", \"refute\" the claim, or \"not \nenough info\" to decide which category it \nfall into. \nGive me the label only:\n\u2026\nOverPrompt \nPlease read through these pairs of claim \nand evidence:  \n1. [Claim]:\u2026 [Evidence]:\u2026 \n    \u2026 \nn. [Claim]:\u2026 [Evidence]:\u2026 \nand determine whether the evidence \n\"support\", \"refute\" the claim, or \"not \nenough info\" to decide which category it \nfall into. \nReturn in JSON format, such as: {\"1\": \n\u201cc_j\", \"2\":\"c_k\"}:\nNot Enough Info\nRefute\nSupport                             \nSupport \nSupport \nSupport\nSingle Instance Input\nBatch Process Multiple Instances Input\nFigure 2: Illustration of ChatGPT struggling with similar sentences when input individually. Employing the OverPrompt strategy and cohesively grouping synthetic data from the \"VitaminC\" dataset may improve the performance of zero-shot inference.\nExamples\nMixed\nGrouped\nEvidence\nSamsung entered the electronics industry in the late 1960s and the construction and shipbuilding\nindustries in the mid-1970s; these areas would drive its subsequent growth.\nClaims\nSamsung entered the electronics industry in the late 1970s.\nSUPPORTS \u2717\nREFUTES \u2713\nSamsung never entered the shipbuilding industries.\nSUPPORTS \u2717\nREFUTES \u2713\nSamsung entered the construction and shipbuilding industries in the mid-1950s.\nSUPPORTS \u2717\nREFUTES \u2713\nSamsung exited the construction and shipbuilding industries in the mid-1970s.\nSUPPORTS \u2717\nREFUTES \u2713\nSamsung never entered the electronics industry.\nSUPPORTS \u2717\nREFUTES \u2713\nTable 5 offers an in-depth case study on the topic \u201cSamsung\u201d. This table illustrates that when similar claims are grouped together in the same query, the LLM is better equipped to analyze the context of the claims by comparing them across different instances. The data shows that all claims incorrectly classified as \u201cSUPPORTS\u201d under the mix condition were accurately classified as \u201cREFUTES\u201d under the grp condition. This suggests that using a grouping strategy could considerably enhance the model\u2019s performance in fact-checking tasks.\nThe internal workings and decision-making processes of LLMs and ChatGPT\u2019s non-open-sourced structures are complex and difficult to investigate. However, the results of these studies provide valuable insights into the significance of context and instance grouping in LLMs. These studies also suggest that performing data augmentation along with task input can be a viable solution to improve LLMs\u2019 zero-shot classification performance. One way to achieve this is for human annotators to manually create instances with similar topics to take advantage of leveraging ICL. This can benefit tasks such as zero-shot text classification and fact-checking.\nTopic\ngrp\nmix\nfil\nAccuracy\nF1\nAccuracy\nF1\nAccuracy\nF1\nGlobal Warming\n0.8750\n0.5994\n0.7250\n0.4849\n0.8125\n0.8057\nGeorge Harrison\n0.8462\n0.8452\n0.7769\n0.5202\n0.6923\n0.6750\nSamsung\n0.7500\n0.5132\n0.6900\n0.4614\n0.5500\n0.4872\nColombiana\n0.9000\n0.9000\n0.7900\n0.5303\n0.8000\n0.5399\narative analysis of sampling strategies: inputs grouping by same topic\nTable 6: Comparative analysis of sampling strategies: inputs grouping by same topics in one query (grp), inputs mixing with random samples from other topics in one query (mix), and filtering samples from mix to retain the same group of single topic inputs as grp for comparison (fil).\nAs part of the evaluation of the FEVER dataset, two data entry categories, \u201cSamsung\u201d and \u201cColombiana\u201d, were randomly selected. The evaluation results showed that the grp method was the most accurate and had the highest F1 scores across all topics. This suggests that maintaining topic consistency leads to more accurate results as it helps the model gain a deeper and more consistent understanding of the subject, making complex inference generation and precise predictions easier. However, it\u2019s worth noting that for the \u201cGlobal Warming\u201d topic, the fil method had the highest F1 score despite the grp method having the highest accuracy. This observation highlights that different strategies may outperform others depending on the chosen performance metric. For example, the mix method may offer a better balance in predicting labels, making it more effective for certain contexts.\n# 4 Related Work\nEmergent abilities from LLMs have significantly impacted current NLP research (Wei et al., 2022). The capabilities of LLMs to generalize well to new, unseen tasks with minimal or no task-specific data has led to the development of various prompting methods, such as zero-shot and few-shot learning (Radford et al., 2019; Brown et al., 2020). Much research has been conducted on how to use LLMs\u2019 in-context learning ability to enhance their task performance without training the model: Brown et al. (2020) studied by providing few-shot demonstration examples, LLM can achieve superior task performance without fine-tuning. Built on that idea, Zhang et al. (2023); Wang et al. (2023b) explored ways to select better few-shot examples and Madaan et al. (2022) explored better prompting structure to maximize the in-context learning performance. More recently, Xie et al. (2022); Wies et al. (2023) tend to interpret the efficacy of those methods through a Bayesian inference perspective. However, due to the unicity of the zero-shot prompting on LLMs, research on ways to improve zero-shot prompting performance mostly focused on finding appropriate prompt messages to activate LLMs performance (Wei et al., 2022; Yang et al., 2023). Other research explores zero-shot prompting from a perspective of the application, including robustness and prediction consistency (Wang et al., 2023a; Zhu et al., 2023; Reiss, 2023), or as expert data annotator (Gilardi et al., 2023; Kuzman et al., 2023). More recently, Chen et al. (2023) proposed FrugalGPT, a cost-saving approach that differentiates its input queries. It starts with a cheaper model and only resorts to a larger model when it is not confident about its answer. In our research, we leverage LLMs\u2019 instruction following ability to reduce the query costs, and interpret the efficacy of our strategy from a theoretical perspective.\n# 5 Discussion, Limitation and Future Work\nWe present OverPrompt, a novel ICL prompting method specifically tailored for zero-shot text classification. Our findings demonstrate that OverPrompt considerably diminishes both time and token cost, thereby enhancing effience and reducing the carbon footprint. Remarkably, when we grouped unlabelled instances, we observed performance enhancements in some areas such as factchecking and sentiment analysis. Delving deeper, our experiments revealed a particular synergy\nbetween OverPrompt and the gpt-X models. This affinity might be attributed to these LLMs\u2019 unique training methodologies or data utilization. In contrast, another ablation study underscored that the sequence of task inputs exerts minimal influence on performance outcomes. This observation diverges from earlier findings obtained using few-shot prompting, underscoring OverPrompt\u2019s robustness. Our approach broadens the comprehension of zero-shot classification through in-context learning and paves the way for forthcoming LLM innovations. OverPrompt minimizes the token counts by stating instructions just once for multiple instances, leading to computational savings by decreasing the repetition of task descriptions. However, its efficiency might be restricted for datasets where the length of each instance dwarfs the instruction (e.g., summarisation, closed-book QA with lengthy contexts, or reasoning tasks that require detailed intermediate rationale). In such cases, the number of tokens processed is not predominantly by instructions. In addition, for these tasks, the combined input length might surpass the context length limits of LLMs, which would restrict the grouping capability of OverPrompt. We also observed that both lengthy prompts and intricate instructions negatively impacts ChatGPT\u2019s performance. Therefore, two promising directions for future research arise: First, determining the optimal strategy to segment the input while retaining essential context from other segments, in order to enhance the performance of LLMs. Second, deconstructing instructions into subtasks or step-by-step guidelines to further improve LLMs\u2019 efficiency.\n# Acknowledgements\nThis work was supported in part by the UK Engineering and Physical Sciences Research Council (grant no. EP/T017112/2, EP/V048597/1, EP/X019063/1). JL is funded by a PhD scholarship provided by AQA. YH is supported by a Turing AI Fellowship funded by the UK Research and Innovation (grant no. EP/V020579/2).\n# Contribution Statements\nJiazheng Li developed and refined the platform, formulated and designed the initial experimental pipeline, carried out the experiments, and drafted the initial version of manuscript. Dr. Runcong Zhao conceived of the presented the original idea, built the prototype of the model, refined the code of platform, and drafted the initial version of manuscript. Dr. Yongxin Yang gave valuable feedback on the first version, re-designed the experimental pipelines, and help with the drafting of the updated manuscript. Professor Yulan He and Dr. Lin Gui are the principle investigators of this project as well as the supervisor, who help with conceiving the original idea, formulating the research problem, interpreting of the experimental results, and refining the paper.\n# References\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proc. of NeurIPS 2020. Lingjiao Chen, Matei A. Zaharia, and James Y. Zou. 2023. Frugalgpt: How to use large language models while reducing cost and improving performance. ArXiv. Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. 2023. Chatgpt outperforms crowd-workers for text-annotation tasks. ArXiv. Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of EMNLP.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. ArXiv. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. ArXiv. Sawan Kumar and Partha Talukdar. 2021. Reordering examples helps during priming-based few-shot learning. In Findings of ACL. Taja Kuzman, Nikola Ljube\u0161i\u00b4c, and Igor Mozeti\u02c7c. 2023. Chatgpt: Beginning of an end of manual annotation? use case of automatic genre identification. ArXiv. Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. ArXiv. Gr\u00e9goire Mialon, Roberto Dess\u00ec, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozi\u00e8re, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey. ArXiv. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Michael V Reiss. 2023. Testing the reliability of chatgpt for text annotation and classification: A cautionary remark. ArXiv. Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proc. of NAACL 2021. James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proc. of NAACL-HLT. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proc. of ICLR. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. 2023a. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. ArXiv. Xinyi Wang, Wanrong Zhu, Michael Stephen Saxon, and William Yang Wang. 2023b. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. ArXiv. Xinyi Wang, Wanrong Zhu, and William Yang Wang. 2023c. Large language models are implicitly topic models: Explaining and finding good demonstrations for in-context learning. ArXiv. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Huai hsin Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. TMLR. Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C. Schmidt. 2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. Noam Wies, Yoav Levine, and Amnon Shashua. 2023. The learnability of in-context learning. ArXiv.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022. An explanation of in-context learning as implicit bayesian inference. In Proc. of ICLR. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. ArXiv. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In In Proc. of ICLR. Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. ArXiv.\n# A Further Experiments\n# A.1 Experimental Setup\nDatasets We selected 10 text classification datasets that covered a wide range of various aspects. These datasets were chosen to evaluate the performance of our proposed method across various tasks and domains:\nDatasets We selected 10 text classification datasets that covered a wide range of various aspects. These datasets were chosen to evaluate the performance of our proposed method across various tasks and domains:\n\u2022 Natural Language Inference: We included three datasets from the General Language Understanding Evaluation benchmark (GLUE) Wang et al. (2019), Recognizing Textual Entailment (RTE), MultiNLI (MNLI), Question NLI (QNLI), and Winograd NLI (WNLI). These datasets involve determining the relationship between pairs of sentences, such as entailment, contradiction, or neutral. \u2022 Sentiment Analysis: The Stanford Sentiment Treebank (SST-2) dataset Wang et al. (2019), which is designed for assessing the sentiment of movie reviews as either positive or negative. \u2022 Opinion Analysis: MPQA Opinion dataset2 contains news articles and other text documents manually annotated for opinions as either good for or bad for. \u2022 Fact Checking: To assess our method\u2019s effectiveness in fact-checking tasks, we selected three datasets, Fever Thorne et al. (2018), VitaminC Schuster et al. (2021), Hover Jiang et al. (2020). These datasets involve verifying the accuracy of claims based on relevant evidence from various sources.\nIn some cases, the test set may not have labels, but there is a significant amount of data available in the training set. To evaluate zero-shot text classification, we use the validation set from each dataset. Our main objective is to analyze fact-checking tasks and study how LLMs contextualize information using evidence.\nParameter Setting We utilise the OpenAI API, and the model is set to be the latest ChatGPT model gpt-3.5-turbo. We follow the official text classification example3 to set the temperature as 0 for reproducibility. All the experiment results are obtained during April 2023 - May 2023.\ngpt-3.5-turbo. We follow the official text classification example3 to set the temperature as 0 for reproducibility. All the experiment results are obtained during April 2023 - May 2023. Evaluation Metrics We use two classical evaluation metrics for text classification: Accuracy and Macro-F1 scores.\nEvaluation Metrics We use two classical evaluation metrics for text classification: Accuracy and Macro-F1 scores.\n# A.2 Results on Natural Language Inference\nWhile the OverPrompt strategy has proven to be effective in tasks such as fact-checking and sentiment analysis, it is important to keep in mind that it may not always result in improved performance. The accuracy of four NLI datasets, for instance, significantly decreased as a result of the strategy, as shown in Table A1. It is worth noting that increasing the number of parallel inputs can lengthen the prompt, which may complicate language comprehension. Tasks like sentence entailment, which do not benefit from contextual inputs, are particularly vulnerable to a drop in performance due to the elongated prompt.\n2https://mpqa.cs.pitt.edu/corpora/mpqa_corpus/ 3https://platform.openai.com/examples/default-classificatio\nDataset\nLabels\nSize\nn=1\nn=10\nPerformance\nQQP\n2\n40,430\n79.35\n75.48\n\u2193\nMNLIm\n3\n9,815\n68.37\n66.29\n\u2193\nQNLI\n2\n5,463\n77.39\n70.09\n\u2193\nW-NLI\n2\n71\n71.83\n70.42\n\u2193\nmparison of OverPrompt on different natural language inferen\n\u2193 Table A1: Comparison of OverPrompt on different natural language inference datasets.\n# A.3 Generalizbility of OverPrompt on over other LLMs\nDuring our study, we delved deeper into the effectiveness of our prompting strategy across various LLMs. Surprisingly, we discovered that only the gpt-x series of language models and Baidu\u2019s Ernie Bot performed well with the OverPrompt strategy. This finding suggests that their pre-training data may have included more structured data that varies from the data used for Flan T5, LLama and Bard models. Here we present one OverPrompt query sampled from the SST-2 dataset as an example and included each LLMs\u2019 responses:\nPlease\nread\nthrough\nthese\nsentences:\n0: \"in exactly 89 minutes , most of which\npassed as slowly as if i \u2019d\nbeen\nsitting\nnaked on an igloo , formula 51 sank from\nquirky to jerky\nto utter\nturkey . \"\n1: \"a sometimes\ntedious\nfilm . \"\n2: \"it \u2019s a charming\nand often\naffecting\njourney . \"\n3: \"the acting , costumes , music , cinematography\nand sound are all\nastounding\ngiven the\nproduction \u2019s austere\nlocales . \"\n4: \"unflinchingly\nbleak and\ndesperate \"\n5: \"it \u2019s slow -- very , very slow . \"\n6: \"or doing\nlast year \u2019s taxes\nwith your ex -wife . \"\n7: \"you do n\u2019t have to know\nabout\nmusic to appreciate\nthe film \u2019s\neasygoing\nblend of comedy and\nromance . \"\n8: \"allows us to hope that\nnolan is poised to embark a major\ncareer as\na commercial\nyet\ninventive\nfilmmaker . \"\n9: \"although\nlaced\nwith\nhumor and a few\nfanciful\ntouches , the film is\na refreshingly\nserious\nlook at young\nwomen . \"\nand\ndetermine\nthe\nsentiment of sentences\nare \"positive\" or \"negative \".\nReturn in JSON format , such as: {\"1\": \"c_1\", \"2\":\" c_2 \"}:\nPlease\nread\nthrough\nthese\nsentences:\n0: \"in exactly 89 minutes , most of which\npassed as slowly as if i \u2019d\nbeen\nsitting\nnaked on an igloo , formula 51 sank from\nquirky to jerky\nto utter\nturkey . \"\n1: \"a sometimes\ntedious\nfilm . \"\n2: \"it \u2019s a charming\nand often\naffecting\njourney . \"\n3: \"the acting , costumes , music , cinematography\nand sound are all\nastounding\ngiven the\nproduction \u2019s austere\nlocales . \"\n4: \"unflinchingly\nbleak and\ndesperate \"\n5: \"it \u2019s slow -- very , very slow . \"\n6: \"or doing\nlast year \u2019s taxes\nwith your ex -wife . \"\n7: \"you do n\u2019t have to know\nabout\nmusic to appreciate\nthe film \u2019s\neasygoing\nblend of comedy and\nromance . \"\n8: \"allows us to hope that\nnolan is poised to embark a major\ncareer as\na commercial\nyet\ninventive\nfilmmaker . \"\n9: \"although\nlaced\nwith\nhumor and a few\nfanciful\ntouches , the film is\na refreshingly\nserious\nlook at young\nwomen . \"\nand\ndetermine\nthe\nsentiment of sentences\nare \"positive\" or \"negative \".\nReturn in JSON format , such as: {\"1\": \"c_1\", \"2\":\" c_2 \"}:\nChatGPT:\n{\"0\": \"negative\", \"1\": \"negative\", \"2\": \"positive\", \"3\": \"positive\",\n\"4\": \"negative\", \"5\": \"negative\", \"6\": \"negative\", \"7\": \"positive\",\n\"8\": \"positive\", \"9\": \"positive \"}\nGPT-4:\n{\"0\": \"negative\", \"1\": \"negative\", \"2\": \"positive\", \"3\": \"positive\",\n\"4\": \"negative\", \"5\": \"negative\", \"6\": \"negative\", \"7\": \"positive\",\n\"8\": \"positive\", \"9\": \"positive \"}\nText-davinci-003:\n{\"0\": \"negative\", \"1\": \"negative\", \"2\": \"positive\", \"3\": \"positive\",\n\"4\": \"negative\", \"5\": \"negative\", \"6\": \"negative\", \"7\": \"positive\",\n\"8\": \"positive\", \"9\": \"positive \"}\nGoogle Bard:\nSure , here is the\nsentiment of the\nsentences\nyou provided , in JSON\nformat:\nChatGPT:\nGoogle Bard:\nSure , here is the sentiment of the sentences you provid format:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d70/6d7066c2-8698-4a35-bae1-67159077c754.png\" style=\"width: 50%;\"></div>\nBaidu Ernie Bot: The output format is correct but missed the number 0 instance.\n{\"1\": \"negative\", \"2\": \"negative\", \"3\": \"positive\", \"4\": \"negative\",\n\"5\": \"negative\", \"6\": \"negative\", \"7\": \"positive\", \"8\": \"positive\",\n\"9\": \"positive \"}\nAfter the prompt instruction is adjusted to\nReturn in JSON format , such as: {\"0\": \"c_1\", \"1\":\" c_2 \"}:\nErnie Bot can then output the correct order.\n{\"0\": \"negative\",\n\"1\": \"negative\",\n\"2\": \"positive\",\n\"3\": \"positive\n\",\n\"4\": \"negative\",\n\"5\": \"negative\",\n\"6\": \"negative\",\n\"7\": \"\npositive\",\n\"8\": \"positive\",\n\"9\": \"positive \"}\nLlama-2-70b-hf:\n1 is the\nsentence\nnumber , c_1 is the\nsentiment of sentence 1.\nThe data is from the movie\nreview\ndataset.\nThe data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is\nfrom the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset.\nThe data\n...\nBaidu Ernie Bot: The output format is correct but missed the number 0 instance.\n{\"1\": \"negative\", \"2\": \"negative\", \"3\": \"positive\", \"4\": \"negative\",\n\"5\": \"negative\", \"6\": \"negative\", \"7\": \"positive\", \"8\": \"positive\",\n\"9\": \"positive \"}\nAfter the prompt instruction is adjusted to\nReturn in JSON format , such as: {\"0\": \"c_1\", \"1\":\" c_2 \"}:\nErnie Bot can then output the correct order.\n{\"0\": \"negative\",\n\"1\": \"negative\",\n\"2\": \"positive\",\n\"3\": \"positive\n\",\n\"4\": \"negative\",\n\"5\": \"negative\",\n\"6\": \"negative\",\n\"7\": \"\npositive\",\n\"8\": \"positive\",\n\"9\": \"positive \"}\nLlama-2-70b-hf:\n1 is the\nsentence\nnumber , c_1 is the\nsentiment of sentence 1.\nThe data is from the movie\nreview\ndataset.\nThe data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is\nfrom the movie\nreview\ndataset. The data is from the movie\nreview\ndataset. The data is from the movie\nreview\ndataset.\nThe data\n...\nLlama-2-70b-chat-hf:\nwhere c_1\nrepresents\nthe\nsentiment of the first\nsentence , and so on.\nI\u2019m not sure what you \u2019re asking. It seems\nlike you \u2019ve provided a list\nof movie\nreviews\nand you want to know the\nsentiment of each\nreview. Is\nthat\ncorrect? If so , I can\ncertainly\nhelp you with that.\nHere are the\nsentiments of each\nreview:\n11\nLlama-2-70b-chat-hf:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/41ce/41ceef31-8b3a-497f-80eb-8f645a06c1a8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Flan-t5-xxl:</div>\nFlan-t5-xxl:\npositive , positive , positive , positive , positive , positive , positive ,\npositive , positive , positive , positive , positive , positive , positive ,\npositive , positive , positive , positive , positive , positive , positive ,\npositive , positive , positive , positive , positive\n...\n# A.4 Explore the influence of permutation on OverPrompt\nIn the OverPrompt, we included multiple task inputs. To investigate the impact of permutation, we conducted an ablation study in this section. With 10 task inputs, there are over 3.6 million possible orders, which is too complex to consider all permutations. Therefore, we randomly selected 100 orders and calculated the mean accuracy, variance, maximum and minimum values.\nDataset\nMean\nVariance\nMax Accuracy\nMin Accuracy\nSST-2\n1.0\n0.0\n1.0\n1.0\nFever\n0.9\n0.0\n0.9\n0.9\nHover\n0.4\n0.0\n0.4\n0.4\nMPQA\n0.5\n0.0\n0.5\n0.5\nTable A2: Ablation Study on the Influence of Order\nTable A2: Ablation Study on the Influence of Order\nOur ablation study shows that the ordering of task inputs in a single batch does not influence the performance of OverPrompt, highlighting the robustness of our prompting strategy (Table A2). Interestingly, this finding differs from previous experiments carried on few-shot example ordering (Kumar and Talukdar, 2021).\n# B Prompt messages\nIn this section, we report the prompt message we designed for OverPrompt for reproducibility:\n# B.1 SST-2\n<div style=\"text-align: center;\">Single Task Input</div>\nSingle Task Input\nSingle Task Input\nPlease\nread\nthrough\nthis\nsentence:\n[Single\nInstance]\nand\ndetermine\nthe\nsentiment of the\nsentence is \\\" positive \\\" or \\\"\nnegative \\\". Give me the label\nonly:\nMultiple Task Inputs\nPlease\nread\nthrough\nthese\nsentences:\n[Multiple\nInstances]\nand\ndetermine\nthe\nsentiment of sentences\nare \\\" positive \\\" or \\\"\nnegative \\\". Return in JSON format , such as: {\\\"1\\\": \\\"c_1\\\", \\\"2\\\":\\\"\nc_2 \\\"}:\nPlease\nread\nthrough\nthis\nsentence:\n[Single\nInstance]\nand\ndetermine\nthe\nsentiment of the\nsentence is \\\" positive \\\" or \\\"\nnegative \\\". Give me the label\nonly:\n<div style=\"text-align: center;\">Multiple Task Inputs</div>\nPlease\nread\nthrough\nthese\nsentences:\n[Multiple\nInstances]\nand\ndetermine\nthe\nsentiment of sentences\nare \\\" positive \\\" or \\\"\nnegative \\\". Return in JSON format , such as: {\\\"1\\\": \\\"c_1\\\", \\\"2\\\":\\\"\nc_2 \\\"}:\nMultiple Task Inputs\n<div style=\"text-align: center;\">Multiple Task Inputs</div>\n# B.3 VITAMINC\n<div style=\"text-align: center;\">Single Task Input</div>\nSingle Task Input\nPlease\nread\nthrough\nthis pair of claim and\nevidence\n[Single\nInstance]\nand\ndetermine\nwhether\nthe\nevidence \\\" support \\\", \\\" refute \\\" the claim ,\nor \\\"not enough\ninfo \\\" to decide\nwhich\ncategory it fall into .\\ nGive me\nthe label\nonly:\nMultiple Task Inputs\nPlease\nread\nthrough\nthese\npairs of claim and\nevidence\n[Multiple\nInstances]\nand\ndetermine\nwhether\nthe\nevidence \\\" support \\\", \\\" refute \\\" the claim ,\nor \\\"not enough\ninfo \\\" to decide\nwhich\ncategory it fall into .\\ nReturn\nMultiple Task Inputs\nPlease\nread\nthrough\nthese\npairs of claim and\nevidence\n[Multiple\nInstances]\nand\ndetermine\nwhether\nthe\nevidence \\\" support \\\", \\\" refute \\\" the claim ,\nor \\\"not enough\ninfo \\\" to decide\nwhich\ncategory it fall into .\\ nReturn\nin JSON format , such as: {\\\"1\\\": \\\"c_1\\\", \\\"2\\\":\\\" c_2 \\\"}:\n# B.4 MPQA\nPlease\nread\nthrough\nthe given\nsentence\n[Single\nInstance]\nand\ndetermine\nwhether\nthe\nsentence \\\" positively \\\" or \\\" negatively \\\"\naffects\nobjects. Give me the label\nonly:\nMultiple Task Inputs\nPlease\nread\nthrough\nthe given\nsentences\n[Multiple\nInstances]\nand for each sentence , determine\nwhether\nthe\nsentence \\\" positively \\\"\nor \\\" negatively \\\" affects\nobjects. Return in JSON format , such as:\n{\\\"1\\\": \\\"c_1\\\", \\\"2\\\":\\\" c_2 \\\"}:\nMultiple Task Inputs\nPlease\nread\nthrough\nthe given\nsentences\n[Multiple\nInstances]\nand for each sentence , determine\nwhether\nthe\nsentence \\\" positively \\\"\nor \\\" negatively \\\" affects\nobjects. Return in JSON format , such as:\n{\\\"1\\\": \\\"c_1\\\", \\\"2\\\":\\\" c_2 \\\"}:\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of high token and time costs associated with using large language models (LLMs) in zero-shot prompting, which has hindered their adoption in practical applications. Previous methods have focused on few-shot prompting or improving task performance without fine-tuning, but cost-efficient prompting strategies remain under-explored. OverPrompt is proposed to enhance efficiency by processing multiple instances in a single query.",
        "problem": {
            "definition": "The problem is the prohibitive costs of tokens and time when using LLMs for zero-shot classification, particularly in production environments where efficiency is crucial.",
            "key obstacle": "The main challenge is the substantial token and time costs incurred from repetitive task descriptions and the need for multiple API calls, which limits the practical use of LLMs."
        },
        "idea": {
            "intuition": "The idea was inspired by the observation that LLMs can leverage in-context learning (ICL) to improve efficiency and performance by processing multiple task inputs simultaneously.",
            "opinion": "OverPrompt is a zero-shot prompting strategy that utilizes ICL to reduce token and time costs while maintaining or improving task performance.",
            "innovation": "The primary innovation of OverPrompt lies in its ability to batch process multiple instances in a single query, significantly reducing the number of API calls and token usage compared to traditional zero-shot prompting methods."
        },
        "method": {
            "method name": "OverPrompt",
            "method abbreviation": "OP",
            "method definition": "OverPrompt is a zero-shot classification strategy designed to efficiently handle multiple task inputs in a single query by leveraging the in-context learning capabilities of LLMs.",
            "method description": "The method involves grouping multiple instances of task inputs and reusing task descriptions to minimize token consumption and processing time.",
            "method steps": [
                "Identify the task description and label set for the classification task.",
                "Group multiple instances of input texts into a single query.",
                "Construct the prompt by incorporating the task description and the grouped inputs.",
                "Send the constructed prompt to the LLM for classification.",
                "Receive and format the output in JSON format."
            ],
            "principle": "The effectiveness of OverPrompt is based on the principle that providing multiple instances in a single query allows the LLM to better approximate input task distributions, improving the accuracy of predictions while reducing costs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted across ten different text classification datasets, including Fever, Vitamin C, and HoVer, comparing OverPrompt with traditional zero-shot prompting methods.",
            "evaluation method": "Performance was measured by calculating the average time and token costs per instance under both traditional and OverPrompt settings, as well as accuracy and Macro-F1 scores across datasets."
        },
        "conclusion": "The experiments demonstrate that OverPrompt significantly reduces token and time costs while achieving comparable or improved task performance in zero-shot classification tasks. The method shows promise for enhancing the efficiency of LLMs in practical applications.",
        "discussion": {
            "advantage": "The key advantages of OverPrompt include reduced API calls, lower token costs, and improved efficiency in processing multiple task inputs simultaneously, leading to better resource management.",
            "limitation": "The method may face limitations in scenarios where the length of task descriptions significantly exceeds that of the input instances, potentially leading to performance degradation.",
            "future work": "Future research should focus on optimizing the input segmentation strategy to maintain context and exploring ways to further enhance the efficiency of LLMs in various tasks."
        },
        "other info": {
            "acknowledgements": "This work was supported by the UK Engineering and Physical Sciences Research Council and various PhD scholarships.",
            "contribution statements": {
                "Jiazheng Li": "Developed the platform and conducted experiments.",
                "Runcong Zhao": "Conceived the original idea and refined the model.",
                "Yongxin Yang": "Provided feedback and redesigned experimental pipelines.",
                "Yulan He": "Supervised the project and helped interpret results.",
                "Lin Gui": "Co-supervised the project and contributed to research formulation."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.2",
            "key information": "The problem is the prohibitive costs of tokens and time when using LLMs for zero-shot classification, particularly in production environments where efficiency is crucial."
        },
        {
            "section number": "1.3",
            "key information": "OverPrompt is a zero-shot prompting strategy that utilizes in-context learning (ICL) to reduce token and time costs while maintaining or improving task performance."
        },
        {
            "section number": "3.1",
            "key information": "The effectiveness of OverPrompt is based on the principle that providing multiple instances in a single query allows the LLM to better approximate input task distributions, improving the accuracy of predictions while reducing costs."
        },
        {
            "section number": "4.1",
            "key information": "The key advantages of OverPrompt include reduced API calls, lower token costs, and improved efficiency in processing multiple task inputs simultaneously."
        },
        {
            "section number": "6.2",
            "key information": "The main challenge is the substantial token and time costs incurred from repetitive task descriptions and the need for multiple API calls, which limits the practical use of LLMs."
        },
        {
            "section number": "7",
            "key information": "The experiments demonstrate that OverPrompt significantly reduces token and time costs while achieving comparable or improved task performance in zero-shot classification tasks."
        }
    ],
    "similarity_score": 0.7051509242639071,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/OverPrompt_ Enhancing ChatGPT through Efficient In-Context Learning.json"
}