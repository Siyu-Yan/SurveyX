{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2212.10559",
    "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
    "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
    "bib_name": "dai2023gptlearnincontextlanguage",
    "md_text": "# Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers\nDamai Dai\u2020\u2217, Yutao Sun\u2225\u2217, Li Dong\u2021, Yaru Hao\u2021, Shuming Ma\u2021, Zhifang Sui\u2020, Furu Wei\u2021 \u2020 MOE Key Lab of Computational Linguistics, Peking University \u2225Tsinghua University \u2021 Microsoft Research {daidamai,szf}@pku.edu.cn {lidong1,fuwei}@microsoft.com\n{daidamai,szf}@pku.edu.cn {lidong1,fuwei}@microsoft.com\nAbstract\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0070/00703be9-4cea-483e-b771-ed8f4f177327.png\" style=\"width: 50%;\"></div>\nLarge pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as metaoptimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at https://aka.ms/icl.\n# 1 Introduction\nIn recent years, large pretrained language models, especially in Transformer-based architectures (e.g., GPT; Brown et al. 2020), have shown strong emergent in-context learning (ICL) ability (Wei et al., 2022; Dong et al., 2023). Different from finetuning which needs additional parameter updates, ICL just needs several demonstration examples prepended\nFigure 1: According to the demonstration examples, GPT produces meta-gradients for in-context learning (ICL) through forward computation. ICL works by applying these meta-gradients to the model through attention. The meta-optimization process of ICL shares a dual view with finetuning that explicitly updates the model parameters with back-propagated gradients.\nbefore the query input, and then the model can predict labels for unseen inputs. On numerous downstream tasks, large GPT models can achieve surprising performance, which even exceeds smaller models with supervised finetuning. However, although ICL has achieved great performance, its working mechanism is still an open question to be investigated. In this paper, we explain in-context learning as a process of meta-optimization and analyze connections between GPT-based in-context learning and finetuning. Concentrating on the attention modules, we figure out that the Transformer attention has a dual form of gradient descent. On top of it, we propose a novel perspective to explain in-\ncontext learning: (1) a pretrained GPT serves as a meta-optimizer; (2) it produces meta-gradients according to the demonstration examples through forward computation; (3) the meta-gradients are applied to the original language model through attention to build an ICL model. As illustrated in Figure 1, in-context learning and explicit finetuning share a dual view of gradient descent, where ICL produces meta-gradients through forward computation, while finetuning computes gradients by back-propagation. Therefore, it is reasonable to understand in-context learning as implicit finetuning. In order to provide empirical evidence to support our understanding, we conduct comprehensive experiments based on real tasks. On six classification tasks, we compare the model predictions, attention outputs, attention weights to query tokens, and attention weights to training tokens between in-context learning and finetuning. Experimental results validate that the behavior of in-context learning is similar to explicit finetuning from multiple perspectives. These results are strong evidence to prove the reasonability of our understanding of in-context learning as implicit finetuning. Further, inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention, which regards the attention values as meta-gradients and applies the momentum mechanism (Polyak, 1964; Sutskever et al., 2013) to them. Experiments on both language modeling and in-context learning show that our momentum-based attention consistently outperforms vanilla attention, which supports our understanding of meta-optimization again from another perspective. We note that beyond this preliminary attempt, our understanding may have more potential to enlighten model design, which is worth investigating in the future. Our contributions are summarized as follows: \u2022 We figure out a dual form between Transformer attention and gradient descent, and explain ICL as a process of meta-optimization. \u2022 We analyze connections between in-context learning and explicit finetuning and propose to understand ICL as implicit finetuning. \u2022 We provide several lines of empirical evidence to prove that ICL and explicit finetuning behave similarly from multiple perspectives. \u2022 We design a momentum-based attention and validate its effectiveness, which supports our\nunderstanding of meta-optimization again and shows the potential of our understanding to enlighten future model design.\n# 2 Background\n# 2.1 In-Context Learning with GPT\nIn this paper, we focus on ICL for classification tasks using GPT (Brown et al., 2020). A GPT model is stacked with L identical Transformer (Vaswani et al., 2017) decoder layers where each layer consists of an attention module and a feed-forward network. For a classification task, given a query input text x and a candidate answer set Y = {y1, y2, . . . , ym}, we need to predict a label \u02c6y conditional on n demonstration examples C = {(x\u2032 1, y\u2032 1), (x\u2032 2, y\u2032 2), . . . , (x\u2032 n, y\u2032 n)}, where (x\u2032 i, y\u2032 i) is an input-label pair different from the query one. Formally, given a GPT model M, we first compute the probability of each answer yj:\n(1)\nSince the label space is restricted for classification, we predict the final answer \u02c6y by selecting the answer with the highest probability from the candidate answer set Y :\n(2)\nIn practice, we usually use a pre-defined template to format the demonstrations and prepend them before the query input. Let T (\u00b7) be the function that formats an example, e.g.:\n(3)\n (4)\nFeeding this contextual input into M, the probability of an answer yj is computed as\n(5) (6)\nwhere M(I) denotes the output hidden state at the last token position; eyj denotes the output word embedding of yj; and lj is the logit corresponding to the j-th answer.\n# 2.2 Dual Form Between Attention and Linear Layers Optimized by Gradient Descent\nThe idea in this paper to explain language models as meta-optimizers is inspired by Aizerman et al. (1964); Irie et al. (2022). They present that linear layers optimized by gradient descent have a dual form of linear attention. Let W0, \u2206W \u2208Rdout\u00d7din be the initialized parameter matrix and the update matrix, respectively, and x \u2208Rdin be the input representation. A linear layer optimized by gradient descent can be formulated as\n(7)\nIn the back-propagation algorithm, \u2206W is computed by accumulating the outer products of historic input representations x\u2032T i \u2208Rdin and the error signals ei \u2208Rdout of their corresponding outputs:\n(8)\nwhere ei is derived from the historic output gradients by multiplying \u2212\u03b3, the negative learning rate. Combing Equation (7) and Equation (8), we can derive the dual form of linear layers optimized by gradient descent:\n(9)\n\ufffd \ufffd where LinearAttn(V, K, q) denotes the linear attention operation, in which we regard the historic output error signals E as values, the historic inputs X\u2032 as keys, and the current input x as the query.\n# 3 Understanding In-Context Learning (ICL) as Implicit Finetuning\nWe first qualitatively analyze the Transformer attention under a relaxed linear attention form to figure out a dual form between it and gradient descent. Then, we compare in-context learning with explicit finetuning to analyze connections between these two optimization forms. Based on these theoretical findings, we propose to understand in-context learning as implicit finetuning.\n# 3.1 Understanding Transformer Attention as Meta-Optimization\nLet x \u2208Rd be the input representation of a query token t, and q = WQx \u2208Rd\u2032 be the attention query vector. In the ICL setting, the attention result of a head is formulated as FICL(q) = Attn(V, K, q) \ufffd \ufffd\nwhere WQ, WK, WV \u2208Rd\u2032\u00d7d are the projection matrices for computing the attention queries, keys, and values, respectively; \u221a d denotes the scaling factor; X denotes the input representations of query tokens before t; X\u2032 denotes the input representations of the demonstration tokens; and [X\u2032; X] denotes the matrix concatenation. For ease of qualitative analysis, we approximate the standard attention to relaxed linear attention by removing the softmax operation and the scaling factor:\nFICL(q) \u2248WV [X\u2032; X] \ufffd WK[X\u2032; X] \ufffdT q = WV X (WKX)T q + WV X\u2032 \ufffd WKX\u2032\ufffdT q = \ufffd FICL(q). (1\n(11)\n(12)\nAs shown in the above equations, the attention to the demonstration tokens is equivalent to parameter updates \u2206WICL that take effect on WZSL. In addition, by analogy with E in Equation (9), we regard WV X\u2032 as meta-gradients, which are used to compute the update matrix \u2206WICL. In summary, we explain in-context learning as a process of meta-optimization: (1) a pretrained GPT model serves as a meta-optimizer; (2) it produces meta-gradients according to the demonstration examples through forward computation; (3) through attention, the meta-gradients are applied to the original language model to build an ICL model.\n# 3.2 Comparing ICL with Finetuning\nBased on the above understanding of in-context learning, we further compare the meta-optimization of in-context learning with the explicit optimization of finetuning to analyze connections between them. Considering that ICL directly takes effect on only the attention keys and values, we design a specific finetuning setting as the compared baseline, which also updates only the parameters for the key and value projection. Also in the relaxed linear attention form, the attention result of a finetuned head is formulated as\n(13)\nwhere \u2206WK and \u2206WV denote the parameter updates to WK and WV , respectively, which are acquired by back-propagation from task-specific training objectives; and \u2206WFT is the updates to WZSL introduced by finetuning. For a more fair comparison with in-context learning, we further restrict the finetuning setting as follows: (1) we specify the training examples as the demonstration examples for in-context learning; (2) we train each example for only one step in the same order as demonstrated for in-context learning; (3) we format each training example with the same template used for ICL T (x\u2032 i, y\u2032 i) and use the causal language modeling objective for finetuning. Comparing in-context learning and this finetuning setting, we find that ICL has many properties in common with finetuning. We organize these common properties into the following four aspects. Both Perform Gradient Descent Comparing Equation (12) and Equation (13), we find that both in-context learning and finetuning introduce updates (\u2206WICL v.s. \u2206WFT) to WZSL, which drive from implicit and explicit gradient descent, respectively. The main difference is that ICL produces meta-gradients by forward computation while finetuning acquires real gradients by back-propagation. Same Training Information The metagradients of ICL are produced according to the demonstration examples. The gradients of finetuning are also derived from the same training examples. That is to say, in-context learning and finetuning share the same source of training information.\nSame Causal Order of Training Examples Incontext learning and our finetuning setting share\nthe same causal order of training examples. ICL uses decoder-only Transformers so the subsequent tokens in the demonstrations will not affect the preceding ones. For our finetuning setting, we use the same order of training examples and train only one epoch, so we can also guarantee that the subsequent examples have no effect on the preceding ones. Both Aim at Attention Compared with zeroshot learning, the direct effect of in-context learning and our finetuning are both restricted to the computation of attention keys and values. For ICL, the model parameters are unchanged and it encodes demonstration information into additional keys and values to change the attention behavior. For finetuning, due to our restriction, the training information can be introduced to only the projection matrices for attention keys and values as well. Considering the above common properties between in-context learning and finetuning, we show that it is reasonable to understand in-context learning as implicit finetuning. In the rest of this paper, we compare ICL and explicit finetuning empirically from multiple perspectives to provide quantitative results to support this understanding.\n# 4 Experiments\n# 4.1 Experimental Settings\nWe analyze two off-the-shelf pretrained GPT models with 1.3 billion and 2.7 billion model parameters, respectively, which are released by fairseq1. In the rest of this paper, we call them GPT 1.3B and GPT 2.7B for short. All experiments are conducted on NVIDIA V100 GPUs with 32 GB memory. For each task, we use the same template to format examples for zero-shot learning (ZSL), finetuning (FT), and in-context learning (ICL). Details of the templates used for each task are provided in Appendix A. The answer prediction processes for ZSL and finetuning are the same with ICL as described in Section 2.1, except that they do not have demonstration examples. For in-context learning, we fix the max number of demonstration examples to 32 and tune the random seed for each task to find a set of demonstration examples that achieves the best validation performance. For explicit finetuning, we use the same demonstration examples for in-context learning as the training examples and use SGD as the optimizer. For a fair comparison, we fine-tune the 1https://github.com/facebookresearch/fairseq\nSST2\nSST5\nMR\nSubj\nAGNews\nCB\n# Validation Examples\n872\n1101\n1066\n2000\n7600\n56\n# Label Types\n2\n5\n2\n2\n4\n3\nZSL Accuracy (GPT 1.3B)\n70.5\n39.3\n65.9\n72.6\n46.3\n37.5\nFT Accuracy (GPT 1.3B)\n73.9\n39.5\n73.0\n77.8\n65.3\n55.4\nICL Accuracy (GPT 1.3B)\n92.7\n45.0\n89.0\n90.0\n79.2\n57.1\nZSL Accuracy (GPT 2.7B)\n71.4\n35.9\n60.9\n75.2\n39.8\n42.9\nFT Accuracy (GPT 2.7B)\n76.9\n39.1\n80.0\n86.1\n65.7\n57.1\nICL Accuracy (GPT 2.7B)\n95.0\n46.5\n91.3\n90.3\n80.3\n55.4\n<div style=\"text-align: center;\">Table 1: Statistics of six classification datasets (rows 1-2) and validation accuracy in the zero-shot learning (ZSL), finetuning (FT), and in-context learning (ICL) settings on these datasets (rows 3-8).</div>\nModel\nSST2\nSST5\nMR\nSubj\nAGNews\nCB\nAverage\nGPT 1.3B\n91.84\n66.67\n97.08\n87.17\n83.08\n87.50\n85.56\nGPT 2.7B\n96.83\n71.60\n95.83\n87.63\n84.44\n100.00\n89.39\nTable 2: Rec2FTP for two GPT models on six datasets. From the perspective of model prediction, ICL can cover most of the correct behavior of finetuning.\nmodel for only one epoch and the training examples are provided in the same order as demonstrated for in-context learning. We tune the learning rate for finetuning and select the one that achieves the best validation performance. Details of the search range and selected value for the random seeds and learning rates are shown in Appendix B.\n# 4.2 Evaluation Datasets\nWe compare in-context learning and finetuning based on six datasets spanning three sorts of classification tasks. SST2 (Socher et al., 2013), SST5 (Socher et al., 2013), MR (Pang and Lee, 2005) and Subj (Pang and Lee, 2004) are four datasets for sentiment classification; AGNews (Zhang et al., 2015) is a topic classification dataset; and CB (De Marneffe et al., 2019) is used for natural language inference. Statistics of the number of validation examples and label types are summarized in Table 1. For reference, we present the validation accuracy in the ZSL, finetuning, and ICL settings on six classification datasets in Table 1. Compared with ZSL, ICL and finetuning both achieve considerable improvements, which means the optimizations they make are both helpful to these downstream tasks.\n# 4.3 ICL Covers Most of Correct Predictions of Finetuning\nWe compute a recall to finetuning prediction (Rec2FTP) to measure ICL can cover how much behavior of finetuning from the perspective of the model prediction. We first count NFT>ZSL, the number of query examples that finetuning can predict correctly but ZSL cannot. Then, among these examples, we count N(FT>ZSL)\u2227(ICL>ZSL), the number that ICL can also predict correctly. Finally, we compute the Rec2FTP score as N(FT>ZSL)\u2227(ICL>ZSL) NFT>ZSL . A higher Rec2FTP score suggests that ICL covers more correct behavior of finetuning from the perspective of the model prediction. We show the Rec2FTP scores for two GPT models on six datasets in Table 2. As shown in the table, on average, ICL can correctly predict more than 85% of the examples that finetuning can correct from ZSL. These results indicate that from the perspective of model prediction, ICL can cover most of the correct behavior of finetuning.\n# 4.4 ICL Tends to Change Attention Outputs in the Same Direction as Finetuning\nFrom the perspective of representation, we compute a similarity of the attention output updates (SimAOU) to measure the similarity between the updates that ICL and finetuning make. For a query example, let h(l) X denote the normalized output rep-\nModel\nMetric\nSST2\nSST5\nMR\nSubj\nAGNews\nCB\nAverage\nGPT 1.3B\nSimAOU (Random \u2206)\n0.002\n0.003\n0.001\n0.002\n0.002\n0.003\n0.002\nSimAOU (\u2206FT)\n0.110\n0.080\n0.222\n0.191\n0.281\n0.234\n0.186\nGPT 2.7B\nSimAOU (Random \u2206)\n0.000\n-0.002\n0.000\n0.001\n-0.002\n0.000\n-0.001\nSimAOU (\u2206FT)\n0.195\n0.323\n0.157\n0.212\n0.333\n0.130\n0.225\nTable 3: SimAOU for two GPT models on six datasets. ICL updates are much more similar to finetuning updates than to random updates. From the perspective of representation, ICL tends to change attention output representations in the same direction as finetuning changes.\nModel\nMetric\nSST2 SST5\nMR\nSubj\nAGNews\nCB\nAverage\nGPT 1.3B SimAM (Before Finetuning)\n0.555\n0.391 0.398 0.378\n0.152\n0.152\n0.338\nSimAM (After Finetuning)\n0.585\n0.404 0.498 0.490\n0.496\n0.177\n0.442\nGPT 2.7B SimAM (Before Finetuning)\n0.687\n0.380\n0.314 0.346\n0.172\n0.228\n0.355\nSimAM (After Finetuning)\n0.687\n0.492 0.347 0.374\n0.485\n0.217\n0.434\nTable 4: SimAM for two models on six datasets. From the perspective of attention behavior, compared with attention weights before finetuning, ICL is more inclined to generate similar attention weights to those after finetuning.\nresentation of the last token at the l-th attention layer in setting X. The updates of ICL and finetuning compared with ZSL are h(l) ICL \u2212h(l) ZSL and h(l) FT \u2212h(l) ZSL, respectively. We compute the cosine between these two updates to get SimAOU (\u2206FT) at the l-th layer. A higher SimAOU (\u2206FT) means ICL is more inclined to update the attention output in the same direction as finetuning. For comparison, we also compute a baseline metric called SimAOU (Random \u2206) that computes the similarity between ICL updates and randomly generated updates. We present the SimAOU scores averaged across examples and layers for two GPT models on six datasets in Table 3. From the table, we find that SimAOU (Random \u2206) is always around zero, while SimAOU (\u2206FT) remains much more positive. These results indicate that ICL updates are much more similar to finetuning updates than to random updates. From the perspective of representation, we prove that ICL tends to change the attention outputs in the same direction as finetuning.\n# 4.5 ICL Is Inclined to Generate Similar Attention Weights to Finetuning\nFrom the perspective of attention behavior, we compute a similarity of the attention map (SimAM) to measure the similarity of the attention map to query tokens for ICL and finetuning. For a query example, let m(l,h) X denote the attention weights before softmax of the last token at the h-th attention\nhead in the l-th attention layer in setting X. For ICL, we omit the attention to the demonstration tokens and only monitor the attention weights to the query tokens. First, before finetuning, we compute the cosine between m(l,h) ICL and m(l,h) ZSL and then average the similarity across attention heads to get SimAM (Before Finetuning) at each layer. Similarly, after finetuning, we compute the cosine between m(l,h) ICL and m(l,h) FT to get SimAM (After Finetuning). A higher SimAM (After Finetuning) over SimAM (Before Finetuning) indicates that the attention behavior of ICL is more similar to a finetuned model than a non-finetuned one. Table 4 demonstrates the SimAM scores averaged across examples and layers for two GPT models on six datasets. We observe that compared with attention weights before finetuning, ICL is more inclined to generate similar attention weights to attention weights after finetuning. Again, from the perspective of attention behavior, we prove that ICL behaves similarly to finetuning.\n# 4.6 ICL and Finetuning Tend to Pay Similar Attention to Training Tokens\nSince we understand ICL as a process of metaoptimization, we also compare the attention to training tokens for ICL and finetuning with the Kendall rank correlation coefficient (Kendall, 1948). For a query example, let m(l) ICL denote the ICL attention weights to the demonstration tokens\nModel\nMetric\nSST2\nSST5\nMR\nSubj\nAGNews\nCB\nAverage\nGPT 1.3B\nKendall (ICL, Random)\n0.000\n-0.001\n0.000\n0.001\n-0.001\n0.000\n0.000\nKendall (ICL, FT)\n0.192\n0.151\n0.173\n0.181\n0.190\n0.274\n0.193\nGPT 2.7B\nKendall (ICL, Random)\n-0.001\n0.000\n0.000\n0.000\n0.000\n-0.001\n0.000\nKendall (ICL, FT)\n0.213\n0.177\n0.264\n0.203\n0.201\n0.225\n0.214\nTable 5: Kendall rank correlation coefficients for two GPT models on six datasets. Compared with random attention weights, ICL attention weights to training tokens are much more similar to finetuning attention weights.\nof the last query token in the l-th attention layer, which is summed across attention heads. For finetuning, we first record all the attention queries Q\u2032(l,h) \u2208Rd\u2032\u00d7N of the training tokens, and then use the inner product between them and the attention query q(l,h) \u2208Rd\u2032 of the last token in the query example as the finetuning attention weights to the training tokens: m(l) FT = \ufffd h Q\u2032(l,h)T q(l,h), which is also summed across attention heads. The Kendall coefficient between m(l) ICL and m(l) FT is computed as Kendall (ICL, FT) = Pc\u2212Pd N(N\u22121)/2, where N denotes the number of training tokens, Pc denotes the number of concordant pairs, and Pd denotes the number of discordant pairs. A higher Kendall coefficient means that the orders of attention weights to training tokens of ICL and finetuning are more similar. For comparison, we also compute the Kendall coefficient between m(l) ICL and randomly generated attention weights m(l) Random, which we call Kendall (ICL, Random). Table 5 shows the Kendall correlation coefficients averaged across examples and layers for two GPT models on six datasets. We find that Kendall (ICL, Random) is always near zero, while Kendall (ICL, FT) always maintains a distinctly positive value. These results suggest that ICL and finetuning tend to pay similar attention to training tokens.\n# 5 Momentum-Based Attention Inspired by Dual Form of Transformer Attention\nWe have figured out the dual form between Transformer attention and gradient descent. As illustrated in Figure 2, inspired by this dual view, we investigate whether we can utilize momentum (Polyak, 1964; Sutskever et al., 2013), a widely used technique for optimization algorithms, to improve Transformer attention. Gradient descent with momentum averages gra-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bcfa/bcfab989-0d9c-41ef-90b4-7ff16f952bed.png\" style=\"width: 50%;\"></div>\nFigure 2: Inspired by the dual form between attention and gradient descent, we introduce the momentum mechanism into Transformer attention by analogy with gradient descent with momentum.\ndients among timestamps:\nwhere \u03b3 is the learning rate and \u03b7 is a scalar between 0 and 1. As stated in Section 3.1, the attention values serve as meta-gradients. By analogy with gradient descent with momentum, we try to use Exponential Moving Average (EMA; Hunter 1986) to average the attention values to build the momentum-based attention:\nMoAttn(V, K, qt) = Attn(V, K, qt) + EMA(V )\nwhere vi is the i-th attention value vector. The momentum of attention value vectors explicitly strengthens the recency bias of attention, which has been shown helpful for language modeling (Press et al., 2022). Therefore, we assume that introducing momentum into attention will contribute to faster convergence and better performance.\nExperiments on Language Modeling First, we evaluate the effect of momentum-based attention on language modeling. We train two GPT models with 350M parameters from scratch, where one is the vanilla Transformer, and another applies momentum to attention. More training details are provided in Appendix C. We evaluate the perplexity\nModel\nTrain1024\nValid256\nValid512\nValid1024\nTransformer\n17.61\n19.50\n16.87\n15.14\nTransformerMoAttn\n17.55\n19.37\n16.73\n15.02\nModel\nSST5\nIMDB\nMR\nCB\nARC-E\nPIQA\nAverage\nTransformer\n25.3\n64.0\n61.2\n43.9\n48.2\n68.7\n51.9\nTransformerMoAttn\n27.4\n70.3\n64.8\n46.8\n50.0\n69.0\n54.7\nTable 7: Accuracy on six in-context learning datasets. Introducing momentum into attention improves the accuracy of the vanilla Transformer by 2.8 on average.\nof these two models on the training set and three validation sets with input lengths of 256, 512, and 1024, respectively. The results are shown in Table 6. On all of the validation sets, applying momentum to attention introduces a consistent perplexity improvement compared with the vanilla Transformer. Experiments on In-Context Learning We also evaluate the in-context learning ability of the above language models to verify the effectiveness of momentum-based attention on downstream tasks. We consider six datasets for sentiment analysis (SST5 (Socher et al., 2013), IMDB (Maas et al., 2011), and MR (Pang and Lee, 2005)), natural language inference (CB (De Marneffe et al., 2019)), and multi-choice selection (ARC-E (Clark et al., 2018) and PIQA (Bisk et al., 2020)). For all of these datasets, we use up to 32 examples as demonstrations. As shown in Table 7, compared with vanilla Transformer, using momentum-based attention achieves consistently higher accuracy on all of these datasets. The performance improvements on both language modeling and in-context learning prove our deduction that introducing momentum will improve Transformer attention. From another perspective, these results further support our understanding of Transformer attention as meta-optimization.\n# 6 Related Work\nRecently, some pieces of work have attempted to understand the inference mechanism of in-context learning. Xie et al. (2022) explain in-context learning as implicit Bayesian inference. They state that in-context learning emerges when language models can infer the shared latent concept among the demonstration examples, which is learned during\npretraining. On another aspect, Olsson et al. (2022) focus on specific modules in Transformers. They find some induction heads in Transformers that refer to abstract patterns in previous sequences to help predict the next token. They indicate that the induction heads drive the ability of in-context learning. Different from them, we concentrate on the learning algorithm of ICL and explain it as a process of meta-optimization. Some other work also studies the learning algorithm of ICL. As a case study, Garg et al. (2022) show that Transformers can be trained to in-context learn a class of linear functions and the performance is comparable to the least squares estimator. Based on linear regression, Aky\u00fcrek et al. (2022) prove that they can construct parameters of Transformers to implement gradient-descent-based learning algorithms. Further, they show that models trained with an in-context learning objective tend to match the behavior of models computed by explicit learning algorithms. Also based on regression tasks, von Oswald et al. (2022) show that linear attention-only Transformers with constructed parameters that implement gradient descent and models learned by an in-context learning objective are highly related. Compared with them, we are the first ones to explain in-context learning in real scenarios. To be specific, (1) we analyze in-context learning for off-the-shelf GPT models, instead of models trained from scratch by an ICL objective; (2) our experiments are based on real NLP tasks, instead of toy ones like linear regression.\n# 7 Conclusion\nIn this paper, we aim to explain the working mechanism of GPT-based ICL. Theoretically, we figure\nout a dual form between Transformer attention and gradient descent, and propose to understand ICL as a process of meta-optimization. Further, we analyze connections between ICL and explicit finetuning and show the reasonability to regard ICL as implicit finetuning. Empirically, we comprehensively compare ICL and finetuning based on six real NLP tasks. The results prove that ICL behaves similarly to explicit finetuning from multiple perspectives. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention that achieves consistent performance improvements over vanilla attention. We believe our understanding will have more potential to enlighten ICL applications and model design in the future.\n# Limitations\nAlthough the ability of in-context learning has been found for different architectures (e.g., Transformer and LSTM), we consider only Transformer-based in-context learning in this paper because Transformer is the current mainstream architecture of NLP. However, as for in-context learning itself, figuring out how it works for other architectures is also a meaningful problem, which we encourage to study in the future. As for the dual form we point out between Transformer attention and gradient descent, we consider a relaxed form of linear attention for qualitative analysis. Although the experimental results support our understanding well, the mechanism of standard Transformer attention without approximation may be more complex and should be studied more clearly in the future. As for empirical experiments, our analysis needs to record a large number of intermediate results (e.g., attention output representations, and attention weights to query tokens and demonstration tokens) for thousands of validation examples. Considering the storage space and computational cost of analysis, we only analyze GPT models with up to 2.7B parameters and leave larger models such as GPT 13B for future work. In addition, for the clarity of the problem definition and the convenience of experiments, our analysis is based on only classification tasks. Although classification is a representative application of in-context learning, other tasks like multiple choice and open-ended generation are not considered in this paper and could be investigated in the future.\n# Acknowledgement\nDamai Dai and Zhifang Sui are supported by the National Key Research and Development Program of China 2020AAA0106700 and NSFC project U19A2065.\n# References\nMark A Aizerman, Emmanuil M Braverman, and Lev I Rozonoer. 1964. Theoretical foundation of potential functions method in pattern recognition. Avtomatika i Telemekhanika, 25(6):917\u2013936. Ekin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. 2022. What learning algorithm is in-context learning? investigations with linear models. CoRR, abs/2211.15661. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. PIQA: reasoning about physical commonsense in natural language. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, pages 7432\u20137439. AAAI Press. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457. Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring discourse. In proceedings of Sinn und Bedeutung, volume 23, pages 107\u2013124. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A survey for in-context learning. CoRR, abs/2301.00234. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. 2022. What can transformers learn incontext? A case study of simple function classes. CoRR, abs/2208.01066. J Stuart Hunter. 1986. The exponentially weighted moving average. Journal of quality technology, 18(4):203\u2013210.\n# Appendix\n# A Templates for In-Context Learning\nWe demonstrate the templates used to format examples and the candidate answer sets for six classification datasets used in our experiments in Table 8.\n# B Hyper-Parameters for In-Context Learning and Finetuning\nWe perform grid search to find the best random seed for ICL and the best learning rate for finetuning. The search range for all the datasets is the same. For random seeds, we search in {1, 2, 3, 4, 5, 6, 7}. For learning rates, the search base values are {1, 2, 3, 4, 5, 6, 7, 8, 9} and we scale them to 0.1, 0.01, 0.001, and 0.0001 times, i.e., we have 9\u00d74 = 36 values to search. As an exception, for GPT 1.3B finetuned on SST5, we perform a more fine-grained search and finally set its learning rate to 0.00016 since the finetuned model cannot outperform the zero-shot learning with the above 36 learning rates. In Table 9, we present the details of the selected random seeds and learning rates for two GPT models on six classification datasets.\n# C Hyper-Parameters for Training Language Models from Scratch\nThe hyper-parameters for training two language models from scratch are summarized in Table 10.\nDataset\nTemplate\nCandidate Answer Set\nSST2\nSentence: {Sentence}\n{ Negative, Positive }\nLabel: {Label}\nSST5\nSentence: {Sentence}\n{ terrible, bad, neutral, good, great }\nLabel: {Label}\nMR\nReview: {Sentence}\n{ Negative, Positive }\nSentiment: {Label}\nSubj\nInput: {Sentence}\n{ objective, subjective }\nType: {Label}\nAGNews\nClassify the news articles into the categories\nof World, Sports, Business, and Technology.\n{ World, Sports, Business, Technology }\nNews: {Sentence}\nType: {Label}\nCB\n{Premise}\n{ True, False, Neither }\nQuestion: {Hypothesis} True, False, or Nei-\nther?\nAnswer: {Label}\nHyper-Parameter\nDataset\nGPT 1.3B\nGPT 2.7B\nRandom Seed\nSST2\n2\n7\nSST5\n5\n5\nMR\n5\n1\nSubj\n4\n4\nAGNews\n3\n3\nCB\n3\n3\nLearning Rate\nSST2\n0.0005\n0.007\nSST5\n0.00016\n0.04\nMR\n0.003\n0.001\nSubj\n0.003\n0.002\nAGNews\n0.2\n0.2\nCB\n0.08\n0.01\nHyper-parameter\nValue\nEmbedding & Hidden Dimension\n1024\nFFN Inner Hidden Dimension\n4096\nNumber of Attention Heads\n16\nNumber of Transformer Layers\n24\nNumber of Parameters\n350M\nSequence Length\n1024\nBatch Size\n512K Tokens\nOptimizer\nAdam\nAdam Betas\n(0.9, 0.98)\nAdam Epsilon\n1e-6\nMaximum Learning Rate\n3e-4\nLearning Rate Scheduler\nPolynomial Decay\nTotal Training Steps\n500K\nWarm-up Steps\n20K\nGradient Clip Norm\n2.0\n",
    "paper_type": "theory",
    "attri": {
        "background": "This paper addresses the issue of understanding the working mechanism of in-context learning (ICL) in large pretrained language models, particularly those based on the Transformer architecture, and explains how these models can perform ICL without parameter updates.",
        "problem": {
            "definition": "The core problem is to elucidate the underlying mechanism of in-context learning in large language models, which allows them to make predictions based on demonstration examples without explicit finetuning.",
            "key obstacle": "Despite the impressive performance of ICL, its operational principles remain poorly understood, which hinders further advancements in model design and application."
        },
        "idea": {
            "intuition": "The idea was inspired by the duality between Transformer attention mechanisms and gradient descent optimization techniques.",
            "opinion": "The authors propose to view in-context learning as a form of meta-optimization, where the model generates meta-gradients based on demonstration examples.",
            "innovation": "The primary innovation lies in understanding ICL as implicit finetuning, revealing connections between ICL and explicit parameter updates typically seen in traditional machine learning."
        },
        "Theory": {
            "perspective": "The theoretical perspective is that Transformer attention operates similarly to gradient descent, allowing for a deeper understanding of how ICL functions.",
            "opinion": "The authors assume that by interpreting ICL through the lens of meta-optimization, one can gain insights into its operational mechanics.",
            "proof": "The authors derive a dual form of attention that parallels gradient descent, providing a theoretical basis for their claims about ICL."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using two pretrained GPT models with 1.3 billion and 2.7 billion parameters across six classification tasks.",
            "evaluation method": "The evaluation involved comparing predictions, attention outputs, and weights between ICL and explicit finetuning across multiple datasets."
        },
        "conclusion": "The experimental results confirm that in-context learning exhibits behavior similar to explicit finetuning, supporting the view of ICL as implicit finetuning, and the introduction of momentum-based attention demonstrates improved performance.",
        "discussion": {
            "advantage": "The paper provides a novel theoretical framework that enhances understanding of in-context learning, potentially guiding future model designs.",
            "limitation": "The study is limited to Transformer-based models, leaving the mechanisms of ICL in other architectures unexplored.",
            "future work": "Future research could investigate the applicability of these findings to other neural network architectures and explore the complexities of standard Transformer attention without approximations."
        },
        "other info": [
            {
                "info1": "The code for the experiments is available at https://aka.ms/icl."
            },
            {
                "info2": {
                    "info2.1": "The authors acknowledge support from the National Key Research and Development Program of China and NSFC projects.",
                    "info2.2": "The paper includes extensive experiments validating the theoretical claims with empirical data."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of understanding the working mechanism of in-context learning (ICL) in large pretrained language models, particularly those based on the Transformer architecture."
        },
        {
            "section number": "1.3",
            "key information": "The core problem is to elucidate the underlying mechanism of in-context learning in large language models, which allows them to make predictions based on demonstration examples without explicit finetuning."
        },
        {
            "section number": "3.2",
            "key information": "The theoretical perspective is that Transformer attention operates similarly to gradient descent, allowing for a deeper understanding of how ICL functions."
        },
        {
            "section number": "3.4",
            "key information": "The authors derive a dual form of attention that parallels gradient descent, providing a theoretical basis for their claims about ICL."
        },
        {
            "section number": "4.1",
            "key information": "The experimental results confirm that in-context learning exhibits behavior similar to explicit finetuning, supporting the view of ICL as implicit finetuning."
        },
        {
            "section number": "6.1",
            "key information": "The study is limited to Transformer-based models, leaving the mechanisms of ICL in other architectures unexplored."
        },
        {
            "section number": "7",
            "key information": "Future research could investigate the applicability of these findings to other neural network architectures and explore the complexities of standard Transformer attention without approximations."
        }
    ],
    "similarity_score": 0.7005141239127909,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Why Can GPT Learn In-Context_ Language Models Implicitly Perform Gradient Descent as Meta-Optimizers.json"
}