{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1802.03471",
    "title": "Certified Robustness to Adversarial Examples with Differential Privacy",
    "abstract": "Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks, but they either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google's Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired formalism, that provides a rigorous, generic, and flexible foundation for defense.",
    "bib_name": "lecuyer2019certifiedrobustnessadversarialexamples",
    "md_text": "# Certified Robustness to Adversarial Examples with Differential Privacy\nMathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana Columbia University\nAbstract\u2014Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to normbounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google\u2019s Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.\n[stat.ML\n# I. Introduction\nDeep neural networks (DNNs) perform exceptionally well on many machine learning tasks, including safety- and security-sensitive applications such as self-driving cars [5], malware classification [48], face recognition [47], and critical infrastructure [71]. Robustness against malicious behavior is important in many of these applications, yet in recent years it has become clear that DNNs are vulnerable to a broad range of attacks. Among these attacks \u2013 broadly surveyed in [46] \u2013 are adversarial examples: the adversary finds small perturbations to correctly classified inputs that cause a DNN to produce an erroneous prediction, possibly of the adversary\u2019s choosing [56]. Adversarial examples pose serious threats to security-critical applications. A classic example is an adversary attaching a small, human-imperceptible sticker onto a stop sign that causes a self-driving car to recognize it as a yield sign. Adversarial examples have also been demonstrated in domains such as reinforcement learning [32] and generative models [31]. Since the initial demonstration of adversarial examples [56], numerous attacks and defenses have been proposed, each building on one another. Initially, most defenses used best-effort approaches and were broken soon after introduction. Model distillation, proposed as a robust defense in [45], was subsequently broken in [7]. Other work [36] claimed that adversarial examples are unlikely to fool machine learning (ML) models in the real-world, due to the rotation and scaling introduced by even the slightest camera movements. However, [3] demonstrated a new attack strategy that is robust to rotation and scaling. While this\nback-and-forth has advanced the state of the art, recently the community has started to recognize that rigorous, theorybacked, defensive approaches are required to put us off this arms race. Accordingly, a new set of certified defenses have emerged over the past year, that provide rigorous guarantees of robustness against norm-bounded attacks [12], [52], [65]. These works alter the learning methods to both optimize for robustness against attack at training time and permit provable robustness checks at inference time. At present, these methods tend to be tied to internal network details, such as the type of activation functions and the network architecture. They struggle to generalize across different types of DNNs and have only been evaluated on small networks and datasets. We propose a new and orthogonal approach to certified robustness against adversarial examples that is broadly applicable, generic, and scalable. We observe for the first time a connection between differential privacy (DP), a cryptography-inspired formalism, and a definition of robustness against norm-bounded adversarial examples in ML. We leverage this connection to develop PixelDP, the first certified defense we are aware of that both scales to large networks and datasets (such as Google\u2019s Inception network trained on ImageNet) and can be adapted broadly to arbitrary DNN architectures. Our approach can even be incorporated with no structural changes in the target network (e.g., through a separate auto-encoder as described in Section III-B). We provide a brief overview of our approach below along with the section references that detail the corresponding parts. \u00a7II establishes the DP-robustness connection formally (our first contribution). To give the intuition, DP is a framework for randomizing computations running on databases such that a small change in the database (removing or altering one row or a small set of rows) is guaranteed to result in a bounded change in the distribution over the algorithm\u2019s outputs. Separately, robustness against adversarial examples can be defined as ensuring that small changes in the input of an ML predictor (such as changing a few pixels in an image in the case of an l0-norm attack) will not result in drastic changes to its predictions (such as changing its label from a stop to a yield sign). Thus, if we think of a DNN\u2019s inputs (e.g., images) as databases in DP parlance, and individual features (e.g., pixels) as rows in DP, we observe that randomizing the outputs of a DNN\u2019s prediction function to enforce\nDP on a small number of pixels in an image guarantees robustness of predictions against adversarial examples that can change up to that number of pixels. The connection can be expanded to standard attack norms, including l1, l2, and l\u221enorms. \u00a7III describes PixelDP, the first certified defense against norm-bounded adversarial examples based on differential privacy (our second contribution). Incorporating DP into the learning procedure to increase robustness to adversarial examples requires is completely different and orthogonal to using DP to preserve the privacy of the training set, the focus of prior DP ML literature [40], [1], [9] (as \u00a7 VI explains). A PixelDP DNN includes in its architecture a DP noise layer that randomizes the network\u2019s computation, to enforce DP bounds on how much the distribution over its predictions can change with small, norm-bounded changes in the input. At inference time, we leverage these DP bounds to implement a certified robustness check for individual predictions. Passing the check for a given input guarantees that no perturbation exists up to a particular size that causes the network to change its prediction. The robustness certificate can be used to either act exclusively on robust predictions, or to lowerbound the network\u2019s accuracy under attack on a test set. \u00a7IV presents the first experimental evaluation of a certified adversarial-examples defense for the Inception network trained on the ImageNet dataset (our third contribution). We additionally evaluate PixelDP on various network architectures for four other datasets (CIFAR-10, CIFAR-100, SVHN, MNIST), on which previous defenses \u2013 both best effort and certified \u2013 are usually evaluated. Our results indicate that PixelDP is (1) as effective at defending against attacks as today\u2019s state-of-the-art, best-effort defense [37] and (2) more scalable and broadly applicable than a prior certified defense. Our experience points to DP as a uniquely generic, broadly applicable, and flexible foundation for certified defense against norm-bounded adversarial examples (\u00a7V, \u00a7VI). We credit these properties to the post-processing property of DP, which lets us incorporate the certified defense in a network-agnostic way.\n# II. DP-Robustness Connection A. Adversarial ML Background\nAn ML model can be viewed as a function mapping inputs \u2013 typically a vector of numerical feature values \u2013 to an output (a label for multiclass classification and a real number for regression). Focusing on multiclass classification, we define a model as a function f : Rn \u2192K that maps ndimensional inputs to a label in the set K = {1, . . . , K} of all possible labels. Such models typically map an input x to a vector of scores y(x) = (y1(x), . . . , yK(x)), such that yk(x) \u2208[0, 1] and \ufffdK k=1 yk(x) = 1. These scores are interpreted as a probability distribution over the labels, and the model returns the label with highest probability, i.e., f(x) = arg maxk\u2208K yk(x). We denote the function that\nmaps input x to y as Q and call it the scoring function; we denote the function that gives the ultimate prediction for input x as f and call it the prediction procedure. Adversarial Examples. Adversarial examples are a class of attack against ML models, studied particularly on deep neural networks for multiclass image classification. The attacker constructs a small change to a given, fixed input, that wildly changes the predicted output. Notationally, if the input is x, we denote an adversarial version of that input by x + \u03b1, where \u03b1 is the change or perturbation introduced by the attacker. When x is a vector of pixels (for images), then xi is the i\u2019th pixel in the image and \u03b1i is the change to the i\u2019th pixel. It is natural to constrain the amount of change an attacker is allowed to make to the input, and often this is measured by the p-norm of the change, denoted by \u2225\u03b1\u2225p. For 1 \u2264p < \u221e, the p-norm of \u03b1 is defined by \u2225\u03b1\u2225p = (\ufffdn i=1 |\u03b1i|p)1/p; for p = \u221e, it is \u2225\u03b1\u2225\u221e= maxi |\u03b1i|. Also commonly used is the 0-norm (which is technically not a norm): \u2225\u03b1\u22250 = |{i : \u03b1i \u0338= 0}|. A small 0-norm attack is permitted to arbitrarily change a few entries of the input; for example, an attack on the image recognition system for self-driving cars based on putting a sticker in the field of vision is such an attack [19]. Small p-norm attacks for larger values of p (including p = \u221e) require the changes to the pixels to be small in an aggregate sense, but the changes may be spread out over many or all features. A change in the lighting condition of an image may correspond to such an attack [34], [50]. The latter attacks are generally considered more powerful, as they can easily remain invisible to human observers. Other attacks that are not amenable to norm bounding exist [67], [54], [66], but this paper deals exclusively with norm-bounded attacks. Let Bp(r) := {\u03b1 \u2208Rn : \u2225\u03b1\u2225p \u2264r} be the p-norm ball of radius r. For a given classification model, f, and a fixed input, x \u2208Rn, an attacker is able to craft a successful adversarial example of size L for a given p-norm if they find \u03b1 \u2208Bp(L) such that f(x + \u03b1) \u0338= f(x). The attacker thus tries to find a small change to x that will change the predicted label. Robustness Definition. Intuitively, a predictive model may be regarded as robust to adversarial examples if its output is insensitive to small changes to any plausible input that may be encountered in deployment. To formalize this notion, we must first establish what qualifies as a plausible input. This is difficult: the adversarial examples literature has not settled on such a definition. Instead, model robustness is typically assessed on inputs from a test set that are not used in model training \u2013 similar to how accuracy is assessed on a test set and not a property on all plausible inputs. We adopt this view of robustness. Next, given an input, we must establish a definition for insensitivity to small changes to the input. We say a model f is insensitive, or robust, to attacks of p-norm L on a given\ninput x if f(x) = f(x + \u03b1) for all \u03b1 \u2208Bp(L). If f is a multiclass classification model based on label scores (as in \u00a7II-A), this is equivalent to:\n\u2200\u03b1 \u2208 Bp(L) \ufffdyk(x + \u03b1) > max i:i\u0338=k yi(x + \u03b1), (1\n(1)\nwhere k := f(x). A small change in the input does not alter the scores so much as to change the predicted label.\n# B. DP Background\nDP is concerned with whether the output of a computation over a database can reveal information about individual records in the database. To prevent such information leakage, randomness is introduced into the computation to hide details of individual records.\nDP is concerned with whether the output of a computation over a database can reveal information about individual records in the database. To prevent such information leakage, randomness is introduced into the computation to hide details of individual records. A randomized algorithm A that takes as input a database d and outputs a value in a space O is said to satisfy (\u03f5, \u03b4)DP with respect to a metric \u03c1 over databases if, for any databases d and d\u2032 with \u03c1(d, d\u2032) \u22641, and for any subset of possible outputs S \u2286O, we have\n(2)\nHere, \u03f5 > 0 and \u03b4 \u2208[0, 1] are parameters that quantify the strength of the privacy guarantee. In the standard DP definition, the metric \u03c1 is the Hamming metric, which simply counts the number of entries that differ in the two databases. For small \u03f5 and \u03b4, the standard (\u03f5, \u03b4)-DP guarantee implies that changing a single entry in the database cannot change the output distribution very much. DP also applies to general metrics \u03c1 [8], including p-norms relevant to norm-based adversarial examples. Our approach relies on two key properties of DP. First is the well-known post-processing property: any computation applied to the output of an (\u03f5, \u03b4)-DP algorithm remains (\u03f5, \u03b4)-DP. Second is the expected output stability property, a rather obvious but not previously enunciated property that we prove in Lemma 1: the expected value of an (\u03f5, \u03b4)DP algorithm with bounded output is not sensitive to small changes in the input.\nLemma 1. (Expected Output Stability Bound) Suppose a randomized function A, with bounded output A(x) \u2208 [0, b], b \u2208R+, satisfies (\u03f5, \u03b4)-DP. Then the expected value of its output meets the following property:\n\u2200\u03b1 \u2208Bp(1) \ufffdE(A(x)) \u2264e\u03f5E(A(x + \u03b1)) + b\u03b4. The expectation is taken over the randomness in A.\nThe expectation is taken over the randomness in A.\nProof: Consider any \u03b1 \u2208Bp(1), and let x\u2032 := x + \u03b1. We write the expected output as: E(A(x)) = \ufffdb 0 P(A(x) > t)dt.\nProof: Consider any \u03b1 \u2208Bp(1), and let x\u2032 := x + \u03b1. We write the expected output as:\nWe next apply Equation (2) from the (\u03f5, \u03b4)-DP property:\nSince \u03b4 is a constant, \ufffdb 0 \u03b4dt = b\u03b4.\nSince \u03b4 is a constant, \ufffdb 0 \u03b4dt = b\u03b4\n# \ufffd C. DP-Robustness Connection\n\ufffd The intuition behind using DP to provide robustness to adversarial examples is to create a DP scoring function such that, given an input example, the predictions are DP with regards to the features of the input (e.g. the pixels of an image). In this setting, we can derive stability bounds for the expected output of the DP function using Lemma 1. The bounds, combined with Equation (1), give a rigorous condition (or certification) for robustness to adversarial examples. Formally, regard the feature values (e.g., pixels) of an input x as the records in a database, and consider a randomized scoring function A that, on input x, outputs scores (y1(x), . . . , yK(x)) (with yk(x) \u2208[0, 1] and \ufffdK k=1 yk(x) = 1). We say that A is an (\u03f5, \u03b4)-pixel-level differentially private (or (\u03f5, \u03b4)-PixelDP) function if it satisfies (\u03f5, \u03b4)-DP (for a given metric). This is formally equivalent to the standard definition of DP, but we use this terminology to emphasize the context in which we apply the definition, which is fundamentally different than the context in which DP is traditionally applied in ML (see \u00a7VI for distinction). Lemma 1 directly implies bounds on the expected outcome on an (\u03f5, \u03b4)-PixelDP scoring function:\nCorollary 1. Suppose a randomized function A satisfies (\u03f5, \u03b4)-PixelDP with respect to a p-norm metric, and where A(x) = (y1(x), . . . , yK(x)), yk(x) \u2208[0, 1]:\n\u2200k, \u2200\u03b1 \u2208Bp(1) \ufffdE(yk(x)) \u2264e\u03f5E(yk(x + \u03b1)) + \u03b4. (\n(3)\nProof: For any k apply Lemma 1 with b = 1. Our approach is to transform a model\u2019s scoring function into a randomized (\u03f5, \u03b4)-PixelDP scoring function, A(x), and then have the model\u2019s prediction procedure, f, use A\u2019s expected output over the DP noise, E(A(x)), as the label probability vector from which to pick the arg max. I.e., f(x) = arg maxk\u2208K E(Ak(x)). We prove that a model constructed this way allows the following robustness certification to adversarial examples:\nProposition 1. (Robustness Condition) Suppose A satisfies (\u03f5, \u03b4)-PixelDP with respect to a p-norm metric. For any input x, if for some k \u2208K,\n(4)\nthen the multiclass classification model based on label probability vector y(x) = (E(A1(x)), . . . , E(AK(x))) is\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d407/d407b6df-d11c-4078-968c-cde4031121bb.png\" style=\"width: 50%;\"></div>\nFig. 1: Architecture. (a) In blue, the original DNN. In red, the noise layer that provides the (\u03f5, \u03b4)-DP guarantees. The noise can be added to the inputs or any of the following layers, but the distribution is rescaled by the sensitivity \u2206p,q of the computation performed by each layer before the noise layer. The DNN is trained with the original loss and optimizer (e.g., Momentum stochastic gradient descent). Predictions repeatedly call the (\u03f5, \u03b4)-DP DNN o measure its empirical expectation over the scores. (b) After adding the bounds for the measurement error between the empirical and true expectation green) and the stability bounds from Lemma 1 for a given attack size Lattack (red), the prediction is certified robust to this attack size if the lower bound of the arg max label does not overlap with the upper bound of any other labels.\nrobust to attacks \u03b1 of size \u2225\u03b1\u2225p \u22641 on input x.\n# Proof: Consider any \u03b1 \u2208Bp(1), and let x\u2032 := x + \u03b1 From Equation (3), we have:\nProof: Consider any \u03b1 \u2208Bp(1), and let x\u2032 := x + \u03b1. From Equation (3), we have:\nE(Ak(x)) \u2264e\u03f5E(Ak(x\u2032)) + \u03b4, E(Ai(x\u2032)) \u2264e\u03f5E(Ai(x)) + \u03b4, i \u0338= k.\n(a) (b)\nEquation (a) gives a lower-bound on E(Ak(x\u2032)); Equation (b) gives an upper-bound on maxi\u0338=k E(Ai(x\u2032)). The hypothesis in the proposition statement (Equation (4)) implies that the lower-bound of the expected score for label k is strictly higher than the upper-bound for the expected score for any other label, which in turn implies the condition from Equation (1) for robustness at x. To spell it out:\n\u0338 =\u21d2E(Ak(x\u2032)) > max i:i\u0338=k E(Ai(x + \u03b1)) \u2200\u03b1 \u2208Bp(1),\nThe preceding certification test is exact regardless of the value of the \u03b4 parameter of differential privacy: there is no failure probability in this test. The test applies only to attacks of p-norm size of 1, however all preceding results generalize to attacks of p-norm size L, i.e., when \u2225\u03b1\u2225p \u2264L, by applying group privacy [18]. The next section shows how to apply group privacy (\u00a7III-B) and generalize the certification test to make it practical (\u00a7III-D).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9533/9533ed37-362e-4a6b-9f3c-0859376eccdc.png\" style=\"width: 50%;\"></div>\n# III. PixelDP Certified Defense\n# A. Architecture\nPixelDP is a certified defense against p-norm bounded adversarial example attacks built on the preceding DProbustness connection. Fig. 1(a) shows an example PixelDP DNN architecture for multi-class image classification. The original architecture is shown in blue; the changes introduced to make it PixelDP are shown in red. Denote Q the original DNN\u2019s scoring function; it is a deterministic map from images x to a probability distribution over the K labels Q(x) = (y1(x), . . . , yK(x)). The vulnerability to adversarial examples stems from the unbounded sensitivity of Q with respect to p-norm changes in the input. Making the DNN (\u03f5, \u03b4)-PixelDP involves adding calibrated noise to turn Q into an (\u03f5, \u03b4)-DP randomized function AQ; the expected output of that function will have bounded sensitivity to pnorm changes in the input. We achieve this by introducing a noise layer (shown in red in Fig. 1(a)) that adds zeromean noise to the output of the layer preceding it (layer1 in Fig. 1(a)). The noise is drawn from a Laplace or Gaussian distribution and its standard deviation is proportional to: (1) L, the p-norm attack bound for which we are constructing the network and (2) \u2206, the sensitivity of the pre-noise computation (the grey box in Fig. 1(a)) with respect to pnorm input changes. Training an (\u03f5, \u03b4)-PixelDP network is similar to training the original network: we use the original loss and optimizer, such as stochastic gradient descent. The major difference is that we alter the pre-noise computation to constrain its sensitivity with regards to p-norm input changes. Denote Q(x) = h(g(x)), where g is the pre-noise computation and h is the subsequent computation that produces Q(x) in the original network. We leverage known techniques, reviewed in \u00a7III-C, to transform g into another function, \u02dcg, that has a fixed sensitivity (\u2206) to p-norm input changes. We then add the noise layer to the output of \u02dcg, with a standard deviation scaled by \u2206and L to ensure (\u03f5, \u03b4)-PixelDP for p-norm\nchanges of size L. Denote the resulting scoring function of the PixelDP network: AQ(x) = h(\u02dcg(x)+noise(\u2206, L, \u03f5, \u03b4)), where noise(.) is the function implementing the Laplace/Gaussian draw. Assuming that the noise layer is placed such that h only processes the DP output of \u02dcg(x) without accessing x again (i.e., no skip layers exist from pre-noise to post-noise computation), the post-processing property of DP ensures that AQ(x) also satisfies (\u03f5, \u03b4)-PixelDP for p-norm changes of size L. Prediction on the (\u03f5, \u03b4)-PixelDP scoring function, AQ(x), affords the robustness certification in Proposition 1 if the prediction procedure uses the expected scores, E(AQ(x)), to select the winning label for any input x. Unfortunately, due to the potentially complex nature of the post-noise computation, h, we cannot compute this output expectation analytically. We therefore resort to Monte Carlo methods to estimate it at prediction time and develop an approximate version of the robustness certification in Proposition 1 that uses standard techniques from probability theory to account for the estimation error (\u00a7III-D). Specifically, given input x, PixelDP\u2019s prediction procedure invokes AQ(x) multiple times with new draws of the noise layer. It then averages the results for each label, thereby computing an estimation \u02c6E(AQ(x)) of the expected score E(AQ(x)). It then computes an \u03b7-confidence interval for \u02c6E(AQ(x)) that holds with probability \u03b7. Finally, it integrates this confidence interval into the stability bound for the expectation of a DP computation (Lemma 1) to obtain \u03b7-confidence upper and lower bounds on the change an adversary can make to the average score of any label with a p-norm input change of size up to L. Fig. 1(b) illustrates the upper and lower bounds applied to the average score of each label by the PixelDP prediction procedure. If the lower bound for the label with the top average score is strictly greater than the upper bound for every other label, then, with probability \u03b7, the PixelDP network\u2019s prediction for input x is robust to arbitrary attacks of p-norm size L. The failure probability of this robustness certification, 1\u2212\u03b7, can be made arbitrarily small by increasing the number of invocations of AQ(x). One can use PixelDP\u2019s certification check in two ways: (1) one can decide only to actuate on predictions that are deemed robust to attacks of a particular size; or (2) one can compute, on a test set, a lower bound of a PixelDP network\u2019s accuracy under p-norm bounded attack, independent of how the attack is implemented. This bound, called certified accuracy, will hold no matter how effective future generations of the attack are. The remainder of this section details the noise layer, training, and certified prediction procedures. To simplify notation, we will henceforth use A instead of AQ. B. DP Noise Layer The noise layer enforces (\u03f5, \u03b4)-PixelDP by inserting noise inside the DNN using one of two well-known DP mecha-\n# B. DP Noise Layer\nThe noise layer enforces (\u03f5, \u03b4)-PixelDP by inserting noise inside the DNN using one of two well-known DP mecha-\nnisms: the Laplacian and Gaussian mechanisms. Both rely upon the sensitivity of the pre-noise layers (function g). The sensitivity of a function g is defined as the maximum change in output that can be produced by a change in the input, given some distance metrics for the input and output (pnorm and q-norm, respectively):\nAssuming we can compute the sensitivity of the prenoise layers (addressed shortly), the noise layer leverages the Laplace and Gaussian mechanisms as follows. On every invocation of the network on an input x (whether for training or prediction) the noise layer computes g(x) + Z, where the coordinates Z = (Z1, . . . , Zm) are independent random variables from a noise distribution defined by the function noise(\u2206, L, \u03f5, \u03b4). \u2022 Laplacian mechanism: noise(\u2206, L, \u03f5, \u03b4) uses the Laplace distribution with mean zero and standard deviation \u03c3 = \u221a 2\u2206p,1L/\u03f5; it gives (\u03f5, 0)-DP. \u2022 Gaussian mechanism: noise(\u2206, L, \u03f5, \u03b4) uses the Gaussian distribution with mean zero and standard deviation \u03c3 = \ufffd 2 ln( 1.25 \u03b4 )\u2206p,2L/\u03f5; it gives (\u03f5, \u03b4)-DP for \u03f5 \u22641. Here, L denotes the p-norm size of the attack against which the PixelDP network provides (\u03f5, \u03b4)-DP; we call it the construction attack bound. The noise formulas show that for a fixed noise standard deviation \u03c3, the guarantee degrades gracefully: attacks twice as big halve the \u03f5 in the DP guarantee (L \u21902L \u21d2\u03f5 \u21902\u03f5). This property is often referred as group privacy in the DP literature [18]. Computing the sensitivity of the pre-noise function g depends on where we choose to place the noise layer in the DNN. Because the post-processing property of DP carries the (\u03f5, \u03b4)-PixelDP guarantee from the noise layer through the end of the network, a DNN designer has great flexibility in placing the noise layer anywhere in the DNN, as long as no skip connection exists from pre-noise to post-noise layers. We discuss here several options for noise layer placement and how to compute sensitivity for each. Our methods are not closely tied to particular network architectures and can therefore be applied on a wide variety of networks. Option 1: Noise in the Image. The most straightforward placement of the noise layer is right after the input layer, which is equivalent to adding noise to individual pixels of the image. This case makes sensitivity analysis trivial: g is the identity function, \u22061,1 = 1, and \u22062,2 = 1. Option 2: Noise after First Layer. Another option is to place the noise after the first hidden layer, which is usually simple and standard for many DNNs. For example, in image classification, networks often start with a convolution layer. In other cases, DNNs start with fully connected layer. These linear initial layers can be analyzed and their sensitivity computed as follows.\n\u03c3 = \ufffd 2 ln( \u03b4 )\u2206p,2L/\u03f5; it gives (\u03f5, \u03b4)-DP for \u03f5 \u22641. Here, L denotes the p-norm size of the attack against which the PixelDP network provides (\u03f5, \u03b4)-DP; we call it the construction attack bound. The noise formulas show that for a fixed noise standard deviation \u03c3, the guarantee degrades gracefully: attacks twice as big halve the \u03f5 in the DP guarantee (L \u21902L \u21d2\u03f5 \u21902\u03f5). This property is often referred as group privacy in the DP literature [18]. Computing the sensitivity of the pre-noise function g depends on where we choose to place the noise layer in the DNN. Because the post-processing property of DP carries the (\u03f5, \u03b4)-PixelDP guarantee from the noise layer through the end of the network, a DNN designer has great flexibility in placing the noise layer anywhere in the DNN, as long as no skip connection exists from pre-noise to post-noise layers. We discuss here several options for noise layer placement and how to compute sensitivity for each. Our methods are not closely tied to particular network architectures and can therefore be applied on a wide variety of networks. Option 1: Noise in the Image. The most straightforward placement of the noise layer is right after the input layer, which is equivalent to adding noise to individual pixels of the image. This case makes sensitivity analysis trivial: g is the identity function, \u22061,1 = 1, and \u22062,2 = 1. Option 2: Noise after First Layer. Another option is to place the noise after the first hidden layer, which is usually simple and standard for many DNNs. For example, in image classification, networks often start with a convolution layer. In other cases, DNNs start with fully connected layer. These linear initial layers can be analyzed and their sensitivity computed as follows.\nFor linear layers, which consist of a linear operator with matrix form W \u2208Rm,n, the sensitivity is the matrix norm, defined as: \u2225W\u2225p,q = supx:\u2225x\u2225p\u22641 \u2225Wx\u2225q. Indeed, the definition and linearity of W directly imply that \u2225W x\u2225q \u2225x\u2225p \u2264 \u2225W\u2225p,q, which means that: \u2206p,q = \u2225W\u2225p,q. We use the following matrix norms [64]: \u2225W\u22251,1 is the maximum 1norm of W\u2019s columns; \u2225W\u22251,2 is the maximum 2-norm of W\u2019s columns; and \u2225W\u22252,2 is the maximum singular value of W. For \u221e-norm attacks, we need to bound \u2225W\u2225\u221e,1 or \u2225W\u2225\u221e,2, as our DP mechanisms require q \u2208{1, 2}. However, tight bounds are computationally hard, so we currently use the following bounds: \u221an\u2225W\u22252,2 or \u221am\u2225W\u2225\u221e,\u221e where \u2225W\u2225\u221e,\u221eis the maximum 1-norm of W\u2019s rows. While these bounds are suboptimal and lead to results that are not as good as for 1-norm or 2-norm attacks, they allow us to include \u221e-norm attacks in our frameworks. We leave the study of better approximate bounds to future work. For a convolution layer, which is linear but usually not expressed in matrix form, we reshape the input (e.g. the image) as an Rndin vector, where n is the input size (e.g. number of pixels) and din the number of input channels (e.g. 3 for the RGB channels of an image). We write the convolution as an Rndout\u00d7ndin matrix where each column has all filter maps corresponding to a given input channel, and zero values. This way, a \u201ccolumn\u201d of a convolution consists of all coefficients in the kernel that correspond to a single input channel. Reshaping the input does not change sensitivity. Option 3: Noise Deeper in the Network. One can consider adding noise later in the network using the fact that when applying two functions in a row f1(f2(x)) we have: \u2206(f1\u25e6f2) p,q \u2264\u2206(f2) p,r \u2206(f1) r,q . For instance, ReLU has a sensitivity of 1 for p, q \u2208{1, 2, \u221e}, hence a linear layer followed by a ReLU has the same bound on the sensitivity as the linear layer alone. However, we find that this approach for sensitivity analysis is difficult to generalize. Combining bounds in this way leads to looser and looser approximations. Moreover, layers such as batch normalization [28], which are popular in image classification networks, do not appear amenable to such bounds (indeed, they are assumed away by some previous defenses [12]). Thus, our general recommendation is to add the DP noise layer early in the network \u2013 where bounding the sensitivity is easy \u2013 and taking advantage of DP\u2019s post-processing property to carry the sensitivity bound through the end of the network. Option 4: Noise in Auto-encoder. Pushing this reasoning further, we uncover an interesting placement possibility that underscores the broad applicability and flexibility of our approach: adding noise \u201cbefore\u201d the DNN in a separately trained auto-encoder. An auto-encoder is a special form of DNN trained to predict its own input, essentially learning the identity function f(x) = x. Auto-encoders are typically used to de-noise inputs [60], and are thus a good fit for PixelDP.\nGiven an image dataset, we can train a (\u03f5, \u03b4)-PixelDP autoencoder using the previous noise layer options. We stack it before the predictive DNN doing the classification and finetune the predictive DNN by running a few training steps on the combined auto-encoder and DNN. Thanks to the decidedly useful post-processing property of DP, the stacked DNN and auto-encoder are (\u03f5, \u03b4)-PixelDP. This approach has two advantages. First, the auto-encoder can be developed independently of the DNN, separating the concerns of learning a good PixelDP model and a good predictive DNN. Second, PixelDP auto-encoders are much smaller than predictive DNNs, and are thus much faster to train. We leverage this property to train the first certified model for the large ImageNet dataset, using an auto-encoder and the pre-trained Inception-v3 model, a substantial relief in terms of experimental work (\u00a7IV-A).\n# C. Training Procedure\nThe soundness of PixelDP\u2019s certifications rely only on enforcing DP at prediction time. Theoretically, one could remove the noise layer during training. However, doing so results in near-zero certified accuracy in our experience. Unfortunately, training with noise anywhere except in the image itself raises a new challenge: left unchecked the training procedure will scale up the sensitivity of the prenoise layers, voiding the DP guarantees. To avoid this, we alter the pre-noise computation to keep its sensitivity constant (e.g. \u2206p,q \u22641) during training. The specific technique we use depends on the type of sensitivity we need to bound, i.e. on the values of p and q. For \u22061,1, \u22061,2, or \u2206\u221e,\u221e, we normalize the columns, or rows, of linear layers and use the regular optimization process with fixed noise variance. For \u22062,2, we run the projection step described in [12] after each gradient step from the stochastic gradient descent (SGD). This makes the pre-noise layers Parseval tight frames, enforcing \u22062,2 = 1. For the pre-noise layers, we thus alternate between an SGD step with fixed noise variance and a projection step. Subsequent layers from the original DNN are left unchanged. It is important to note that during training, we optimize for a single draw of noise to predict the true label for a training example x. We estimate E(A(x)) using multiple draws of noise only at prediction time. We can interpret this as pushing the DNN to increase the margin between the expected score for the true label versus others. Recall from Equation (4) that the bounds on predicted outputs give robustness only when the true label has a large enough margin compared to other labels. By pushing the DNN to give high scores to the true label k at points around x likely under the noise distribution, we increase E(Ak(x)) and decrease E(Ai\u0338=k(x)).\n# \u0338 D. Certified Prediction Procedure\nFor a given input x, the prediction procedure in a traditional DNN chooses the arg max label based on the\nscore vector obtained from a single execution of the DNN\u2019s deterministic scoring function, Q(x). In a PixelDP network, the prediction procedure differs in two ways. First, it chooses the arg max label based on a Monte Carlo estimation of the expected value of the randomized DNN\u2019s scoring function, \u02c6E(A(x)). This estimation is obtained by invoking A(x) multiple times with independent draws in the noise layer. Denote ak,n(x) the nth draw from the distribution of the randomized function A on the kth label, given x (so ak,n(x) \u223cAk(x)). In Lemma 1 we replace E(Ak(x)) with \u02c6E(Ak(x)) = 1 n \ufffd n ak,n(x), where n is the number of invocations of A(x). We compute \u03b7-confidence error bounds to account for the estimation error in our robustness bounds, treating each label\u2019s score as a random variable in [0, 1]. We use Hoeffding\u2019s inequality [25] or Empirical Bernstein bounds [39] to bound the error in \u02c6E(A(x)). We then apply a union bound so that the bounds for each label are all valid together. For instance, using Hoeffding\u2019s inequality, with probability \u03b7, \u02c6Elb(A(x)) \u225c\u02c6E(A(x)) \u2212 \ufffd 1 2nln( 2k 1\u2212\u03b7 ) \u2264 E(A(x)) \u2264\u02c6E(A(x)) + \ufffd 1 2nln( 2k 1\u2212\u03b7 ) \u225c\u02c6Eub(A(x)). Second, PixelDP returns not only the prediction for x (arg max(\u02c6E(A(x)))) but also a robustness size certificate for that prediction. To compute the certificate, we extend Proposition 1 to account for the measurement error:\nProposition 2. (Generalized Robustness Condition) Suppose A satisfies (\u03f5, \u03b4)-PixelDP with respect to changes of size L in p-norm metric. Using the notation from Proposition 1 further let \u02c6Eub(Ai(x)) and \u02c6Elb(Ai(x)) be the \u03b7confidence upper and lower bound, respectively, for the Monte Carlo estimate \u02c6E(Ai(x)). For any input x, if for some k \u2208K,\nthen the multiclass classification model based on label probabilities (\u02c6E(A1(x)), . . . , \u02c6E(AK(x))) is robust to attacks of p-norm L on input x with probability \u2265\u03b7.\nThe proof is similar to the one for Proposition 1 and is detailed in Appendix A. Note that the DP bounds are not probabilistic even for \u03b4 > 0; the failure probability 1 \u2212 \u03b7 comes from the Monte Carlo estimate and can be made arbitrarily small with more invocations of A(x). Thus far, we have described PixelDP certificates as binary with respect to a fixed attack bound, L: we either meet or do not meet a robustness check for L. In fact, our formalism allows for a more nuanced certificate, which gives the maximum attack size Lmax (measured in p-norm) against which the prediction on input x is guaranteed to be robust: no attack within this size from x will be able to change the highest probability. Lmax can differ for different inputs. We compute the robustness size certificate for input x as follows. Recall from III-B that the DP mechanisms have a noise standard deviation \u03c3 that grows in \u2206p,qL \u03f5 . For a given\n<div style=\"text-align: center;\">\u03c3 used at prediction time, we solve for the maximum L for which the robustness condition in Proposition 2 checks out:</div>\n\u03c3 used at prediction time, we solve for the maximum L for which the robustness condition in Proposition 2 checks out:\nLmax = maxL\u2208R+ L such that \u02c6Elb(Ak(x)) > e2\u03f5\u02c6Eub(Ai:i\u0338=k(x)) + (1 + e\u03f5)\u03b4 AND either \u2022 \u03c3 = \u2206p,1L/\u03f5 and \u03b4 = 0 (for Laplace) OR \u2022 \u03c3 = \ufffd 2 ln(1.25/\u03b4)\u2206p,2L/\u03f5 and \u03f5 \u22641 (for Gaussian).\n\ufffd The prediction on x is robust to attacks up to Lmax, so we award a robustness size certificate of Lmax for x. We envision two ways of using robustness size certifications. First, when it makes sense to only take actions on the subset of robust predictions (e.g., a human can intervene for the rest), an application can use PixelDP\u2019s certified robustness on each prediction. Second, when all points must be classified, PixelDP gives a lower bound on the accuracy under attack. Like in regular ML, the testing set is used as a proxy for the accuracy on new examples. We can certify the minimum accuracy under attacks up to a threshold size T, that we call the prediction robustness threshold. T is an inference-time parameter that can differ from the construction attack bound parameter, \u0141, that is used to configure the standard deviation of the DP noise. In this setting the certification is computed only on the testing set, and is not required for each prediction. We only need the highest probability label, which requires fewer noise draws. \u00a7IV-E shows that in practice a few hundred draws are sufficient to retain a large fraction of the certified predictions, while a few dozen are needed for simple predictions.\n# IV. Evaluation\nWe evaluate PixelDP by answering four key questions: Q1: How does DP noise affect model accuracy? Q2: What accuracy can PixelDP certify? Q3: What is PixelDP\u2019s accuracy under attack and how does it compare to that of other best-effort and certified defenses? Q4: What is PixelDP\u2019s computational overhead? We answer these questions by evaluating PixelDP on five standard image classification datasets and networks \u2013 both large and small \u2013 and comparing it with one prior certified defense [65] and one best-effort defense [37]. \u00a7IV-A describes the datasets, prior defenses, and our evaluation methodology; subsequent sections address each question in turn. Evaluation highlights: PixelDP provides meaningful certified robustness bounds for reasonable degradation in model accuracy on all datasets and DNNs. To the best of our knowledge, these include the first certified bounds for large, complex datasets/networks such as the Inception network on ImageNet and Residual Networks on CIFAR-10. There, PixelDP gives 60% certified accuracy for 2-norm attacks up to 0.1 at the cost of 8.5 and 9.2 percentage-point accuracy degradation respectively. Comparing PixelDP to the prior certified defense on smaller datasets, PixelDP models give\nhigher accuracy on clean examples (e.g., 92.9% vs. 79.6% accuracy SVHN dataset), and higher robustness to 2-norm attacks (e.g., 55% vs. 17% accuracy on SVHN for 2-norm attacks of 0.5), thanks to the ability to scale to larger models. Comparing PixelDP to the best-effort defense on larger models and datasets, PixelDP matches its accuracy (e.g., 87% for PixelDP vs. 87.3% on CIFAR-10) and robustness to 2-norm bounded attacks.\nA. Methodology Datasets. We evaluate PixelDP on image classification tasks from five pubic datasets listed in Table I. The datasets are listed in descending order of size and complexity for classification tasks. MNIST [69] consists of greyscale handwritten digits and is the easiest to classify. SVHN [44] contains small, real-world digit images cropped from Google Street View photos of house numbers. CIFAR-10 and CIFAR100 [33] consist of small color images that are each centered on one object of one of 10 or 100 classes, respectively. ImageNet [13] is a large, production-scale image dataset with over 1 million images spread across 1,000 classes. Models: Baselines and PixelDP. We use existing DNN architectures to train a high-performing baseline model for each dataset. Table I shows the accuracy of the baseline models. We then make each of these networks PixelDP with regards to 1-norm and 2-norm bounded attacks. We also did rudimentary evaluation of \u221e-norm bounded attacks, shown in Appendix D. While the PixelDP formalism can support \u221e-norm attacks, our results show that tighter bounds are needed to achieve a practical defense. We leave the development and evaluation of these bounds for future work. Table II shows the PixelDP configurations we used for the 1-norm and 2-norm defenses. The code is available at https://github.com/columbia/pixeldp. Since most of this section focuses on models with 2-norm attack bounds, we detail only those configurations here. ImageNet: We use as baseline a pre-trained version of Inception-v3 [55] available in Tensorflow [22]. To make it PixelDP, we use the autoencoder approach from \u00a7III-B, which does not require a full retraining of Inception and was instrumental in our support of ImageNet. The encoder has three convolutional layers and tied encoder/decoder weights. The convolution kernels are 10 \u00d7 10 \u00d7 32, 8 \u00d7 8 \u00d7 32, and 5\u00d75\u00d764, with stride 2. We make the autoencoder PixelDP by adding the DP noise after the first convolution. We then stack the baseline Inception-v3 on the PixelDP autoencoder and fine-tune it for 20k steps, keeping the autoencoder weights constant. CIFAR-10, CIFAR-100, SVHN: We use the same baseline architecture, a state-of-the-art Residual Network (ResNet) [70]. Specifically we use the Tensorflow implementation of a 28-10 wide ResNet [57], with the default parameters. To make it PixelDP, we slightly alter the architecture to remove the image standardization step. This step makes sensitivity input dependent, which is harder to deal with in\nPixelDP. Interestingly, removing this step also increases the baseline\u2019s own accuracy for all three datasets. In this section, we therefore report the accuracy of the changed networks as baselines. MNIST: We train a Convolutional Neural Network (CNN) with two 5 \u00d7 5 convolutions (stride 2, 32 and 64 filters) followed by a 1024 nodes fully connected layer. Evaluation Metrics. We use two accuracy metrics to evaluate PixelDP models: conventional accuracy and certified accuracy. Conventional accuracy (or simply accuracy) denotes the fraction of a testing set on which a model is correct; it is the standard accuracy metric used to evaluate any DNN, defended or not. Certified accuracy denotes the fraction of the testing set on which a certified model\u2019s predictions are both correct and certified robust for a given prediction robustness threshold; it has become a standard metric to evaluate models trained with certified defenses [65], [52], [16]. We also use precision on certified examples, which measures the number of correct predictions exclusively on examples that are certified robust for a given prediction robustness threshold. Formally, the metrics are defined as follows: 1) Conventional accuracy \ufffdn i=1 isCorrect(xi) n , where n is the testing set size and isCorrect(xi) denotes a function returning 1 if the prediction on test sample xi returns the correct label, and 0 otherwise. 2) Certified accuracy \ufffdn i=1(isCorrect(xi)&robustSize(scores,\u03f5,\u03b4,L)\u2265T ) n , where robustSize(scores, \u03f5, \u03b4, L) returns the certified robustness size, which is then compared to the prediction robustness threshold T. 3) Precision on certified examples \ufffdn i=1(isCorrect(xi)&robustSize(pi,\u03f5,\u03b4,L)\u2265T )) \ufffdn i=1 robustSize(pi,\u03f5,\u03b4,L)\u2265T ) . For T = 0 all predictions are robust, so certified accuracy is equivalent to conventional accuracy. Each time we report L or T, we use a [0, 1] pixel range. Attack Methodology. Certified accuracy \u2013 as provided by PixelDP and other certified defense \u2013 constitutes a guar-\n\ufffd \ufffd \u2265 For T = 0 all predictions are robust, so certified accuracy is equivalent to conventional accuracy. Each time we report L or T, we use a [0, 1] pixel range. Attack Methodology. Certified accuracy \u2013 as provided by PixelDP and other certified defense \u2013 constitutes a guaranteed lower-bound on accuracy under any norm-bounded attack. However, the accuracy obtained in practice when faced with a specific attack can be much better. How much better depends on the attack, which we evaluate in two steps. We first perform an attack on 1,000 randomly picked samples (as is customary in defense evaluation [37]) from the testing set. We then measure conventional accuracy on the attacked test examples. For our evaluation, we use the state-of-the art attack from Carlini and Wagner [7], that we run for 9 iterations of binary search, 100 gradient steps without early stopping (which we empirically validated to be sufficient), and learning rate 0.01. We also adapt the attack to our specific defense following [2]: since PixelDP adds noise to the DNN, attacks based on optimization may fail due to the high variance\nDataset\nImage\nsize\nTraining\nset size\nTesting\nset size\nTarget\nlabels\nClassifier\narchitecture\nBaseline\naccuracy\nImageNet [13]\n299x299x3 1.4M\n50K\n1000\nInception V3\n77.5%\nCIFAR-100 [33]\n32x32x3\n50K\n10K\n100\nResNet\n78.6%\nCIFAR-10 [33]\n32x32x3\n50K\n10K\n10\nResNet\n95.5%\nSVHN [44]\n32x32x3\n73K\n26K\n10\nResNet\n96.3%\nMNIST [69]\n28x28x1\n60K\n10K\n10\nCNN\n99.2%\nTable I: Evaluation datasets and baseline models. Last column shows the accuracy\nTable I: Evaluation datasets and baseline models. Last column shows the accuracy of the baseline, undefended models. The datasets are sorted based on descending order of scale or complexity.\nTable I: Evaluation datasets and baseline models. Last column shows the accuracy of the baseline, undefended models. The datasets are sorted based on descending order of scale or complexity.\nDataset\nBaseline\nL = 0.03\nL = 0.1\nL = 0.3\nL = 1.0\nImageNet\n77.5%\n\u2013\n68.3%\n57.7%\n37.7%\nCIFAR-10\n95.5%\n93.3%\n87.0%\n70.9%\n44.3%\nCIFAR-100\n78.6%\n73.4%\n62.4%\n44.3%\n22.1%\nSVHN\n96.3%\n96.1%\n93.1%\n79.6%\n28.2%\nMNIST\n99.2%\n99.1%\n99.1%\n98.2%\n11%\nTable III: Impact of PixelDP noise on conventional accuracy. For each DNN, we show different levels of construction attack size L. Conventional accuracy degrades with noise level.\nof gradients, which would not be a sign of the absence of adversarial examples, but merely of them being harder to find. We address this concern by averaging the gradients over 20 noise draws at each gradient step. Appendix \u00a7C contains more details about the attack, including sanity checks and another attack we ran similar to the one used in [37]. Prior Defenses for Comparison. We use two state-of-art defenses as comparisons. First, we use the empirical defense model provided by the Madry Lab for CIFAR-10 [38]. This model is developed in the context of \u221e-norm attacks. It uses an adversarial training strategy to approximately minimize the worst case error under malicious samples [37]. While inspired by robust optmization theory, this methodology is best effort (see \u00a7VI) and supports no formal notion of robustness for individual predictions, as we do in PixelDP. However, the Madry model performs better under the latest attacks than other best-effort defenses (it is in fact the only one not yet broken) [2], and represents a good comparison point. Second, we compare with another approach for certified robustness against \u221e-norm attacks [65], based on robust optimization. This method does not yet scale to the largest datasets (e.g. ImageNet), or the more complex DNNs (e.g. ResNet, Inception) both for computational reasons and because not all necessary layers are yet supported (e.g. BatchNorm). We thus use their largest released model/dataset, namely a CNN with two convolutions and a 100 nodes fully connected layer for the SVHN dataset, and compare their robustness guarantees with our own networks\u2019 robustness guarantees. We call this SVHN CNN model RobustOpt.\n# B. Impact of Noise (Q1)\nQ1: How does DP noise affect the conventional accuracy of our models? To answer, for each dataset we train up to four (1.0, 0.05)-PixelDP DNN, for construction attack bound \u0141 \u2208{0.03, 0.1, 0.3, 1}. Higher values of L correspond to\np-norm\nDP\nNoise\nSensitivity\nused\nmechanism\nlocation\napproach\n1-norm\nLaplace\n1st conv.\n\u22061,1 = 1\n1-norm\nGaussian\n1st conv.\n\u22061,2 = 1\n2-norm\nGaussian\n1st conv.\n\u22062,2 \u22641\n1-norm\nLaplace\nAutoencoder\n\u22061,1 = 1\n2-norm\nGaussian\nAutoencoder\n\u22062,2 \u22641\n \u2264 Table II: Noise layers in PixelDP DNNs. For each DNN, we implement defenses for different attack bound norms and DP mechanisms.\nrobustness against larger attacks and larger noise standard deviation \u03c3. Table III shows the conventional accuracy of these networks and highlights two parts of an answer to Q1. First, at fairly low but meaningful construction attack bound (e.g., L = 0.1), all of our DNNs exhibit reasonable accuracy loss \u2013 even on ImageNet, a dataset on which no guarantees have been made to date! ImageNet: The Inception-v3 model stacked on the PixelDP auto-encoder has an accuracy of 68.3% for L = 0.1, which is reasonable degradation compared to the baseline of 77.5% for the unprotected network. CIFAR-10: Accuracy goes from 95.5% without defense to 87% with the L = 0.1 defense. For comparison, the Madry model has an accuracy of 87.3% on CIFAR-10. SVHN: our L = 0.1 PixelDP network achieves 93.1% conventional accuracy, down from 96.3% for the unprotected network. For comparison, the L = 0.1 RobustOpt network has an accuracy of 79.6%, although they use a smaller DNN due to the computationally intensive method. Second, as expected, constructing the network for larger attacks (higher L) progressively degrades accuracy. ImageNet: Increasing L to 0.3 and then 1.0 drops the accuracy to 57.7% and 37.7%, respectively. CIFAR-10: The ResNet with the least noise (L = 0.03) reaches 93.3% accuracy, close to the baseline of 95.5%; increasing noise levels (L = (0.1, 0.3, 1.0)) yields 87%, 70.9%, and 37.7%, respectively. Yet, as shown in \u00a7IV-D, PixelDP networks trained with fairly low \u0141 values (such as \u0141 = 0.1) already provide meaningful empirical protection against larger attacks.\n# C. Certified Accuracy (Q2)\nQ2: What accuracy can PixelDP certify on a test set? Fig. 2 shows the certified robust accuracy bounds for ImageNet and CIFAR-10 models, trained with various values of the construction attack bound \u0141. The certified accuracy is shown as a function of the prediction robustness threshold, T. We make two observations. First, PixelDP yields meaningful robust accuracy bounds even on large networks for ImageNet (see Fig. 2(a)), attesting the scalability of our approach. The \u0141 = 0.1 network has a certified accuracy of 59% for attacks smaller than 0.09 in 2-norm. The \u0141 = 0.3 network has a certified accuracy of 40% to attacks up to size 0.2. To our knowledge, PixelDP is the first defense to yield DNNs with certified bounds on accuracy under 2-norm attacks on datasets of ImageNet\u2019s size and for large networks\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/230f/230f9742-a3eb-4945-8bae-07f3159c8a19.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">accuracy, varying the construction attack bound (L) and prediction robustness threshold (T), on ImageNet ion and CIFAR-10 ResNet, 2-norm bounds. Robust accuracy at high Robustness thresholds (high T) increases with s (high L). Low noise networks are both more accurate and more certifiably robust for low T.</div>\nFig. 2: Certified accuracy, varying the construction attack bound (L) and prediction robustness threshold (T), on ImageNet auto-encoder/Inception and CIFAR-10 ResNet, 2-norm bounds. Robust accuracy at high Robustness thresholds (high T) increases with high-noise networks (high L). Low noise networks are both more accurate and more certifiably robust for low T.\n# like Inception.\nlike Inception. Second, PixelDP networks constructed for larger attacks (higher \u0141, hence higher noise) tend to yield higher certified accuracy for high thresholds T. For example, the ResNet on CIFAR-10 (see Fig. 2(b)) constructed with L = 0.03 has the highest robust accuracy up to T = 0.03, but the ResNet constructed with L = 0.1 becomes better past that threshold. Similarly, the L = 0.3 ResNet has higher robust accuracy than the L = 0.1 ResNet above the 0.14 2-norm prediction robustness threshold. We ran the same experiments on SVHN, CIFAR-100 and MNIST models but omit the graphs for space reasons. Our main conclusion \u2013 that adding more noise (higher L) hurts both conventional and low T certified accuracy, but enhances the quality of its high T predictions \u2013 holds in all cases. Appendix B discusses the impact of some design choices on robust accuracy, and Appendix D discusses PixelDP guarantees as compared with previous certified defenses for \u221e-norm attacks. While PixelDP does not yet yield strong \u221e-norm bounds, it provides meaningful certified accuracy bounds for 2-norm attacks, including on much larger and more complex datasets and networks than those supported by previous approaches.\n# D. Accuracy Under Attack (Q3)\nA standard method to evaluate the strength of a defense is to measure the conventional accuracy of a defended model on malicious samples obtained by running a state-of-theart attack against samples in a held-out testing set [37]. We apply this method to answer three aspects of question Q3: (1) Can PixelDP help defend complex models on large datasets in practice? (2) How does PixelDP\u2019s accuracy under attack compare to state-of-the-art defenses? (3) How does the accuracy under attack change for certified predictions? Accuracy under Attack on ImageNet. We first study conventional accuracy under attack for PixelDP models on ImageNet. Fig. 3 shows this metric for 2-norm attacks on the baseline Inception-v3 model, as well as three defended\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/386f/386f7a06-b7ac-4085-bc34-3dbf764cefcd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 3: Accuracy under attack on ImageNet. For the ImageNet auto-encoder plus Inception-v3, L \u2208{0.1, 0.3, 1.0} 2norm attacks. The PixelDP auto-encoder increases the robustness of Inception against 2-norm attacks.</div>\nversions, with a stacked PixelDP auto-encoder trained with construction attack bound L \u2208{0.1, 0.3, 1.0}. PixelDP makes the model significantly more robust to attacks. For attacks of size Lattack = 0.5, the baseline model\u2019s accuracy drops to 11%, whereas the L = 0.1 PixelDP model\u2019s accuracy remains above 60%. At Lattack = 1.5, the baseline model has an accuracy of 0, but the L = 0.1 PixelDP is still at 30%, while the L = 0.3 PixelDP model have more that 39% accuracy. Accuracy under Attack Compared to Madry. Fig. 4(a) compares conventional accuracy of a PixelDP model to that of a Madry model on CIFAR-10, as the empirical attack bound increases for 2-norm attacks. For 2-norm attacks, our model achieves conventional accuracy on par with, or slightly higher than, that of the Madry model. Both models are dramatically more robust under this attack compared to the baseline (undefended) model. For \u221e-norm attacks our model does not fare as well, which is expected as the PixelDP model is trained to defend against 2-norm attacks, while the Madry model is optimized for \u221e-norm attacks. For Lattack = 0.01, PixelDP\u2019s accuracy is 69%, 8 percentage points lower than Madry\u2019s. The gap increases until PixelDP arrives at 0 accuracy for Lattack = 0.06, with Madry still having 22%. Appendix \u00a7D details this evaluation.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7daa/7daa4668-cd09-45df-8b00-2febab2d53af.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: Accuracy under 2-norm attack for PixelDP vs. Madry and RobustOpt, CIFAR-10 and SVHN. For 2-norm attacks, PixelD is on par with Madry until Lattack \u22651.2; RobustOpt support only small models, and has lower accuracy.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/68aa/68aa72df-d133-4c7c-b109-d999ebfbfd89.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 5: PixelDP certified predictions vs. Madry accuracy, under attack, CIFAR-10 ResNets, 2-norm attack. PixelDP makes fewer but more correct predictions up to Lattack = 1.0.</div>\nAccuracy under Attack Compared to RobustOpt. Fig. 4(b) shows a similar comparison with the RobustOpt defense [65], which provides certified accuracy bounds for \u221enorm attacks. We use the SVHN dataset for the comparison as the RobustOpt defense has not yet been applied to larger datasets. Due to our support of larger DNN (ResNet), PixelDP starts with higher accuracy, which it maintains under 2norm attacks. For attacks of Lattack = 0.5, RobustOpt is bellow 20% accuracy, and PixelDP above 55%. Under \u221e-norm attacks, the behavior is different: PixelDP has the advantage up to Lattack = 0.015 (58.8% to 57.1%), and RobustOpt is better thereafter. For instance, at Lattack = 0.03, PixelDP has 22.8% accuracy, to RobustOpt\u2019s 32.7%. Appendix \u00a7D details the \u221e-norm attack evaluation. Precision on Certified Predictions Under Attack. Another interesting feature of PixelDP is its ability to make certifiably robust predictions. We compute the accuracy of these certified predictions under attack \u2013 which we term robust precision \u2013 and compare them to predictions of the Madry network that do not provide such a certification. Fig. 5 shows the results of considering only predictions with a certified robustness above 0.05 and 0.1. It reflects the benefit to be gained by applications that can leverage our theoretical guarantees to filter out non-robust predictions. We observe that PixelDP\u2019s robust predictions are substantially more correct than Madry\u2019s predictions up to an empirical attack\n<div style=\"text-align: center;\">(b) SVHN</div>\nbound of 1.1. For T = 0.05 PixelDP\u2019s robust predictions are 93.9% accurate, and up to 10 percentage points more correct under attack for Lattack \u22641.1. A robust prediction is given for above 60% of the data points. The more conservative the robustness test is (higher T), the more correct PixelDP\u2019s predictions are, although it makes fewer of them (Certified Fraction lines). Thus, for applications that can afford to not act on a minority of the predictions, PixelDP\u2019s robust predictions under 2-norm attack are substantially more precise than Madry\u2019s. For applications that need to act on every prediction, PixelDP offers on-par accuracy under 2-norm attack to Madry\u2019s. Interestingly, although our defense is trained for 2-norm attacks, the first conclusion still holds for \u221e-norm attacks; the second (as we saw) does not.\n# E. Computational Overhead (Q4)\nQ4: What is PixelDP\u2019s computational overhead? We evaluate overheads for training and prediction. PixelDP adds little overhead for training, as the only additions are a random noise tensor and sensitivity computations. On our GPU, the CIFAR-10 ResNet baseline takes on average 0.65s per training step. PixelDP versions take at most 0.66s per training step (1.5% overhead). This represents a significant benefit over adversarial training (e.g. Madry) that requires finding good adversarial attacks for each image in the minibatch at each gradient step, and over robust optimization (e.g. RobustOpt) that requires solving a constrained optimization problem at each gradient step. The low training overhead is instrumental to our support of large models and datasets. PixelDP impacts prediction more substantially, since it uses multiple noise draws to estimate the label scores. Making a prediction for a single image with 1 noise draw takes 0.01s on average. Making 10 draws brings it only to 0.02s, but 100 requires 0.13s, and 1000, 1.23s. It is possible to use Hoeffding\u2019s inequality [25] to bound the number of draws necessary to distinguish the highest score with probability at least \u03b7, given the difference between the top two scores ymax \u2212ysecond\u2212max. Empirically, we\nfound that 300 draws were typically necessary to properly certify a prediction, implying a prediction time of 0.42s seconds, a 42\u00d7 overhead. This is parallelizable, but resource consumption is still substantial. To make simple predictions \u2013 distinguish the top label when we must make a prediction on all inputs \u2013 25 draws are enough in practice, reducing the overhead to 3\u00d7.\n# V. Analysis\nWe make three points about PixelDP\u2019s guarantees and applicability. First, we emphasize that our Monte Carlo approximation of the function x \ufffd\u2192E(A(x)) is not intended to be a DP procedure. Hence, there is no need to apply composition rules from DP, because we do not need this randomized procedure to be DP. Rather, the Monte Carlo approximation x \ufffd\u2192\u02c6E(A(x)) is just that: an approximation to a function x \ufffd\u2192E(A(x)) whose robustness guarantees come from Lemma 1. The function x \ufffd\u2192\u02c6E(A(x)) does not satisfy DP, but because we can control the Monte Carlo estimation error using standard tools from probability theory, it is also robust to small changes in the input, just like x \ufffd\u2192E(A(x)). Second, Proposition 1 is not a high probability result; it is valid with probability 1 even when A is (\u03f5, \u03b4 > 0)-DP. The \u03b4 parameter can be thought of as a \u201cfailure probability\u201d of an (\u03f5, \u03b4)-DP mechanism: a chance that a small change in input will cause a big change in the probability of some of its outputs. However, since we know that Ak(x) \u2208[0, 1], the worst-case impact of such failures on the expectation of the output of the (\u03f5, \u03b4)-DP mechanism is at most \u03b4, as proven in Lemma 1. Proposition 1 explicitly accounts for this worst-case impact (term (1 + e\u03f5)\u03b4 in Equation (4)). Were we able to compute E(A(x)) analytically, PixelDP would output deterministic robustness certificates. In practice however, the exact value is too complex to compute, and hence we approximate it using a Monte Carlo method. This adds probabilistic measurement error bounds, making the final certification (Proposition 2) a high probability result. However, the uncertainty comes exclusively from the Monte Carlo integration \u2013 and can be made arbitrarily small with more runs of the PixelDP DNN \u2013 and not from the underlying (\u03f5, \u03b4)-DP mechanism A. Making the uncertainty small gives an adversary a small chance to fool a PixelDP network into thinking that its prediction is robust when it is not. The only ways an attacker can increase that chance is by either submitting the same attack payload many times or gaining control over PixelDP\u2019s source of randomness. Third, PixelDP applies to any task for which we can measure changes to input in a meaningful p-norm, and bound the sensitivity to such changes at a given layer in the DNN (e.g. sensitivity to a bounded change in a word frequency vector, or a change of class for categorical attributes). PixelDP also applies to multiclass classification where the prediction procedure returns several top-scoring\nlabels. Finally, Lemma 1 can be extended to apply to DP mechanism with (bounded) output that can also be negative, as shown in Appendix E. PixelDP thus directly applies to DNNs for regression tasks (i.e. predicting a real value instead of a category) as long as the output is bounded (or unbounded if \u03b4 = 0). The output can be bounded due to the specific task, or by truncating the results to a large range of values and using a comparatively small \u03b4.\n# VI. Related Work\nOur work relates to a significant body of work in adversarial examples and beyond. Our main contribution to this space is to introduce a new and very different direction for building certified defenses. Previous attempts have built on robust optimization theory. In PixelDP we propose a new approach built on differential privacy theory which exhibits a level of flexibility, broad applicability, and scalability that exceeds what robust optimization-based certified defenses have demonstrated. While the most promising way to defend against adversarial examples is still an open question, we observe undebatable benefits unique to our DP based approach, such as the post-processing guarantee of our defense. In particular, the ability to prepend a defense to unmodified networks via a PixelDP auto-encoder, as we did to defend Inception with no structural changes, is unique among certified (and best-effort) defenses. Best-effort Defenses. Defenders have used multiple heuristics to empirically increase DNNs\u2019 robustness. These defenses include model distillation [45], automated detection of adversarial examples [24], [42], [41], application of various input transformations [29], [10], randomization [23], [11], and generative models [51], [27], [68]. Most of these defenses have been broken, sometimes months after their publication [7], [6], [2]. The main empirical defense that still holds is Madry et al. [37], based on adversarial training [21]. Madry et al. motivate their approach with robust optimization, a rigorous theory. However not all the assumptions are met, as this approach runs a best-effort attack on each image in the minibatch at each gradient step, when the theory requires finding the best possible adversarial attack. And indeed, finding this worst case adversarial example for ReLU DNNs, used in [37], was proven to be NP-hard in [53]. Therefore, while this defense works well in practice, it gives no theoretical guarantees for individual predictions or for the models accuracy under attack. PixelDP leverages DP theory to provide guarantees of robustness to arbitrary, norm-based attacks for individual predictions. Randomization-based defenses are closest in method to our work [23], [11], [35]. For example, Liu et al. [35] randomizes the entire DNN and predicts using an ensemble of multiple copies of the DNN, essentially using draws to roughly estimate the expected arg max prediction. They observe empirically that randomization smoothens the pre-\ndiction function, improving robustness to adversarial examples. However, randomization-based prior work provides limited formalism that is insufficient to answer important defense design questions: where to add noise, in what quantities, and what formal guarantees can be obtained from randomization? The lack of formalism has caused some works [23], [11] to add insufficient amounts of noise (e.g., noise not calibrated to pre-noise sensitivity), which makes them vulnerable to attack [6]. On the contrary, [35] inserts randomness into every layer of the DNN: our work shows that adding the right amount of calibrated noise at a single layer is sufficient to leverage DP\u2019s post-processing guarantee and carry the bounds through the end of the network. Our paper formalizes randomization-based defenses using DP theory, and in doing so helps answer many of these design questions. Our formalism also lets us reason about the guarantees obtained through randomization and enables us to elevate randomization-based approaches from the class of best-effort defenses to that of certified defenses. Certified Defenses and Robustness Evaluations. PixelDP offers two functions: (1) a strategy for learning robust models and (2) a method for evaluating the robustness of these models against adversarial examples. Both of these approaches have been explored in the literature. First, several certified defenses modify the neural network training process to minimize the number of robustness violations [65], [52], [12]. These approaches, though promising, do not yet scale to larger networks like Google Inception [65], [52]. In fact, all published certified defenses have been evaluated on small models and datasets [65], [52], [12], [43], and at least in one case, the authors directly acknowledge that some components of their defense would be \u201ccompletely infeasible\u201d on ImageNet [65]. A recent paper [16] presents a certified defense evaluated on the CIFAR-10 dataset [33] for multi-layer DNNs (but smaller than ResNets). Their approach is completely different from ours and, based on the current results we see no evidence that it can readily scale to large datasets like ImageNet. Another approach [53] combines robust optimization and adversarial training in a way that gives formal guarantees and has lower computational complexity than previous robust optimization work, hence it has the potential to scale better. This approach requires smooth DNNs (e.g., no ReLU or max pooling) and robustness guarantees are over the expected loss (e.g., log loss), whereas PixelDP can certify each specific prediction, and also provides intuitive metrics like robust accuracy, which is not supported by [53]. Finally, unlike PixelDP, which we evaluated on five datasets of increasing size and complexity, this technique was evaluated only on MNIST, a small dataset that is notoriously amenable to robust optimization (due to being almost black and white). Since the effectiveness of all defenses depends on the model and dataset, it is hard to conclude anything about how well it will work on more complex datasets.\nSecond, several works seek to formally verify [26], [30], [61], [62], [15], [20], [58] or lower bound [49], [63] the robustness of pre-trained ML models against adversarial attacks. Some of these works scale to large networks [49], [63], but they are insufficient from a defense perspective as they provide no scalable way to train robust models. Differentially Private ML. Significant work focuses on making ML algorithms DP to preserve the privacy of training sets [40], [1], [9]. PixelDP is orthogonal to these works, differing in goals, semantic, and algorithms. The only thing we share with DP ML (and most other applied DP literature) are DP theory and mechanisms. The goal of DP ML is to learn the parameters of a model while ensuring DP with respect to the training data. Public release of model parameters trained using a DP learning algorithm (such as DP empirical risk minimization or ERM) is guaranteed to not reveal much information about individual training examples. PixelDP\u2019s goal is to create a robust predictive model where a small change to any input example does not drastically change the model\u2019s prediction on that example. We achieve this by ensuring that the model\u2019s scoring function is a DP function with respect to the features of an input example (eg, pixels). DP ML algorithms (e.g., DP ERM) do not necessarily produce models that satisfy PixelDP\u2019s semantic, and our training algorithm for producing PixelDP models does not ensure DP of training data. Previous DP-Robustness Connections. Previous work studies generalization properties of DP [4]. It is shown that learning algorithms that satisfy DP with respect to the training data have statistical benefits in terms of out-ofsample performance; or that DP has a deep connection to robustness at the dataset level [14], [17]. Our work is rather different. Our learning algorithm is not DP; rather, the predictor we learn satisfies DP with respect to the atomic units (e.g., pixels) of a given test point.\n# VII. Conclusion\nWe demonstrated a connection between robustness against adversarial examples and differential privacy theory. We showed how the connection can be leveraged to develop a certified defense against such attacks that is (1) as effective at defending against 2-norm attacks as today\u2019s state-of-theart best-effort defense and (2) more scalable and broadly applicable to large networks compared to any prior certified defense. Finally, we presented the first evaluation of a certified 2-norm defense on the large-scale ImageNet dataset. In addition to offering encouraging results, the evaluation highlighted the substantial flexibility of our approach by leveraging a convenient autoencoder-based architecture to make the experiments possible with limited resources.\n# VIII. Acknowledgments\nWe thank our shepherd, Abhi Shelat, and the anonymous reviewers, whose comments helped us improve the paper significantly. This work was funded through NSF CNS-\n1351089, CNS-1514437, and CCF-1740833, ONR N0001417-1-2010, two Sloan Fellowships, a Google Faculty Fellowship, and a Microsoft Faculty Fellowship.\n[1] M. Abadi, A. Chu, I. Goodfellow, H. Brendan McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep Learning with Differential Privacy. ArXiv e-prints, 2016. [2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. 2018. [3] A. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017. [4] R. Bassily, K. Nissim, A. Smith, T. Steinke, U. Stemmer, and J. Ullman. Algorithmic stability for adaptive data analysis. In Proceedings of the forty-eighth annual ACM symposium on Theory of Computing, 2016. [5] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. End to end learning for self-driving cars. CoRR, 2016. [6] N. Carlini and D. Wagner. Adversarial examples are not easily detected: Bypassing ten detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security. ACM, 2017. [7] N. Carlini and D. A. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), 2017. [8] K. Chatzikokolakis, M. E. Andr\u00b4es, N. E. Bordenabe, and C. Palamidessi. Broadening the scope of differential privacy using metrics. In International Symposium on Privacy Enhancing Technologies Symposium, 2013. [9] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. J. Mach. Learn. Res., 2011. 10] Chuan Guo, Mayank Rana, Moustapha Cisse, Laurens van der Maaten. Countering adversarial images using input transformations. International Conference on Learning Representations, 2018. 11] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, Alan Yuille. Mitigating adversarial effects through randomization. International Conference on Learning Representations, 2018. 12] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to adversarial examples. In Proceedings of the 34th International Conference on Machine Learning, 2017. 13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, 2009. 14] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. Rubinstein. Bayesian Differential Privacy through Posterior Sampling. arXiv preprint arXiv:1306.1066v5, 2016. 15] S. Dutta, S. Jha, S. Sankaranarayanan, and A. Tiwari. Output range analysis for deep feedforward neural networks. In NASA Formal Methods Symposium, 2018.\n[16] K. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O\u2019Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned verifiers. ArXiv e-prints, 2018. [17] C. Dwork and J. Lei. Differential privacy and robust statistics. In Proceedings of the forty-first annual ACM symposium on Theory of computing, 2009. [18] C. Dwork, A. Roth, et al. The algorithmic foundations of differential privacy. Foundations and Trends R\u20ddin Theoretical Computer Science, 2014. [19] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physicalworld attacks on machine learning models. arXiv preprint arXiv:1707.08945, 2017. [20] T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. Ai 2: Safety and robustness certification of neural networks with abstract interpretation. In IEEE Symposium on Security and Privacy (SP), 2018. [21] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In Proceedings of the 3rd ICLR, 2015. [22] Google. Inception v3. https://github.com/tensorflow/models/ tree/master/research/inception. Accessed: 2018. [23] Guneet S. Dhillon, Kamyar Azizzadenesheli, Jeremy D. Bernstein, Jean Kossaifi, Aran Khanna, Zachary C. Lipton, Animashree Anandkumar. Stochastic activation pruning for robust adversarial defense. International Conference on Learning Representations, 2018. [24] D. Hendrycks and K. Gimpel. Early methods for detecting adversarial images. In ICLR (Workshop Track), 2017. [25] W. Hoeffding. Probability inequalities for sums of bounded random variables. Journal of the American statistical association, 1963. [26] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety verification of deep neural networks. In Proceedings of the 29th International Conference on Computer Aided Verification, 2017. [27] A. Ilyas, A. Jalal, E. Asteri, C. Daskalakis, and A. G. Dimakis. The robust manifold defense: Adversarial training using generative models. CoRR, abs/1712.09196, 2017. [28] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, 2015. [29] Jacob Buckman, Aurko Roy, Colin Raffel, Ian Goodfellow. Thermometer encoding: One hot way to resist adversarial examples. International Conference on Learning Representations, 2018. [30] G. Katz, C. W. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. CoRR, 2017. [31] J. Kos, I. Fischer, and D. Song. Adversarial examples for generative models. arXiv preprint arXiv:1702.06832, 2017. [32] Kos, Jernej and Song, Dawn. Delving into adversarial attacks on deep policies. arXiv preprint arXiv:1705.06452, 2017.\n[33] A. Krizhevsky. Learning multiple layers of features from tiny images. 2009. [34] A. Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial examples in the physical world. arXiv preprint 1607.02533, 2016. [35] X. Liu, M. Cheng, H. Zhang, and C. Hsieh. Towards robust neural networks via random self-ensemble. Technical report, 2017. [36] J. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. CVPR, 2017. [37] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. CoRR, abs/1706.06083, 2017. [38] Madry Lab. CIFAR-10 Adversarial Examples Challenge. https://github.com/MadryLab/cifar10 challenge. Accessed: 1/22/2017. [39] A. Maurer and M. Pontil. Empirical bernstein bounds and sample-variance penalization. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009, 2009. [40] F. McSherry and I. Mironov. Differentially private recommender systems: Building privacy into the netflix prize contenders. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2009. [41] D. Meng and H. Chen. Magnet: A two-pronged defense against adversarial examples. In CCS, 2017. [42] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff. On detecting adversarial perturbations. In Proceedings of the 6th International Conference on Learning Representations, 2017. [43] M. Mirman, T. Gehr, and M. Vechev. Differentiable abstract interpretation for provably robust neural networks. In International Conference on Machine Learning (ICML), 2018. [44] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng. Reading digits in natural images with unsupervised feature learning. [45] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In Proc. of IEEE Symposium on Security and Privacy (Oakland), 2016. [46] N. Papernot, P. D. McDaniel, A. Sinha, and M. P. Wellman. Towards the science of security and privacy in machine learning. CoRR, abs/1611.03814, 2016. [47] O. M. Parkhi, A. Vedaldi, A. Zisserman, et al. Deep face recognition. [48] R. Pascanu, J. W. Stokes, H. Sanossian, M. Marinescu, and A. Thomas. Malware classification with recurrent networks. In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on. IEEE, 2015. [49] J. Peck, J. Roels, B. Goossens, and Y. Saeys. Lower bounds on the robustness to adversarial perturbations. In Advances in Neural Information Processing Systems, 2017.\n[50] K. Pei, Y. Cao, J. Yang, and S. Jana. Deepxplore: Automated whitebox testing of deep learning systems. In Proceedings of the 26th Symposium on Operating Systems Principles (SOSP), 2017. [51] Pouya Samangouei, Maya Kabkab, Rama Chellappa. Defense-GAN: Protecting classifiers against adversarial attacks using generative models. International Conference on Learning Representations, 2018. [52] A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples. arXiv preprint arXiv:1801.09344, 2018. [53] A. Sinha, H. Namkoong, and J. Duchi. Certifying Some Distributional Robustness with Principled Adversarial Training. 2017. [54] Y. Song, R. Shu, N. Kushman, and S. Ermon. Generative adversarial examples. 2018. [55] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. [56] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In Proceedings of the 2",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of adversarial examples that fool machine learning models, particularly deep neural networks, highlighting the limitations of previous defenses and the necessity for a new approach that is both scalable and broadly applicable.",
        "problem": {
            "definition": "The problem defined in this paper is the vulnerability of deep neural networks to adversarial examples, which are small perturbations to inputs that can lead to incorrect predictions.",
            "key obstacle": "The main obstacle is that existing defenses either do not scale to large datasets or are limited in the types of models they can support, making them ineffective against sophisticated attacks."
        },
        "idea": {
            "intuition": "The intuition behind the proposed idea is the realization of a connection between differential privacy and robustness against adversarial examples, which can be leveraged to create a certified defense.",
            "opinion": "The proposed idea, PixelDP, is a method that incorporates differential privacy into the training of deep neural networks to ensure robustness against norm-bounded adversarial attacks.",
            "innovation": "The innovation lies in the use of differential privacy as a foundation for certified robustness, which allows for a more generalized and scalable approach compared to existing defenses."
        },
        "method": {
            "method name": "PixelDP",
            "method abbreviation": "DP",
            "method definition": "PixelDP is defined as a method that incorporates a differential privacy noise layer into the architecture of deep neural networks to guarantee robustness against adversarial examples.",
            "method description": "The core of PixelDP involves adding a noise layer that randomizes the outputs of the network, ensuring that small changes in input do not drastically change predictions.",
            "method steps": [
                "Introduce a differential privacy noise layer to the network.",
                "Train the network using standard techniques while ensuring the sensitivity of the pre-noise layers is controlled.",
                "At inference time, use Monte Carlo methods to estimate the expected outputs and certify robustness."
            ],
            "principle": "The principle of PixelDP's effectiveness is based on the post-processing property of differential privacy, which guarantees that the output distribution remains stable under small changes to the input."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted on multiple datasets including ImageNet, CIFAR-10, CIFAR-100, SVHN, and MNIST, using various deep learning architectures to evaluate the performance of PixelDP.",
            "evaluation method": "Performance was assessed using conventional accuracy and certified accuracy metrics, comparing PixelDP against state-of-the-art defenses under norm-bounded adversarial attacks."
        },
        "conclusion": "The outcomes indicate that PixelDP provides a robust defense against adversarial examples, achieving competitive performance compared to existing methods while being scalable to large datasets and diverse model architectures.",
        "discussion": {
            "advantage": "The key advantages of PixelDP include its scalability to large models and datasets, as well as its ability to offer certified robustness guarantees for individual predictions.",
            "limitation": "A limitation of the method is the computational overhead introduced by the noise layer, which can increase prediction time significantly depending on the number of noise draws.",
            "future work": "Future work should focus on optimizing the computational efficiency of PixelDP and exploring its applicability to other types of neural network architectures and tasks."
        },
        "other info": {
            "funding": "This work was funded through NSF CNS-1351089, CNS-1514437, and CCF-1740833, ONR N0001417-1-2010, two Sloan Fellowships, a Google Faculty Fellowship, and a Microsoft Faculty Fellowship."
        }
    },
    "mount_outline": [
        {
            "section number": "2.4",
            "key information": "The problem defined in this paper is the vulnerability of deep neural networks to adversarial examples, which are small perturbations to inputs that can lead to incorrect predictions."
        },
        {
            "section number": "5.2",
            "key information": "The outcomes indicate that PixelDP provides a robust defense against adversarial examples, achieving competitive performance compared to existing methods while being scalable to large datasets and diverse model architectures."
        },
        {
            "section number": "3.4",
            "key information": "PixelDP is defined as a method that incorporates a differential privacy noise layer into the architecture of deep neural networks to guarantee robustness against adversarial examples."
        },
        {
            "section number": "5.3",
            "key information": "The key advantages of PixelDP include its scalability to large models and datasets, as well as its ability to offer certified robustness guarantees for individual predictions."
        },
        {
            "section number": "7.2",
            "key information": "Future work should focus on optimizing the computational efficiency of PixelDP and exploring its applicability to other types of neural network architectures and tasks."
        }
    ],
    "similarity_score": 0.6453658930212969,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2332_Out-o/papers/Certified Robustness to Adversarial Examples with Differential Privacy.json"
}