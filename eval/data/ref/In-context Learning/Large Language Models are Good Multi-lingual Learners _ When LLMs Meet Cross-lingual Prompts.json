{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2409.11056",
    "title": "Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts",
    "abstract": "With the advent of Large Language Models (LLMs), generating rule-based data for realworld applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-toMIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.",
    "bib_name": "wang2024largelanguagemodelsgood",
    "md_text": "# Large Language Models are Good Multi-lingual Learners : When LLMs Meet Cross-lingual Prompts\nTeng Wang1, Zhenqi He1, Wing-Yin Yu2, Xiaojin Fu2, Xiongwei Han3 1Department of Mathematics, The University of Hong Kong, Hong Kong SAR, China 2Noah\u2019s Ark Lab, Huawei, Hong Kong SAR, China 3Noah\u2019s Ark Lab, Huawei, Shenzhen, China\nespondence: {wt0318, zhenqi_he}@connect.hku.hk, {rocket.YuWingYin, fuxiaojin, hanxiongwei}@huawei.com,\n# Abstract\nWith the advent of Large Language Models (LLMs), generating rule-based data for realworld applications has become more accessible. Due to the inherent ambiguity of natural language and the complexity of rule sets, especially in long contexts, LLMs often struggle to follow all specified rules, frequently omitting at least one. To enhance the reasoning and understanding of LLMs on long and complex contexts, we propose a novel prompting strategy Multi-Lingual Prompt, namely MLPrompt, which automatically translates the error-prone rule that an LLM struggles to follow into another language, thus drawing greater attention to it. Experimental results on public datasets across various tasks have shown MLPrompt can outperform state-of-the-art prompting methods such as Chain of Thought, Tree of Thought, and Self-Consistency. Additionally, we introduce a framework integrating MLPrompt with an auto-checking mechanism for structured data generation, with a specific case study in text-toMIP instances. Further, we extend the proposed framework for text-to-SQL to demonstrate its generation ability towards structured data synthesis.\narXiv:2409.11056v1\n# 1 Introduction\nMixed Integer Programming (MIP) is a significant part of Operations Research (OR), which aims to solve optimization problems where some decision variables are constrained to be integers. It has been widely applied in industrial fields including logistics (Hulagu and Celikoglu, 2020), scheduling (Keha et al., 2009), and supply chain management (Sawik, 2011). Nowadays, benefiting from the development of Large Language Models (LLMs), automatically modeling complex and practical OR problems in plain text description to mathematical optimization formulas is no longer an impossible mission (Xiao et al., 2024; Wei et al., 2022; Yao et al., 2023a). CoE (Xiao et al., 2024)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8178/817864a1-c402-4487-a09e-4203dec894bc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 1: Semantic illustrations of the potential of LLMs in generating data following real distributions. (a) Distribution of Constraint Coefficients in Real-World Factory Location Problems (Cornuejols et al., 1977). (b) Distribution of Constraint Coefficients generated by random simulation. (c) Distribution of Constraint Coefficients generated by GPT-4 (Achiam et al., 2023) with different hyper-parameters. For each case: (demand points, candidate locations) = (4, 4), (5, 4).\nproposes a multi-agent system leveraging LLMs to model and program complex operations research problems and constructs a new dataset named ComplexOR involving intricate constraints, domainspecific terminology, and multi-step reasoning for various domains e. g.supply chain, scheduling, and logistics. Although the ComplexOR dataset provides foundational model metadata including \u2018set\u2019, \u2018parameter\u2019, \u2018hyper-parameter\u2019, \u2018variable\u2019, \u2018objective function\u2019, and \u2018constraint\u2019, concrete values of \u2018set\u2019 and \u2018parameter\u2019 are missing to generate MIP instances, causing limited applicability in developing autonomous MIP solvers for real-world optimization tasks. With the success of data synthesis by LLMs in various domains (Yang et al., 2024b; Almashor et al., 2024; Li et al., 2023; He et al., 2023), LLMs have shown great capabilities in synthesizing real-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cdea/cdea922d-e19c-4485-8812-88c08fc2473f.png\" style=\"width: 50%;\"></div>\nFigure 2: Illustration of the MIP instance generation process using key modeling components, including sets, parameters, variables, constraints, and objective functions. Only when compliant with the parser rules can the parser generate the modeling code, and the modeling tools produce instances.\n<div style=\"text-align: center;\">Figure 2: Illustration of the MIP instance generation process using key modeling components, including sets, parameters, variables, constraints, and objective functions. Only when compliant with the parser rules can the parser generate the modeling code, and the modeling tools produce instances.</div>\nistic data in multiple modalities while the potential of LLMs in generating MIP instances data has not been fully explored. By comparing the synthetic distributions generated by GPT-4 (Achiam et al., 2023) with the real distribution of Constraint Coefficients of MIP instances and randomly generated distributions for Factory Location Problem (Cornuejols et al., 1977), shown in Fig. 1, LLMs have demonstrated its superiority in MIP instances generating compared with random simulation. Fig. 2 demonstrates the autonomous MIP instance generation pipeline, incorporating modeling information such as sets, parameters, variables, constraints, and objective functions across multiple solvers. Commercial MIP solvers such as Gurobi (Achterberg, 2019), OptVerse (Li et al., 2024), and CPLEX (Bliek1\u00fa et al., 2014) usually have its unique data representation with different parser rules for MIP instances, requiring various intricate rules in natural language to condition the generation of data that can be imported by the solvers for successful modeling. While direct handcrafted rules may lead to long and ambiguous contexts, LLMs have a high risk of neglecting certain rules to generate unsatisfactory instances given the complicated contexts. To facilitate the autonomous generation of MIP instances within industrial pipelines, we propose a novel Multi-Lingual Prompt algorithm, namely MLPrompt, specifically designed for structural data synthesis and adaptability for various solvers with different formats. Additionally, we propose a framework incorporating MLPrompt with an auto-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fa7a/fa7a9d61-9b1b-4a07-947d-3f136301a166.png\" style=\"width: 50%;\"></div>\nFigure 3: The dominant language refers to the majority language in the pretraining dataset, while \"others\" refer to all remaining languages. This figure demonstrates the language imbalance phenomenon in the pretraining data of GPT-3.5 (Brown et al., 2020), Llama3 (Dubey et al., 2024), Deepseek-V2 (Liu et al., 2024), Bloom (Le Scao et al., 2023), Nemotron-4 (Adler et al., 2024), OPT (Zhang et al., 2022), and Falcon (Almazrouei et al., 2023).\n# checking mechanism to enable iterative prompt updates to ensure compliance with input constraints.\nThe design of MLPrompt is motivated by the following observations. The trilingual parallel language processing experiments (Pathak et al., 2024) have shown that more dominant languages receive greater cognitive focus, which facilitates faster response times and reduces the mental load when processing in non-dominant languages for polyglots. These findings are derived from mousetracking experiments (Pathak et al., 2024) that observed participants\u2019 language processing while listening to words in different languages and selecting corresponding images. Similarly, LLMs, such as ChatGPT (Achiam et al., 2023), function as polyglots, supporting over 80 languages. The scale of pretraining data varies across languages, as depicted in Fig. 3, where dominant languages frequently appear in the pretraining data, while non-dominant languages are comparatively underrepresented (Achiam et al., 2023). Hence, deriving from phenomena in human polyglots where the existence of non-dominant languages helps the process of dominant languages, MLPrompt is proposed to leverage the non-dominant languages of LLMs to strengthen the understanding on dominant languages. Existing LLM-based reasoning approaches, such as Chain-of-Thought (CoT) (Wei et al., 2022), Tree-of-Thoughts (ToT) (Yao et al., 2023a), SelfConsistency (SC) (Wang et al., 2023), and Re-\nAct (Yao et al., 2023b), primarily focus on enhancing reasoning through iterative steps. These approaches aim to improve the quality of generated outputs by verifying the correctness of intermediate steps and selecting the best path forward. However, when generating structured outputs like JSON that must adhere to given constraints, it poses challenges to break the task into smaller and independent parts due to the interconnections within each part. In contrast, MLPrompt tackles structured data synthesis with introducing non-dominant languages to raise LLMs\u2019 attention in error-prone constraints reduce inference time without requiring multiple inference steps, all while preserving the interconnected relevance of the constraints. In this paper, we propose a general MIP instances synthesis pipeline with a novel prompting method. Our contributions are three-fold: (1) We introduce MLPrompt, a simple yet effective multiple-lingual prompting strategy, for enhancing LLM reasoning, inspired by the capability of crosslingual understanding in polyglots. (2) We make the first attempt at a LLM for MIP instance generation which serves as a bridge to connect existing research-based datasets with industrial needs, and it is easy to be extended into a general pipeline for structured data synthesis. (3) Extensive experiments on the ComplexOR dataset demonstrate the superiority of our prompt strategy compared to existing prompting strategies for MIP instance generation. Moreover, additional demonstration on the text2SQL(Yu et al., 2018; Cao et al., 2024) task highlights the broader applicability of our framework in other structured data generation tasks.\n# 2 Related Work\n# 2.1 MIP Instance Generation\nMIP instance generation plays a crucial role in the development of commercial MILP solvers (Rimmi Anand and Kumar, 2017). Traditional mathematical-formulation-based methods for MIP instance generation include TSP (Wiel and Sahinidis, 1995), structure-based instance generation (Bixby et al.; Applegate et al., 2006), mixed-integer knapsack (Atamt\u00fcrk, 2003), and set covering (Balas and Ho, 1980; Anureet Saxena and Lejeune, 2010). G2MILP (Geng et al., 2023) introduces the first learning-based MILP instance generation framework, representing MILP problems as bipartite graphs and using a masked variational autoencoder (VAE) (Kingma\nand Welling, 2019) to iteratively generate new instances. ACM-MILP (Guo et al., 2024) presents an adaptive and structure-preserving approach by using a community detection algorithm to group strongly related constraints for collective modification. MILPGen (Yang et al., 2024c) simplifies bipartite graph representations of MILP instances into tree-like structures and uses graph convolutional networks (GCNs) to calculate node embeddings, allowing for optimal node pair merging. This enables the creation of larger, more complex instances while maintaining the original structural characteristics. Building upon these approaches, our method generates general MILP instances for a variety of applications, such as scheduling, logistics, and product management, through the use of input text descriptions. Specifically, we leverage the modeling information from the ComplexOR dataset to produce MIP instances, aligning with the broader goal of addressing real-world industrial needs.\n# 2.2 Prompt Engineering for LLMs\nPrompts are gradient-free strategy to strengthen LLMs\u2019 reasoning for complex tasks. The Chain-ofThought (CoT) (Wei et al., 2022) decouples complex reasoning tasks into intermediate reasoning steps. Auto-CoT (Zhang et al., 2023) automate CoT by by encouraging LLMs to think step by step to reduce manual operations in prompting. SelfConsistency (SC) (Wang et al., 2023) introduces a novel decoding strategy to enhance the reasoning capabilities of LLMs when using CoT prompting. Instead of relying on a single reasoning path derived from greedy decoding, it samples multiple diverse reasoning paths and determines the final answer by marginalizing over these paths to select the most consistent one. Tree-of-thought (ToT) (Yao et al., 2023a) and Graph-of-thought (GoT) (Besta et al., 2024) enhance the reasoning capabilities of LLMs through structured prompting schemes that go beyond traditional linear approaches like CoT.\n# 2.3 Multilingual LLMs\nWith the success of English-center LLMs across various NLP tasks such as Question Answering (Ko\u02c7cisk\u00fd et al., 2018) and Summarization (Hermann et al., 2015), increasing attention has been drawn to multilingual LLMs due to globalization. A multilingual LLM possesses the ability to process and produce content in multiple languages simultaneously. Existing approaches for Multi-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/abcd/abcd0c67-8cde-414e-9b3e-8e3350e60d81.png\" style=\"width: 50%;\"></div>\nFigure 4: The workflow of the proposed framework for structured data generation. The process includes prompt construction with predefined rules, data generation by the LLM, evaluation of compliance with the rules, and iterative refinement of the prompt by translating rules into other languages if necessary.\nlingual LLMs are mainly split into two ways (1) Parameter-tuning the LLMs with multilingual data (Du et al., 2022; Mendonca et al., 2023) (2) Parameter-frozen with prompting (Hoang et al., 2024). In this paper, we focus on the analysis of increasing the understanding and reasoning capabilities of LLMs trained with multilingual data such as GPT (Achiam et al., 2023) with proposed MLPrompt.\n# 3 Methodology\n# 3 Methodology 3.1 Problem Statement\nAn Mixed-Linear-Integer-Programming (MLIP) problem can be formulated as follows:\n(1)\nwhere c \u2208Rn is the vector of objective coefficients, A \u2208Rm\u00d7n is the matrix of constraint coefficients, b \u2208Rm is the vector of constraint bounds, and x \u2208Rn represents the decision variable. The index set I indicates the decision variables xi constrained to be integers. Here, A, b, and c are parameters to be generated, and their dimensions n, m will be defined in the given modeling problem. To simplify the generation and avoid unexpected long outputs, we formulate the MLIP instances into JSON data to record the data types, along with the lower and upper bounds for sets, parameters, and hyper-parameters\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5dc6/5dc6abf0-3855-4a3a-bf2a-3106a3b14253.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: A demo of how MLPrompt builds prompts.</div>\nas shown in Fig. 4. Then a random process generates values within the generated bounds to be fed into the solver\u2019s parser to construct the MLIP instance. By that, we successfully convert the MLIP instance generation problem into rule-based structured data generation, and the core is to employ the LLM to produce the corresponding JSON.\n# 3.2 MLPrompt\nGenerally, user-specific parsers are used in industries and academic teams for mathematical modeling and MIP instance generation (Xing et al., 2024; Wang et al., 2024b), leading to complex and interdependent rules to condition the generation of desired structured data. To address the complex constraints in natural language, we propose MLPrompt to leverage the multilingual capabilities of LLMs to draw LLMs attention to error-prone rules by translating any rule that is not followed into a non-dominant language of LLMs and hence to improve the quality of the generated data. MLPrompt Inspired by the phenomenon in human polyglots where the existence of nondominant languages helps the process of dominant languages (Pathak et al., 2024), we propose the MLPrompt to translate the error-prone rule into a non-dominant language of LLM to strengthen the understanding and reasoning of LLM in given complex contexts. A demonstration of this approach is presented in Fig. 5. Here, we define the dominant language as the most frequently occurring language within the pretraining dataset, while all other languages, except the dominant one, are referred to as \"others.\". Prior work on Falcon (Almazrouei et al., 2023) highlights the challenge that arises when incorporating a substantial amount of multilingual data (e.g., exceeding 10%) in language models\u2014leading to a decline in performance on tasks that are more aligned with the dominant language. Consequently, many state-of-the-art LLMs prioritize training on large volumes of dominant language data, further exacerbating the imbalance across languages. Fig. 3 demonstrates that while LLMs are designed with multilingual capabilities in mind, there is a preva-\nlent tendency to disproportionately prioritize dominant language exposure, primarily to optimize performance on tasks aligned with the dominant language. Auto-MLPrompt Strategy As introduced in Sec. 3.1, we formulate the MIP instance generation as a conditioned structured data synthesis constrained by natural language rules. It is relatively straightforward to validate whether JSON data adheres to the given constraints and to identify the specific rule or rules that have been violated. With such property in the rule-based JSON generation, we design an autonomous MIP generation pipeline with the proposed MLPrompt strategy to automatically detect the violated rule and translate it into a non-dominant language. The generation flow can be summarized as follows: Initially, we incorporate the predefined rules and input modeling information directly to generate the initial prompts. Then, the LLM generates structured data, like JSON, based on the initial prompt, and an evaluation function assesses whether the generated data complies with the given rules. If the error or misgeneration is detected, the corresponding rule will be translated into a non-dominant language to update the prompt, and the data generation process will be repeated.\n# 4 Experiments\n# 4.1 Dataset\nComplexOR (Xiao et al., 2024) contains 60 complex operations research problems with natural language description and corresponding mathematical formula. For each problem, it contains comprehensive information (an example is shown in Appendix A.1) for constructing models, including problem backgrounds, sets, parameters, hyperparameters, variables, objective functions, and constraint functions. To generate the data following given conditions, we convert this extensive context into a set of rules for the LLMs to follow, requiring the output JSON to include the data type, lower bound, and upper bound for each set, parameter, and hyper-parameter. Subsequently, a random process is simulated to generate values for these sets and parameters. The solver\u2019s parser then reads the model-building information, loads the generated random data, and constructs the MIP instance accordingly.\n# 4.2 Experimental Settings\n4.2 Experimental Settings As described in Section 4.1, we use the ComplexOR (Xiao et al., 2024) dataset (an example of binpacking problem is shown in Appendix A.1) for our experiments and utilize an LLM to generate a JSON file constrained by input rules. The rules are outlined in Appendix A.2, and we evaluate the model\u2019s performance based on rules 4, 7, and 8. A detailed analysis of the model\u2019s compliance with these rules is provided in Appendix A.4. We also implement an evaluation function to check whether the generated JSON complies with each rule, compute the accuracy for each rule, and calculate the final score by averaging the accuracies of the three rules. We evaluate our proposed prompting strategy, MLPrompt, on various LLMs of different sizes, categorizing them into three groups based on the number of parameters. (1) Small-scale LLMs: LLMs with fewer than 10 billion parameters are classified as small. We use opensource models such as Mistral-7B (Jiang et al., 2023), Llama-3-8B (Dubey et al., 2024), Gemma2-9B (Team et al., 2024), Qwen2-7B (Yang et al., 2024a), Llama-3.1-8B (Dubey et al., 2024), Qwen1.5-7B (Bai et al., 2023) and Openchat3.5-7B (Wang et al., 2024a). (2) Medium-scale LLMs: LLMs with parameter sizes between 50 billion and 200 billion are considered mediumsized. These include GPT-3.5 (Brown et al., 2020), Mixtral-8\u00d77B (Jiang et al., 2024), Llama-3.170B (Dubey et al., 2024), Deepseek-67B (Liu et al., 2024), Llama-3-70B (Dubey et al., 2024), Qwen272B (Yang et al., 2024a), WizardLM-8\u00d722B (Xu et al., 2023) and Mixtral-8\u00d722B (Jiang et al., 2024). (3) Large-scale LLMs: We define LLMs with more than 200 billion parameters as large models. For this category, we focus on the GPT-4 series and conduct experiments using GPT-4o, GPT-4o mini, and GPT-4-Turbo (Achiam et al., 2023). For each model scale, we compare MLPrompt with CoT (Wei et al., 2022), ToT (Yao et al., 2023a) and SC (Wang et al., 2023)\n# 4.3 Small-scale LLMs\nIn our experiments, the input contexts are long and comprehensive, requiring LLMs to have a strong capability of understanding and reasoning of long contexts, while small models often fail to comprehend our intentions and fail to generate the correct JSON format by input constraints. Hence, for\n<div style=\"text-align: center;\">Table 1: The table presents the success rate of small-scale LLMs generating correctly formatted JSON under variou prompting methods, without considering rule compliance.</div>\nMethods\nMistral-7B\nLlama-3-8B\nLlama-3.1-8B\nGemma-2-9B\nQwen1.5-7B\nQwen2-7B\nOpenchat-3.5-7B\nZero-shot\n0.211\n0.268\n0.016\n0.302\n0.017\n0.100\n0.250\nFew-shots\n0.178\n0.062\n0.032\n0.326\n0.017\n0.033\n0.100\nCoT(Wei et al., 2022)\n0.350\n0.300\n0.167\n0.717\n0.000\n0.133\n0.233\nToT(Yao et al., 2023a)\n0.300\n0.033\n0.350\n0.033\n0.000\n0.233\n0.433\nSC(Wang et al., 2023)\n0.267\n0.067\n0.333\n0.050\n0.000\n0.250\n0.100\nexperiments on the small-scale model, we only consider the success rate of generating the correct JSON format by LLMs, without assessing rule compliance. Natural language, unlike formal logic, lacks clear evaluative properties, making it difficult to assess the accuracy of generated outputs. This limitation poses a challenge for methods like CoT, ToT, and SC, which struggle to handle this ambiguity effectively. Additionally, the interconnected nature of rules in structured outputs, such as JSON, makes it difficult to verify each step in isolation\u2014correctness in one part does not necessarily guarantee overall correctness when the parts are combined. To effectively utilize SOTA prompting methods, in the following experiments, CoT, ToT, and SC are applied without evaluating intermediate results. Detailed explanations are provided in Appendix A.5. The performance, as shown in Table 1, indicates that these small LLMs struggle to generate the desired JSON format and improve the quality of the generated data under prompting methods such as CoT, ToT, and SC. Due to their limitations in handling complex tasks, we will not use these small LLMs in further experiments.\n# 4.4 Medium-scale LLMs\nMedium LLMs have the capability to comprehend our requirements and generate correctly formatted JSON. We evaluate these medium LLMs to determine whether the generated data adhere to the rules we have defined. The final results, as shown in Table 2, demonstrate that MLPrompt effectively improves the quality of generating complex data by translating a single rule from one language to another. This approach is more efficient compared to methods like ToT and SC, which involve multiple steps to obtain intermediate results, then combine and infer from these results to reach the final output. The poor performance of CoT, ToT, and SC, as shown in Table 2, can be attributed to our inabil-\nity to evaluate the intermediate results required by these methods in generating complex data task. As discussed in Section 4.3, these medium LLMs can be considered as weak learners, and their intermediate outputs often contain errors. As a result, combining multiple weak learners not only fails to improve the quality of the final results but may even degrade them. In contrast, similar to bagging, only when combining the results of strong learners can performance improvements be expected (Bhavan et al., 2019). Furthermore, as shown in Table 2, adding the missing rule in another language alongside the existing one not only fails to outperform the replacement method but, in some cases, performs worse than the baseline. This approach also appears influenced by the re-adding prompt method, which undermines the purpose of the ablation experiment. Therefore, in the subsequent experiments, we will focus solely on the MLPrompt strategy using the replacement approach.\n# 4.5 Large-scale Models\nIn this section, we evaluate the performance of large-scale models, specifically from the GPT-4 series, across various prompting strategies. GPT4o served as the base model, and the results, as shown in Table 3, demonstrate several key findings. The baseline performance in a zero-shot setting is reasonable, and improvements are achieved using CoT, ToT, and SC prompting methods, which showcase their ability to refine model performance by structuring reasoning steps. However, the most significant gains are observed with the MLPrompt strategy, which replaces key rules with alternative languages such as Mandarin, Thai, and Korean. Across all model variations, MLPrompt consistently yielded the highest performance, with Mandarin replacement proving particularly effective. This strategy outperformed traditional multi-step prompting methods like CoT, ToT, and SC, not only improving accuracy but also enhancing the efficiency of the models by reducing\nTable 2: The table presents the accuracy of each medium-scale model across various settings. MLPrompt consistentl enhances data quality. The \"+\" symbol denotes the addition of a new language rule to complement an unmet rul while \"\u2194\" signifies the replacement of the original rule with an existing one.\n\u2194\nMethods\nGPT-3.5\nWizardLM-8\u00d722B\nMixtral-8\u00d77B\nMixtral-8\u00d722B\nLlama-3.1-70B\nLlama-3-70B\nDeepseek-67B\nQwen2-72B\nBaseline\n0.436\n0.250\n0.133\n0.239\n0.378\n0.461\n0.083\n0.383\nCoT(Wei et al., 2022)\n0.156\n0.389\n0.128\n0.261\n0.428\n0.150\n0.128\n0.583\nToT(Yao et al., 2023a)\n0.294\n0.339\n0.011\n0.128\n0.167\n0.083\n0.061\n0.606\nSC(Wang et al., 2023)\n0.131\n0.217\n0.011\n0.078\n0.167\n0.117\n0.000\n0.611\nRepeated Missed Rule\n0.133\n0.244\n0.067\n0.228\n0.328\n0.183\n0.056\n0.528\nMLPrompt + Mandarin\n0.378\n0.283 \u21913.3%\n0.039\n0.239\n0.472 \u21919.4%\n0.194\n0.094 \u21911.1%\n0.633 \u219125.0%\nMLPrompt + Thai\n0.383\n0.272 \u21912.2%\n0.011\n0.306 \u21916.7%\n0.500 \u219112.2%\n0.372\n0.044\n0.500 \u219111.7%\nMLPrompt + Korean\n0.372\n0.350 \u219110.0%\n0.044\n0.256 \u21911.7%\n0.483 \u219110.5%\n0.233\n0.050\n0.572 \u219118.9%\nMLPrompt \u2194Mandarin\n0.414\n0.383 \u219113.3%\n0.211 \u21917.8%\n0.289 \u21915.0%\n0.411 \u21913.3%\n0.483 \u21912.2%\n0.117 \u21913.4%\n0.422 \u21913.9%\nMLPrompt \u2194Thai\n0.454 \u21911.8%\n0.294 \u21914.4%\n0.150 \u21911.7%\n0.294 \u21915.5%\n0.394 \u21911.6%\n0.456\n0.128 \u21914.5%\n0.428 \u21914.5%\nMLPrompt \u2194Korean\n0.591 \u219115.5%\n0.406 \u219115.6%\n0.089\n0.267 \u21912.8%\n0.444 \u21916.6%\n0.533 \u21917.2%\n0.106 \u21912.3%\n0.339\nTable 3: Performance evaluation of large-scale LLMs, comparing GPT-4 series under different prompting strategies, including CoT, ToT, SC, and Repeatedmissed-rule.\nMethods\nGPT-4o\nGPT-4o mini\nGPT-4-Turbo\nBaseline\n0.472\n0.625\n0.753\nCoT(Wei et al., 2022)\n0.492\n-\n-\nToT(Yao et al., 2023a)\n0.530\n-\n-\nSC(Wang et al., 2023)\n0.619\n-\n-\nRepeated Missed Rule\n0.536\n-\n-\nMLPrompt \u2194Mandarin\n0.844 \u219137.2%\n0.888 \u219126.3%\n0.874 \u219112.1%\nMLPrompt \u2194Thai\n0.813 \u219134.1%\n0.816 \u219119.1%\n0.777 \u21912.4%\nMLPrompt \u2194Korea\n0.796 \u219132.4%\n0.613\n0.892 \u219113.9%\ninference time and improving the overall quality of the generated data. As such, MLPrompt demonstrates its superiority in handling complex data generation tasks in large-scale models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8de5/8de5acf0-6f15-4335-8985-b94f9ba1030f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nFigure 6: Heatmap of PCA-transformed final layer of LLaMA-3-70B with different prompts. (a) With original single-lingual Prompt. (b) Multi-lingual Prompts with the rule 8 in maindarin (Ours). The X-axis represents input tokens, and the Y-axis shows the 50 PCA components.\n# 4.6 Attention Verification\nTo verify whether MLPrompt increases the attention of LLMs of error-prone rules, we visualize the attention map of the final layer of the open-source LLaMA-3-70B (Dubey et al., 2024) when inputting the English prompt and our MLPrompt with rule 8 being translated into Mandarin. The attention map is firstly downsampled from 8192 to 50 for better representation using PCA. Fig. 6 shows the downsampled attention map, and the corresponding error-prone rule 8 positioned between 84 to 128 has been zoomed in for clear representations. The first few principal components of our proposed prompts predominantly focus on the translated Rule 8 while the original English prompts do not exhibit a similar concentration on any specific rule. The attention map visualization from the final layer of the LLM illustrates the efficacy of our proposed MLPrompt, which effectively directs the LLM\u2019s attention towards error-prone rules by incorporating non-dominant languages.\n# 4.7 Text-to-SQL\nWe further expand our MLPrompt in text-to-SQL tasks to validate the generalization ability. Text-toSQL is more challenging than text-to-JSON due to the difficulty in detecting rule violations in SQL, which in turn complicates the identification of errorprone rules. However, it is straightforward to manually identify rule violations from the SQL results. To demonstrate the effectiveness of our approach, we present an example where MLPrompt outperforms other methods in text-to-SQL, and the rules for LLMs and the process by which these rules are derived are detailed in Appendix A.6. The task is: \"Find the first name and age of students who have a pet,\" using the pets_1 database from the Spider V1.0 dataset (Yu et al., 2018). We generate the prompt for GPT-4 by combining the SQL schema with predefined rules. Manual anal-\nTable 4: Error rates for different prompt configurations in the text-to-SQL task, based on 20 runs of the given sample.\nPrompt combination\nError Rate (%)\nEnglish\n0.35\nEnglish w Repetitive Rule 4\n0.50\nRule 4 in Mandarin (Ours)\n0.10\nRule 4 in Japanese (Ours)\n0.00\nRule 4 in Korean (Ours)\n0.00\nysis reveals that GPT-4 frequently violates rule 4 in this task. To address it, we translate rule 4 into Korean, Japanese, and Mandarin respectively using an online translation. We also present ablation studies examining repetitive error-prone rules. The performance, evaluated by the error rate for SQL execution based on 20 runs of the provided example, is shown in Tab. 4, which shows that GPT-4 would fail to follow long and comprehensive rules. Even when the error-prone rules are repeated for emphasis, GPT-4 still fails. A potential strategy to mitigate this challenge is to split the rules within the prompt. However, isolating them is not feasible due to their interleaved nature, which could compromise the integrity of the prompt structure. Our proposed MLPrompt, utilizing various nondominant languages, consistently exhibits low error rates, demonstrating the superiority of MLPrompt in text-to-SQL tasks.\n# 5 Conclusion\nIn this work, we tackle the challenge of generating structured data using LLMs in real-world applications, where complex rules and natural language ambiguity often hinder the effectiveness of traditional methods. To address this, we introduce MLPrompt, a novel method that improves LLM reasoning to generate structured data by translating error-prone rules into another language, enhancing attention from LLM, and overall data quality. In comparison with state-of-the-art prompting strategies like CoT, ToT, and SC, MLPrompt demonstrates faster inference times and lower error rates. Additionally, we utilize MLPrompt to bridge the gap between LLM and autonomous industrial MIP generation, conducting extensive experiments on Text-to-MIP to prove MLPrompt\u2019s effectiveness. Finally, we explore the possibility of applying MLPrompt to structured data generation tasks, such as Text-to-SQL.\nDifficulty in Identifying Rule Violations in Natural Language Prompts: While MLPrompt can enhance the quality of generated data after localizing the rule LLM would fail to follow, the identification of omitting or violated rules remains a significant challenge. One limitation of our approach arises from the abstract nature of natural language, which often leads to rule inter-dependencies. Unlike programming languages, which can be translated into executable code to verify rule compliance, natural language is an abstract representation that cannot be executed directly. In structured data generation tasks like text-to-SQL (see Appendix A.6, we define rules for LLMs to follow and validate the generated SQL by comparing it to the expected output. However, when discrepancies occur, pinpointing the specific rule violation is difficult, often requiring manual analysis. However, enforcing rules with mathematical constraints, such as checking the data values, is relatively straightforward. The mechanism or model that can pinpoint which rule is violated is crucial for the effectiveness of MLPrompt.\nWhich non-dominant language should we use: In our experiment, the dominant language for LLMs is English, and we implement MLPrompt by translating the error-prone rule in English to another non-dominant language, such as German, French, Mandarin, Thai, Japanese, and Korean. The performance of German and French is notably lower, while Mandarin, Thai, and Korean show better results. Since our method relies on the GPT-4 series, and OpenAI has not disclosed the language distribution in GPT-4\u2019s training data, we reference the language distribution from GPT-3 (Brown et al., 2020). According to this, English, German, and French are among the top three languages in the training dataset, while Mandarin, Thai, and Korean rank much lower, around the 20s. We hypothesize that MLPrompt is most effective when the distribution difference between dominant and non-dominant languages in the training data is neither too large nor too small. However, this hypothesis remains unproven, and selecting the appropriate language combination in the MLPrompt generation remains a key challenge.\n# References\nDavid L Applegate, Robert E Bixby, Va\u0161ek Chv\u00e1tal, and William J Cook. 2006. The Traveling Salesman Problem: A Computational Study. Princeton University Press.\nAlper Atamt\u00fcrk. 2003. On the facets of the mixed\u2013integer knapsack polyhedron. Mathematical Programming.\nnze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609.\nEgon Balas and Andrew Ho. 1980. Set covering algorithms using cutting planes, heuristics, and subgradient optimization: A computational study. Combinatorial Optimization.\nEgon Balas and Andrew Ho. 1980. Set covering algorithms using cutting planes, heuristics, and subgradient optimization: A computational study. Combinatorial Optimization.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. In AAAI. Anjali Bhavan, Pankaj Chauhan, Rajiv Ratn Shah, et al. 2019. Bagged support vector machines for emotion recognition from speech. Knowledge-Based Systems. Robert E Bixby, Mark Fenelon, Zonghao Gu, Edward Rothberg, and Roland Wunderling. Mip: Theory and practice\u2014closing the gap. In System Modelling and Optimization, pages 19\u201349. Christian Bliek1\u00fa, Pierre Bonami, and Andrea Lodi. 2014. Solving mixed-integer quadratic programming problems with ibm-cplex: a progress report. In Proceedings of the twenty-sixth RAMP symposium, pages 16\u201317. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS, volume 33, pages 1877\u20131901. Curran Associates, Inc. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, et al. 2024. Spider2-v: How far are multimodal agents from automating data science and engineering workflows? arXiv preprint arXiv:2407.10956. Gerard Cornuejols, Marshall L Fisher, and George L Nemhauser. 1977. Exceptional paper\u2014location of bank accounts to optimize float: An analytic study of exact and approximate algorithms. Management science, 23(8):789\u2013810. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, and Feng Wu. 2023. A deep instance generative framework for milp solvers under limited data availability. In NeurIPS.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of thoughts: Solving elaborate problems with large language models. In AAAI. Anjali Bhavan, Pankaj Chauhan, Rajiv Ratn Shah, et al. 2019. Bagged support vector machines for emotion recognition from speech. Knowledge-Based Systems. Robert E Bixby, Mark Fenelon, Zonghao Gu, Edward Rothberg, and Roland Wunderling. Mip: Theory and practice\u2014closing the gap. In System Modelling and Optimization, pages 19\u201349. Christian Bliek1\u00fa, Pierre Bonami, and Andrea Lodi. 2014. Solving mixed-integer quadratic programming problems with ibm-cplex: a progress report. In Proceedings of the twenty-sixth RAMP symposium, pages 16\u201317. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS, volume 33, pages 1877\u20131901. Curran Associates, Inc. Ruisheng Cao, Fangyu Lei, Haoyuan Wu, Jixuan Chen, Yeqiao Fu, Hongcheng Gao, Xinzhuang Xiong, Hanchong Zhang, Yuchen Mao, Wenjing Hu, et al. 2024. Spider2-v: How far are multimodal agents from automating data science and engineering workflows? arXiv preprint arXiv:2407.10956. Gerard Cornuejols, Marshall L Fisher, and George L Nemhauser. 1977. Exceptional paper\u2014location of bank accounts to optimize float: An analytic study of exact and approximate algorithms. Management science, 23(8):789\u2013810. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, and Feng Wu. 2023. A deep instance generative framework for milp solvers under limited data availability. In NeurIPS.\nZiao Guo, Yang Li, Chang Liu, Wenli Ouyang, and Junchi Yan. 2024. ACM-MILP: Adaptive constraint modification via grouping and selection for hardnesspreserving MILP instance generation. In ICML. Zhenqi He, Junjun He, Jin Ye, and Yiqing Shen. 2023. Artifact restoration in histology images with diffusion probabilistic models. In Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI, pages 518\u2013527. Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In NeurIPS. Hieu Hoang, Huda Khayrallah, and Marcin JunczysDowmunt. 2024. On-the-fly fusion of large language models and machine translation. In Findings of the Association for Computational Linguistics: NAACL 2024. Selin Hulagu and Hilmi Berk Celikoglu. 2020. A mixed integer linear programming formulation for green vehicle routing problem: Case for shuttle services. In Computer Aided Systems Theory\u2013EUROCAST 2019: 17th International Conference, Las Palmas de Gran Canaria, Spain, February 17\u201322, 2019, Revised Selected Papers, Part II 17, pages 153\u2013160. Springer. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Ahmet B. Keha, Ketan Khowala, and John W. Fowler. 2009. Mixed integer programming formulations for single machine scheduling problems. Computers & Industrial Engineering. Diederik P. Kingma and Max Welling. 2019. Tom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2023. Bloom: A 176bparameter open-access multilingual language model. Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin Huang, Zhenan Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, et al. 2024. Machine\n# Diederik P. Kingma and Max Welling. 2019.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA Reading Comprehension Challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328.\nDivya Aggarwal Rimmi Anand and Vijay Kumar. 2017. A comparative analysis of optimization solvers. Journal of Statistics and Management Systems.\n# Tadeusz Sawik. 2011. Scheduling in Supply Chains Using Mixed Integer Programming.\nGemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L\u00e9onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram\u00e9, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118. Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024a. Openchat: Advancing open-source language models with mixed-quality data. In ICLR. Teng Wang, Wing-Yin Yu, Ruifeng She, Wenhan Yang, Taijie Chen, and Jianping Zhang. 2024b. Leveraging large language models for solving rare mip challenges. arXiv preprint arXiv:2409.04464. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In NeurIPS. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS. Russ J Vander Wiel and Nikolaos V Sahinidis. 1995. Heuristic bounds and test problem generation for the time-dependent traveling salesman problem. Transportation Science.\nRuss J Vander Wiel and Nikolaos V Sahinidis. 1995. Heuristic bounds and test problem generation for the time-dependent traveling salesman problem. Transportation Science.\nZiyang Xiao, Dongxiang Zhang, Yangjun Wu, Lilin Xu, Yuan Jessica Wang, Xiongwei Han, Xiaojin Fu, Tao Zhong, Jia Zeng, Mingli Song, and Gang Chen. 2024. Chain-of-experts: When LLMs meet complex operations research problems. In ICLR. Linzi Xing, Xinglu Wang, Yuxi Feng, Zhenan Fan, Jing Xiong, Zhijiang Guo, Xiaojin Fu, Rindra Ramamonjison, Mahdi Mostajabdaveh, Xiongwei Han, et al. 2024. Towards human-aligned evaluation for linear programming word problems. In LREC-COLING, pages 16550\u201316556. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024a. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Jiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, and Chang Zhou. 2024b. Synthesizing text-tosql data from weak and strong llms. In ACL. Tianxing Yang, Huigen Ye, and Hua Xu. 2024c. Learning to generate scalable milp instances. In Proceedings of the Genetic and Evolutionary Computation Conference Companion. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing reasoning and acting in language models. In ICLR. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In ACL. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nJiaxi Yang, Binyuan Hui, Min Yang, Jian Yang, Junyang Lin, and Chang Zhou. 2024b. Synthesizing text-tosql data from weak and strong llms. In ACL.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023. Automatic chain of thought prompting in large language models. In ICLR.\n# A Appendix\n# A.1 An Example for ComplexOR Dataset\nBinpacking modeling problem in the ComplexOR dataset is shown blew:\n\"id\": 3, \"title\": \"Binpacking Problem\", \"description\": \"The bin packing problem involves assigning items of known weights to bins with uniform capacity. The objective is to minimize the total number of bins utilized while ensuring that all items are allocated and each bin\u2019s total weight does not exceed the bin capacity.\", \"category\": [\"Binpacking Problem\"], \"model\": { \"set\": [ { \"name\": \"I\", \"description\": \"Set of items\" } ], \"parameter\": [ { \"name\": \"s\", \"description\": \"weight of item \u2018 i\u2018\", \"domain\": \"{i <in> I}\" }, { \"name\": \"c\", \"description\": \"Capacity of a bin\" } ], \"variable\": [ { \"name\": \"y\", \"description\": \"Binary variable, 1 if we use bin \u2018j\u2018\", \"domain\": \"{j <in> I}\", \"type\": \"binary\" }, { \"name\": \"x\", \"description\": \"Binary variable, 1 if we assign item \u2018i\u2018 to bin \u2018j\u2018\",\n\"id\": 3, \"title\": \"Binpacking Problem\", \"description\": \"The bin packing problem involves assigning items of known weights to bins with uniform capacity. The objective is to minimize the total number of bins utilized while ensuring that all items are allocated and each bin\u2019s total weight does not exceed the bin capacity.\", \"category\": [\"Binpacking Problem\"],\n\"type\": \"binary\" } ], \"objective\": [ { \"name\": \"MinBins\", \"description\": \"Minimize the total number of used bins\", \"sense\": \"min\", \"function\": \"<sum>_{j <in> I} y_{ j}\" } ], \"constraint\": [ { \"name\": \"CapConstraint\", \"description\": \"The total weight of assigned items to a bin should not exceed the bin capacity\", \"domain\": \"{j <in> I}\", \"function\": \"<sum>_{i <in> I} s_{ i} * x_{i,j} <= c * y_{j}\" }, { \"name\": \"AssignConstraint\", \"description\": \"Every items should be assigned to a bin\", \"domain\": \"{i <in> I}\", \"function\": \"<sum>_{j <in> I} x_{ i,j} = 1\" }\n<div style=\"text-align: center;\">A.2 Rule for Text-to-MIP</div>\n# A.2 Rule for Text-to-MIP\nThe following rules are to be obeyed by the LLM, combined with modeling information (shown in Appendix A.1 from the ComplexOR dataset), to generate the prompt.\nYou must follow the following rules:\n# You must follow the following rules:\n: If the set in the model.json file has the \u2019range\u2019 key, this set has a hyper-parameter. For example, {range : [1,T]}, where T is the hyperparameter. If the set in the model.\njson file doesn\u2019t have the \u2019range\u2019 key, the set doesn\u2019t have a hyperparameter. 2: The number of set equals to the number of hyper-parameter. 3: In the set key, you need to append each set\u2019s lower bound and upper bound to the set value in order. The value of bound is generated by you after reading the model json file. 4: In the above case, if json_obj[\u2019model \u2019][\u2019set\u2019] does not have the range field, you need to add [null, null] to the hyper-parameter. If json_obj [\u2019model\u2019][\u2019set\u2019]} has the range field, you need to add the predicted lower and upper bounds to the hyper -parameter. 5: The lower bound and upper bound of the set must be numbers, not null! The lb of set can start from non-one number. 6: When you provide the lower and upper bounds of the parameter, the value includes the lower bound but does not include the upper bound. 7: You should specify the data type of each parameter in order. The lb and ub of a parameter must either both be integers or both be float. If you think the type of this parameter is int, then you should append \u2019 integer\u2019. If you think the type of this parameter is float, then you should append \u2019float\u2019. 8: You should ensure that the gap between ub and lb of the parameter should be less than or equal to 15. ub-lb <= 15! At the same time, diversify the upper and lower bound of parameter. 9: You must follow the json data format {\u2019set\u2019: [[lb1, ub1], [lb2,ub2]...], \u2019hyper-parameter\u2019: [[lb1, ub1], [lb2 ,ub2]...], \u2019parameter\u2019: [[lb1, ub1 ],[lb2, ub2]...], \u2019parameter_types \u2019:[integer, integer, float, ...]}.\n\nFigure 7: The figure shows the translations of rules 4, 7, and 8 into Mandarin, Korean, and Thai, illustrating the language-specific versions of these rules.\nSince our experiment primarily focuses on generating data according to rules 4, 7, and 8, we also present the corresponding dictionary in Fig. 7, which illustrates how these rules are translated into Mandarin, Korean, and Thai.\n# A.3 Prompt Template for Text-to-MIP\nThe MLPrompt approach involves simply replacing a specific rule with its equivalent in another language. The dominant language in the prompt is English, and during application, only the target rule needs to be swapped with its counterpart in the desired language. The prompt template is as follows:\n# Prompt Template\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a654/a6544d5e-1be2-4198-bb6b-acbcc17dbb7e.png\" style=\"width: 50%;\"></div>\n# A.4 Analysis rules for Text to MIP\nRule 4 requires the LLM to read through the detailed modeling information, which consists of a long text, analyze the modeling process, and determine whether each set contains a corresponding hyper-parameter after reading the rule. Rule 4 evaluates the LLM\u2019s ability to comprehend the lengthy text and reason the result based on both the given question and the context. Rule 7 requires consistency between the generated parameter values and their respective data types. Since the LLM must first output all parameter values and then generate their corresponding data types, it often forgets previous outputs, leading to struggles in maintaining coherence across the generated values. Rule 7 assesses the LLM\u2019s ability to ensure coherence throughout the process. Rule 8 requires the LLM to diversify its output while adhering to specific mathematical constraints. The LLM often fails by generating common or repeated data values and not complying with the necessary constraints. Rule 8 evaluates the LLM\u2019s ability to generate diverse numbers, perform calculations, and adhere to given constraints.\n# A.5 Details of CoT, ToT, and SC\nDue to the inherent ambiguity in natural language, evaluating intermediate steps during the reasoning process becomes problematic for methods like CoT, ToT, and SC. These methods rely on breaking down complex tasks into smaller steps and verifying them, which works well in structured environments like logic expressions. However, in structured data generation tasks, it is challenging to clearly define whether an intermediate result is accurate, as language is often subject to interpretation. Additionally, the interconnected nature of rules in structured outputs, such as JSON, makes it difficult to verify each step in isolation\u2014correctness in one part does not necessarily guarantee overall correctness when the parts are combined. In our experiment, we made slight modifications to CoT, ToT, and SC to adapt them to our specific task. The details of these modifications are as follows: CoT We implement CoT using zero-shot prompting by simply adding the phrase, \"Let\u2019s think it step\nby step,\" to guide the LLM in reasoning through the task. ToT We implement ToT by having the LLM generate each part of the desired JSON, similar to the structure of a tree, one by one. The previous output is combined into the prompt at each step until the entire requirement is fulfilled. SC We implement SC by generating the full JSON multiple times, evaluating each result as either correct or incorrect. These labeled outputs are then combined into a new prompt, which is provided to the LLM to generate a more consistent and accurate final output.\n# A.6 Text to SQL\nTo demonstrate the generalization and robustness of MLPrompt, we apply our method to the Text-toSQL task using the Spider V1.0 dataset (Yu et al., 2018). Given that each entity\u2014such as companies, individuals, and datasets\u2014follows its own coding style, we filter questions that GPT-4 answered incorrectly by having it generate SQL queries directly based on the SQL schema used in database construction, without any specific rules. We manually analyzed these incorrect cases and summarized a set of rules for GPT-4 to follow. Given the large size of the dataset, we used only 20% of it and identified the types of questions GPT-4 struggles to handle effectively. By combining these rules with the SQL schema, we employed zero-shot learning with GPT-4 to test whether it could generate the correct queries. The rules are as follows:\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c88/5c886f81-a231-407b-b9e2-adea2b1623fd.png\" style=\"width: 50%;\"></div>\nRules for Text-to-SQL\n1.\nWhen handling queries involving\ncounting-related issues, avoid using \"LEFT\nJOIN\" or \"RIGHT JOIN\" to generate non-\nexistent records.\nInstead, use \"INNER\nJOIN\".\n2. Pay attention to the order of values re-\nquested in the question! Make sure the \"SE-\nLECT\" clause provides the appropriate field\norder. If the question does not ask for a\ncount, do not include it. If the requirement\nis \"xxx1 for each xxx2,\" the first field in\n\"SELECT\" should contain the data (xxx1)\nand the last field should return the category\n(xxx2). If the target is a primary key and\nthe question asks how many xxx each pri-\nmary key has, put the primary key last and\nthe count first. There\u2019s no need to include\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c87c/c87ca636-e3c5-4962-92fe-c3cb1f4c9583.png\" style=\"width: 50%;\"></div>\nnon-key fields like names. 3. If the logic is complex, use subqueries instead of joining tables. Avoid using \"DISTINCT\" unless necessary. When considering relationships between tables based on the query and the table\u2019s information, determine whether it\u2019s a \"has-a\" or \"is-a\" relationship. If the \"SELECT\" fields include a \"has-a\" scenario, use \"DISTINCT\" to avoid duplicates. If the query asks to list all xxx, do not use \"DISTINCT\". 4. If the user wants to find the maximum/minimum/average value in a table, consider using subqueries instead of grouping by different categories. If the query requires finding the maximum/minimum/average value within categories, use aggregation functions and \"GROUP BY\". 5. If the query requires listing all information, use \"SELECT *\".\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of generating structured data using Large Language Models (LLMs) in real-world applications, where the inherent ambiguity of natural language and the complexity of rule sets often hinder the effectiveness of traditional methods. Previous methods have struggled to follow specified rules in long contexts, necessitating a new approach to enhance reasoning and understanding.",
        "problem": {
            "definition": "The problem is the generation of Mixed Integer Programming (MIP) instances from natural language descriptions, where LLMs often omit rules due to complexity and ambiguity in the instructions provided.",
            "key obstacle": "The main difficulty lies in the interdependent nature of rules and the tendency of LLMs to neglect certain rules when generating data, leading to unsatisfactory instances."
        },
        "idea": {
            "intuition": "The idea is inspired by human polyglots, where understanding non-dominant languages aids in processing dominant languages. This insight suggests that LLMs can similarly benefit from leveraging their multilingual capabilities.",
            "opinion": "MLPrompt is proposed as a novel prompting strategy that translates error-prone rules into non-dominant languages, thereby drawing greater attention to these rules and improving the accuracy of generated outputs.",
            "innovation": "The key innovation of MLPrompt lies in its ability to enhance LLM reasoning by introducing non-dominant languages to focus attention on error-prone constraints, contrasting with existing methods that primarily rely on iterative reasoning steps."
        },
        "method": {
            "method name": "Multi-Lingual Prompt",
            "method abbreviation": "MLPrompt",
            "method definition": "MLPrompt is a prompting strategy that translates error-prone rules into non-dominant languages to improve the understanding and reasoning of LLMs in generating structured data.",
            "method description": "The core of MLPrompt involves translating rules into non-dominant languages to enhance LLM attention and data quality.",
            "method steps": [
                "Incorporate predefined rules and input modeling information to generate initial prompts.",
                "Generate structured data (e.g., JSON) based on the initial prompt using the LLM.",
                "Evaluate the generated data against the predefined rules.",
                "If violations are detected, translate the corresponding rule into a non-dominant language and update the prompt.",
                "Repeat the data generation process with the updated prompt."
            ],
            "principle": "The effectiveness of MLPrompt is based on the premise that introducing non-dominant languages helps LLMs focus on and adhere to complex rules, thereby improving the overall quality of generated structured data."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the ComplexOR dataset, which contains 60 complex operations research problems with natural language descriptions and corresponding mathematical formulations. Various LLMs were evaluated against baseline methods for generating structured data.",
            "evaluation method": "The evaluation involved measuring the accuracy of generated JSON outputs against predefined rules, with comparisons made between MLPrompt and other prompting strategies like Chain-of-Thought, Tree-of-Thought, and Self-Consistency."
        },
        "conclusion": "The experiments demonstrated that MLPrompt significantly improves the generation of structured data from LLMs, achieving higher accuracy and faster inference times compared to traditional prompting methods. It effectively bridges the gap between LLM capabilities and industrial needs for MIP generation.",
        "discussion": {
            "advantage": "MLPrompt stands out by effectively utilizing multilingual capabilities to enhance LLM reasoning, leading to improved attention on critical rules and better data generation quality.",
            "limitation": "A notable limitation is the challenge of identifying specific rule violations in natural language prompts, which can complicate the validation of generated outputs.",
            "future work": "Future research should focus on developing mechanisms to better identify and address rule violations in natural language, as well as exploring additional applications of MLPrompt in other structured data generation tasks."
        },
        "other info": {
            "additional insights": "The study found that the effectiveness of MLPrompt is influenced by the choice of non-dominant languages used for translations, with certain languages yielding better results than others."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of generating structured data using Large Language Models (LLMs) in real-world applications, highlighting the inherent ambiguity of natural language and the complexity of rule sets."
        },
        {
            "section number": "1.3",
            "key information": "The paper discusses how LLMs often omit rules due to complexity and ambiguity in instructions, necessitating new approaches to enhance reasoning and understanding."
        },
        {
            "section number": "3.1",
            "key information": "The method proposed, MLPrompt, improves LLM reasoning by translating error-prone rules into non-dominant languages, which enhances the model's attention to complex rules."
        },
        {
            "section number": "4.1",
            "key information": "MLPrompt is a novel prompting strategy that effectively utilizes multilingual capabilities to improve the accuracy of generated outputs by focusing on error-prone constraints."
        },
        {
            "section number": "6.1",
            "key information": "The limitation of identifying specific rule violations in natural language prompts complicates the validation of generated outputs, highlighting challenges in the application of in-context learning."
        },
        {
            "section number": "7",
            "key information": "The paper concludes that MLPrompt significantly improves the generation of structured data from LLMs, achieving higher accuracy and faster inference times compared to traditional methods."
        }
    ],
    "similarity_score": 0.6967948226841173,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Large Language Models are Good Multi-lingual Learners _ When LLMs Meet Cross-lingual Prompts.json"
}