{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2410.06768",
    "title": "Patterns of Creativity: How User Input Shapes AI-Generated Visual Diversity",
    "abstract": "Recent critiques of Artificial-intelligence (AI)-generated visual content highlight concerns about the erosion of artistic originality, as these systems often replicate patterns from their training datasets, leading to significant uniformity and reduced diversity. Our research adopts a novel approach by focusing on user behavior during interactions with Text-to-Image models. Instead of solely analyzing training data patterns, we examine how users' tendencies to create original prompts or rely on common templates influence content homogenization. We developed three originality metrics -- lexical, thematic, and word-sequence originality -- and applied them to user-generated prompts from two datasets, DiffusionDB and Civiverse. Additionally, we explored how characteristics such as topic choice, language originality, and the presence of NSFW content affect image popularity, using a linear regression model to predict user engagement. Our research enhances the discourse on AI's impact on creativity by emphasizing the critical role of user behavior in shaping the diversity of AI-generated visual content.",
    "bib_name": "palmini2024patternscreativityuserinput",
    "md_text": "# Patterns of Creativity: How User Input Shapes AI-Generated Visual Diversity\nMaria-Teresa De Rosa Palmini and Eva Cetinic\nUniversity of Zurich, Zurich, Switzerland maria-teresa.derosa-palmini@uzh.ch eva.cetinic@uzh.ch\nAbstract. Recent critiques of Artificial-intelligence (AI)-generated visual content highlight concerns about the erosion of artistic originality, as these systems often replicate patterns from their training datasets, leading to significant uniformity and reduced diversity. Our research adopts a novel approach by focusing on user behavior during interactions with Text-to-Image models. Instead of solely analyzing training data patterns, we examine how users\u2019 tendencies to create original prompts or rely on common templates influence content homogenization. We developed three originality metrics\u2014lexical, thematic, and word-sequence originality\u2014and applied them to user-generated prompts from two datasets, DiffusionDB and Civiverse. Additionally, we explored how characteristics such as topic choice, language originality, and the presence of NSFW content affect image popularity, using a linear regression model to predict user engagement. Our research enhances the discourse on AI\u2019s impact on creativity by emphasizing the critical role of user behavior in shaping the diversity of AI-generated visual content.\nKeywords: Text-to-Image \u00b7 Generative AI \u00b7 Prompt Engineering\n# 1 Introduction\nThe latest advancements in Computer Vision and Natural Language Processing have paved the way for numerous applications in generative models, particularly in text-guided image generation. Innovations like DALL-E 2 [36] and Stable Diffusion [37] have built upon advanced techniques in combined image and text embedding learning, such as Contrastive Language and Image Pretraining (CLIP) [35], enabling the generation of photorealistic and aesthetically appealing images from textual prompts. The applications of generative Artificialintelligence (AI) Text-to-Image (TTI) models are currently utilized in diverse sectors, such as news media graphics [24] and product design [21]. However, despite these advancements, concerns are growing about their heavy reliance on training data, which often leads to the perpetuation of Western-centric biases related to culture, race, and gender [3], [11], as well as a lack of authenticity in the generated content, resulting in the emergence of a visually homogenous culture.\nWhile much of the existing research has focused on algorithmic biases and dataset limitations in TTI models [9], [25], [40] less attention has been given to the role of human agency, particularly through the user-provided text descriptions, also known as \"prompts\", in shaping the AI-generated visual culture. Research in prompt engineering has proposed various guidelines for structuring prompts, recommending specific language and techniques to optimize image quality by offering effective strategies for prompt formulation [23], [31], [47]. Online communities, such as the subreddit r/StableDiffusion1, and resources like A Beginner\u2019s Guide to Prompt Design for Text-to-Image Generative Models2 and The Art and Science of Prompt Engineering3 provide users with prompt examples and strategies, helping them achieve impressive results with minimal trial and error. However, this standardized prompting process, while effective for creating visually appealing images, may also lead to the repetitive use of similar language structures, reinforcing a culture of visual uniformity. Additionally, the persistent use of a coding-syntax-like language in prompts may have established a fixed linguistic template, constraining users to narrow parameters and limiting their ability to explore and experiment with a broader and more diverse range of linguistic expressions and thematic subjects. This study aims to bridge the gap in understanding how user-generated prompts influence the originality of AI-generated visual content. It suggests that uniformity in prompts, both in terms of topics and linguistic choices, may be one of the factors contributing to visual homogenization, potentially limiting stylistic diversity in AI outputs while also perpetuating certain cultural biases. To do so, we focus not only on the topics users prompt about, but also on the originality of the language choices used in these prompts. To achieve this, we developed three originality metrics: lexical originality (the uniqueness of words), thematic originality (the diversity of topics), and word-sequence originality (the novelty of word combinations). These metrics are based on the concept of \"novelty\" defined by Shah et al. [39], which considers an idea novel if it significantly deviates from existing ones, as determined by the rarity of its attributes compared to a predefined set or across a collection of ideas. In our study, we apply this concept to analyze the frequency of lexical choices, thematic elements, and word sequences within two extensive prompt datasets, DiffusionDB [49] and Civiverse [33], identifying as original those that deviate significantly from the dataset norm. In addition to assessing prompt originality, we aimed to identify which other prompt features influence the popularity of AI-generated images on open-source platforms like CivitAI [12], as the type of content that resonates with users often reflects broader aesthetic trends in emerging AI-generated visual culture. To achieve this, we developed a Linear Regression model to predict user en-\n1 https://www.reddit.com/r/StableDiffusion/ 2 https://towardsdatascience.com/a-beginners-guide-to-prompt-design-fortext-to-image-generative-models-8242e1361580 3 https : / / medium . com / @alimelki / the - art - and - science - of - prompt engineering-mastering-ai-communication-cc55261bfe24\ngagement based on various prompt characteristics, including originality metrics, specific topics identified through topic modeling, and the presence of NSFW content. By analyzing the impact of these features on image popularity, we sought to determine which elements are most likely to shape aesthetic preferences in AI-generated content. This analysis provides insight into whether popular content in open-source platforms contributes to visual homogenization and bias reinforcement, or promotes greater diversity in AI outputs. By examining the interplay between user behavior and AI outputs, this research addresses a critical gap in understanding how human agency influences the cultural products of AI systems. It contributes to the field of human-computer interaction (HCI) by highlighting the social and creative dynamics that may drive visual uniformity in AI-generated content. Ultimately, this study offers new insights into how human-AI collaboration can either foster diversity in visual outputs or reinforce existing biases and patterns, both artistic and cultural. These findings underscore the need for platform developers and online communities to revisit current guidelines, fostering a creative environment that encourages exploration, innovation, and the representation of a wider range of perspectives in AI-generated visual culture.\n# 2 Theoretical Background and Related Work\nThe emergence of generative AI technologies, particularly TTI models, has transformed creative processes, enabling users to generate visuals based on textual descriptions. While these tools open new avenues for artistic expression, they also raise important questions about how technology and human agency interact in shaping the creative outcomes. In this section, we explore key debates on technological determinism, the critical role of human agency, and how prompt engineering practices might impact user creativity. By examining these issues, we aim to contextualize the relationship between user-generated prompts and originality of AI-generated outputs.\n# 3 The Notion of Technological Determinism\nA central debate surrounding technology\u2019s role in society is framed by the theory of technological determinism, which asserts that technology is the primary force driving social, cultural, and artistic change [15]. However, this view has been critiqued for oversimplifying the intricate relationship between humans and technology, ignoring the broader context in which technology operates [13]. Critics argue that technology is not an autonomous force but is shaped by an array of social, cultural, and political factors that influence its development and use. As a result, its effects are not solely determined by technological capabilities but are also the product of human decisions, values, and the specific contexts in which it is applied [43].\n# 3.1 The Role of Human Agency\nGiven the critique of technological determinism, the role of human agency becomes crucial in understanding how AI technologies influence creative outputs. While generative AI has been praised for its collaborative potential, particularly in ideation and art-making, recent studies have shown that human-AI interaction remains central to producing meaningful creative outcomes [8], [10], [19], [32]. Models of human-AI collaboration range from systems with elevated computational awareness [14] to frameworks that emphasize real-time, bidirectional communication [26], [44]. Additionally, mixed-initiative approaches, where both human and AI agents contribute to the creative outcome, have been shown to effectively support creative processes [22], [53]. Despite these advancements, there is an increasing need to reestablish human agency as central to the creative process, particularly as the reliance on Large Language Models (LLMs) like ChatGPT [30] and other generative technologies grows significantly. While these tools offer new possibilities for artistic expression, they also introduce significant risks, most notably the potential erosion of creativity through the repetitive generation of formulaic, uninspired outputs. Critical AI studies have emphasized that TTI generated images often lack a clear authorial figure, raising complex questions about the notion of authorship in the context of AI-generated art. Specifically, some scholars suggest that the concept of authorship is being displaced, with the role of the prompt-maker being minimized or overlooked entirely in favor of assigning creative identity to the scientists and programmers behind the models themselves [51]. This debate revisits older disputes from the field of computational art, where advanced technologies often led to a blurring of boundaries between human creative decision-making and the highly formalized processes that seek to emulate it [18]. Additioanly, the depiction of AI systems as \"creative collaborators\" has amplified these uncertainties, reinforcing misconceptions about the concept of machine agency and concealing the genuine relationship between human creators and AI-generated processes [2, pp.27\u201328, 241\u201343], [17, pp.3\u20135]. In this context, the risk of visual homogenization is particularly evident in TTI models, where users often rely on standardized prompts and familiar visual styles, resulting in repetitive and uninspired imagery. This issue reflects broader challenges in the design of Creativity Support Tools (CSTs), which, while one the hand intend to foster and enhance creativity, can impose rigid structures that limit creative exploration and diversity. To mitigate this, maintaining active human engagement in AI-assisted creative processes is crucial, as it ensures the creation of original and meaningful content while also enhancing the effectiveness of CSTs [34]. Furthermore, research in both Creativity Research (CR) and Human-Computer Interaction (HCI) emphasizes the importance of promoting divergent thinking and encouraging exploratory practices, which are often overshadowed with the role of creativity being constrained by restrictive and technology-guided frameworks [16].\n# 3.2 Prompt Engineering and Its Impact on Originality\nThe rise of generative models has highlighted the importance of prompt design, resulting in numerous techniques being developed for prompt engineering. The concept gained prominence from a well-known post by Gwern Branwen [5] about GPT-3\u2019s ability to generate creative fiction, where it was suggested that mastering prompt design could become a new mode of interacting with models. Users would simply need to craft prompts in a way that draws out the required information and abstractions for completing specific tasks. As the field has evolved, prompt engineering has greatly improved the capabilities of generative models, particularly in TTI systems. Many TTI platforms now offer guidelines to help users create prompts that enhance image quality. For instance, in earlier iterations of Stable Diffusion, phrases like cinematic, highly detailed, and 8k were essential for generating highquality images, though recent advancements in the model have made these terms less critical. Similarly, OpenAI\u2019s DALL-E offers guidance for users to improve image generation4, recommending the inclusion of specific details like elements, settings, artistic styles, or emotions to produce more accurate and refined outputs. In addition to prompting guidelines, several techniques have been developed to optimize prompt creation and improve output quality. Methods like few-shot prompting [6], [52] and chain-of-thought prompting [50], [52] help guide models by providing limited examples or by breaking down tasks into logical steps. Research has also emphasized the importance of style keywords and modifiers for generating high-quality images [23], [31]. In more, recent advancements in the field include automated prompt generation techniques with reinforcement learning-based tools like Promptist [20], which enhance prompt effectiveness across different models. These automated methods are further supported by interactive tools like Promptify [4] or PromptCharm [48], which refine prompts iteratively based on user feedback. Although these aforementioned developments have increased the sophistication of prompt engineering, this increasing emphasis on prompt optimization has inadvertently led to the development of standardized \"prompt templates,\" where users tend to rely on familiar structures and themes, contributing to a growing visual uniformity in AI-generated content. Research into the thematic trends of user-generated prompts on platforms like Midjourney has shown that popular styles and surface-level aesthetics\u2014such as fantasy, game art, and anime\u2014are often prioritized over deeper artistic elements like narrative and authenticity [27], [38]. This tendency has resulted in a cultural uniformity in the topics generated, reinforcing stylistic norms and leading to the emergence of predominantly homogenized outputs. However, while much of the existing research has focused on what users are prompting about, there has been limited attention to the linguistic diversity in how these prompts are crafted. Exploring therefore language originality\u2014through patterns in lexical choices, thematic elements, and  \n4 https://platform.openai.com/docs/guides/prompt-engineering/\nword combinations\u2014is crucial in understanding how prompt engineering might unintentionally limit visual diversity. Addressing this gap can reveal whether current prompt practices and guidelines are contributing to or counteracting the trend toward homogenization in the AI-generated visual culture.\n# 4 Methodology\nIn this section, the approach to evaluating the impact of prompt originality on the diversity of AI-generated content is outlined. Lexical, word-sequence, and thematic originality are analyzed across two datasets to assess the diversity of user-generated prompts between them. Furthermore, the correlations between prompt characteristics and user engagement are also explored, providing insights into how human input is shaping creative outcomes and popular trends in AIgenerated visuals.\nIn this study, prompt originality is examined through the use of two large-scale datasets: DiffusionDB and Civiverse. These datasets capture user interactions with TTI models but differ significantly in their origins, purposes, and content regulations. DiffusionDB comes from a strictly moderated environment, while Civiverse represents a more open platform with fewer restrictions. This variation is essential for understanding whether different content guidelines might influence not only the linguistic originality in user-generated prompts but also the diversity of the resulting AI-generated visuals.\nDiffusionDB The DiffusionDB dataset [49] was collected over a two-week period, from October 6 to October 20, 2022, by gathering user-generated images and their associated metadata, such as seed values, step counts, CFG scale, and image size, from the official Stable Diffusion Discord server. With approximately 1.8 million unique text prompts, primarily contributed by experienced users and early adopters of Stable Diffusion, the dataset may exhibit a bias toward more advanced prompting techniques, as noted by the dataset creators. As Stable Diffusion is a widely-used open-source, large-scale TTI model, DiffusionDB is released under a CC0 1.0 license, allowing unrestricted public use of the content [41]. The Discord server enforces strict moderation policies, prohibiting illegal, hateful, NSFW, and personally identifiable content [42], ensuring that the dataset remains focused on safe-for-work, creative outputs. Civiverse The Civiverse dataset [33] is sourced from the CivitAI platform [12], which facilitates image generation and the exchange of Stable Diffusion model derivatives. The platform has experienced rapid growth, with a substantial volume of user-generated content uploaded daily. The dataset, assembled from this platform, contains metadata for over 6.5 million images posted between October 2023 and April 2024, such as the web URL, image hash, post ID, timestamp,\nanonymized user data, model information, and content rating. It is worth noting that CivitAI imposes fewer content restrictions, which is reflected in the significant proportion of NSFW content on the platform. To be more specific, the percentage of content rated above PG-13 increased from 55.19% in October 2023 to 72.94% in April 2024, highlighting the platform\u2019s more permissive content policies.\n# 4.2 Topic Modeling\nInspired by previous work in prompt analysis [27], [38], the thematic coverage of both the DiffusionDB and Civiverse datasets was first explored. This was considered essential not only for comparison but also as a critical step in developing the thematic originality metric (4.3), as the thematic clusters and their associated keywords from both datasets would be used as labels for the corresponding prompts. Given the disparity in dataset sizes\u20141.8 million prompts from DiffusionDB and over 6 million from Civiverse\u2014a direct comparison was deemed non-representative. To address this, a random sample of 1.8 million prompts from Civiverse was taken to ensure a balanced analysis. Due to the variability in raw prompt formats, individual prompt specifiers, text fragments specifying desired image characteristics (e.g., \"highly detailed,\" \"cinematic\"), were analyzed. Following a method similar to Sanchez [38], prompts were segmented by commas, and specifiers that appeared at least 300 times and were under 35 characters in length were selected. This process resulted in approximately 11,000 specifiers for DiffusionDB and 19,000 for Civiverse prompts. Additionally, the MiniLMv2 model [46] was used to transform the specifiers into 384-dimensional embeddings. For topic modeling and clustering, BERTopic was employed, utilizing UMAP [28] for dimensionality reduction and HDBSCAN [7] for clustering. Additionally, class-specific term frequency-inverse document frequency (c-TF-IDF) was further applied to extract key terms within clusters, facilitating the manual labeling of thematic groups.\n# 4.3 Prompt Originality Metrics\nTo comprehensively assess the diversity of user-generated prompts in AI-generated visual content, three originality metrics, lexical originality, word-sequence originality, and thematic originality, were calculated. These metrics draw on the concept of \"novelty\" as outlined by Shah et al. [39], which defines an idea as novel if it deviates considerably from existing ones, measured by the rarity of its characteristics in relation to a predefined set or a broader collection of ideas. Each metric captures a different dimension of prompt originality by evaluating the uniqueness of individual words, the contextual relationships between words, and the novelty of thematic combinations, respectively. By applying these metrics, the goal was to quantify the level of novelty introduced by the prompts in the two datasets, providing valuable insights into the originality of user interactions with TTI models.\nBefore calculating the originality scores, all text prompts underwent preprocessing steps, including stopword and special character removal, as well conversion to lowercase to ensure consistency in analysis.\nLexical Originality To assess the lexical originality of user-generated prompts, a metric that evaluates the rarity of individual words within each prompt relative to the entire dataset, is introduced. This measure plays a crucial role in distinguishing prompts that rely on common words from those that use more distinctive vocabulary, offering insights into how creatively users are interacting with TTI models. The process begins by calculating the frequency of each word across the dataset to determine how common or rare it is. The rarity score for each word is then computed as the inverse of its frequency, meaning that frequently appearing words receive a low rarity score, while rarely used words are assigned a higher score. To ensure stability in the calculation and prevent division by zero, a small constant  \\epsilon is added to the frequency, allowing the formula to function smoothly even for extremely common words. To discourage excessive repetition within a prompt, a penalty is applied based on how often a word is repeated. The more frequently a word appears in the same prompt, the higher the penalty:\nwhere  n is the total number of words in the prompt, and  \\text {count}(w_i) represents how many times word  w_i appears. In addition to the repetition penalty, another penalty is imposed for prompts that rely heavily on the most frequently used words across the entire dataset. These commonly used words further reduce the originality score, as they indicate less creative variation. To ensure that longer prompts do not automatically receive higher scores, the lexical originality score is adjusted based on the length of the prompt. This adjustment is proportional to the ratio of the number of words in the prompt to the maximum number of words in any prompt within the dataset:\nThe final lexical originality score is then determined by summing the rarity scores of all the words, applying both the repetition and common word penalties, and adjusting for the prompt\u2019s length:\n  S_{\\ t ext   {final}} =  \\max \\lef t  (S_{\\text {ad j u\nWord-Sequence Originality To address the limitations of lexical originality, an additional metric, that is word-sequence originality, is introduced. While lexical originality measures the rarity of individual words within a dataset, it might\nstill overlook how those words are combined in a broader context. As a result, a prompt might contain rare words but still follow conventional word pairings, something that can be limiting its creative potential. By focusing on the probabilities of transitions between words, word-sequence originality captures the contextual relationships, assessing how likely one word is to follow another and providing a deeper layer of originality evaluation in word sequences. For example, in a TTI prompt like \"beautiful sunset\", the transition from \"beautiful\" to \"sunset\" is common, contributing less to the sequence originality score. In contrast, a phrase like \"sunset underwater\" may be rare, even if \"sunset\" and \"underwater\" are common individually. Since the transition between these two words is unexpected, it results in a higher sequence originality score. The sequence originality of each prompt was calculated using a Markov Chain-based model [45]. First, each prompt was tokenized into words, stopwords are removed, and bigrams (pairs of consecutive words) were generated. The transition probability  P(w _ 2 \\mid w_1) , representing the likelihood of  w_2 following  w_1 , was calculated as:\nf The sequence originality score  S for each prompt was then calculated by summing the negative logarithms of these transition probabilities:\nRare transitions (low probability ones) contribute more to the score, while common transitions contribute less. The score is then normalized by dividing the total by the number of bigrams to ensure comparability across prompts of varying lengths. If a bigram is not found in the dataset, a small probability of  1  \\times 10^{-5} is assigned to prevent zero probabilities, ensuring the calculation remains unbiased.\nThematic Originality To evaluate the novelty of topics within prompts, a metric called thematic originality is developed. This metric measures how unique a set of topics within a prompt is relative to the overall dataset by analyzing both individual topic frequency and the co-occurrence of topics. This is essential for understanding the diversity of user-generated content and determining whether prompts follow repetitive patterns or introduce more original thematic combinations. The calculation of thematic originality begins with the assignment of thematic labels to each prompt based on the results of topic modeling. For instance, a label like High Resolution may be associated with keywords such as \"8k\", \"ultra detailed\", \"4k\" and \"high definition.\" These labels are then analyzed for their frequency, both individually and in combination with others. For example, while Female Subjects may be a frequent label, its combination with less\ncommon ones may still indicate novelty. The frequency of individual labels and their combinations is then calculated, capturing both standalone occurrences and relationships. The thematic originality score for a prompt is calculated by summing the negative logarithms of the probabilities of its topics and their combinations, as follows:\nHere,  P(t_i) represents the probability of the ith topic in the dataset, and  P(t_ i, t_j) is the probability of the combination of topics i and j. The negative logarithm is used because it increases the weight of rarer topics and combinations, making them contribute more to the originality score. Conversely, common topics lead to smaller values, lowering the score. For example, if a prompt contains the labels \"Urban Development\" and \"Sustainable Design\", and these labels, individually and together, appear infrequently in the dataset, the prompt will receive a higher originality score. In contrast, if the topics are common, the score will be lower.\nClustering and Categorization To evaluate the originality of prompts across the three metrics introduced, K-Means clustering is used to group the prompts into three categories: Low, Moderate, and High Originality. Instead of applying predefined originality thresholds, which may not generalize well across datasets with different characteristics, K-Means dynamically adjusts to the dataset by clustering based on the centroids of the originality scores. This approach ensures that the method remains flexible and adaptable, regardless of the specific properties of each dataset.\n# 4.4 Prediction of User Engagement\nIn the Civiverse dataset, LikeCount serves as a key metadata element, capturing the number of likes each generated image receives on the CivitAI platform. Users typically award likes to images that resonate with them on aesthetic, humorous, or intellectual levels, making the number of likes not only a reflection of an image\u2019s appeal but also an indicator of how effectively its prompt engages users, thus providing a crucial measure of a prompt\u2019s success within the platform. Consequently, understanding whether specific prompt characteristics can predict user engagement becomes essential, as it not only highlights the types of content garnering the most attention but also reveals which images are more frequently displayed and circulated across open-source platforms. As these images gain prominence and visibility, they play a significant role in shaping the overall visual culture of the platform, thereby influencing the dominant aesthetics in AI-generated content and potentially setting trends within the community. To investigate the relationship between prompt characteristics and user engagement, a subset of approximately 200,000 images and their corresponding\nmetadata from the Civiverse dataset is utilized, given that the DiffusionDB dataset does not include image popularity scores. From this dataset, a balanced sample of images, representing both high and low engagement levels, measured by whether the like count falls above or below the average threshold, is selected, following a similar approach to that of Arazzi et al. [1], who predicted user engagement in Twitter-related posts. In order to determine which prompt characteristics exert the most influence on user engagement, an model based on Linear Regression was developed, as LikeCount is a continuous variable [29, pp. 245\u2013246]. Each prompt was represented as a vector of features, incorporating various prompt-related characteristics, including the previously calculated originality metrics (lexical, wordsequence, and thematic originality), thematic elements identified through topic modeling, and the presence of NSFW content. After conducting topic modeling on the Civiverse dataset, approximately 70 distinct topics were identified, each associated with specific keywords and manually named based on these keywords. These topics were then utilized as features in the predictive model, with binary indicators assigned for each topic, marked as 1 if any of the topic\u2019s keywords were present in the prompt, and 0 otherwise. Additionally, the presence of NSFW content, already included in the metadata of the Civiverse dataset, was represented as a binary feature. A 5-fold cross-validation procedure was then performed, with 80% of the data used for training and 20% for testing, to ensure the robustness of the model. During this process, hyperparameters such as regularization strength and feature selection criteria are fine-tuned to optimize performance while minimizing overfitting. A regularization term of 0.1 is set to control the model\u2019s complexity, ensuring that the coefficients do not become excessively large and that the model captures meaningful patterns rather than fitting to noise. Additionally, a feature selection threshold of 0.05 was applied to focus on statistically significant features, in order for the model to rely on the most relevant predictors.\n# 5 Results\n# 5.1 Civiverse vs DiffusionDB\nThematic Similarities and Differences The comparative analysis of the Civiverse and DiffusionDB datasets reveals distinct thematic emphases, reflecting the different user bases and content policies of these platforms. Although equal dataset sizes were sampled for this study, topic modeling identified approximately 35 topics for DiffusionDB and 70 for Civiverse, highlighting potential variation in user interests and content generation across the two platforms. The designated topics were manually defined after reviewing the results of the topic modeling process, ensuring a representative and focused interpretation of the thematic outputs. DiffusionDB places a stronger emphasis on artistic themes, with a focus on traditional and contemporary art forms, as well as detailed explorations of character design and artistic techniques. Topics such as Artists and Famous Art\nFigures, Painting Techniques and Styles, as well as Portraits are particularly prevalent. This thematic focus, in contrast to Civiverse, suggests a more specialized environment where users delve deeply into particular artistic practices and engage in creative experimentation within those fields. In contrast, Civiverse, though more thematically diverse, is characterized by a stronger emphasis on explicit and sensual content, with topics such as Female Genitalia, Sensual Clothing, and Nudity being disproportionately represented. This suggests that Civiverse\u2019s users are more inclined to produce and share adult-oriented material, with the broader range of explicit content potentially stemming from the platform\u2019s more permissive nature. Despite their thematic differences, both platforms demonstrate significant overlap in topics related to visual and aesthetic quality. Specifically, topics such as Cinematic Lighting and Effects, Aesthetics and Beauty, Photography and Realism, and Intricate and Complex Details are prominent across both datasets. This shared emphasis on creating visually striking and high-quality content reflects a common aim of artistic refinement and appeal, suggesting that these elements are consistently important to users of TTI models, regardless of the primary thematic focus.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3acf/3acf41d8-f687-4fe1-a543-cbe9ce1353a9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">pin Fig. 2: Categorization of the primary topics and their relative proportions within the Civiverse dataset.</div>\n<div style=\"text-align: center;\">Fig. 1: Categorization of the primary topics and their relative proportions within the DiffusionDB dataset. Fig. 2: Categorization of the primary topics and their relative proportions within the Civiverse dataset.</div>\n<div style=\"text-align: center;\">Fig. 1: Categorization of the primary topics and their relative proportions within the DiffusionDB dataset.</div>\nOriginality Scores The results of the analysis reveal distinct differences in originality between the Civiverse and DiffusionDB datasets, with notable patterns emerging across all three originality metrics: lexical, thematic, and wordsequence originality. As demonstrated in Figure 5.1, prompts from DiffusionDB exhibit a higher proportion of high lexical originality, indicating that users of this platform tend to employ a more diverse and less predictable vocabulary. In contrast, Civiverse displays a greater prevalence of low lexical originality, suggesting that prompts in this dataset frequently rely on common and familiar words. Shifting focus to thematic originality, shown in Figure 5.1, both datasets\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4d3a/4d3a3909-cc0b-40ba-8775-816500df355a.png\" style=\"width: 50%;\"></div>\ndisplay a significant focus on low-originality themes; however, this trend is more pronounced in Civiverse, where nearly 70% of prompts fall into the low thematic originality category, underscoring the platform\u2019s reliance on recurring themes. It is important to mention that the very low proportion of high thematic originality observed in both datasets might be influenced by the fact that the topics measured are based solely on those captured through topic modeling. If a prompt addresses uncommon or niche topics that were not identified by the topic modeling process, it remains unnoticed in the categorization, potentially underestimating the presence of its high thematic originality. Finally, in terms of word-sequence originality (Figure 5.1), both datasets are largely dominated by moderately original word combinations, although Civiverse exhibits a slight advantage in the proportion of high word-sequence originality prompts. Overall, the results indicate that while there are instances of originality in user prompts, both datasets exhibit a strong tendency toward the repetition of common topics and familiar word combinations. This trend is particularly evident in Civiverse, where low levels of thematic and lexical originality are more pronounced, suggesting that users on this platform are more inclined to rely on conventional language patterns and familiar topics. Although there are examples of users experimenting with more creative language and concepts, especially in DiffusionDB, where high lexical and word-sequence originality are more prominent, the majority of prompts across both platforms tend to follow established linguistic and thematic structures. To complement this analysis, Table 1 provides examples of prompts from the DiffusionDB and Civiverse datasets, labeled with their corresponding originality scores, offering a practical illustration of the differences in prompt structures across the two platforms.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c8a/7c8abdbf-b0e4-4c0d-ac2f-fdd655fd1bc5.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Thematic Originality Word-Sequence Origin</div>\n<div style=\"text-align: center;\">Thematic Originality</div>\nFig. 3: Comparison of Lexical, Word-Sequence, and Thematic Originality between DiffusionDB and Civiverse Datasets.\n<div style=\"text-align: center;\">Word-Sequence Originality</div>\n<div style=\"text-align: center;\">iginality Word-Sequence Originality</div>\n<div style=\"text-align: center;\">Table 1: Example Prompts from DiffusionDB and Civiverse with Assigned Originality Labels.</div>\nOriginality Metrics\nPrompt Examples\nDiffusionDB Prompt\nCiviverse Prompt\nLexical Originality\nLow Lexical Original-\nity\na space girl with big and cute eyes, hold-\ning a cat, very anime, fine face, realistic\nshaded perfect face, fine details.\na\ngirl\nconfessing\nher\nlove,\nsmil-\ning at viewer, blushing, masterpiece,\nbest quality, highres, best illumination,\ndepth of field, detailed background, dy-\nnamic angle perspective.\nModerate\nLexical\nOriginality\none cute komodo dragon with very big\neyes, wearing a bandana and chain,\nholding a laser gun, standing on a desk,\ndigital art, award winning, in the style\nof the movie zootopia.\ndark theme low-key mysticism, fantasy,\ngothic, horror glowing flora nocturnal\ncreatures magical phenomena or eerie\ndark settings typical of gothic tales, lush\nroots with Forest Bathed in shadows,\nmacro lens Sepia filter psychedelic.\nHigh Lexical Original-\nity\nlarge colorful balloons with people on\nrope swings underneath, flying high\nover the beautiful countryside land-\nscape,\nprofessional\nphotography,\n80\nmm, telephoto lens, realistic, detailed,\ndigital art.\nbreathtakingly beautiful woman stand-\ning amidst a field of vibrant colorful\nflowers, the fragrance of the flowers is\nheavy in the air mixing with the gen-\ntle breeze that whispers through the field\ncreating an idyllic and enchanting at-\nmosphere.\nThematic Original-\nity\nLow Thematic Origi-\nnality\na painting of a fully dressed girl wear-\ning a jacket upper body with beautiful\npurple galaxy eyes, highly detailed, dig-\nital painting, artstation, sharp focus,\ndreamy illustration.\ngothic style art of two stunningly beau-\ntiful naked 20-year-old girls, turquoise\neyes, light auburn crown, braid hair,\nplaying with jewelry with a flirtatious\nsmile.\nModerate\nThematic\nOriginality\na painting of an orange cat staring\nprofoundly into the window, american\nscene painting, dutch golden age.\n36-year-old cyberpunk girl, dark fairy\ntale gown with hand-embroidered de-\ntails, full skirt and matching acces-\nsories, neon-lit rain-drenched alleyway\nwith reflections creating a disorienting\neffect.\nHigh Thematic Origi-\nnality\ntwo spherical glass plasma lamp heads\nwith normal human bodies and clothes\nhaving an awkward dinner date in a\ndimly lit cafe.\nultra realistic, full body color pencil\ndrawn portrait of Eleanor Rigby, picks\nup the rice in the church where a wed-\nding has been, lives in a dream, waits at\nthe window, ethereal figures from for-\ngotten memories hover behind her.\nWord-Sequence\nOriginality\nLow\nWord-Sequence\nOriginality\nepic gorilla battle, wlop, concept art,\ndigital painting, trending on artstation,\nhighly detailed, epic composition, 8k\nuhd.\nhigh quality, detailed background, 1girl\nsolo, pink hair, short hair short, losed\neyes, looking at viewer, mature female,\nsmall breasts, bikini under clothes, wet\nshirt.\nModerate\nWord-\nSequence Originality\nthe subtle shades of consciousness as an\nabstract painting.\nan eldritch magical tome bound in de-\nmon skin and an elder sign on the\ncover.\nHigh\nWord-Sequence\nOriginality\na portal to the void deep below the mar-\niana trench, fishes swimming towards\nthe portal, eerie, mixed media.\nlisbon street, a model in a beaded flow-\ners intertwining, trimmed by lime pink\nshoelaces through the mesh romper, the\nA-line romper is made of white mesh.\n# 5.2 Case Study: The Impact of Lexical Originality on Visua Homogenization\nIn this case study, the impact of lexical originality on visual homogenization is explored by analyzing a random sample of 15,000 images and their corresponding prompts, drawn from the DiffusionDB dataset. After the thematic originality metrics are calculated, the prompts are categorized based on their respective topic labels, providing a framework for deeper analysis. To maintain consistency and reduce variability introduced by dissimilar subjects, the focus is then narrowed to prompts specifically related to the topic of Portraits. This approach ensures that visual features are compared within a more homogenous subject matter, allowing for more accurate insights into how originality influences visual diversity. As a result, approximately 3,500 prompts and their corresponding images, all labeled under the topic of Portraits, are isolated for further investigation and analysis.\nCalculation of CLIP embeddings The lexical originality scores for the aforementioned prompts are calculated and categorized into three groups: high originality (975), low originality (1,700), and moderate originality for the remainder ones. To ensure a balanced comparison between the extremes of originality, an equal number of high and low originality cases (975 each) is selected, thereby avoiding bias from unequal representation across originality levels. Using this balanced sample, CLIP embeddings, a widely-used method for mapping both images and text into a shared latent space, are computed for both the corresponding images and text prompts. This enables a quantitative analysis of not only visual, but also textual similarity. By comparing the embeddings of both images and prompts generated from different descriptions, this approach allows for a direct examination of whether lexically similar prompts result in more homogeneous visual and textual outputs.\nDimensionality Reduction and Clustering To visualize the clustering of images based on CLIP embeddings, the dimensionality of the high-dimensional embeddings is first reduced using UMAP (Uniform Manifold Approximation and Projection) [28], allowing for a more interpretable two-dimensional representation. The resulting clusters are plotted in scatter plots (see Figures 4 and 5), where each point represents an image or a prompt respectively, with colors assigned based on their lexical originality labels\u2014either high or low. By overlaying the cluster labels onto these visual representations, distinct patterns of grouping emerge, indicating that both images and prompts with higher lexical originality tend to produce more diverse and dispersed clusters, while lower originality prompts and their corresponding images generate more homogeneous clusters, providing evidence of the relationship between lexical originality and the diversity of both the visual and textual outputs.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2fbc/2fbca64c-5cc8-4d88-8d69-61b8e89f1092.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/58ca/58ca4586-4b96-4700-b99b-3116431de07f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 4: UMAP-based visualization of image clusters from CLIP embeddings.</div>\nQuantitative Analysis of Visual Diversity To strengthen the analysis, a set of metrics is calculated to assess the dispersion and clustering behavior of of both the generated images and the textual prompts: 1. Cluster Dispersion: The variance of distances within each cluster is used as an indicator of visual diversity, with higher variance reflecting greater variation among the corresponding data points (images or textual prompts) in the cluster. 2. Distance from Centroid: The average distance of each image from the centroid of its cluster is measured to quantify how much individual data points (images or textual prompts) deviate from the cluster\u2019s center, with greater distances implying higher diversity.\n# Quantitative Analysis of Visual Diversity To strengthen the analysis, a set of metrics is calculated to assess the dispersion and clustering behavior of of both the generated images and the textual prompts:\n1. Cluster Dispersion: The variance of distances within each cluster is used as an indicator of visual diversity, with higher variance reflecting greater variation among the corresponding data points (images or textual prompts) in the cluster. 2. Distance from Centroid: The average distance of each image from the centroid of its cluster is measured to quantify how much individual data points (images or textual prompts) deviate from the cluster\u2019s center, with greater distances implying higher diversity.\nCluster Dispersion The variance analysis, conducted for both the visual outputs and the textual CLIP embeddings, reveals that clusters associated with high lexical originality prompts tend to be more dispersed across both dimensions compared to those with low originality. For the images, variances range from 0.313 to 0.852, while the textual embeddings exhibit variances between 0.140 and 0.651.\nAverage Distance from Centroid Similarly, the average distance from the centroid is found to be larger for both the images and textual prompts generated from high lexical originality prompts, further reinforcing the notion of greater diversity in these outputs. Specifically, for the images, the average distance from the centroid is 6.75, in contrast to 5.42 for those generated from low lexical originality prompts. Likewise, in the textual embedding space, high-originality prompts show an average distance of 5.20, compared to 4.74 for low-originality prompts. Overall, the results indicate that lexical originality can be a contributing factor to both visual and textual homogenization, as prompts with lower lex-\n<div style=\"text-align: center;\">Fig. 5: UMAP-based visualization of text prompt clusters from CLIP embeddings.</div>\nical originality tend to produce more uniform outputs across both domains. This is demonstrated by the tighter clustering, lower variance, and shorter distances from centroids in both the images and textual embeddings generated from low-originality prompts. In contrast, high-originality prompts result in more dispersed clusters and greater diversity, both visually and textually, suggesting that maintaining lexical variation in prompts can be for fostering a broader range of distinct outputs in AI-generated content.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3020/30203d90-38a0-41f7-ac97-ec41f0b9e1f8.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 6: Portrait images generated from prompts low in lexical originality.</div>\n# 5.3 The Impact of Prompt Characteristics on User Engagement\nModel Performance Metrics The performance metrics of the predictive model, which estimates user engagement in the form of LikeCount, indicate both a strong fit and reasonable accuracy. The model, specifically designed to predict the popularity of an image based on various prompt characteristics, is tailored for the Civiverse platform, where LikeCount serves as a key indicator of user interaction and engagement. With an R-squared (R\u00b2) value of 0.8266, the model successfully explains approximately 82.66% of the variance in user engagement, capturing potential underlying patterns that can drive an image\u2019s visual popularity. Additionally, the Mean Absolute Error (MAE) of 5.6015 indicates that, on average, the model\u2019s predictions deviate from the actual number of likes by\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cfbe/cfbe5b96-7e42-4b8d-9f94-aa68706a6385.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ig. 7: Portrait images generated from prompts high in lexical originality</div>\nabout 6 units. This demonstrates that the model is relatively accurate in estimating user engagement, providing valuable insights into how different prompt characteristics influence the popularity of AI-generated images.\n<div style=\"text-align: center;\">Table 2: Model Performance Metrics</div>\nMetric\nValue\nR-squared (R\u00b2)\n0.8266\nMean Absolute Error (MAE)\n5.6015\nRoot Mean Squared Error (RMSE) 6.1683\nFeature Importance A closer examination of the feature coefficients, presented in Table 3, reveals several key factors that strongly influence user engagement on the CivitAI platform, with certain thematic and visual elements, rather than originality metrics, being the most influential drivers of engagement. Among the factors driving user engagement, content featuring feminine subjects emerges as the most significant predictor, with a coefficient of 4.4282. This\nindicates a pronounced user preference for feminine aesthetics, a trend that is further reinforced by the strategic use of charismatic adjectives. With a coefficient of 3.2054, these adjectives underscore the importance of enhancing beauty and appeal, suggesting that users are drawn to content that emphasizes visual attractiveness. Additionally, the inclusion of dynamic visual elements, such as varied angles and perspectives, significantly boosts engagement, as evidenced by a coefficient of 3.7247. This indicates that users are particularly drawn to content that feels more interactive and engaging, as if the images invite the viewer into the scene. Moreover, fantasy elements also contribute notably to user engagement, with a coefficient of 3.3014, highlighting users\u2019 interest in visually imaginative themes. Finally, the analysis reveals that explicit anatomical details, such as the representation of breasts, play a role in attracting user attention, as reflected by the coefficient of 2.7839, indicating that users are drawn to content with nudity and intimate visual focus. Interestingly, these findings align with some of the most popular images in the Civiverse dataset, where user engagement mirrors the trends identified in the feature analysis. As seen in Figure 8, the ten most popular images on the platform as of May 6th, 2024, prominently feature themes such as fantasy, charismatic feminine subjects, and visually dynamic compositions.\n<div style=\"text-align: center;\">Table 3: Feature Coefficients and Statistical Significance</div>\nFeature\nCoefficient Std. Error p-value Confidence Interval\nFeminine Subjects\n4.4282\n0.0293\n0.0026\n[4.3708, 4.4856]\nAngles, Perspectives, and Viewer Interaction\n3.7247\n0.0439\n0.0082\n[3.6386, 3.8108]\nFantasy Elements\n3.3014\n0.0293\n0.0255\n[3.2440, 3.3588]\nCharismatic Adjectives\n3.2054\n0.0365\n0.0404\n[3.1338, 3.2770]\nBreast Details and Chest Anatomy\n2.7839\n0.0305\n0.0099\n[2.7241, 2.8438]\n# 6 Discussion\nThe findings of this study emphasize that human agency, particularly through user-generated text descriptions or prompts, plays a pivotal role in shaping AIgenerated visual culture. This influence extends beyond the dominant patterns learned by algorithms from their training data, as it is also deeply rooted in user behavior, perpetuating biases, particularly in relation to gender representation, and contributing to the homogenization of visual content.\n# 6.1 Surface Aesthetics and Prompting\nAlthough both DiffusionDB and Civiverse exhibit a considerable overlap in their focus on visual and aesthetic quality, the strategies employed by users to achieve\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/14e3/14e34566-9f33-4bc3-87a4-838e3d3a2788.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 8: Most popular images in the Civiverse dataset as of May 6th, 20</div>\nthis are often formulaic and lack creative diversity. Across both platforms, users tend to rely on a predictable set of prompt keywords, such as cinematic, highly detailed, or 8K resolution, to produce visually striking outputs. However, this reliance on standardized terms reveals a broader tendency to prioritize surfacelevel aesthetics over more diverse, experimental, or innovative approaches to image generation. While the prompts generated in the DiffusionDB dataset emphasize artistic themes, particularly focusing on traditional and contemporary art forms, users still frequently rely on mimicking established styles, often incorporating phrases like in the style of or art by. This reliance on familiar artistic frameworks and visual aesthetics indicates that even in a context where users are engaged with artistic themes, there is a tendency to adhere to established styles rather than exploring unconventional or innovative approaches. In contrast, prompts from the Civiverse dataset reveal a pronounced focus on explicit content, a trend that can be explained through the platform\u2019s more permissive content moderation policies. Consequently, the overrepresentation of such content underscores a prioritization of sensationalism over deeper creative exploration, leading to a homogenized visual culture that is narrowly focused on themes related to gender and the human body.\n# 6.2 The Impact of User Interaction\nVisual Homogenization The analysis conducted on both DiffusionDB and Civiverse datasets reveals a consistent user tendency to rely on less original prompts, largely influenced by platform trends and guidelines. The reliance on familiar keywords and stylistic frameworks creates a feedback loop that shapes the generated visuals, resulting in a homogeneous and repetitive visual landscape. This\noveruse of similar subjects, artistic techniques, and color palettes not only contributes to visual monotony but also stifles creative diversity, as prompts with limited thematic exploration and unconventional word combinations shift the focus from innovation to the production of formulaic, visually appealing content. This uniformity not only restricts opportunities for experimentation and the blending of new ideas, but also raises broader concerns about authorship in AI-generated content. Specifically, as familiar visual elements are continually reproduced, the machine begins to appear as the primary creative agent, further complicating the question of authorship in the creative process. Perpetuation of Gender Bias The analysis of user engagement on the Civiverse platform reveals that aesthetic appeal factors, such as the portrayal of feminine subjects, the use of charismatic adjectives, and dynamic visual elements, significantly impact image popularity more than prompt originality, as assessed by the proposed originality metrics. This observation is reinforced by the results from the user engagement prediction model, which reveals that the most popular AI-generated images on the platform often feature female subjects described with adjectives such as beautiful, attractive, or seductive. These images frequently depict women in sexualized poses or interactions designed to appeal directly to viewers, such as making eye contact. This pattern underscores a user-driven preference for content that prioritizes surface-level aesthetics, often narrowing female representation to a limited and objectified form of beauty. Such trends not only reflect broader cultural norms regarding aesthetic ideals but also demonstrate how user input and content popularity perpetuate specific biases, particularly concerning gender, in AI-generated visual content.\noveruse of similar subjects, artistic techniques, and color palettes not only contributes to visual monotony but also stifles creative diversity, as prompts with limited thematic exploration and unconventional word combinations shift the focus from innovation to the production of formulaic, visually appealing content. This uniformity not only restricts opportunities for experimentation and the blending of new ideas, but also raises broader concerns about authorship in AI-generated content. Specifically, as familiar visual elements are continually reproduced, the machine begins to appear as the primary creative agent, further complicating the question of authorship in the creative process.\nPerpetuation of Gender Bias The analysis of user engagement on the Civiverse platform reveals that aesthetic appeal factors, such as the portrayal of feminine subjects, the use of charismatic adjectives, and dynamic visual elements, significantly impact image popularity more than prompt originality, as assessed by the proposed originality metrics. This observation is reinforced by the results from the user engagement prediction model, which reveals that the most popular AI-generated images on the platform often feature female subjects described with adjectives such as beautiful, attractive, or seductive. These images frequently depict women in sexualized poses or interactions designed to appeal directly to viewers, such as making eye contact. This pattern underscores a user-driven preference for content that prioritizes surface-level aesthetics, often narrowing female representation to a limited and objectified form of beauty. Such trends not only reflect broader cultural norms regarding aesthetic ideals but also demonstrate how user input and content popularity perpetuate specific biases, particularly concerning gender, in AI-generated visual content.\n# 7 Limitations and Future Work\nWhile this study provides valuable insights into the role of user behavior in shaping AI-generated content, several limitations must be acknowledged. First, the datasets used, DiffusionDB and Civiverse, are platform-specific, reflecting the characteristics of their respective user communities and content moderation policies. As a result, the findings may not be generalizable to other platforms with different user bases or guidelines. Second, while the originality metrics developed in this study do offer a novel way to assess prompt originality by capturing various aspects of word usage and thematic combinations, they do not fully encompass the broader, more nuanced dimensions of human expression. Specifically, lexical originality measures word uniqueness but overlooks the semantic richness or conceptual depth a prompt might convey, meaning prompts with common words could still reflect innovative ideas yet score low in originality. Similarly, while thematic originality measures the rarity of topics, it doesn\u2019t account for how innovative or coherent the combinations are, allowing prompts with rare but incoherent themes to score highly. Additionally, word-sequence originality, though useful for identifying uncommon word pairings, does not evaluate whether these combinations\ncontribute to a coherent narrative or visual storytelling. Thus, while these metrics offer valuable insights into linguistic originality by examining prompts from a lexical and thematic pattern perspective, assessing how similar or distinct the linguistic choices are among users when interacting with TTI models, they may overlook more complex aspects of human expression, such as conceptual innovation or narrative coherence, which go beyond simple word usage and thematic combinations. Future research could integrate a \"human-in-the-loop\" approach, where human evaluators assess the originality not only of the prompts but also of the images generated from them. This would involve having human assessors rate the creativity, novelty, and artistic value of the outputs, offering subjective insights that go beyond what can be measured by originality metrics alone. By comparing human evaluations with the numerical originality scores, one could better understand whether prompts with higher originality metrics actually result in more diverse and visually compelling images, or if the metrics overlook important elements of human-perceived innovation and conceptual richness.\n# 8 Conclusion\nIn this study, we examined the critical influence of user behavior on the diversity of AI-generated content, particularly focusing on the role of user generated prompts on visual homogenization. Through an in-depth analysis of lexical, thematic, and word-sequence originality in the DiffusionDB and Civiverse datasets, we uncovered that low prompt originality plays a significant role in driving the creation of repetitive and visually uniform content. Our findings highlight that this homogenization is not merely a consequence of the training data alone but is also related to users\u2019 reliance on standardized, formulaic prompting practices, which in turn reinforce visual uniformity and limit creative exploration. However, when users move beyond established templates and incorporate greater creative variability into their prompts, we observed that the resulting AI-generated images demonstrated a higher degree of visual diversity, underscoring the pivotal role of user input in shaping the originality and richness of AI-generated outputs. Beyond visual uniformity, our research also delved into the impact of user input on the reinforcement of cultural biases, particularly those related to gender representation. By examining feature importance in the predictive linear regression model of user engagement, we found that popular images on the Civiverse platform often reflect user preferences for aesthetically appealing content that emphasizes surface-level traits such as beauty, dynamic compositions, and explicit themes. This tendency, especially in the repeated portrayal of feminine subjects in narrow, objectified forms, highlights how user behavior shapes not only the aesthetic qualities of AI-generated images but also perpetuates existing cultural biases within these visual outputs. Ultimately, this study emphasizes the need for platform developers and online communities to reconsider prompt engineering guidelines and foster environments that encourage user exploration when interacting with AI content gener-\nation tools. The aforementioned findings point to the broader implications of human-AI collaboration, advocating for active user involvement to prevent the erosion of creativity and promote a richer, more diverse AI-generated visual culture.\n# 9 Acknowledgements\nThis work was supported by Swiss National Science Foundation (Ambizione Grant 216104).\n# References\n1. Arazzi, M., Nicolazzo, S., Nocera, A., Zippo, M.: The importance of the language for the evolution of online communities: An analysis based on twitter and reddit. Expert Systems with Applications 222, 119847 (2023) 2. Audry, S.: Art in the age of machine learning. Mit Press (2021) 3. Bianchi, F., Kalluri, P., Durmus, E., Ladhak, F., Cheng, M., Nozza, D., Hashimoto, T., Jurafsky, D., Zou, J., Caliskan, A.: Easily accessible text-to-image generation amplifies demographic stereotypes at large scale. In: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency. pp. 1493\u20131504 (2023) 4. Brade, S., Wang, B., Sousa, M., Oore, S., Grossman, T.: Promptify: Text-to-image generation through interactive prompt exploration with large language models. In: Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. pp. 1\u201314 (2023) 5. Branwen, G.: Gpt-3 creative fiction (2020), https://www.gwern.net/GPT-3 6. Brown, T.B.: Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020) 7. Campello, R.J., Moulavi, D., Sander, J.: Density-based clustering based on hierarchical density estimates. In: Pacific-Asia conference on knowledge discovery and data mining. pp. 160\u2013172. Springer (2013) 8. Cetinic, E., She, J.: Understanding and creating art with ai: Review and outlook. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM) 18(2), 1\u201322 (2022) 9. Chauhan, A., Anand, T., Jauhari, T., Shah, A., Singh, R., Rajaram, A., Vanga, R.: Identifying race and gender bias in stable diffusion ai image generation. In: 2024 IEEE 3rd International Conference on AI in Cybersecurity (ICAIC). pp. 1\u20136. IEEE (2024) 10. Chiou, L.Y., Hung, P.K., Liang, R.H., Wang, C.T.: Designing with ai: an exploration of co-ideation with image generators. In: Proceedings of the 2023 ACM designing interactive systems conference. pp. 1941\u20131954 (2023) 11. Cho, J., Zala, A., Bansal, M.: Dall-eval: Probing the reasoning skills and social biases of text-to-image generation models. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 3043\u20133054 (2023) 12. CivitAI: Civitai: The home of open-source generative ai (2024), https://civitai. com/ 13. De la Cruz Paragas, F., Lin, T.T.: Organizing and reframing technological determinism. New Media & Society 18(8), 1528\u20131546 (2016)\n14. Davis, N., Hsiao, C.P., Popova, Y., Magerko, B.: An enactive model of creativity for computational collaboration and co-creation. Creativity in the digital age pp. 109\u2013133 (2015) 15. Drew, R.: Technological determinism. A companion to popular culture pp. 165\u2013183 (2016) 16. Frich, J., Biskjaer, M.M., Dalsgaard, P.: Why hci and creativity research must collaborate to develop new creativity support tools. In: Proceedings of the Technology, Mind, and Society, pp. 1\u20136 (2018) 17. Grba, D.: Deep else: A critical framework for ai art. Digital 2(1), 1\u201332 (2022) 18. Grba, D.: Renegade x: Poetic contingencies in computational art. In: Proceedings of xCoAx 2023, 11th Conference on Computation, Communication, Aesthetics & X. Edited by Mario Verdicchio, Miguel Carvalhais, Luisa Ribas and Andre Rangel. Porto: i2ADS Research Institute in Art, Design and Society. pp. 201\u2013221 (2023) 19. Grba, D.: Art notions in the age of (mis) anthropic ai. In: Arts. vol. 13, p. 137. MDPI (2024) 20. Hao, Y., Chi, Z., Dong, L., Wei, F.: Optimizing prompts for text-to-image generation. Advances in Neural Information Processing Systems 36 (2024) 21. Ko, H.K., Park, G., Jeon, H., Jo, J., Kim, J., Seo, J.: Large-scale text-to-image generation models for visual artists\u2019 creative works. In: Proceedings of the 28th international conference on intelligent user interfaces. pp. 919\u2013933 (2023) 22. Lin, Z., Ehsan, U., Agarwal, R., Dani, S., Vashishth, V., Riedl, M.: Beyond prompts: Exploring the design space of mixed-initiative co-creativity systems. arXiv preprint arXiv:2305.07465 (2023) 23. Liu, V., Chilton, L.B.: Design guidelines for prompt engineering text-to-image generative models. In: Proceedings of the 2022 CHI conference on human factors in computing systems. pp. 1\u201323 (2022) 24. Liu, V., Qiao, H., Chilton, L.: Opal: Multimodal image generation for news illustration. In: Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. pp. 1\u201317 (2022) 25. Luccioni, S., Akiki, C., Mitchell, M., Jernite, Y.: Stable bias: Evaluating societal representations in diffusion models. Advances in Neural Information Processing Systems 36 (2024) 26. McCormack, J., Gifford, T., Hutchings, P., Llano Rodriguez, M.T., Yee-King, M., d\u2019Inverno, M.: In a silent way: Communication between ai and improvising musicians beyond sound. In: Proceedings of the 2019 chi conference on human factors in computing systems. pp. 1\u201311 (2019) 27. McCormack, J., Llano, M.T., Krol, S.J., Rajcic, N.: No longer trending on artstation: Prompt analysis of generative ai art. In: International Conference on Computational Intelligence in Music, Sound, Art and Design (Part of EvoStar). pp. 279\u2013295. Springer (2024) 28. McInnes, L., Healy, J., Melville, J.: Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426 (2018) 29. Mohri, M.: Foundations of machine learning (2018) 30. OpenAI: Chatgpt (mar 14 version) [large language model] (2023), https:// openai.com/chatgpt 31. Oppenlaender, J.: A taxonomy of prompt modifiers for text-to-image generation. Behaviour & Information Technology pp. 1\u201314 (2023) 32. Paananen, V., Oppenlaender, J., Visuri, A.: Using text-to-image generation for architectural design ideation. International Journal of Architectural Computing p. 14780771231222783 (2023)\n33. Palmini, M.T.D.R., Wagner, L., Cetinic, E.: Civiverse: A dataset for analyzing user engagement with open-source text-to-image models. arXiv preprint arXiv:2408.15261 (2024) 34. Peschl, M.F.: Human innovation and the creative agency of the world in the age of generative ai. Possibility Studies & Society 2(1), 49\u201376 (2024) 35. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: International conference on machine learning. pp. 8748\u20138763. PMLR (2021) 36. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 1(2), 3 (2022) 37. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 10684\u201310695 (2022) 38. Sanchez, T.: Examining the text-to-image community of practice: Why and how do people prompt generative ais? In: Proceedings of the 15th Conference on Creativity and Cognition. pp. 43\u201361 (2023) 39. Shah, J.J., Smith, S.M., Vargas-Hernandez, N.: Metrics for measuring ideation effectiveness. Design studies 24(2), 111\u2013134 (2003) 40. Shankman, A.: Staring Bias in the Face: Investigating Emotional Variations in Faces Generated by Text-to-Image Diffusion Models Across Race and Gender. Master\u2019s thesis, The Cooper Union for the Advancement of Science and Art (2024) 41. StabilityAI: Stable diffusion dream studio beta terms of service (2022), https: //beta.dreamstudio.ai/tos 42. StabilityAI: Stable diffusion dream studio beta terms of service (2022), https: //beta.dreamstudio.ai/tos 43. Tessema, D.: Technological determinism versus social determinism, a critical discussion. Ethiopian Journal of Science and Sustainable Development 8(2), 65\u201372 (2021) 44. Thelle, N.J., Pasquier, P.: Spire muse: A virtual musical partner for creative brainstorming. In: NIME 2021. PubPub (2021) 45. Tolver, A.: An introduction to markov chains. Department of Mathematical Sciences, University of Copenhagen (2016) 46. Wang, W., Bao, H., Huang, S., Dong, L., Wei, F.: Minilmv2: Multi-head selfattention relation distillation for compressing pretrained transformers. arXiv preprint arXiv:2012.15828 (2020) 47. Wang, Y., Shen, S., Lim, B.Y.: Reprompt: Automatic prompt editing to refine ai-generative art towards precise expressions. In: Proceedings of the 2023 CHI conference on human factors in computing systems. pp. 1\u201329 (2023) 48. Wang, Z., Huang, Y., Song, D., Ma, L., Zhang, T.: Promptcharm: Text-to-image generation through multi-modal prompting and refinement. In: Proceedings of the CHI Conference on Human Factors in Computing Systems. pp. 1\u201321 (2024) 49. Wang, Z.J., Montoya, E., Munechika, D., Yang, H., Hoover, B., Chau, D.H.: Diffusiondb: A large-scale prompt gallery dataset for text-to-image generative models. arXiv preprint arXiv:2210.14896 (2022) 50. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35, 24824\u201324837 (2022)\n51. Wilde, L.R., Lemmes, M., Sachs-Hombach, K.: Generative imagery: Towards a \u2018new paradigm\u2019of machine learning-based image production. The Interdisciplinary Journal of Image Sciences 37(1) (2023) 52. Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate before use: Improving few-shot performance of language models. In: International conference on machine learning. pp. 12697\u201312706. PMLR (2021) 53. Zhu, J., Liapis, A., Risi, S., Bidarra, R., Youngblood, G.M.: Explainable ai for designers: A human-centered perspective on mixed-initiative co-creation. In: 2018 IEEE conference on computational intelligence and games (CIG). pp. 1\u20138. IEEE (2018)\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of visual homogenization in AI-generated content, emphasizing the role of user behavior in shaping the originality of prompts used in Text-to-Image models. Previous methods focused primarily on training data patterns, leading to concerns about artistic originality and cultural biases.",
        "problem": {
            "definition": "The problem is the erosion of artistic diversity in AI-generated visuals, which is exacerbated by user reliance on standardized and repetitive prompting practices.",
            "key obstacle": "The main challenge is that existing methods do not adequately account for how user-generated prompts influence the originality and diversity of AI outputs."
        },
        "idea": {
            "intuition": "The idea stems from the observation that user behavior, particularly the way prompts are crafted, significantly impacts the originality of AI-generated images.",
            "opinion": "The proposed idea is to analyze user-generated prompts using three originality metrics\u2014lexical, thematic, and word-sequence originality\u2014to understand their influence on AI-generated visual content.",
            "innovation": "The innovation lies in the introduction of these originality metrics that specifically measure the uniqueness of user prompts, providing a new perspective on how user input affects AI-generated visual diversity."
        },
        "method": {
            "method name": "Originality Metrics Analysis",
            "method abbreviation": "OMA",
            "method definition": "This method evaluates the originality of user-generated prompts by analyzing their lexical, thematic, and word-sequence characteristics across two datasets: DiffusionDB and Civiverse.",
            "method description": "The core of the method involves calculating originality scores based on user-generated prompts to assess their impact on AI-generated image diversity.",
            "method steps": [
                "Collect user-generated prompts from the DiffusionDB and Civiverse datasets.",
                "Preprocess the prompts to remove stopwords and special characters.",
                "Calculate lexical originality by measuring the rarity of words.",
                "Evaluate word-sequence originality using transition probabilities of word pairings.",
                "Determine thematic originality based on topic modeling results.",
                "Analyze the relationship between prompt originality and user engagement metrics."
            ],
            "principle": "The method is effective because it quantifies how variations in user prompts can lead to differences in the originality and diversity of AI-generated images, highlighting the importance of user input in the creative process."
        },
        "experiments": {
            "evaluation setting": "The experiments utilized two datasets: DiffusionDB, which consists of 1.8 million unique prompts from a moderated environment, and Civiverse, with over 6.5 million prompts from a less restricted platform. The focus was on analyzing prompt originality and its correlation with user engagement metrics.",
            "evaluation method": "A linear regression model was developed to predict user engagement based on prompt characteristics, including originality metrics and thematic elements identified through topic modeling."
        },
        "conclusion": "The study concludes that low prompt originality significantly contributes to visual homogenization in AI-generated content. It emphasizes the need for users to adopt more diverse prompting practices to enhance the originality and richness of AI outputs, while also addressing the reinforcement of cultural biases in visual representations.",
        "discussion": {
            "advantage": "The proposed approach offers a novel framework for understanding the influence of user-generated prompts on AI-generated visual diversity, highlighting the importance of user agency in the creative process.",
            "limitation": "The findings are limited to the specific datasets analyzed, which may not be generalizable to other platforms with different user behaviors and content policies. Additionally, the originality metrics may not capture the full complexity of human creativity.",
            "future work": "Future research could explore the integration of human evaluations of prompt originality and generated images, providing a more nuanced understanding of creativity and originality in AI-generated content."
        },
        "other info": {
            "acknowledgement": "This work was supported by the Swiss National Science Foundation (Ambizione Grant 216104)."
        }
    },
    "mount_outline": [
        {
            "section number": "2.2",
            "key information": "The problem is the erosion of artistic diversity in AI-generated visuals, which is exacerbated by user reliance on standardized and repetitive prompting practices."
        },
        {
            "section number": "2.3",
            "key information": "The proposed idea is to analyze user-generated prompts using three originality metrics\u2014lexical, thematic, and word-sequence originality\u2014to understand their influence on AI-generated visual content."
        },
        {
            "section number": "4.2",
            "key information": "The study concludes that low prompt originality significantly contributes to visual homogenization in AI-generated content, emphasizing the reinforcement of cultural biases in visual representations."
        },
        {
            "section number": "4.5",
            "key information": "The proposed approach offers a novel framework for understanding the influence of user-generated prompts on AI-generated visual diversity, highlighting the importance of user agency in the creative process."
        },
        {
            "section number": "7.2",
            "key information": "Future research could explore the integration of human evaluations of prompt originality and generated images, providing a more nuanced understanding of creativity and originality in AI-generated content."
        }
    ],
    "similarity_score": 0.580736895297927,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-0632_artif/papers/Patterns of Creativity_ How User Input Shapes AI-Generated Visual Diversity.json"
}