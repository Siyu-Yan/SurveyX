{
    "from": "google",
    "scholar_id": "JbGsyE_po58J",
    "detail_id": null,
    "title": "Large Language Models Augmented Rating Prediction in Recommender System",
    "abstract": " ABSTRACT\n\nRecently, large language models (LLMs) have demonstrated impressive capabilities and gained widespread applications. However, their direct application to recommendation tasks (e.g., rating prediction task) often falls short of optimal results due to a lack of understanding of collaborative information in recommendations. In this paper, we propose L arge l A nguage M odel A ugmented R ecommendation (LAMAR) framework to address this limitation. Instead of relying solely on LLMs, our framework combines their outputs with traditional recommendation models, leveraging both collaborative and semantic information. We further enhance the recommendation performance through an ensemble of diverse prompts and utilize LLMs to extract side information for augmenting traditional recommendation models. Empirical studies on realworld datasets demonstrate that LAMAR outperforms existing approaches, highlighting the benefits of leveraging LLMs in recommendation systems. Code is available at https: //github.com/sichunluo/LAMAR.\nIndex Terms\u2014 recommender system, large language\n\nIndex Terms\u2014 recommender system, large language model, rating prediction\n\n# 1. INTRODUCTION\n\nRecommender systems have become widely adopted in various domains to address the issue of information overload [1, 2, 3]. One crucial task in recommendation is rating prediction, which involves predicting the ratings or preferences that a user would assign to items in a recommender system [4]. Traditional recommender systems often employ neural networks or similar models to generate recommendations based on user preferences [5, 6]. Although these systems have shown effectiveness, they also have inherent limitations. Specifically, two key challenges emerge: First, traditional models typically transform features into embeddings, neglecting the textual semantic information associated with the features. Sec\n\n\u2020 Corresponding Author\n\nond, these systems often lack side information, such as movie directors in movie rat",
    "bib_name": "luo2024large",
    "md_text": "# E LANGUAGE MODELS AUGMENTED RATING PREDICTION IN RECOMMENDER SYSTEM\n\nSichun Luo 1, 2, Jiansheng Wang 3, Aojun Zhou 4, Li Ma 5, Linqi Song 1, 2 \u2020\n\n1 Department of Computer Science, City University of Hong Kong 2 City University of Hong Kong Shenzhen Research Institute 3 College of Water Resources and Architectural Engineering, Northwest A&F University 4 Department of Electronic Engineering, The Chinese University of Hong Kong 5 Library, Tsinghua University\n\n# ABSTRACT\n\nRecently, large language models (LLMs) have demonstrated impressive capabilities and gained widespread applications. However, their direct application to recommendation tasks (e.g., rating prediction task) often falls short of optimal results due to a lack of understanding of collaborative information in recommendations. In this paper, we propose L arge l A nguage M odel A ugmented R ecommendation (LAMAR) framework to address this limitation. Instead of relying solely on LLMs, our framework combines their outputs with traditional recommendation models, leveraging both collaborative and semantic information. We further enhance the recommendation performance through an ensemble of diverse prompts and utilize LLMs to extract side information for augmenting traditional recommendation models. Empirical studies on realworld datasets demonstrate that LAMAR outperforms existing approaches, highlighting the benefits of leveraging LLMs in recommendation systems. Code is available at https: //github.com/sichunluo/LAMAR.\nIndex Terms\u2014 recommender system, large language\n\nIndex Terms\u2014 recommender system, large language model, rating prediction\n\n# 1. INTRODUCTION\n\nRecommender systems have become widely adopted in various domains to address the issue of information overload [1, 2, 3]. One crucial task in recommendation is rating prediction, which involves predicting the ratings or preferences that a user would assign to items in a recommender system [4]. Traditional recommender systems often employ neural networks or similar models to generate recommendations based on user preferences [5, 6]. Although these systems have shown effectiveness, they also have inherent limitations. Specifically, two key challenges emerge: First, traditional models typically transform features into embeddings, neglecting the textual semantic information associated with the features. Sec\n\n\u2020 Corresponding Author\n\nond, these systems often lack side information, such as movie directors in movie rating prediction, which hinders their ability to achieve better performance. In recent years, large language models (LLMs) have demonstrated remarkable capabilities in various domains, including natural language understanding, language generation, and complex reasoning [7, 8]. However, in contrast to computer vision and natural language processing, the field of recommender systems lacks a standardized foundation model, making it challenging to transfer knowledge between different recommendation scenarios [9]. Drawing inspiration from the success of LLMs in other domains, researchers have begun exploring the potential of applying LLMs to recommender systems [10, 11]. Unlike traditional recommender systems that typically rely on training neural networks to model user preferences [5], LLM-based recommendations involve directly prompting the LLMs to generate recommendations. Therefore, we aim to leverage LLMs to enhance the existing traditional recommender systems. Nevertheless, preliminary explorations into the integration of LLMs, such as ChatGPT, into recommendation tasks have yielded less satisfactory outcomes [10, 11]. LLMs still encounter formidable challenges in various recommendation tasks, including sequential and top-k recommendations. Consequently, the direct utilization of LLMs for recommendation purposes may not be the most optimal approach. The limitations in leveraging LLMs for recommendation can be attributed to two pivotal factors. Firstly, LLMs encounter difficulties in comprehending the semantic meaning of user/item IDs, which poses challenges for ID-based recommendations. Secondly, the extensive candidate set involved in recommendation tasks makes it arduous to incorporate and fully comprehend the collaborative information within the LLMs. In this paper, we propose L arge l A nguage M odel A ugment R ecommendation (LAMAR), a general model agnostic framework that augments the traditional recommendation systems by integrating LLMs. LAMAR introduces an adaptive merging module to effectively combine these two methods, which\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c11/4c11f5c8-c437-41c5-b2fa-d2e650f45269.png\" style=\"width: 50%;\"></div>\nallows us to leverage the advantages of traditional recommendation models, which excel at understanding ID features and collaborative information, and LLM-based recommendation models, which excel at comprehending textual information and reasoning. By incorporating an adaptive merging module, we can optimize the performance of the recommendation system. Moreover, we employ a diverse prompt ensemble technique to generate multiple answers from LLMs. By averaging these answers, we can obtain more accurate results, taking advantage of the diverse perspectives provided by the LLMs. Furthermore, we utilize the capabilities of LLMs to provide valuable side information, which supports the traditional recommendation model in making more accurate predictions. By leveraging the additional information provided by LLMs, we enhance the accuracy and effectiveness of the recommendation system. We performed extensive experiments on two real-world datasets, employing various backbone models, to evaluate the performance of our proposed LAMAR framework in the rating prediction task. The experimental results demonstrate the effectiveness of our method in effectively augmenting existing recommendation models.\n\nallows us to leverage the advantages of traditional recommendation models, which excel at understanding ID features and collaborative information, and LLM-based recommendation models, which excel at comprehending textual information and reasoning. By incorporating an adaptive merging module, we can optimize the performance of the recommendation system. Moreover, we employ a diverse prompt ensemble technique to generate multiple answers from LLMs. By averaging these answers, we can obtain more accurate results, taking advantage of the diverse perspectives provided by the LLMs. Furthermore, we utilize the capabilities of LLMs to provide valuable side information, which supports the traditional recommendation model in making more accurate predictions. By leveraging the additional information provided by LLMs, we enhance the accuracy and effectiveness of the recommendation system. We performed extensive experiments on two real-world datasets, employing various backbone models, to evaluate the performance of our proposed LAMAR framework in the rating prediction task. The experimental results demonstrate the effectiveness of our method in effectively augmenting existing recommendation models. In a nutshell, our contribution is threefold.\n\u2022 We introduce a novel and model-agnostic framework called LAMAR, which enhances traditional recommendation models by incorporating LLMs. By leveraging the semantic information comprehension and reasoning abilities of LLMs, our framework extends the capabilities of traditional recommendation models.\n\u2022 LAMAR prompt LLMs to generate side information that augments traditional recommendation models. Furthermore, we propose using diverse prompts ensemble to further improve the recommendation performance of the LAMAR framework.\n\nEmpirical studies conducted on the rating prediction task demonstrate the effectiveness of LAMAR. We observe obvious improvements in recommendation performance through rigorous evaluation and comparison with existing approaches.\n\n# 2. METHOD\n\nFigure 1 illustrates the architecture of our proposed LAMAR framework. In contrast to traditional recommendation models, we integrate the strengths of both traditional and LLMbased recommendation models to leverage their respective advantages. Additionally, we incorporate diverse prompts to further enhance the performance of the framework. Moreover, we prompt LLMs to generate valuable side information that enriches the traditional recommendation model.\n\n# 2.1. Recomemndation with LLMs\n\n2.1.1. Prompt Construction for Rating Prediction Task\n\nFollowing [10], we construct prompts tailored to the specific characteristics of the rating prediction task. These prompts serve as inputs for the LLMs, enabling them to generate recommendation results based on the prompt specifications. The prompt template we employ is as follows:\n\nBased on the rating history below, please predict user\u2019s rating for the movie: {movie name}. The output must predict the user\u2019s rating, and then explain the reasons why the user will give the rating. While predicting the user\u2019s rating for the movie, the user\u2019s preference and the features of the movie should be considered.\n\n### Information: Here is user rating history: {rating history} ### Format Example: title: <Movie Title> rating: x stars reasons: {reasons} ### Answer: {LLM output}\n\n2.1.2. Diverse Prompts\n\n# 2.1.2. Diverse Prompts\n\nMotivated by the work of [12, 13], we incorporate the use of diverse prompts to augment the performance of our LAMAR framework. By employing different prompts, we can elicit different reasoning paths within the LLMs, leading to more reliable and comprehensive results. In our approach, the process of generating rating prediction using diverse prompts can be denoted as r = \u03b8 (p), where p represents the prompt used, and \u03b8 represents the LLM-based recommendation model. We utilize k diverse prompts, resulting in multiple recommendation outputs: {r 1 = \u03b8 (p 1), . . . , r k = \u03b8 (p k)}. To consolidate these multiple outputs and obtain a final score, we employ an averaging mechanism. Specifically, we compute the average of the k recommendation outputs as: r LLM = 1 k \ufffd k i =1 r i. By averaging the diverse recommendation results, we obtain a more robust and accurate final score for the recommendations.\n\n# 2.2. Side Information Augmented Recommendation\n\nIn the traditional recommendation scenario, the available data for modeling is often limited. However, aggregating side information can significantly enhance the recommendation performance. To leverage side information, we utilize LLMs within our LAMAR framework. In the traditional recommendation model, we can denote the recommendation process as r = \u03c8 (f id, f feature), where \u03c8 represents the traditional recommendation model, such as DeepFM [5] or other similar models. Here, f id represents the ID features, and f feature represents additional features. To incorporate side information provided by LLMs, we introduce the LLM denoted as \u03b8. By applying a prompt p \u2032 to the LLM, we obtain the side information f s, which captures additional semantic knowledge related to the recommendation task. Hence, the rating prediction can be expressed as r Rec = \u03c8 (f id, f feature, f s), where we augment the traditional recommendation model with the LLM-generated side information. The prompt template we utilize within our LAMAR framework to generate side information is as follows: ### Instruction: Provide detailed information of the movie {movie name}, including the director, scriptwriters, stars, and keywords of the plot and style. The provided information\n\nmust be correct. ### Format Example: {example} ### Answer: {LLM output}\n\n# 2.3. Adaptive Merging\n\nIn the context of the long-tail phenomenon, where users with a large number of interactions tend to perform better in traditional recommendation models [14, 15], we propose an adaptive merging approach within our LAMAR framework. This adaptive merging combines the results obtained from both LLM-based and traditional recommendation models adaptively. The final score for user u and item v is computed as: r u,v = \u03b1r u,v LLM + (1 \u2212 \u03b1) r u,v Rec, where r u,v LLM represents the rating prediction score for user u and item v from the LLM-based model, and r u,v Rec represents the score from the traditional recommendation model. The hyperparameter \u03b1 controls the weight given to each recommendation model\u2019s score. To adaptively determine \u03b1, we consider the number of interactions for user u. If the number of interactions exceeds a threshold \u03b3, we set \u03b1 = \u03b1 1. Otherwise, we set \u03b1 = \u03b1 2, where \u03b1 1 < \u03b1 2. This adaptive merging mechanism allows us to dynamically adjust the contribution of the LLM-based model and the traditional recommendation model based on the user\u2019s interaction history.\n\n# 3. EXPERIMENT\n\nIn this section, we perform experiments on real-world datasets for evaluating various methods on rating prediction tasks.\n\nDataset\nKaggle-Movie\nMovieLens-100K\n# of user\n670\n943\n# of item\n5,977\n1,682\n# of rating\n96,761\n100,000\ndensity\n0.024162\n0.063046\nUser Features\nN/A\nIDs, Gender, Occupation\nZipCode, Age\nItem Features\nIDs, Title, Genres, Year\nLLM Generated Features\nMovie Director, Scriptwriter, Stars\n# 3.1. Experiment Setup\n\nDataset. To evaluate the effectiveness of proposed LAMAR, we conduct experiments on MovieLens-100K (ML-100K) [16] dataset, which is widely used for evaluating recommendation algorithms in the context of movie rating prediction. We also use Kaggle-Movie [17], which is an extended MovieLens dataset released on Kaggle. The characteristics of datasets are summarized in Table 1.\n\n<div style=\"text-align: center;\">Table 2: Performance achieved by different methods. The better results are highlighted in boldfaces.\n</div>\nBackbone\nw/ LAMAR\nKaggle Movie\nML-100K\nRMSE \u2193\nMAE \u2193\nRMSE \u2193\nMAE \u2193\nLLaMA\n\u2212\n1.0404\n0.7459\n1.1886\n0.9093\nDeepFM\n\u00d7\n0.9873\n0.7765\n1.1204\n0.8729\n\u2713\n0.9582\n0.7505\n1.0778\n0.8399\nNFM\n\u00d7\n1.0379\n0.7806\n1.0581\n0.8323\n\u2713\n0.9882\n0.7500\n1.0345\n0.8157\nDCN\n\u00d7\n1.0299\n0.7794\n1.1121\n0.8591\n\u2713\n0.9907\n0.7531\n1.0765\n0.8343\nAFM\n\u00d7\n1.0378\n0.8016\n1.0938\n0.8385\n\u2713\n0.9980\n0.7722\n1.0623\n0.8203\nxDeepFM\n\u00d7\n1.0625\n0.8134\n1.2528\n0.9766\n\u2713\n1.0094\n0.7712\n1.1497\n0.9007\nAutoInt\n\u00d7\n0.9881\n0.7636\n1.0970\n0.8581\n\u2713\n0.9646\n0.7423\n1.0613\n0.8296\n<div style=\"text-align: center;\">Table 3: Ablation study on ML-100K dataset with backbone model DeepFM. The best results are highlighted in boldfaces.\n</div>\nTable 3: Ablation study on ML-100K dataset with backbone model DeepFM. The best results are highlighted in boldfaces.\n\nVariants\nRMSE \u2193\nMAE \u2193\nLAMAR\n1.0778\n0.8399\nw/o diverse prompt\n1.0952\n0.8547\nw/o side information\n1.0825\n0.8438\nw/o adaptive merging\n1.0958\n0.8541\nEvaluation Metrics.  In line with [18], we employ two evaluation metrics: Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Backbone Models. We incorporate our LAMAR with the following recommendation models that are often used for predicting ratings in recommender systems: CCPM [19], DeepFM [5], NFM [1], DCN [20], AFM [21], xDeepFM [22], and AutoInt [23]. To make our results fully reproductive, we utilize the open-source LLaMA [8] model as the backbone model for LLM. Experiment Settings. To ensure a fair comparison, we adopt the best hyper-parameter settings reported in the original papers of the baselines, and fine-tune all baseline hyperparameters using grid search. We adopt the leave-one-out strategy [17] to use the last interacted item to test, the second last interacted item to validate, and others to train for each user. In the default setting, \u03b1 1 is set to 0.1, \u03b1 2 is set to 0.3, and \u03b3 is set to 80.\n\n# 3.2. Main Result\n\nWe evaluate baselines and our method using two datasets to evaluate the performance under different scenarios. Table 2 summarizes the model performance on these datasets. Through our experiments, we observed that by leverag\n\ning LLMs to augment traditional recommendation models, LAMAR significantly improves the recommendation performance. The collaborative information captured by traditional models, combined with the semantic information provided by LLMs, leads to more accurate rating prediction. Besides, comparing the performance of LAMAR against standalone LLM models, we observed that LAMAR consistently outperforms LLM models in terms of recommendation accuracy. This suggests that the integration of LLMs within the traditional recommendation framework enhances the overall performance, leveraging the strengths of both approaches.\n\n# 3.3. Ablation Study\n\nWe perform ablation studies to analyze different components of our model. The results are shown in Table 3. Specifically, we build three variants: w/o diverse expert, w/o side information, and w/o adaptive merging. By using multiple prompts, we enable the LLMs to capture a broader range of semantics, resulting in more comprehensive and accurate recommendations. In addition, through adaptive merging method, LAMAR achieved superior recommendation performance compared to using standard ones. Moreover, side information merging improves recommendation quality. The utilization of LLMs to extract side information that augments traditional recommendation models proved to be highly effective. This demonstrates the value of leveraging LLMs for extracting relevant and complementary information to enhance recommendations.\n\n# 4. CONCLUSION\n\nIn this paper, we introduces the LAMAR framework, which leverages LLMs to augment recommendation systems. We address the limitation of LLMs in understanding collaborative information by combining their outputs with traditional recommendation models. By incorporating both the collaborative information extracted by traditional models and the semantic information extracted by LLMs, LAMAR achieves improved recommendation performance. We further enhance LLM-based recommendations by employing an ensemble of diverse prompts, which boosts the effectiveness of the framework. Additionally, we utilize LLMs to extract side information that enhances traditional recommendation models, providing a comprehensive approach to recommendation tasks. Empirical studies conducted on real-world datasets validate the effectiveness of our proposed method.\n\n# 5. ACKNOWLEDGEMENT\n\nThis work was supported in part by the National Natural Science Foundation of China under Grant 62371411, the Research Grants Council of the Hong Kong SAR under Grant GRF 11217823, InnoHK initiative, the Government of the HKSAR, Laboratory for AI-Powered Financial Technologies.\n\n[1] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua, \u201cNeural collaborative filtering,\u201d in  Proceedings of the 26th international conference on world wide web, 2017, pp. 173\u2013182.\n[2] Sichun Luo, Yuanzhang Xiao, and Linqi Song, \u201cPersonalized federated recommendation via joint representation learning, user clustering, and model adaptation,\u201d in  Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 2022, pp. 4289\u20134293.\n[3] Sichun Luo, Yuanzhang Xiao, Xinyi Zhang, Yang Liu, Wenbo Ding, and Linqi Song, \u201cPerfedrec++: Enhancing personalized federated recommendation with self-supervised pre-training,\u201d arXiv preprint arXiv:2305.06622, 2023.\n[4] Zahid Younas Khan, Zhendong Niu, Sulis Sandiwarno, and Rukundo Prince, \u201cDeep learning techniques for rating prediction: a survey of the state-of-the-art,\u201d  Artificial Intelligence Review, vol. 54, pp. 95\u2013135, 2021.\n[5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He, \u201cDeepfm: a factorization-machine based neural network for ctr prediction,\u201d arXiv preprint arXiv:1703.04247, 2017.\n[6] Sichun Luo, Chen Ma, Yuanzhang Xiao, and Linqi Song, \u201cImproving long-tail item recommendation with graph augmentation,\u201d in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 1707\u20131716.\n[7] OpenAI, \u201cGpt-4 technical report,\u201d 2023.\n[8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971, 2023.\n[9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al., \u201cOn the opportunities and risks of foundation models,\u201d arXiv preprint arXiv:2108.07258, 2021.\n10] Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang, \u201cIs chatgpt a good recommender? a preliminary study,\u201d arXiv preprint arXiv:2304.10149, 2023.\n11] Lei Wang and Ee-Peng Lim, \u201cZero-shot next-item recommendation using large pretrained language models,\u201d arXiv preprint arXiv:2304.03153, 2023.\n12] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen, \u201cMaking language models better reasoners with step-aware verifier,\u201d in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 5315\u20135333.\n\n[1] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua, \u201cNeural collaborative filtering,\u201d in  Proceedings of the 26th international conference on world wide web, 2017, pp. 173\u2013182.\n[2] Sichun Luo, Yuanzhang Xiao, and Linqi Song, \u201cPersonalized federated recommendation via joint representation learning, user clustering, and model adaptation,\u201d in  Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 2022, pp. 4289\u20134293.\n[3] Sichun Luo, Yuanzhang Xiao, Xinyi Zhang, Yang Liu, Wenbo Ding, and Linqi Song, \u201cPerfedrec++: Enhancing personalized federated recommendation with self-supervised pre-training,\u201d arXiv preprint arXiv:2305.06622, 2023.\n[4] Zahid Younas Khan, Zhendong Niu, Sulis Sandiwarno, and Rukundo Prince, \u201cDeep learning techniques for rating prediction: a survey of the state-of-the-art,\u201d  Artificial Intelligence Review, vol. 54, pp. 95\u2013135, 2021.\n[5] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He, \u201cDeepfm: a factorization-machine based neural network for ctr prediction,\u201d arXiv preprint arXiv:1703.04247, 2017.\n[6] Sichun Luo, Chen Ma, Yuanzhang Xiao, and Linqi Song, \u201cImproving long-tail item recommendation with graph augmentation,\u201d in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 2023, pp. 1707\u20131716.\n\n[13] Sinuo Deng, Lifang Wu, Ge Shi, Lehao Xing, Meng Jian, and Ye Xiang, \u201cLearning to compose diversified prompts for image emotion classification,\u201d arXiv preprint arXiv:2201.10963, 2022.\n[14] Hongzhi Yin, Bin Cui, Jing Li, Junjie Yao, and Chen Chen, \u201cChallenging the long tail recommendation,\u201d arXiv preprint arXiv:1205.6700, 2012.\n[15] Elaheh Malekzadeh Hamedani and Marjan Kaedi, \u201cRecommending the long tail items through personalized diversification,\u201d Knowledge-Based Systems, vol. 164, pp. 348\u2013357, 2019.\n[16] F Maxwell Harper and Joseph A Konstan, \u201cThe movielens datasets: History and context,\u201d Acm transactions on interactive intelligent systems (tiis), vol. 5, no. 4, pp. 1\u201319, 2015.\n[17] Sichun Luo, Xinyi Zhang, Yuanzhang Xiao, and Linqi Song, \u201cHysage: A hybrid static and adaptive graph embedding network for context-drifting recommendations,\u201d in Proceedings of the 31st ACM International Conference on Information & Knowledge Management, 2022, pp. 1389\u20131398.\n[18] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin, \u201cGraph neural networks for social recommendation,\u201d in  The world wide web conference, 2019, pp. 417\u2013426.\n[19] Qiang Liu, Feng Yu, Shu Wu, and Liang Wang, \u201cA convolutional click prediction model,\u201d in Proceedings of the 24th ACM international on conference on information and knowledge management, 2015, pp. 1743\u20131746.\n[20] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang, \u201cDeep & cross network for ad click predictions,\u201d in  Proceedings of the ADKDD\u201917, pp. 1\u20137. 2017.\n[21] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua, \u201cAttentional factorization machines: Learning the weight of feature interactions via attention networks,\u201d arXiv preprint arXiv:1708.04617, 2017.\n[22] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong Sun, \u201cxdeepfm: Combining explicit and implicit feature interactions for recommender systems,\u201d in Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, 2018, pp. 1754\u2013 1763.\n[23] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang, \u201cAutoint: Automatic feature interaction learning via self-attentive neural networks,\u201d in  Proceedings of the 28th ACM international conference on information and knowledge management, 2019, pp. 1161\u20131170.\n\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the limitations of traditional recommender systems in rating prediction tasks, particularly their inability to effectively utilize collaborative information and semantic knowledge from large language models (LLMs).",
        "problem": {
            "definition": "The problem is that traditional recommender systems struggle to incorporate the textual semantic information and side information necessary for accurate rating predictions.",
            "key obstacle": "The main challenge is that traditional models often ignore the semantic meaning of user/item IDs and cannot fully utilize collaborative information due to the extensive candidate set in recommendation tasks."
        },
        "idea": {
            "intuition": "The idea stems from the observation that LLMs excel in understanding textual information, which can be leveraged to enhance traditional recommendation models.",
            "opinion": "The proposed idea is to create the LAMAR framework, which integrates LLMs with traditional recommendation systems to improve rating predictions by combining collaborative and semantic information.",
            "innovation": "The key innovation of LAMAR is the introduction of an adaptive merging module that effectively combines outputs from both traditional and LLM-based recommendation models."
        },
        "method": {
            "method name": "Large Language Model Augmented Recommendation",
            "method abbreviation": "LAMAR",
            "method definition": "LAMAR is a framework that augments traditional recommendation systems by integrating outputs from LLMs to enhance the understanding of user preferences and item characteristics.",
            "method description": "LAMAR combines traditional recommendation techniques with LLM-generated insights to improve rating prediction accuracy.",
            "method steps": [
                "Construct prompts tailored for the rating prediction task.",
                "Utilize diverse prompts to generate multiple outputs from the LLM.",
                "Incorporate LLM-generated side information into the traditional recommendation model.",
                "Apply an adaptive merging strategy to combine predictions from LLM and traditional models."
            ],
            "principle": "The method is effective because it harnesses the strengths of both traditional models in handling collaborative information and LLMs in understanding semantic context."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted on two datasets: MovieLens-100K and Kaggle-Movie, comparing LAMAR to various traditional recommendation models.",
            "evaluation method": "The performance was assessed using RMSE and MAE metrics, with a leave-one-out strategy for training and testing."
        },
        "conclusion": "The LAMAR framework successfully improves recommendation performance by effectively integrating LLMs with traditional models, demonstrating significant enhancements in rating prediction accuracy across empirical studies.",
        "discussion": {
            "advantage": "The main advantages of LAMAR include improved prediction accuracy through the integration of semantic understanding and collaborative information.",
            "limitation": "One limitation is that the effectiveness of LAMAR may depend on the quality of the prompts and the LLM's understanding of the specific recommendation context.",
            "future work": "Future research could explore further enhancements in prompt engineering and the adaptation of LAMAR to different recommendation scenarios."
        },
        "other info": {
            "acknowledgement": "This work was supported by the National Natural Science Foundation of China and the Research Grants Council of the Hong Kong SAR.",
            "code availability": "Code for LAMAR is available at https://github.com/sichunluo/LAMAR."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "Recommendation algorithms are crucial for rating prediction tasks, especially in addressing the limitations of traditional recommender systems."
        },
        {
            "section number": "2.1",
            "key information": "Traditional recommender systems struggle to incorporate textual semantic information and collaborative information necessary for accurate rating predictions."
        },
        {
            "section number": "3.2",
            "key information": "The LAMAR framework integrates LLMs with traditional recommendation systems to enhance semantic understanding for improved rating predictions."
        },
        {
            "section number": "4.1",
            "key information": "LLMs excel in understanding textual information, which can be leveraged to improve traditional recommendation models."
        },
        {
            "section number": "4.2",
            "key information": "The LAMAR framework augments traditional recommendation systems by integrating outputs from LLMs to enhance the understanding of user preferences and item characteristics."
        },
        {
            "section number": "10.2",
            "key information": "Future research could explore enhancements in prompt engineering and adapting LAMAR to different recommendation scenarios."
        },
        {
            "section number": "11",
            "key information": "The LAMAR framework successfully improves recommendation performance by integrating LLMs with traditional models, demonstrating significant enhancements in rating prediction accuracy."
        }
    ],
    "similarity_score": 0.8110654036239794,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4c11/4c11f5c8-c437-41c5-b2fa-d2e650f45269.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large Language Models Augmented Rating Prediction in Recommender System.json"
}