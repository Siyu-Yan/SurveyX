{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2401.14777",
    "title": "Large Language Model Adaptation for Financial Sentiment Analysis",
    "abstract": "Natural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets\u2019 financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.",
    "bib_name": "inserte2024largelanguagemodeladaptation",
    "md_text": "firstname.lastname@linguacustodia.com 2Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\n# Abstract\nNatural language processing (NLP) has recently gained relevance within financial institutions by providing highly valuable insights into companies and markets\u2019 financial documents. However, the landscape of the financial domain presents extra challenges for NLP, due to the complexity of the texts and the use of specific terminology. Generalist language models tend to fall short in tasks specifically tailored for finance, even when using large language models (LLMs) with great natural language understanding and generative capabilities. This paper presents a study on LLM adaptation methods targeted at the financial domain and with high emphasis on financial sentiment analysis. To this purpose, two foundation models with less than 1.5B parameters have been adapted using a wide range of strategies. We show that through careful fine-tuning on both financial documents and instructions, these foundation models can be adapted to the target domain. Moreover, we observe that small LLMs have comparable performance to larger scale models, while being more efficient in terms of parameters and data. In addition to the models, we show how to generate artificial instructions through LLMs to augment the number of samples of the instruction dataset.\n 26 Jan 2024\narXiv:2401.14777v1\nNatural Language Processing (NLP) has become an increasingly important field in the financial industry, with applications ranging from sentiment analysis and named entity recognition to question answering. Information retrieved using machine learning from financial reports, news or posts in social media can be used as indicators of companies\u2019 performance or as insights of a market. Many industry actors are interested in extracting this information to use it as a resource that can provide them\n\u2217Corresponding author.\n\u2217Corresponding author.\nwith a competitive advantage, such as firms forecasting internal future benefits and losses, investors extracting differential information for trading purposes or any practitioner interested in tracking financial assets. Nevertheless, some characteristics of financial text make these tasks especially challenging for models that have been trained on general domain data. The use of specific terminology along with the high complexity of the documents, leads these generalist language models to underperform on financial tasks, which suggests that domain adaptation might be required to improve accuracy of interpretation and analysis. Furthermore, the rapid evolution of large language models (LLMs) and their proven capabilities for NLP tasks has made them stand out and become an interesting option to study. Due to the fact that even the best general language models fall short for some financial tasks, some proposals have been recently presented for a financial domain adaptation of LLMs. These models tailored for finance, such as BloombergGPT (Wu et al., 2023), have been introduced as multitasking generative models specifically designed for financial text understanding and generation. However, these fine-tuned models still show room for improvement, both in performance and in the efficiency of the proposed training strategies. This paper tackles various aspects of adapting LLMs to the financial domain. In particular, we explore diverse strategies of domain adaptation and fine-tuning of LLMs for financial sentiment analysis, and conduct a series of experiments over two different foundation models. The study focuses particularly on smaller manageable models, up to 1.5B parameters, in order to explore the possibilities of models that can be accessible with relatively low hardware requirements. Although the adapted models are smaller than the current state-of-the-art ones, results show that they achieve similar or higher performance. In addition, a curated data collection\nwith two main datasets is also presented. One constructed with financial documents and reports, and the other a set of instructions for financial tasks. We show, step by step, the process of creating these datasets and particularly focus on the use of more powerful LLMs to generate synthetic instructions to fine-tune smaller LLMs. Finally, apart from the main focus of the study which is on financial sentiment analysis, other tasks have also been evaluated to analyze the multitasking capabilities of our models.\n# 2 Related work\nSentiment analysis is one of the most common use cases of NLP. In this task, a model classifies a text according to the sentiment detected, usually between positive, negative and neutral. However, while any sentiment analysis model would be capable of undertaking financial sentiment analysis, an adaptation is required. In this section, the evolution of sentiment analysis in the financial domain is studied using models based on Transformers (Vaswani et al., 2023). FinBERT1 (Araci, 2019) is based on the idea of training a BERT (Devlin et al., 2018) model in two steps to adapt it to the financial domain and the sentiment analysis task. The first step consists of further pre-training the model on financial documents, as this strategy has already be proven to be effective (Howard and Ruder, 2018) for domain adaptation. This step aims at helping the model to understand financial terminologies better than the base model. Authors used a subset of Reuters\u2019 TRC24 dataset2, a collection of news articles published by Reuters that was filtered with keywords related to finance, to fine-tune the model. In the second step, the model is prepared for the sentiment analysis task by adding a dense layer to the last hidden state of the classification token CLS of the encoder-based architecture, a recommended practice for classification with BERT. This task is finetuned using the Financial PhraseBank (FPB) (Malo et al., 2014), a financial sentiment analysis dataset. FinBERT presents remarkable results on financial sentiment analysis, outperforming the state-of-theart. Nevertheless, the model is strongly limited to sentiment analysis and underperforms greatly on other tasks.\n1Several models under the name of FinBERT exist, how ever in this work we only discuss the first model. 2https://trec.nist.gov/data/reuters/reuters. html\n# 2.1 Base large language models\nRecent advances in the field of large language models (LLMs) have shown that these models can achieve remarkable capabilities in understanding complex natural language. They are also capable of performing zero-shot and few-shot learning, in which they can generate accurate responses for tasks that they have not seen during training (Radford et al., 2019). This makes LLMs a great choice in multitask settings where one model is expected to perform several tasks. Most of today\u2019s LLMs are based on Transformer models (Vaswani et al., 2023), typically set in decoder-only architectures. Training of LLMs is typically split in two stages. The first part of the training is the most computationally expensive since the model is trained using large amounts of text. For this reason, conducting the training of a LLM from scratch requires high computational resources. Nevertheless, many research groups and companies are releasing these models to the public to be used as base or foundation for other models, to enable research to move forward. Using these pre-trained models is highly beneficial for researchers with fewer data or hardware resources, as they can be used as a starting point for fine-tuning on specific tasks, such as chatting, following instructions or giving outputs in a specific style or format. Although most LLMs are trained on general domain data, there have been a few works recently to adapt LLMs to the financial domain. In the next subsections two such work are reviewed.\n# 2.2 Financial large language models\nBloombergGPT. One of the first decoderonly LLMs trained specifically for finance is BloombergGPT (Wu et al., 2023), a model of 50B parameters based on BLOOM\u2019s architecture (Scao et al., 2023). The corpus collected for the training of this LLM consisted in the combination of 363 billion tokens from financial documents with 345 billion tokens from general purpose datasets. The model was trained from scratch, without using any foundation model as a base, with the objective of predicting the next token of the documents, and without fine-tuning on instructions. However, the results presented by BloombergGPT are far from the ones achieved by other models, some of them of a much smaller scale. In addition, the results reported did not outperform other generalist LLMs, as we will show later in this paper.\nFinMA. The open model FinMA from PIXIU\u2019s framework (Xie et al., 2023) introduced by ChanceFocus reported better scores on several financial tasks than larger generalist LLMs, such as GPT4 (OpenAI, 2023), and BloombergGPT. They used LLaMA (Touvron et al., 2023a) as the pre-trained model and fine-tuned it with instructions tailored for financial multitasking. The instruction dataset consists of texts formed by an instruction, an input and an answer. The dataset includes a data augmentation strategy in which the inputs of those tasks with few samples were used with 10 different instructions. This augmentation strategy, while increasing the number of samples in the dataset, did not increase its diversity as the same set of 10 instructions were always repeated. In the same paper in which FinMA was presented, the PIXIU framework also included FLARE, a financial evaluation benchmark. This benchmark has been used to evaluate the experiments carried out in this project.\n# 2.3 Financial benchmark\nFor the evaluation of large language models, the FLARE benchmark3 from PIXIU framework has been used. The tasks of this benchmark which are relevant to our work are presented below. Financial Sentiment Analysis. Financial sentiment analysis task over two different benchmarks, the Financial Phrase Bank (FPB) (Malo et al., 2014) and FIQA-SA (Maia et al., 2018). News Headline Classification. Headlines task contains 9 different subtasks, each one associated with 9 different gold questions, in which the expected answers are \u201cyes\u201d or \u201cno\u201d. The inputs analyzed are gold news from the Gold dataset (Sinha and Khandait, 2020). Named Entity Recognition. NER task is based on detecting financial named-entities in U.S. public agreements in the (Salinas Alvarado et al., 2015) dataset. The tagged entities correspond to people, organizations and locations.\n# 3 Methodology\nIn this section, we describe the methods designed to conduct the experiments. First, we list the foundation models that are used as a part of this project. Then, two new dataset collections are introduced, one with data based on documents and the second with instructions. We also give details of designing\n3https://github.com/chancefocus/PIXIU\na data augmentation strategy for the instructions as well as the description of the training process carried out to fine-tune the foundation models.\n# 3.1 Foundation models\nAs stated earlier, the focus in this work is on smaller sized models that can be adapted to achieve performance of larger models. The two models that we use are listed below: OPT. Meta AI\u2019s large language models suite OPT (Open Pre-trained Transformers) (Zhang et al., 2022) were presented as a collection of 9 models ranging from 125M to 175B parameters, being one of the first publicly available LLMs. Pythia. EleutherAI presented Pythia (Biderman et al., 2023), a suite of decoder-only language models with sizes ranging from 70M to 12B parameters. These models are trained on the Pile dataset (Gao et al., 2020), a curated collection of English texts from a wide variety of sources.\n# 3.2 Datasets\nIn order to train LLMs, two main different approaches can be taken with respect to data. When a model is trained from scratch, the data used are collections of documents, for which the model has the objective of predicting the next token. This is usually the training carried out to obtain foundation LLMs. However, these models can be further pre-trained for domain adaptation, in the same way that FinBERT was trained. This approach is based on the idea of continuing the training of the model with financial documents to shift from a general to a financial language model. Moreover, it has been proven that large language models can improve their performances, especially on unseen (or zero-shot) tasks by fine-tuning them to follow instructions. For this fine-tuning method, the training objective is the same, predicting the next token of the text, with the only difference being that the format of this data relies on an \u201cinstruction\u201d, \u201cinput\u201d, \u201canswer\u201d format. For this project, one dataset was collected for each of these two training strategies. In addition, the instruction-based dataset was augmented artificially with samples generated from another LLM (LLaMA 2 13B (Touvron et al., 2023b)). Document dataset. The collection of documents used to further pre-train the base LLMs is a combination of general and financial documents from different sources. The purpose of this mixture is to add diversity to the training data, with finance\nbeing the most represented domain. Having general data in the training set prevents the model to completely drift the domain and result in a model that is unable to understand general language. The data sources of these documents are described below:\n EDGAR Files (Financial). EDGAR is the Electronic Data Gathering, Analysis, and Retrieval system online platform operated by the SEC4 (United States Securities and Exchange Commission). It is used by companies to electronically file registration statements, periodic reports, and other forms required by the SEC. The database of these documents is open to everyone, allowing the retrieval of high-quality financial text.\n Reuters News (Financial). Reuters is a news agency specializing in business and finance that released Reuters Corpora, a collection of financial news made available for use in NLP research. The collection used in this dataset is TRC2 (Thomson Reuters Text Research Collection), that contains more than 1.8 million news.\n# \u2022 In-house Dataset (Financial). As a part o\n In-house Dataset (Financial). As a part of this project, a diverse collection of in-house financial text has been obtained. The text in this dataset is mostly at sentence level, as they were originally used for machine translation. This is the only private set used for the project.\n The Pile (General). The Pile (Gao et al., 2020), from EleutherAI, is a dataset that comprises 22 diverse high-quality subsets, several of which originate from academic or professional sources. The idea behind this dataset\u2019s construction is that diversity enhances general cross-domain knowledge and downstream generalization capability of large language models. It includes data from general news to scientific articles, code, etc...The proportions of these subsets are kept as-is in the subsample used for this project.\nThe lengths of the documents of this dataset had to be adapted to the models\u2019 context length, which corresponds to the longest sequence of tokens that the model can support. In this project the context was limited to 2048 tokens. The pre-processing of\n4https://www.sec.gov/edgar/searchedgar/ companysearch\nthe dataset consisted in concatenation of all documents from the same source, using a special token (<endoftext>) to separate them. The long concatenated text is then sliced in blocks of 2048 tokens, which are mixed and shuffled with all the other blocks of the dataset. Since some datasets used in this project are extremely large, we decided to take a smaller proportion from each one. The summary of the ratio used for each partition is shown in Table 1.\nSubset\nDomain\n# Tokens\n%\nEDGAR\nFinance\n100k\n25.7\nReuters\nFinance\n36k\n9.3\nIn-house\nFinance\n38k\n9.7\nThe Pile\nGeneral\n215k\n55.3\nTotal\n389k\n100\nTable 1: Proportion and absolute number of tokens taken from each dataset.\nInstruction-based dataset. Instruction finetuning is a strategy used to improve LLMs\u2019 performance for specific tasks by teaching them to follow specific format of questions and answers. LLMs learn by being trained on this specific format of text, while keeping the same training objective, predicting the next sequence of tokens. Fine-tuning on instructions is the most common technique to adapt foundation models to specific use-cases, mainly because this method not only improves performance on the trained tasks, but also augments zero-shot and few-shot capabilities. Models trained on instructions are usually consistent in the format in which data is presented. In Table 2 the format used for our dataset is displayed.\nTemplate\n### Instruction: Description of the task\n### Input: Input to analyze\n### Answer: Answer to predict\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ca93/ca936054-466f-4f69-8b0b-8f73450ba69d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 2: Template for instructions.</div>\nTable 2: Template for instructions.\nTo create the instruction dataset, we used the instructions dataset published by PIXIU that targeted the FLARE benchmark. However, this dataset has poorly curated prompts and includes a suboptimal data augmentation strategy. For instance, certain parts of the dataset have been up-sampled by using ten different instructions over the same input. Despite having more samples, the up-sampled version\nSubset\nInstr\nPIXIU\nAugm\nFPB\n4,838\n48,380\n6,633\nFiQA-SA\n1,173\n11,730\n2,825\nNER\n609\n6,090\n2,609\nHeadline\n102,708\n102,708\n102,708\nFinQA\n8,242\n8,242\n8,242\nConvFinQA\n3,892\n3,892\n3,892\nBigData22\n0\n7,164\n0\nACL18\n0\n27,053\n0\nCIKM18\n0\n4,967\n0\nTOTAL\n121,462\n220,226\n126,909\nTable 3: Comparison of the instructions used for each task in the original instructions dataset (Instr) with one input-answer by instruction, the up-sampled version (PIXIU), and the dataset augmented by LLM inference (Augm).\nof dataset lacks diversity which may lead to poor performance as discussed by Zhou et al. (2023). For this reason, the dataset proposed in this project has been designed from scratch, only reusing the unique input - answer pairs from a down-sampled version of PIXIU\u2019s dataset that includes a single instruction for each input. Instruction data augmentation. The main idea behind instruction data augmentation is to bring new inputs to the dataset, so the model has more diverse examples to learn from. Two different methods have been defined for generating these instructions dependent on the target task. For sentiment analysis, the model has to generate an input for a given sentiment given an example with that label. This strategy has been used to augment both FPB and FIQA-SA subsets. In Table 7 of Appendix A the template used for this task is presented. For NER, since it is not a sequence classification task, the inference method is different. The first solution proposed was based on letting the model generates both the new sentence and its NER tags at the same time, only guiding the model by including a few examples in the prompt. However, the variety of the sentences generated by the model was too short and the tags were incorrect, indicating that the task was too hard for the model. Our solution was to use existing unlabeled sentences, which reduced the generative task to a tagging process. The sentences used for this augmentation were in-house financial sentences. Moreover, in this case the example given to the model is fixed in order to make sure all types of entities are present in the prompt. The format of the tagging was chosen using prompt\nengineering. In Table 8 of Appendix A, the template used for NER data augmentation is shown. The Headline task was not augmented since it had enough samples, even when considering that there are 9 subtasks in the benchmark. Using the above-mentioned two templates, new inputs are inferred to be added to the instructions dataset. The model used for the generation of these new samples is LLaMA-2-13B, quantized in 4-bits to reduce the GPU memory required. Table 3 shows a comparison of the number of samples targeting each task before and after the augmentation. The decision on the number of synthetic samples generated was taken considering the number of original samples. The reason behind not generating even a larger number of instructions is that despite their high-quality, artificial samples could introduce some noise to the dataset by generating sentences too different from the original distribution, introducing erratic input-answer pairs or NER tags or to duplicate some inputs after several iterations. In Table 3 there is a comparison between PIXIU\u2019s dataset before and after down-sampling as well as after the data augmentation.\n# 3.3 Training method\nAs stated earlier, in this work, we use two pretrained foundation models, namely Pythia-1.4B and OPT-1.3B, and fine-tune them in two stages as detailed below:\n Further pre-training. The models are finetuned to predict the next token of the text in the document-based dataset, following the same idea as in FinBERT and without being fine-tuned on a specific task. The idea here is to tilt the models to become more familiar with the financial domain. Both models are trained on a total of 389, 000 tokens introduced in context blocks of 2, 048 tokens. The models are trained for two epochs, saving 4 checkpoints at every epoch. The best checkpoint is selected for each model.\nstructed to perform financial tasks using the instructions dataset. Since the length of these instructions is generally shorter than on the document-based dataset, the context length is reduced to 1, 000 tokens to speed up the training. Sequences shorter than this length are padded with a padding token. Instructions\nlonger than 1, 000 tokens are cut off. For instruction fine-tuning, models are trained for 1 epoch.\nFor both set-ups, training is performed with AdamW optimizer (Loshchilov and Hutter, 2019), a batch size of 32, and applying gradient accumulation of 4 for training efficiency. The initial learning rate is set to 1e-4, while the weight decay is adjusted to 0.1. These values remained the same for all the conducted experiments. The training of these models was carried out on a H100 GPU.\n# 4 Results\n# 4.1 Classical algorithms versus LLMs for financial sentiment analysis\nPrior to conducting an evaluation of our fine-tuned LLMs for financial sentiment analysis, we study the performance of the current state-of-the-art models and classical machine learning algorithms. Compared to LLMs, classical algorithms do not require a lot of computation, they could be easily trained and tested. For the sake of simplicity, the evaluation has been carried out only on the FPB. Based on the results in Table 4, the lowest score, unsurprisingly, is obtained by the lexicon approach. Classical machine learning algorithms on the other hand are able to obtain results considerably higher than lexicon, and even match or pass LLM scores in some cases. Overall conclusions that can be depicted from the results can be summarized as follows:\n\u2022 The domain adaptation and training of FinBERT on this specific task, gives the model an advantage over general models. Comparing FinMA-30B with GPT-4, it can be seen that a smaller model fine-tuned for finance has better performance than a generalist one.\n\u2022 BloombergGPT was a good starting point for financial LLMs. However, its performance on tasks like sentiment analysis is poor. One likely reason is that this model has not been fine-tuned on instructions.\n FinMA-30B proves the relevance of finetuning on instructions to improve performance on financial tasks. Nevertheless, as mentioned before, the train dataset might be not sufficiently diverse, which may impact the model\u2019s capability in real-world scenario.\nAlgorithm and features\nAccuracy\nLexicon approach\nLoughran-McDonald dictionary\n0.59\nClassical ML algorithms\nSVM\n0.77\nNaive Bayes\n0.73\nXGB\n0.80\nTransformers approach\nFinBERT\n0.85\nGPT-4\n0.71\nBloombergGPT\n-\nFinMA-30B\n0.87\nTable 4: Performance of financial sentiment analysis. A comparison between traditional approaches and modern transformer based models.\n# 4.2 Financial domain adaptation\nIn this section we show the impacts of the two stage fine-tuning as well as improvements brought by the artificially augmented instruction dataset. Models are evaluated using a subset of the tasks proposed in the FLARE benchmark: FPB, FIQA-SA, Headlines and NER. For the classification tasks (FPB, FIQA-SA and Headlines), the predictions are obtained by forcing the model to generate one of the expected class label. For example, in FPB, this means to choose the next token only amongst the ones needed to generate the labels (positive, negative or neutral), and sticking with the most probable ones (the highest logits). When evaluating on NER, the generation is not constrained. The first experiment that we carry out is to see the effects of fine-tuning on documents versus instructions. Based on the results of Table 5, it is clear that performance of both Pythia and OPT models show similar behaviors and that fine-tuning brings significant improvements over the base models. Particularly, instruction fine-tuning improvement is much higher than just further pre-training on documents. This conclusion seems to be aligned with what we observe in the literature of instruction tuning of other domains. Next, in order to evaluate the effect of augmenting the number of instructions using the strategy designed for this dataset, the models are compared after fine-tuning with the base instructions dataset and with the augmented instructions dataset. The results of using data augmentation are not straightforward. In Table 5, it can be seen that the performance of the models augmented instructions is improved for the sentiment analysis tasks, but the scores goes down for the other two. In the\nF1 scores\nFine-tuning data\nFPB\nFIQA-SA\nHeadlines\nNER\nPythia-1.4B\n(base)\n0.20\n0.29\n0.16\n0\nDocs\n0.41\n0.16\n0.57\n0.30\nInstr\n0.82\n0.73\n0.93\n0.59\nAugmented instr\n0.82\n0.79\n0.90\n0.56\nDocs + Augmented instr\n0.84\n0.83\n0.97\n0.69\nOPT-1.3B\n(base)\n0.19\n0.48\n0.29\n0\nDocs\n0.13\n0.58\n0.39\n0\nInstr\n0.84\n0.77\n0.93\n0.53\nAugmented instr\n0.86\n0.79\n0.97\n0.29\nDocs + Augmented instr\n0.86\n0.81\n0.96\n0.34\nTable 5: Comparison of Pythia-1.4B and OPT-1.3B fine-tuned with different strategies. The results reported correspond to the base models without fine-tuning (base), models with document further pre-training (Docs), models fine-tuned on instructions (Instr), models fine-tuned on augmented instructions dataset (Augmented instr), fine-tuning first with documents and then with augmented instructions (Docs + Augmented instr).\ncase of Headlines, this effect can be caused by the fact that this task is the most represented in the dataset and, by introducing new samples, the model is less focused on this task. For NER, the issue can be explained by the difference between the text of the synthetic samples and the original test set. As explained in previous sections, NER is augmented using in-house data, and even though the chosen sentences were also in the financial domain, the sources are different and that might have introduced errors in the predictions. Finally, we can test the implications of instruction fine-tuning after further pre-training the model with the financial documents. This simply means that the model is fine-tuned two times. Since the augmented instructions proved to be better than the original instructions, this experiment is conducted on the earlier instruction dataset. As shown in the last row of Table 5, this approach seems to lead to a higher score in every task. Therefore, the domain adaptation method inspired by FinBERT\u2019s training strategy, proves to be effective for decoder-only LLMs and not only for financial sentiment analysis, but for multiple financial NLP tasks.\n# 4.3 Comparison with other Financial LLMs\nIn this section, the results of the best models obtained through the previous experiments (Docs + Augmented instr) are compared against the stateof-the-art LLMs for finance. As can be seen in Table 6, both fine-tuned Pythia-1.4B and OPT-1.3B over perform GPT-4 in classification tasks, which includes financial sentiment analysis. This is made possible because of the domain adaptation conducted for these two base models. For NER, which\n<div style=\"text-align: center;\">F1 scores</div>\nis a generative task, GPT-4 is still the LLM with the highest score. When the models of these projects are compared to BloombergGPT, the biggest current LLM tailored for finance, it can be observed that the scores obtained are much higher for classification tasks, specially for sentiment analysis, and that Pythia also obtains better score for NER. In terms of efficiency, these results are achieved with models that have approximately 97% fewer training parameters than BloombergGPT5. In the comparative with the collection of FinMA models, the PIXIU LLMs still outperform the models fine-tuned with our domain adaptation strategy in some tasks, specially when compared to FinMA30B. However, when FinMA-7B, the model with the closest size to the models presented in this project, is evaluated in financial sentiment analysis and Headlines, it can be observed that the scores are almost equivalent to the fine-tuned Pythia-1.4B and OPT-1.3B. In this case, however, the biggest improvement with respect to FinMA-7B is in terms of efficiency. Pythia-1.4B and OPT-1.3B have approximately 78% fewer training parameters than FinMA-7B, and the number of instructions used goes from 220, 226 down to 126, 909, which is only a 57% of the number of samples used for PIXIU models. Therefore, from the general comparison it can be seen that the models fine-tuned in this project over perform most LLMs in financial tasks, with the only exception of FinMA models. In addition, the size of the models and the training strategy\n5BloombergGPT has 50B trainable parameters. Pythia1.4B and OPT-1.3B have approximately 1.5B parameters. The amount of data used is not comparable since BloombergGPT was trained from scratch.\nF1 scores\nModels\nFPB\nFIQA-SA\nHeadlines\nNER\nBloombergGPT\n0.51\n0.75\n0.82\n0.61\nGPT-4\n0.78\n-\n0.86\n0.83\nFinMA-7B\n0.86\n0.84\n0.98\n0.75\nFinMA-30B\n0.88\n0.87\n0.97\n0.62\nPythia-1.4B\n0.84\n0.83\n0.97\n0.69\nOPT-1.3B\n0.86\n0.81\n0.96\n0.34\nTable 6: Comparison of state-of-the-art with Pythia-1.4B and OPT-1.3B fine-tuned on documents and the augmented dataset. Performance of models retrieved from BloombergGPT and PIXIU papers. BloombergGPT is not a publicly available model, so it is not possible to evaluate it under the same conditions as the other models. Thus, ChatGPT, GPT-4 and FinMA-30B are evaluated on zero-shot, BloombergGPT is only reported in a five-shot setting and its accuracy was not published.\nhave been proven to be more efficient than the ones proposed for other models.\n# 5 Conclusion\nThis project has covered a wide range of aspects of financial LLMs. Through a series of experiments, using Pythia-1.4B and OPT-1.3B as base models, we studied the adaptation of relatively small LLMs for finance. The experiments we conducted first show that LLMs adapted to the financial domain through further pre-training followed by instruction fine-tuning perform better than some of the best current generalist LLMs (such as GPT-4) on financial tasks. Second, it validates our training strategy since our LLMs obtain higher or similar scores than other financial LLMs that were trained with much more parameters and larger datasets. Lowering the requirements to fine-tune LLMs for this specific industry can be key for the future of several companies, since it can enable smaller organizations to host their own LLMs or, at least, to make them more accessible. Furthermore, it is worth mentioning that the models used for this project as well as most datasets, except for the inhouse subset (only 9.7% of the documents dataset), are open and publicly available. In addition to the findings related to domain adaptation of LLMs for financial tasks and the models presented, a strategy for the generation of samples for the instructions dataset is introduced. Moreover, the two datasets used for the project are described with enough details to be reproduced by other researchers. Finally, the paper also presented a comprehensive study that delves into the state-of-the-art and the evolution of approaches for financial sentiment analysis, ranging from traditional dictionary-based methods to the more recent advancements in LLMs.\n<div style=\"text-align: center;\">F1 scores</div>\nDespite the fact that the results showed great performance of the small-sized models, in further research these fine-tuning strategies could be applied to larger models and study their impact on different scales and domains. An interesting option to study in the future are Low-Rank Adapters or LoRA (Hu et al., 2021), a method that reduces the number of trainable parameters by freezing the foundation model weights and injecting trainable rank decomposition matrices into each layer of the LLM.\n# Limitations\nThe limitations of this work can be summarized as following:\n\u2022 Generative capabilities: The final fine-tuned model seems to perform very well on classification tasks such as sentiment analysis, while still lagging behind in generative ones.\n\u2022 Unseen tasks: our work concentrates on certain tasks that have been studied in previous similar work, but for a full understanding of its limitations, one needs to test it on unseen tasks.\n\u2022 Large models: we believe that testing the same strategy of multiple fine-tuning stages would yield even better results with larger models such as LLaMA-2-7B or even larger models.\n\u2022 Large models: we believe that testing the same strategy of multiple fine-tuning stages would yield even better results with larger models such as LLaMA-2-7B or even larger models.\n# References\nDogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models.\nDogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d054/d054ce30-5442-4cee-b0d6-311b38aa652f.png\" style=\"width: 50%;\"></div>\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.\n# OpenAI. 2023. Gpt-4 technical report.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, and et al. 2023. Bloom: A 176bparameter open-access multilingual language model. Ankur Sinha and Tanmay Khandait. 2020. Impact of\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2023. Attention is all you need. hijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: A large language model for finance. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. Pixiu: A large language model, instruction data and evaluation benchmark for finance. usan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. A Instruction data augmentation\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment.\n# A Instruction data augmentation examples\nTemplate\nWrite a sentence with a {yi} financial sentiment.\nUse the format <stc> sentence </stc>.\nReuse\nterms from the example.\nExample: \u2019<stc> {xi}\n</stc>\u2019\nExample\nWrite\na\nsentence\nwith\na\npositive\nfinancial\nsentiment. Use the format <stc> sentence </stc>.\nReuse terms from the example. Example: \u2019<stc>\nShares of Standard Chartered ( STAN ) rose 1.2\n% in the FTSE 100 , while Royal Bank of Scotland\n( RBS ) shares rose 2 % and Barclays shares (\nBARC ) ( BCS ) were up 1.7 % . </stc>\u2019\nTable 7: Template for sentiment analysis input generation with financial sentiment fixed and dynamic shot. Example given for positive input inference. {xi, yi} is an input-answer pair sampled from one of the two subsets.\nTemplate\nIdentify the named entities that represent a\nperson (\u2019PER\u2019), an organization (\u2019ORG\u2019), or\na\nlocation\n(\u2019LOC\u2019)\nin\na\nfinancial\ncontext.\nUse the format \u2019Entities: entity name, entity\ntype\u2019.\nSentence: \u2019The Bank gave money to the Borrower\nto open a business in New York.\u2019;\nEntities:\n\u2019Bank, ORG | Borrower, PER | New York, LOC\u2019\nDo the same with this sentence, identifying\n\u2019PER\u2019, \u2019ORG\u2019, \u2019LOC\u2019 entities.\nSentence: {xi}; Entities:\nExample\nIdentify the named entities that represent a\nperson (\u2019PER\u2019), an organization (\u2019ORG\u2019), or\na\nlocation\n(\u2019LOC\u2019)\nin\na\nfinancial\ncontext.\nUse the format \u2019Entities: entity name, entity\ntype\u2019.\nSentence: \u2019The Bank gave money to the Borrower\nto open a business in New York.\u2019;\nEntities:\n\u2019Bank, ORG | Borrower, PER | New York, LOC\u2019\nDo the same with this sentence, identifying\n\u2019PER\u2019, \u2019ORG\u2019, \u2019LOC\u2019 entities.\nSentence:\n\u2018350 ,\nWellesley ,\nMassachusetts\n02481 doing business as \" Silicon Valley East\n\" and AKAMAI TECHNOLOGIES , INC . (\" Borrower\n\"), whose address is 201 Broadway , 4th Floor\n, Cambridge , Massachusetts 02139 provides the\nterms on which Bank will lend to Borrower and\nBorrower will repay Bank\u2019; Entities:\nTable 8: Template for NER tags generation given a sentence of the financial domain.\n",
    "paper_type": "method",
    "attri": {
        "background": "Natural language processing (NLP) has gained relevance in financial institutions for extracting insights from financial documents. However, generalist language models face challenges in this domain due to complex texts and specific terminology, necessitating new adaptation methods for improved performance.",
        "problem": {
            "definition": "The issue addressed in this paper is the underperformance of generalist language models in financial sentiment analysis due to the unique characteristics of financial texts.",
            "key obstacle": "The main challenge is the complexity and specificity of financial terminology, which existing models struggle to interpret accurately."
        },
        "idea": {
            "intuition": "The idea was inspired by the need for specialized models that can effectively handle financial texts and tasks, leveraging recent advances in large language models (LLMs).",
            "opinion": "The proposed idea involves adapting smaller LLMs for financial sentiment analysis through fine-tuning with financial documents and synthetic instructions.",
            "innovation": "The key innovation lies in the adaptation of smaller LLMs, achieving comparable performance to larger models while being more efficient in terms of parameters and data."
        },
        "method": {
            "method name": "Financial Sentiment Analysis Adaptation",
            "method abbreviation": "FSAA",
            "method definition": "FSAA involves adapting foundation models for financial sentiment analysis through domain-specific fine-tuning and instruction-based training.",
            "method description": "The method focuses on fine-tuning smaller LLMs using financial documents and augmented instruction datasets to enhance their performance on financial tasks.",
            "method steps": [
                "Select foundation models (e.g., Pythia and OPT).",
                "Collect and preprocess financial document datasets.",
                "Create instruction-based datasets for training.",
                "Fine-tune models on financial documents.",
                "Fine-tune models using instruction datasets."
            ],
            "principle": "This method is effective because it leverages both domain-specific knowledge from financial texts and structured instruction formats to improve the model's ability to understand and generate relevant outputs."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using two foundation models (Pythia-1.4B and OPT-1.3B) on financial sentiment analysis tasks, comparing their performance against classical algorithms and larger models.",
            "evaluation method": "The performance was assessed using accuracy and F1 scores across various tasks, including sentiment analysis, named entity recognition, and headline classification."
        },
        "conclusion": "The study demonstrates that smaller LLMs fine-tuned for the financial domain can achieve performance levels comparable to larger models, highlighting the effectiveness of the proposed adaptation strategies.",
        "discussion": {
            "advantage": "The main advantages of the proposed approach include improved performance on financial tasks with lower resource requirements and the ability to generate synthetic instructions for training.",
            "limitation": "Limitations include the models' generative capabilities lagging behind in some tasks and the need for further testing on unseen tasks to fully understand their limitations.",
            "future work": "Future research could explore applying the fine-tuning strategies to larger models and investigate the use of Low-Rank Adapters (LoRA) to optimize performance."
        },
        "other info": {
            "info1": "The datasets used are publicly available, except for a small in-house subset.",
            "info2": {
                "info2.1": "The models are designed to be efficient, requiring significantly fewer parameters than existing state-of-the-art models.",
                "info2.2": "The paper includes comprehensive details on dataset creation and training processes to facilitate reproducibility."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "5.1",
            "key information": "The paper discusses how NLP techniques are used to interpret user preferences, specifically in the context of financial sentiment analysis."
        },
        {
            "section number": "4.1",
            "key information": "The study highlights the capabilities of smaller LLMs in processing financial texts through domain-specific fine-tuning."
        },
        {
            "section number": "4.2",
            "key information": "The integration of smaller LLMs into financial sentiment analysis is proposed, emphasizing their adaptability through fine-tuning with financial documents."
        },
        {
            "section number": "3.2",
            "key information": "The paper explores AI-driven techniques for enhancing semantic understanding in financial recommendations through the adaptation of LLMs."
        },
        {
            "section number": "10.1",
            "key information": "The paper identifies challenges faced by generalist language models in financial sentiment analysis, particularly the complexity and specificity of financial terminology."
        },
        {
            "section number": "10.2",
            "key information": "Future research opportunities include applying fine-tuning strategies to larger models and investigating optimization techniques like Low-Rank Adapters (LoRA)."
        }
    ],
    "similarity_score": 0.7409907864661692,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/Large Language Model Adaptation for Financial Sentiment Analysis.json"
}