{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2206.12131",
    "title": "MVP: Multi-task Supervised Pre-training for Natural Language Generation",
    "abstract": "Pre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e., \u201csupervised pre-training\u201d) showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multitask superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model\u2019s capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3% and Flan-T5 by 5.8%.",
    "bib_name": "tang2023mvpmultitasksupervisedpretraining",
    "md_text": "# MVP: Multi-task Supervised Pre-training for Natural Language Generation\nTianyi Tang1,4, Junyi Li1,3, Wayne Xin Zhao1,4 \ufffdand Ji-Rong Wen1,2,4 1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Information, Renmin University of China 3DIRO, Universit\u00e9 de Montr\u00e9al\n4Beijing Key Laboratory of Big Data Management and Analysis Methods eventianyitang@outlook.com lijunyi@ruc.edu.cn batmanfly@gmail.co\n# Abstract\nPre-trained language models (PLMs) have achieved remarkable success in natural language generation (NLG) tasks. Up to now, most NLG-oriented PLMs are pre-trained in an unsupervised manner using the large-scale general corpus. In the meanwhile, an increasing number of models pre-trained with labeled data (i.e., \u201csupervised pre-training\u201d) showcase superior performance compared to unsupervised pre-trained models. Motivated by the success of supervised pre-training, we propose Multitask superVised Pre-training (MVP) for natural language generation. We collect a large-scale natural language generation corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. Then we unify these examples into a general text-to-text format to pre-train the text generation model MVP in a supervised manner. For each task, we further pre-train specific soft prompts to stimulate the model\u2019s capacity to perform a specific task. Our MVP model can be seen as a practice that utilizes recent instruction tuning on relatively small PLMs. Extensive experiments have demonstrated the effectiveness and generality of our MVP model in a number of NLG tasks, which achieves state-of-the-art performance on 13 out of 17 datasets, outperforming BART by 9.3% and Flan-T5 by 5.8%.\n# 1 Introduction\nNatural language generation (NLG, also known as text generation) is a crucial capacity for language intelligence, which aims to generate human-like texts on demand (Garbacea and Mei, 2020). Since the emergence of the pre-training and fine-tuning paradigm, pre-trained language models (PLMs) have dominated mainstream approaches for NLG tasks (Lewis et al., 2020; Brown et al., 2020). With a large-scale general corpus, the majority of PLMs are pre-trained in an unsupervised (self-supervised) manner by leveraging intrinsic data correlations as\nsupervision signals. However, unsupervised pretraining is likely to incorporate noise that affects the performance of downstream tasks (Feng et al., 2022), also leading to a slower rate of acquiring knowledge (Zhang et al., 2021). In the meanwhile, more and more large-scale labeled datasets have become easily accessible (Deng et al., 2009; Liu et al., 2020). There is growing evidence that pre-training with labeled data can further improve the performance of PLMs, both in the fields of computer vision (He et al., 2016; Dosovitskiy et al., 2021) and natural language processing (Lin et al., 2020b; Su et al., 2022). These promising developments motivate us to consider pre-training text generation models with labeled data, which is called \u201csupervised pretraining\u201d (Feng et al., 2022). Existing work has shown that supervised pre-training can explicitly learn task-specific characteristics and alleviate the discrepancy between unsupervised pre-training and supervised fine-tuning (Lin et al., 2020b). Furthermore, most NLG systems are often trained in a supervised way, requiring supervision signals to learn the input-to-output transformation. For example, dialogue systems learn to generate appropriate responses based on historical utterances, and text summarization systems learn to extract essential information from long documents according to human-written summaries. Therefore, we suspect that supervised pre-training is more suited for NLG-oriented PLMs in essence since it can provide task-related instructions early in the pre-training stage instead of a later fine-tuning stage. Inspired by the recent success of supervised pre-training, we propose Multi-task superVised Pre-training (MVP) for natural language generation by leveraging a variety of labeled text generation datasets. Specially, we collect a largescale labeled corpus, MVPCorpus, consisting of 77 datasets over 11 text generation tasks. Since recent research shows that an extensive scale of\nSettings\nSupervised Pre-training\nUnsupervised Pre-training\nNLG\nMVP (ours)\nGPT-2, MASS, BART, T5\nNLU\nFLAN, T0, Muppet, ExT5\nBERT, XLNet, RoBERTa, T5\nTable 1: Representative PLMs for NLG and NLU tasks using (un)supervised pre-training. We present a more detailed comparison and discussion about supervised pre-training in Section 5.\nmulti-task pre-training (Aribandi et al., 2022) is the key to generalizing to new tasks for large PLMs, we combine these labeled datasets for multi-task pre-training. Existing popular works, as shown in Table 1, mainly focus on NLU tasks (Sanh et al., 2022; Aribandi et al., 2022) or use unsupervised pre-training (Lewis et al., 2020; Raffel et al., 2020), with no consideration of supervised pre-training on NLG tasks. To fill this gap, we explore supervised pre-training and multi-task learning for deriving both effective and general NLG models. To develop our approach, we adopt a Transformer-based (Vaswani et al., 2017) sequenceto-sequence model as the backbone. In multi-task training, different tasks may \u201cneutralize\u201d the ability learned through other tasks (He and Choi, 2021). To mitigate this potential issue, we propose to learn task-specific prompts based on the MVP model, following the structure of prefix-tuning (Li and Liang, 2021). Task-specific pre-training enables prompts to \u201cstore\u201d specialized knowledge for each corresponding task. Integrating MVP with task-specific prompts can further stimulate the model\u2019s capacity to perform some specific tasks. To summarize, our main contributions center around the following research questions:\n How to train an NLG-oriented PLM in a supervised pre-training way? In order to prepare the supervised corpus, we collect a massive labeled MVPCorpus, consisting of 77 datasets over 11 NLG tasks across various domains and specific objectives. To the best of our knowledge, MVPCorpus is the largest collection of NLG datasets. Firstly, we formulate different NLG tasks as a general text-to-text form using task instructions so that the supervised corpus can be used in a unified way for pre-training an NLG model. Our work presents a simple yet general approach for pre-training a more capable NLG model by leveraging various labeled NLG datasets.  Can supervised pre-trained NLG models be both effective and general? Extensive experiments\nshow that the supervised pre-trained MVP outperforms its unsupervised pre-trained counterpart BART in both full tuning (+9.3% in ratio) and parameter-efficient tuning (+4.3% in ratio) settings. Our MVP model achieves state-of-the-art performance on 13 out of 17 datasets and outperforms Flan-T5 (Chung et al., 2022) by 5.8%. Our zero-shot performance also surpasses T011B (Sanh et al., 2022) by a large margin. Furthermore, the experiments on unseen NLG and NLU tasks demonstrate that our supervised MVP model has a strong generality for unseen tasks.\nFor reproducing and reusing our work, we release the MVPCorpus collection, all the MVP model variants, and accordingly codes at the link: https://github.com/RUCAIBox/MVP.\n# 2 Related Work\nPre-trained Language Models. Pre-trained language models have achieved exceptional success in a wide range of tasks, and the majority of them are pre-trained in an unsupervised manner (Devlin et al., 2019; Brown et al., 2020). For example, with large-scale plain texts as the unsupervised pre-training corpus (570GB), GPT-3 (Brown et al., 2020) employs language modeling as the pretraining task, i.e., predicting the next token conditioned on previous tokens. In the meanwhile, the computer vision community benefits a lot from the labeled dataset ImageNet (Deng et al., 2009). Influential models, such as ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2021), leverage ImageNet for pre-training. Inspired by the success of pretraining with labeled data, machine translation researchers explore supervised pre-training (McCann et al., 2017; Lin et al., 2020b). Lin et al. (2020b) attempt to pre-train a translation model with parallel data in multiple languages. Despite using much less pre-trained data, mRASP still achieves better performance than translation models pre-trained in an unsupervised manner (Liu et al., 2020). In this paper, we propose to pre-train a universal NLG model in a supervised manner with collections of labeled datasets (23GB).\nMulti-task Learning. Our pre-training process is also related to multi-task learning (MTL), a method of mixing multiple tasks into a single training process (Collobert and Weston, 2008). A model trained with MTL can benefit from helpful knowledge of relevant tasks, resulting in improved perfor-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/52c9/52c948fd-5ec3-4002-8e96-8b2d1b7bdbf2.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: The overview of the pre-training process of our MVP model and task-specific prompts.</div>\nmance (Subramanian et al., 2018). Recently, MTDNN (Liu et al., 2019a) and Muppet (Aghajanyan et al., 2021) collect tens of datasets in the multi-task procedure and achieve better performance in downstream tasks. The pre-finetuning schema proposed in Muppet shares a similar idea with our study. Aribandi et al. (2022) further combine the denoising pre-training task of T5 (Raffel et al., 2020) and multi-task learning to pre-train a new model, ExT5. MTL has also contributed to sub-fields of text generation, such as open-ended dialogue system (Zhang et al., 2020), task-oriented dialogue system (Su et al., 2022), text style transfer (Bujnowski et al., 2020), and question answering (Khashabi et al., 2020). At the same time, researchers explore the transferability of models trained on multi-task datasets (Mishra et al., 2022). FLAN (Wei et al., 2022), T0 (Sanh et al., 2022), ZeroPrompt (Xu et al., 2022), and FLAN-T5 (Chung et al., 2022) investigate the zero-shot or few-shot generalization abilities of large language models (LLMs) (Zhao et al., 2023) trained on numerous task datasets with well-designed prompts. Compared with these works, we aim to explore multi-task learning to derive both effective and general NLG models in a supervised pre-training manner.\nPrompt Learning. Prompt learning is a thriving method in the field of NLP. Prompt learning converts fine-tuning text into a format similar to pre-training to leverage implicit pre-training knowledge and alleviate the discrepancy between pretraining and fine-tuning (Liu et al., 2021b). GPT2 (Radford et al., 2019) and T5 (Raffel et al., 2020) add human-written task prompts to the input text. For instance, T5 prepends \u201cSummarize:\u201d to the input document for summarization tasks. Some researchers also design elaborate prompts for each task and dataset and investigate their effectiveness and robustness (Wei et al., 2022; Sanh et al., 2022). To overcome the constraints of manually\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fecd/fecdfff1-f295-4828-864e-8b9298fcfb88.png\" style=\"width: 50%;\"></div>\nconstructed prompts, researchers develop continuous (soft) prompts that can be optimized in continuous space (Lester et al., 2021; Qin and Eisner, 2021; Tang et al., 2022b). Considering the random initialization of soft prompts, Gu et al. (2022) propose PPT to pre-train continuous prompts using unlabeled data. SPoT (Vu et al., 2022), UnifiedSKG (Xie et al., 2022), and PTG (Li et al., 2022a) further learn the prompts on related tasks and transfer the prompts to new tasks.\n# 3 The MVP Model\nThis section introduces our MVP model: a Multitask superVised Pre-trained model for natural language generation. The overview of our model is illustrated in Figure 1.\n# 3.1 Data Collection\nFormally, the natural language generation (NLG) task aims to generate a sequence of tokens Y = (y1, y2, . . . , yn) conditioned on input data X (e.g., a piece of text or structured data) (Li et al., 2022b). In this paper, we collect a large-scale labeled MVPCorpus consisting of 77 labeled datasets from 11 representative NLG tasks1, including commonsense generation, data-to-text generation, openended dialogue system, paraphrase generation, question answering, question generation, story generation, task-oriented dialogue system, text simplification, text style transfer, and text summarization. These datasets come from various domains and are of different sizes. Some datasets are elaborately hand-crafted and thus relatively small in size, while others are created for large-scale weak supervision. The detailed descriptions of these tasks can be found in Appendix A.1. Next, we convert the different input data X of each task into a unified text-to-text format. For\ninstance, we linearize structured data (e.g., knowledge graph or table) by concatenating triples or key-value pairs using the special token \u201c[SEP]\u201d for data-to-text generation, and we utilize the special token \u201c[X_SEP]\u201d to separate answer and paragraph for question generation. The transformed input format for each task can be found in Appendix E. We divide MVPCorpus into two parts, which are used for pre-training and fine-tuning (evaluation), respectively. For supervised pre-training, we utilize 50 datasets from 7 tasks, including data-to-text generation, open-ended dialogue system, question answering, question generation, story generation, task-oriented dialogue system, and text summarization. We also eliminate pre-training examples overlapping with evaluation data to avoid data leakage (more details in Appendix A.2). Finally, we have a 25GB supervised pre-training corpus containing 32M examples. The statistics of the datasets for pre-training are listed in Table 9. For evaluation, we utilize the rest of the 27 datasets, which are more commonly used in the literature. Among these datasets, 23 datasets are from the 7 tasks used in pre-training. We refer to them as seen tasks and use them to test the effectiveness of our model. The remaining 4 datasets are from the tasks of commonsense generation, paraphrase generation, simplification, and style transfer, respectively. We call them unseen tasks and use them to examine the generality of our model.\n# 3.2 Model Architecture\nOur MVP model is built on the standard Transformer encoder-decoder architecture (Vaswani et al., 2017). Compared to decoder-only PLMs such as GPT-3 (Brown et al., 2020) and prefix LMs such as UniLM (Dong et al., 2019), the encoderdecoder architecture is more effective for text generation tasks (Raffel et al., 2020). In the first stage, we pre-train the MVP backbone using a mixture of labeled datasets from seven tasks. To indicate each task, we apply human-written instructions to each task instance. For example, we write \u201cSummarize:\u201d as the prompt for summarization tasks. The manual instructions for each task are shown in Appendix E. In the second stage, we freeze the MVP backbone and pre-train a set of task-specific prompts (i.e., continuous vectors) to stimulate the model\u2019s capacity to perform some specific task. Specially, we follow prefix-tuning (Li and Liang, 2021) to insert continuous vectors at each Transformer layer\nand learn them using a mixture of corresponding intra-task datasets (i.e., datasets under the same task2). Compared to prompt tuning (Lester et al., 2021), which only adds prompts to the input layer, layer-wise prompts are more effective and stable (Liu et al., 2022), especially for NLG tasks. These soft prompts, which are not shared between tasks, encode task-specific semantic knowledge to alleviate the blurring-out problem induced by multitask learning (He and Choi, 2021).\n# 3.3 Training Details\nOur MVP model adopts a Transformer with 12 layers in both the encoder and decoder (406M parameters), the same as the model size of BARTLARGE (Lewis et al., 2020). We initialize the backbone with the BART parameters to provide a good starting point for NLG tasks following previous work (Dong et al., 2019; Zhang et al., 2020). We pre-train the model with a batch size of 8,192 and adopt a temperature-scaled mixing strategy (Raffel et al., 2020) with a rate of T = 2 to mitigate the disparity in tasks and datasets. We follow prefix-tuning (Li and Liang, 2021) to pre-train task-specific prompts by prepending trainable vectors to multi-head attention modules at each layer. The prompt length is set to 100, and we utilize the MLP reparameterization function with a hidden size of 800 to improve the training robustness and performance (Li and Liang, 2021). Hence, every task prompts have approximately 62M parameters. Then, we freeze the MVP model and train seven groups of task-specific prompts, each of which corresponds to a different task. In the two stages, the maximum length of both input and output sequences is set to 1,024 for supporting examples to contain more tokens. We optimize the model with a constant learning rate of 3 \u00d7 10\u22125 using standard sequence-to-sequence cross-entropy loss. We apply the AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.98, \u03f5 = 1 \u00d7 10\u22126 to improve training stability (Liu et al., 2019b). The weight decay coefficient is 0.1. For testing, we select the checkpoint with the highest validation performance. All the experiments are conducted on 32 NVIDIA Tesla V100 32GB GPUs. We implement our model using the text generation library TextBox (Tang et al., 2022a).\nMethods\nCNN/DailyMail\nWebNLG\nSQuAD (QG)\nCoQA\nR-1\nR-2\nR-L\nB-4\nME\nR-L\nB-4\nME\nR-L\nF1\nEM\nMVP\n44.52\n21.62\n41.10\n67.82\n47.47\n76.88\n26.26\n27.35\n53.49\n86.43\n77.78\nBART\n44.16e\n21.28\n40.90\n64.55b\n46.51\n75.13\n22.00f\n26.40\n52.55\n68.60f\n\u2013\nFlan-T5\n43.45\n21.01\n40.03\n66.60\n46.93\n75.76\n25.55\n26.90\n53.51\n84.18\n75.44\nSingle\n44.36\n21.54\n40.88\n67.74\n46.89\n76.94\n26.09\n27.15\n53.29\n86.20\n77.26\nMVP+S\n44.63\n21.72\n41.21\n68.19\n47.75\n76.81\n25.69\n27.04\n53.20\n86.65\n77.93\nMVP+R\n44.14\n21.45\n40.72\n67.61\n47.65\n76.70\n25.71\n27.03\n53.09\n85.95\n77.22\nMVP+M\n43.97\n21.16\n40.46\n67.45\n47.57\n76.81\n25.46\n26.79\n52.95\n86.28\n77.26\nSOTA\n47.16a\n22.55\n43.87\n66.14b\n47.25\n76.10\n25.97c\n27.33\n53.43\n84.50d\n\u2013\nMethods\nROCStories\nPersonaChat\nMultiWOZ\nB-1\nB-2\nD-1\nD-4\nB-1\nB-2\nD-1\nD-2\nB-4\nSuccess\nInform\nMVP\n33.79\n15.76\n3.02\n75.65\n50.73\n40.69\n1.65\n11.23\n20.26\n76.40\n85.00\nBART\n30.70g\n13.30\n\u2013\n69.90\n49.90f\n40.00\n1.30\n8.00\n17.89j\n74.91\n84.88\nFlan-T5\n32.72\n15.23\n2.97\n68.97\n48.55\n40.22\n1.40\n7.85\n19.73\n70.20\n78.70\nSingle\n32.67\n15.29\n2.72\n72.97\n49.96\n40.53\n1.27\n7.63\n19.73\n75.60\n83.70\nMVP+S\n33.92\n15.60\n3.44\n80.58\n47.91\n39.97\n1.52\n9.54\n20.32\n79.90\n86.80\nMVP+R\n32.93\n15.32\n2.88\n73.83\n48.45\n40.09\n1.30\n7.95\n19.02\n73.30\n81.80\nMVP+M\n33.30\n15.51\n2.71\n74.24\n46.26\n39.30\n1.36\n8.07\n19.93\n72.70\n79.70\nSOTA\n33.40g\n15.40\n\u2013\n69.30\n49.90f\n40.00\n1.50h\n9.40\n20.50i\n85.30\n94.40\nTable 2: The main results on seven seen tasks under full tuning settings. The best and second-best results among all the methods are marked in bold and underlined, respectively. The SQuAD dataset here is used for the question generation task. The letters B, R, D, and ME denote BLEU, ROUGE, Distinct, and METEOR, respectively. \u201c\u2013\u201d means the work does not compute the corresponding result. a (Ravaut et al., 2022) b (Ke et al., 2021) c (Bao et al., 2021) d (Xiao et al., 2020) e (Lewis et al., 2020) f (Liu et al., 2021a) g (Guan et al., 2021) h (Chen et al., 2022) i (He et al., 2022) j (Lin et al., 2020c)\nIn summary, we pre-train a 406M generation model MVP and seven groups of 62M task-specific prompts. For each downstream task, users can either utilize the backbone (406M) directly or further combine MVP with task-specific prompts (468M).\n# 4 Experiment Results\nIn this section, we mainly investigate the effectiveness and generality of our MVP model. We conduct extensive experiments in different settings:\n\u2022 Under full tuning scenarios, we employ the 27 generation datasets and the GLUE benchmark (Wang et al., 2019) for evaluation. Section 4.1 and Appendix C analyze the results on 23 datasets from 7 seen tasks. Section 4.3 includes the results of 4 unseen generation tasks and 8 understanding tasks. To better compare with ExT5, we conduct experiments on the GEM benchmark (Gehrmann et al., 2021) in Appendix C.2. \u2022 In zero-shot learning, we compare our models with T0 in Section 4.2. \u2022 In parameter-efficient tuning settings, we utilize the same datasets as in Section 4.1, and the\n# results can be found in Section 4.4. \u2022 We conduct a human evaluation in Section 4.5.\nFor the full tuning setting (Tables 2 and 11), we fine-tune the entire model (including the backbone MVP and prompts), while for the parameterefficient tuning (Table 6), we only fine-tune prompts but freeze the parameter weights of MVP. We optimize the model via the seq2seq loss with label smoothing (Szegedy et al., 2016) factor of 0.1 and the AdamW optimizer with default hyper-parameters. We sweep over the batch size in {16, 64, 256} and the learning rate in {5 \u00d7 10\u22126, 1\u00d710\u22125, 3\u00d710\u22125} to find the optimal hyperparameters for each evaluation task. We utilize the checkpoint with the best validation performance for test set inference. During inference, we set the beam size to 5 and the no-repetitive ngram size to 3. Details regarding fine-tuning and evaluation can be found in Appendix B.\n# 4.1 Full Tuning Performance\nWe conduct experiments on seven new datasets of seven seen tasks to verify the effectiveness of our two-stage pre-training method. We design several\nMethods\nCNN/DailyMail\nWebNLG\nSQuAD (QG)\nCoQA\nR-1\nR-2\nR-L\nB-4\nME\nR-L\nB-4\nME\nR-L\nF1\nEM\nFT BART\n44.16\n21.28\n40.90\n64.55\n46.51\n75.13\n22.00\n26.40\n52.55\n68.60\n\u2013\nFT MVP\n44.52\n21.62\n41.10\n67.82\n47.47\n76.88\n26.26\n27.35\n53.49\n86.43\n77.78\nT0-3B\n\u2013\n\u2013\n\u2013\n01.40\n10.20\n18.43\n3.06\n12.43\n14.91\n13.30\n06.60\nT0-11B\n\u2013\n\u2013\n\u2013\n00.26\n06.13\n14.12\n2.63\n07.00\n15.25\n09.18\n04.36\nMVP\n29.50\n11.29\n25.92\n34.42\n31.33\n52.33\n2.90\n13.94\n15.48\n29.40\n18.20\nMVP+S\n25.60\n09.51\n22.67\n39.43\n34.32\n55.34\n2.96\n15.23\n18.23\n52.40\n37.30\nMethods\nROCStories\nPersonaChat\nMultiWOZ\nB-1\nB-2\nD-1\nD-4\nB-1\nB-2\nD-1\nD-2\nB-4\nSuccess\nInform\nFT BART\n30.70\n13.30\n\u2013\n69.90\n49.90\n40.00\n1.30\n8.00\n17.89\n74.91\n84.88\nFT MVP\n33.79\n15.76\n3.02\n75.65\n50.73\n40.69\n1.65\n11.23\n20.26\n76.40\n85.00\nT0-3B\n08.69\n3.02\n04.37\n35.49\n23.20\n23.57\n2.56\n12.06\n0.02\n2.50\n22.10\nT0-11B\n00.63\n0.16\n12.41\n92.86\n32.17\n28.35\n1.56\n07.19\n0.00\n3.90\n22.10\nMVP\n01.01\n0.31\n07.18\n86.26\n35.54\n32.71\n2.87\n16.38\n3.08\n2.50\n22.20\nMVP+S\n10.52\n3.54\n02.13\n69.55\n37.04\n33.38\n2.66\n14.84\n0.38\n2.50\n22.10\nFor the second stage that integrates single-task pre-trained prompts (denoted as MVP+S), we compare it with two variants using different prompts:\n\u2022 Randomly initialized prompts (MVP+R): The layer-wise prompts for the MVP model are randomly initialized without pre-training. \u2022 Multi-Task pre-trained prompts (MVP+M): We only pre-train one group of prompts for all tasks, using the same mixed datasets as in the backbone pre-training.\nBesides these variants, we further include the best-reported results from original papers in the literature for comparison (denoted as SOTA). From the results in Table 2, we can see that: First, supervised pre-training models (i.e., MVP, Flan-T5, and Single) achieve better performance than the unsupervised pre-trained model BART, yielding an average improvement of 9.3%, 3.13%, and 4.4% (in ratio), respectively. This finding verifies the effectiveness of our supervised pre-training method, which enables the model to acquire more task-specific information. Regarding multi-task pre-training (MVP) and single-task (Single), our MVP model outperforms its single-task counterparts by 5.0%. This result indicates that the multitask learning approach can enhance single-task performance by learning transferable semantic information across tasks. Notably, our MVP model outperforms Flan-T5 by 5.8%, which shows the significance of training on our NLG dataset collection, MVPCorpus. Second, task-specific prompt learning is effective to alleviate the \u201cblurring-out\u201d issue of multitask learning. For tasks such as data-to-text generation and question answering, MVP with the singletask prompt (MVP+S) consistently surpasses the other two variants (MVP+R and MVP+M). This verifies that task-specific prompts can acquire taskspecialized knowledge and stimulate the capacity of the MVP model to perform certain tasks. Finally, our supervised pre-training approach achieves five new SOTA results on data-to-text gen-\nAESOP\nQuora\nB-4\nR-1\nR-2\nR-L\nME\n+BART\n47.30a\n73.30\n54.10\n75.10\n49.70\n+MVP\n49.81\n74.78\n56.84\n76.34\n53.40\nSC & BLEU\nGYAFC E&M\nGYAFC F&R\nB-4\nAccuracy\nHM\nB-4\nAccuracy\nHM\n+BART\n76.50b\n93.70\n83.90\n79.30\n92.00\n85.20\n+MVP\n77.18\n94.49\n84.96\n79.43\n92.12\n85.31\n<div style=\"text-align: center;\">Table 4: The results of unseen NLG tasks. We use AESOP and SC & BLEU to denote the methods proposed by Sun et al. (2021) and Lai et al. (2021), respectively. a (Sun et al., 2021) b (Lai et al., 2021)</div>\nMethods\nCoLA\nSST-2\nMRPC\nSTS-B\nQQP\nMNLI\nQNLI\nRTE\nAverage\nMatt.\nAcc.\nF1/Acc.\nP/S Corr.\nF1/Acc.\nm./mm.\nAcc.\nAcc.\nBART\n60.30\n96.30\n90.47 / 86.70\n90.97 / 90.30\n73.03 / 89.87\n90.03 / 89.27\n94.60\n79.83\n85.17\nMVP\n59.87\n96.43\n92.07 / 89.43\n91.37 / 90.90\n73.20 / 90.13\n89.70 / 88.73\n95.10\n82.87\n85.88\neration, question generation, question answering, story generation, and open-ended dialogue tasks. We also achieve SOTA performance in six out of eight datasets in Table 11, which shows the strong text generation capability of our MVP model. As for the remaining tasks, the SOTA models incorporate tailored techniques, e.g., the re-ranking framework (Ravaut et al., 2022) and various task-specific objectives (He et al., 2022), which yield better performance. In contrast, our MVP model can produce competitive results just with a general architecture and a unified learning objective.\n# 4.2 Zero-shot Performance\nSince we do not pre-train MVP on the seven commonly used datasets, we further conduct zero-shot experiments to see the domain transfer abilities of our models. We include T0-3B and T0-11B (Sanh et al., 2022) as our baselines, which are large models trained on various downstream tasks. The results are listed in Table 3. We can observe that our small MVP model (406M) outperforms T03B and T0-11B in all metrics with a large margin, except for few metrics on ROCStories and MultiWOZ. This demonstrates the effectiveness of using supervised pre-training on our MVPCorpus. However, all tasks demonstrate that models in the zero-shot setting perform significantly worse than those with full tuning settings. This suggests that training strategies that are effective for NLU tasks may not produce satisfactory results for NLG tasks. Even though our model has acquired task knowledge, it struggles to perform well in a new domain without being fine-tuned. Hence, it is still necessary to develop specific NLG models for certain tasks and domains. Our MVP models can be effective models for further investigation.\n<div style=\"text-align: center;\">GYAFC F&R</div>\n# 4.3 Generality to Unseen Tasks\nIn this subsection, we test our MVP model on unseen NLG and NLU tasks to verify its generality.\nUnseen NLG Tasks. According to Deng et al. (2021), an NLG task can be assigned to one of the following three categories: compression (e.g., summarization), transduction (e.g., translation), or creation (e.g., story generation). Since we do not include any transduction tasks during pre-training, we evaluate our MVP model using two unseen transduction NLG tasks: paraphrase generation and text style transfer. We select the SOTA methods for these two tasks, i.e., AESOP (Sun et al., 2021) for paraphrase generation and SC & BLEU (Lai et al., 2021) for text style transfer, and replace their backbone BART with our MVP model for comparison. From the results in Table 4, we can see that our model outperforms BART by a ratio of 2.3% and achieves two new SOTA results, which verifies the strong generality of our model. This finding shows that our MVP model is more capable than BART and can serve as a general yet effective backbone. Unseen NLU Tasks. Although MVP is designed especially for NLG tasks, we also evaluate its performance on unseen NLU tasks using the widely used GLUE benchmark (Wang et al., 2019). We compare our model to BARTLARGE using its sequence classification method (Lewis et al., 2020). According to the results presented in Table 5, our MVP model outperforms BART on 9 of 12 metrics and has a superior overall performance of 0.71%. This result indicates the generality ability of our MVP model and further demonstrates that supervised pre-training not only learns generation ability but also improves overall semantic representations.\nMethods\nCNN/DailyMail\nWebNLG\nSQuAD (QG)\nCoQA\nR-1\nR-2\nR-L\nB-4\nME\nR-L\nB-4\nME\nR-L\nF1\nEM\nMVP+S\n43.03\n20.27\n39.72\n66.73\n47.42\n76.36\n25.28\n26.66\n52.69\n86.44\n76.84\nBART+R\n42.47\n19.82\n39.15\n65.54\n46.86\n75.24\n24.27\n26.07\n52.03\n82.22\n71.92\nMVP+R\n42.84\n20.21\n39.61\n66.12\n47.12\n75.83\n25.05\n26.34\n52.57\n85.51\n75.56\nMVP+M\n42.99\n20.36\n39.70\n66.40\n47.16\n75.89\n25.24\n26.49\n52.88\n85.90\n76.34\nFT BART\n44.16\n21.28\n40.90\n64.55\n46.51\n75.13\n22.00\n26.40\n52.55\n68.60\n\u2013\nFT MVP\n44.52\n21.62\n41.10\n67.82\n47.47\n76.88\n26.26\n27.35\n53.49\n86.43\n77.78\nMethods\nROCStories\nPersonaChat\nMultiWOZ\nB-1\nB-2\nD-1\nD-4\nB-1\nB-2\nD-1\nD-2\nB-4\nSuccess\nInform\nMVP+S\n32.94\n15.12\n2.98\n71.09\n47.11\n39.51\n1.39\n7.28\n19.24\n71.40\n77.80\nBART+R\n32.14\n14.71\n2.85\n68.94\n46.23\n38.98\n1.30\n6.82\n17.94\n62.20\n69.20\nMVP+R\n32.28\n14.85\n2.97\n70.29\n46.70\n39.23\n1.31\n6.98\n18.86\n64.40\n71.40\nMVP+M\n32.62\n15.28\n2.95\n69.58\n46.78\n39.40\n1.33\n7.13\n19.13\n67.20\n72.90\nFT BART\n30.70\n13.30\n\u2013\n69.90\n49.90\n40.00\n1.30\n8.00\n17.89\n74.91\n84.88\nFT MVP\n33.79\n15.76\n3.02\n75.65\n50.73\n40.69\n1.65\n11.23\n20.26\n76.40\n85.00\nTable 6: The results on seven seen tasks under parameter-efficient settings. We also include the results of BART and MVP under the full tuning setting (denoted as FT) for comparison.\n# 4.4 Parameter-Efficient Tuning Performance\nIn the lightweight fine-tuning setting, we only tune the prompts while freezing the backbone MVP model to verify its effectiveness in resourceconstrained situations. Besides our MVP+S model, we consider comparing the following methods:\nFrom the experimental results in Table 6, we can see that: the good performance of the MVP model in lightweight settings further demonstrates the effectiveness of supervised pre-training. By comparing two randomly initialized prompting methods (BART+R and MVP+R), we can see that MVP+R achieves superior performance to BART+R (+2.0%) due to its multi-task supervised backbone. Furthermore, when initialized with pretrained prompts, MVP+S and MVP+M achieve improved results over MVP+R, which is consistent with the findings of SPoT (Vu et al., 2022).\nDatasets\nMVP wins (%)\nTies (%)\nBART wins (%)\nCNN/DM\n46.50\n10.67\n42.83\nWebNLG\n32.17\n45.67\n22.17\nROCStories\n46.50\n11.33\n42.17\nPersonaChat\n35.33\n34.00\n30.67\nTable 7: Human evaluation on four tasks with Krippendorff\u2019s \u03b1 = 0.418, which measures the inter-annotator correlation of human judges.\nWhen compared with MVP+M, MVP+S performs marginally better by 1.2%, indicating that taskspecific prompts are useful to improve the model in generation tasks. Surprisingly, our lightweight MVP+S can even outperform fully tuned BART on tasks such as question generation and question answering, showcasing the effectiveness of the proposed supervised pre-training approach.\n# 4.5 Human Evaluation\nConsidering that there exists a certain gap between automatic metrics and human judgments (Sai et al., 2022), we further conduct a human evaluation to better demonstrate the generation capabilities of our MVP model. We compare MVP with BART on four tasks, including text summarization, datato-text generation, open-ended dialog system, and story generation. Following the practices of van der Lee et al. (2021), we utilize a stratified sample of 100 inputs of low, medium, and high word frequency for each task. We invite six human judges to evaluate the generated texts of MVP and BART. Then they need to choose which one is better or\nMethods\n#NLG (PT)\n#NLU (PT)\n#NLG (FT)\n#NLU (FT)\nSP model\nSP prompts\nOpen source\nFLAN\n3\n9\n2\n9\n\u2713\n\u2717\n\u2717\nT0\n2\n6\n0\n4\n\u2713\n\u2717\n\u2713\nMuppet\n1\n3\n1\n3\n\u2713\n\u2717\n\u2713\nExT5\n3\n8\n6\n8\n\u2713\n\u2717\n\u2717\nSPoT\n1\n4\n0\n6\n\u2717\n\u2713\n\u2717\nMVP (ours)\n7\n0\n11\n3\n\u2713\n\u2713\n\u2713\nTable 8: Comparison of MVP with existing supervised pre-training works. #NLG/#NLU are the number of NLG and NLU tasks, respectively. PT, FT, and SP denote pre-training, fine-tuning, and supervised pre-training, respectively.\nchoose a tie according to fluency, informativeness, consistency, task features, etc. More human evaluation details are listed in Appendix D. Table 7 showcases the proportions of \u201cMVP wins\u201d, \u201cTies\u201d, and \u201cBART wins\u201d for each dataset. From the results, we can see that MVP can generate overall better texts than BART from a human perspective.\n# 5 Discussion\nDifferences with Existing Methods. To the best of our knowledge, existing supervised pre-training works mainly focus on NLU tasks (Aghajanyan et al., 2021; Aribandi et al., 2022) or a small number of NLG tasks (Lin et al., 2020b; Su et al., 2022). Given the superior performance achieved by supervised pre-training approaches, it is important to explore supervised pre-training for deriving both effective and general NLG models. Our work makes a significant contribution in this direction, achieving SOTA performance with a single model on 13 of 17 datasets. Compared with its strong counterpart, ExT5 (Aribandi et al., 2022), our MVP model outperforms it in 26 out of 27 metrics (detailed in Appendix C.2). In order to better understand the difference between our work and previous supervised (multi-task) pre-training studies, we present a detailed comparison in Table 8. As we can see, our work conducts the study with the largest number of NLG tasks for both supervised pre-training and fine-tuning, incorporates task-specific prompts, and also releases all the important resources for reproducing or reusing our work.\nApplicability. To facilitate the application of our work, we have released the collection corpus, pretrained models, task-specific prompts, and generated texts. Our collected MVPCorpus is the largest NLG task collection, which can be a high-quality resource for recent LLMs (Zhao et al., 2023). We can use all the data to pre-train a general model or select a subset to continue pre-training a domain- or task-specific model (Gururangan et al., 2020) Our\nMVPCorpus can also be considered as the evaluation benchmark for different NLG tasks. Furthermore, our MVP model can be employed to achieve competitive results in various NLG tasks. Users can fine-tune the MVP model or integrate it with task-specific prompts based on sufficient labeled data. Notably, our MVP model can be directly employed to obtain good performance in zero-shot learning. In addition, our MVP model can provide effective parameter initialization for improving existing methods, as described in Section 4.3. Finally, the task-specific prompts and the generated texts can be further used to study the task similarity and their effect on the multi-task pre-training.\n# 6 Conclusion\nIn this paper, we present Multi-task superVised Pre-training (MVP) for natural language generation. Firstly, we collect a large-scale NLG corpus, MVPCorpus, from 77 datasets over 11 diverse NLG tasks. After converting various NLG tasks into a unified text-to-text format, we propose multi-task supervised pre-training to learn an effective and general model MVP with task-specific prompts for NLG tasks. Extensive experiments have demonstrated that: (1) supervised pre-training is beneficial for NLG tasks as an effective solution. Our MVP model outperforms its strong counterparts BART and Flan-T5 and even achieves SOTA performance on 13 out of 17 datasets; (2) supervised pre-trained models have strong generality on unseen generation or even understanding tasks. In future work, we will explore the multilingual version of our MVP model by covering more datasets in other languages. Such a model is expected to capture language-independent task characteristics and improve generation tasks in the minority language. Besides, it is interesting to study how different tasks relate to each other in the unified semantic space, which can inspire methods that incorporate task relations as prior.\n# Acknowledgements\nThis work was partially supported by National Natural Science Foundation of China under Grant No. 62222215, Beijing Natural Science Foundation under Grant No. 4222027, and Beijing Outstanding Young Scientist Program under Grant No. BJJWZYJH012019100020098. Xin Zhao is the corresponding author.\n# Limitations\nDespite our efforts to collect as many generation tasks and datasets as possible, we only evaluate the generation quality and generality of our models on a small number of tasks and datasets. The interpretability and robustness of our models require further analysis. Besides, there exists subjectivity when collecting downstream tasks and intratask datasets, albeit our attempts to employ widelyrecognized categorizations from the literature. Due to the limitation of computing power, we do not study the performance of our method at different model scales. The effectiveness of multi-task pretraining from scratch, similar to ExT5 (Aribandi et al., 2022), also merits an in-depth study.\n# Broader Impacts\nIn this paper, we pre-trained a language model MVP using labeled NLG datasets. According to the research (Bender et al., 2021; Bommasani et al., 2021), PLMs tend to \u201cremember\u201d what they have \u201cseen\u201d in the pre-training corpus. This could result in the reproduction of undesirable biases from pretraining data on downstream tasks. Training data intervention could be a solution to alleviate this issue (Lu et al., 2020). It is also interesting to investigate whether supervised pre-training produces fewer biases than unsupervised pre-training. Environmental impact is another factor we should consider. We attempt a more efficient pretraining strategy and released our PLM for future work. In contrast to large PLMs with tens of billions of parameters, such as T5 (Raffel et al., 2020) and GPT-3 (Brown et al., 2020), we pre-train only a small model with hundreds of millions of parameters. In addition, we utilize supervised pretraining data and initialize our model with pretrained BART, both of which improve the convergence of our model. Ultimately, our model is pretrained for about 20, 000 steps, whereas the BART of the same size is pre-trained for 500, 000 steps.\n# Reproducibility\nFor reproducing and reusing our work, we have released the collection MVPCorpus, the models (e.g., MVP, task-specific prompts, and multitask variants), intermediate results (e.g., the generated texts), and source codes for pre-training and fine-tuning at the link: https://github.com/ RUCAIBox/MVP. The detailed settings of the experiments are listed in Appendix B. We hope that these open-source resources will facilitate future work on supervised pre-training and contribute to the advancement of NLG research.\n# References\nEmily M. Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA. Association for Computing Machinery.\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. 2009. The fifth pascal recognizing textual entailment challenge. In In Proc Text Analysis Conference (TAC\u201909.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\nBill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young\nKim, and Andy Cedilnik. 2019. Taskmaster-1: Toward a realistic and diverse dialog dataset. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4516\u20134525, Hong Kong, China. Association for Computational Linguistics.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 179\u2013188, Vancouver, Canada. Association for Computational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ond\u02c7rej Du\u0161ek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo\u00e3o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96\u2013120, Online. Association for Computational Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1\u20139, Prague. Association for Computational Linguistics.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 70\u201379, Hong Kong, China. Association for Computational Linguistics.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinglang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek HakkaniT\u00fcr. 2019. Topical-chat: Towards knowledgegrounded open-domain conversations. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, pages 1891\u2013 1895. ISCA.\nDavid Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2003. English gigaword. Linguistic Data Consortium, Philadelphia, 4(1):34.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018. Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 708\u2013719, New Orleans, Louisiana. Association for Computational Linguistics.\ning Gu, Mostafa Mirshekari, Zhou Yu, and Aaron Sisto. 2021. ChainCQG: Flow-aware conversational question generation. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 2061\u20132070, Online. Association for Computational Linguistics.\nlong text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 781\u2013793, Online. Association for Computational Linguistics.\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural CRF model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7943\u20137960, Online. Association for Computational Linguistics.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The NarrativeQA reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284\u20132293, Minneapolis, Minnesota. Association for Computational Linguistics.\nMahnaz Koupaee and William Yang Wang. 2018. Wikihow: A large scale text summarization dataset. arXiv preprint arXiv:1810.09305.\nInformation Processing Systems, volume 30. Curran Associates, Inc.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland. Association for Computational Linguistics.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Rajen Subba. 2019. OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 845\u2013854, Florence, Italy. Association for Computational Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 839\u2013849, San Diego, California. Association for Computational Linguistics.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021. DART: Opendomain structured data record to text generation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 432\u2013447, Online. Association for Computational Linguistics.\nThong Nguyen, Anh Tuan Luu, Truc Lu, and Tho Quan. 2021. Enriching and controlling global semantics for text summarization. In Proceedings of the 2021\nConference on Empirical Methods in Natural Language Processing, pages 9443\u20139456, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. 2022. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations.\nof the 58th Annual Meeting of the Association for Computational Linguistics, pages 1970\u20131978, Online. Association for Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u2013 1083, Vancouver, Canada. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics.\n# Karl Stratos. 2019. Mutual information maximization\nKarl Stratos. 2019. Mutual information maximization for simple and accurate part-of-speech induction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1095\u20131104, Minneapolis, Minnesota. Association for Computational Linguistics.\nfor Computational Linguistics (Volume 1: Long Papers), pages 5039\u20135059, Dublin, Ireland. Association for Computational Linguistics.\non Empirical Methods in Natural Language Processing, pages 708\u2013713, Brussels, Belgium. Association for Computational Linguistics.\n# A Tasks and Datasets\n# A.1 Description of Tasks and Datasets\nWe provide the details of the tasks and datasets used in our paper for pre-training and fine-tuning in Tables 9 and 10. If the dataset for pre-training does not have a valid set, we divide 10% of the training set for validation. We list the licenses for all datasets if they have them. All datasets are publicly available. The majority of them can be directly downloaded from GitHub or Google Drive. ROCStories (Mostafazadeh et al., 2016) and CommonGen (Lin et al., 2020a) can be obtained after filling out a form. GYAFC (Rao and Tetreault, 2018) is accessible after requesting Yahoo and the authors of the dataset. The tasks and datasets we use in this paper are as follows:\n\u2022 Paraphrase generation involves rewriting a sentence with the same semantic meaning but a different syntactic or lexical form. We utilize the following datasets for fine-tuning evaluation:\n1. Quora (also known as QQP-Pos) (Kumar et al., 2020), which is a subset of Quora Question Pairs3.\n1. HotpotQA (Yang et al., 2018); 2. MS MARCO (Nguyen et al., 2016); 3. MSQG (Liu et al., 2021a); 4. NarrativeQA (Ko\u02c7cisk\u00fd et al., 2018); 5. NewsQA (Trischler et al., 2017); 6. QuAC (Choi et al., 2018). Most of them are QA tasks, and we invert the question and answer to enrich QG examples. We utilize the following datasets for fine-tuning evaluation: 1. CoQA (Reddy et al., 2019); 2. SQuAD (Rajpurkar et al., 2016), we utilize version 1.1.  Story generation creates a long and informative text with a short title. We use the following datasets for pre-training: 1. ChangeMyView (Hua and Wang, 2020); 2. English Gigaword (Rush et al., 2015); 3. Hippocorpus (Sap et al., 2020); 4. WikiPlots (Markriedl); 5. WritingPrompts (Fan et al., 2018), we split the original training set for pre-training and corresponding validation. Considering English Gigaword is a large summarization dataset, we use the summary as the title to generate the passage in turn to enrich the examples of story generation. We utilize the following datasets for fine-tuning evaluation: 1. ROCStories (Mostafazadeh et al., 2016); 2. WritingPrompts (Fan et al., 2018), we use the sets created by Guan et al. (2021) (who split the original valid and test sets for training, validation, and testing) to fine-tune our model for a fair comparison.  Task-oriented dialogue system meets the reallife needs of users, such as restaurant reservations and airplane bookings. We use the datasets for pre-training, following Su et al. (2022): 1. CamRest676 (Wen et al., 2017); 2. Frames (El Asri et al., 2017); 3. KVRET (Eric et al., 2017); 4. MetaLWOZ (Lee et al., 2019); 5. MSR-E2E (Li et al., 2018); 6. MultiWOZ (Budzianowski et al., 2018);\n# 7. Schema-Guided (Rastogi et al., 2020a); 8. TaskMaster (Byrne et al., 2019); 9. WOZ (Mrk\u0161i\u00b4c et al., 2017).\n# \u2022 Commonsense generation:\n1. CommonGen (CG) (Lin et al., 2020a). \u2022 Data-to-text generation: 1. DART (Nan et al., 2021); 2. E2E NLG cleaned (Novikova et al., 2017); 3. ToTTo (Su et al., 2021); 4. WebNLG (Gardent et al., 2017).\n# \u2022 Dialogue system:\n1. Schema-Guided Dialog (SGD) (Rastogi et al., 2020b). \u2022 Text simplification: 1. WikiAuto + Turk/ASSET (WiA-T/A) (Jiang et al., 2020; Xu et al., 2016; Alva-Manchego et al., 2020). \u2022 Text summarization: 1. Wiki-Lingua (WLE) (Ladhak et al., 2020). To test the generalization ability of our model, we also utilize the natural language standing benchmark GLUE (Wang et al., 2019), which is composed of three tasks:\n1. Schema-Guided Dialog (SGD) (Rastogi et al., 2020b).\n# \u2022 Text simplification:\n1. WikiAuto + Turk/ASSET (WiA-T/A) (Jiang et al., 2020; Xu et al., 2016; Alva-Manchego et al., 2020).\n# \u2022 Text summarization:\nTo test the generalization ability of our model, we also utilize the natural language standing benchmark GLUE (Wang et al., 2019), which is composed of three tasks:\n# \u2022 Natural language inference:\n# \u2022 Paraphrase detection:\n# \u2022 Text classification:\n1. CoLA (Warstadt et al., 2019); 2. SST-2 (Socher et al., 2013).\n# A.2 Data Leakage\nSince our model is pre-trained on a large number of labeled datasets, it may have \u201cseen\u201d examples from fine-tuning test sets during pre-training, which leads to an unfair comparison with other methods. Hence, we eliminate the pre-training examples that share n-gram overlap with either of the test datasets. Following Brown et al. (2020), n is the 5th percentile example length in words, and the maximum value of n is set to 13. Finally, we have removed 17, 848 examples from the pre-training datasets. The number of \u201ccleaned\u201d examples for each dataset can be found in Table 9.\nDataset\n#Train\nCleaned #Train\n#Valid\n#Test\nInput\nOutput\nLicense\nAGENDA\n38,720\n38,720\n1,000\n1,000\n52.1\n141.2\nN/A\nENT-DESC\n88,652\n88,652\n11,081\n11,081\n279.9\n31.0\nN/A\nGenWiki\n681,436\n681,436\n75,716\n1,000\n21.4\n29.5\nMIT\nLogicNLG\n28,450\n28,450\n4,260\n4,305\n178.4\n14.2\nMIT\nTEKGEN\n6,310,061\n6,307,995\n788,746\n796,982\n17.0\n21.2\nCC BY-SA 2.0\nWEATHERGOV\n25,000\n25,000\n1,000\n3,528\n148.7\n30.6\nN/A\nWikiTableT\n1,453,794\n1,452,778\n4,533\n4,351\n81.0\n99.7\nMIT\nCleaned OS Dialogs\n13,355,487\n13,355,368\n1,483,944\n-\n75.5\n16.7\nN/A\nCMUDoG\n82,818\n82,818\n5,555\n14,510\n433.0\n12.2\nN/A\nCuriosity\n64,930\n64,551\n8,539\n8,495\n144.4\n20.2\nCC BY-NC 4.0\nDREAM\n14,264\n14,242\n4,709\n4,766\n75.6\n13.6\nN/A\nEmpathetic Dialogues\n64,636\n64,636\n9,308\n8,426\n52.7\n12.9\nCC BY-NC 4.0\nMovie Dialog\n762,751\n762,711\n8,216\n8,066\n126.9\n44.0\nN/A\nMuTual\n33,691\n33,691\n4,090\n3,248\n53.6\n14.5\nN/A\nOpenDialKG\n69,680\n69,680\n7,743\n-\n54.2\n12.4\nCC BY-NC 4.0\nTopical-Chat\n179,750\n179,750\n22,295\n22,452\n223.3\n20.0\nCDLA-Sharing-1.0\nWizard of Wikipedia\n148,357\n147,702\n15,767\n15,564\n297.0\n16.7\nMIT\nHotpotQA\n90,447\n87,815\n7,405\n-\n187.9\n2.2\nCC BY-SA 4.0\nMS MARCO\n681,445\n681,226\n77,580\n-\n68.7\n13.3\nN/A\nMSQG\n198,058\n198,029\n11,008\n-\n48.1\n3.7\nCC BY-SA 4.0\nNarrativeQA\n65,494\n65,494\n6,922\n21,114\n584.1\n4.2\nApache 2.0\nNatural Questions\n96,676\n96,676\n10,693\n6,490\n9.0\n2.1\nCC BY-SA 3.0\nNewsQA\n97,850\n97,700\n5,486\n5,396\n726.8\n5.0\nMIT\nQuAC\n83,568\n83,485\n31,906\n-\n487.9\n12.5\nCC BY-SA 4.0\nTriviaQA\n78,785\n78,785\n8,837\n11,313\n14.0\n2.0\nApache 2.0\nWebQuestions\n8,933\n8,933\n4,863\n4,863\n6.7\n2.4\nCC BY 4.0\nHotpotQA\n90,440\n87,808\n6,972\n-\n79.6\n19.8\nCC BY-SA 4.0\nMS MARCO\n681,445\n681,226\n77,580\n-\n75.9\n6.0\nN/A\nMSQG\n198,058\n198,029\n11,008\n11,022\n45.9\n6.0\nCC BY-SA 4.0\nNarrativeQA\n65,494\n65,494\n6,922\n21,114\n579.7\n8.6\nApache 2.0\nNewsQA\n97,850\n97,700\n5,486\n5,396\n724.2\n7.6\nMIT\nQuAC\n69,109\n69,026\n26,301\n-\n496.7\n6.5\nCC BY-SA 4.0\nChangeMyView\n42,462\n42,459\n6,480\n7,562\n17.9\n104.1\nMIT\nEnglish Gigaword\n3,803,957\n3,802,620\n189,651\n1,951\n8.8\n33.3\nMIT\nHippocorpus\n6,168\n6,168\n686\n-\n34.1\n262.6\nCDLA-Permissive 2.0\nWikiPlots\n101,642\n101,641\n11,294\n-\n3.4\n338.5\nN/A\nWritingPrompts\n272,600\n272,518\n15,620\n15,138\n28.4\n630.8\nMIT\nCamRest676\n4,872\n4,872\n616\n-\n55.3\n9.4\nN/A\nFrames\n26,631\n26,631\n2,106\n-\n116.1\n13.0\nMIT\nKVRET\n14,136\n14,136\n1,616\n-\n30.5\n9.3\nN/A\nMetaLWOZ\n176,073\n176,073\n17,912\n-\n45.6\n8.0\nN/A\nMSR-E2E\n103,362\n103,362\n5,235\n-\n51.3\n12.8\nMicrosoft\nSchema-Guided\n494,946\n494,933\n73,089\n-\n120.8\n12.5\nCC BY-SA 4.0\nTaskMaster\n249,664\n249,662\n20,680\n-\n95.6\n12.0\nCC BY 4.0\nWOZ\n6,364\n6,359\n1,260\n-\n47.0\n10.6\nN/A\nEnglish Gigaword\n3,803,957\n3,802,620\n189,651\n1,951\n33.3\n8.8\nMIT\nMediaSum\n443,596\n442,021\n10,000\n10,000\n1641.0\n14.4\nN/A\nMSNews\n136,082\n135,937\n7,496\n7,562\n309.9\n9.8\nCC BY-SA 4.0\nNewsroom\n995,041\n989,351\n108,837\n108,862\n642.4\n26.7\nN/A\nWikiHow\n157,252\n157,247\n5,599\n5,577\n502.6\n45.6\nCC BY-NC-SA\nTable 9: The statistics and licenses of datasets for pre-training our MVP model. The #Train, #Valid, and #Test\nTable 9: The statistics and licenses of datasets for pre-training our MVP model. The #Train, #Valid, and #Test denote the number of examples in the train, valid, and test sets, respectively. Cleaned #Train represents the number of training examples after filtering. Input and Output are the average number of words (split by space) in the input and output sequences, respectively.\nTask\nDataset\n#Train\n#Valid\n#Test\nInput\nOutput\nLicense\nCommonsen generation\nCommonGen\n67,389\n993\n\u2013\n5.5\n11.6\nMIT\nData-to-text generation\nDART\n62,659\n2,768\n\u2013\n27.5\n21.5\nMIT\nE2E\n33,525\n4,299\n\u2013\n9.5\n20.6\nCC BY-SA 4.0\nToTTo\n120,761\n7,700\n\u2013\n37.8\n18.0\nCC BY-SA 3.0\nWebNLG\n34,338\n4,313\n4,222\n18.0\n19.9\nCC BY-NA-SA 4.0\nWebNLG (GEM)\n35,426\n1,667\n\u2013\n17.7\n22.7\nCC BY-NA-SA 4.0\nWikiBio\n582,659\n72,831\n72,831\n81.6\n26.1\nCC BY-SA 3.0\nOpen-ended dialogue\nDailyDialog\n76,052\n7,069\n6,740\n72.5\n13.9\nCC BY-NC-SA 4.0\nDSTC7-AVSD\n76,590\n17,870\n1,710\n148.2\n11.5\nMIT\nPersonaChat\n122,499\n14,602\n14,056\n132.1\n11.9\nMIT\nSGD\n164,982\n10,000\n\u2013\n134.7\n11.3\nCC BY-SA 4.0\nNatural language inference\nMNLI-m\n392,702\n9,815\n9,796\n29.8\n\u2013\nMixed\nMNLI-mm\n9,832\n9,847\nQNLI\n104,743\n5,463\n5,463\n36.6\n\u2013\nCC BY-SA 4.0\nRTE\n2,490\n277\n3,000\n51.0\n\u2013\nN/A\nParaphrase generation\nQuora\n137,185\n3,000\n3,000\n10.9\n10.8\nN/A\nParaphrase detection\nMRPC\n3,668\n408\n1,725\n43.8\n\u2013\nN/A\nQQP\n363,846\n40,430\n390,965\n22.3\n\u2013\nN/A\nSTS-B\n5,749\n1,500\n1,379\n20.3\n\u2013\nN/A\nQuestion answering\nCoQA\n107,286\n31,621\n\u2013\n349.4\n2.6\nMixed\nSQuAD\n75,722\n10,570\n11,877\n156.2\n3.6\nCC BY-SA 4.0\nQuestion generation\nCoQA\n107,286\n31,621\n\u2013\n346.6\n5.5\nMixed\nSQuAD\n75,722\n10,570\n11,877\n148.3\n11.6\nCC BY-SA 4.0\nStory generation\nROCStories\n176,688\n9,816\n4,909\n9.0\n40.7\nN/A\nWritingPrompts\n53,516\n4,000\n2,000\n25.5\n150.4\nMIT\nTask-oriented dialogue\nMultiWOZ\n170,220\n22,074\n22,116\n128.3\n11.3\nMIT\nText classification\nCoLA\n8,551\n1,043\n1,063\n7.7\n\u2013\nN/A\nSST-2\n67,349\n872\n1,821\n9.8\n\u2013\nN/A\nText simplification\nWiA-A\n483,801\n20,000\n359\n26.2\n21.5\nMixed\nWiA-T\n359\nText style transfer\nGYAFC-E&M\n52,595\n11,508\n1,416\n9.9\n10.6\nN/A\nGYAFC-F&R\n51,967\n11,152\n1,332\n10.7\n11.3\nText summarization\nCNN/DailyMail\n287,227\n13,368\n11,490\n679.8\n48.3\nMIT\nSAMSum\n14,732\n818\n819\n103.4\n20.3\nCC BY-NC-ND 4.0\nWLE\n99,020\n28,614\n\u2013\n367.6\n33.4\nCC0 1.0\nXSum\n204,045\n11,332\n11,334\n373.7\n21.1\nMIT\nTable 10: The statistics and licenses of datasets for evaluating our MVP model. The license of the MNLI dataset is\nTable 10: The statistics and licenses of datasets for evaluating our MVP model. The license of the MNLI dataset is composed of OANC, CC BY-SA 3.0, and CC BY 3.0. The license of the CoQA dataset is composed of CC BY-SA 4.0, MSR-LA, and Apache 2.0. The license of the WiA-A/T datasets is composed of CC BY-NC 3.0, CC BY-NC 4.0, and GNU General Public License v3.0.\nMethods\nXSum\nSAMSum\nCoQA QG\nR-1\nR-2\nR-L\nR-1\nR-2\nR-L\nB-4\nME\nR-L\nBART\n45.14d\n22.27\n37.25\n51.74b\n26.46\n48.72\n12.34c\n35.78\n46.88\nMVP\n45.60\n22.47\n37.42\n53.78\n29.12\n49.37\n23.48\n47.79\n55.09\nMVP+S\n45.67\n22.63\n37.50\n53.81\n29.75\n49.43\n23.43\n47.49\n55.25\nSOTA\n49.57a\n25.08\n41.81\n53.89b\n28.85\n49.29\n15.78c\n40.15\n50.98\nMethods\nWritingPrompts\nDailyDialog\nWikiBio\nB-1\nB-2\nD-1\nD-4\nB-1\nB-2\nD-1\nD-2\nB-4\nBART\n22.40e\n8.40\n\u2013\n31.30\n44.30f\n39.20\n3.90\n21.10\n\u2013\nMVP\n32.34\n13.11\n2.12\n64.58\n46.19\n41.81\n4.61\n25.06\n48.42\nMVP+S\n30.12\n11.46\n3.97\n83.70\n45.71\n42.92\n5.10\n27.14\n48.19\nSOTA\n22.40e\n8.40\n\u2013\n31.30\n46.10f\n40.70\n4.10\n22.20\n45.10g\nMethods\nDSTC7-AVSD\nSQuAD\nB-1\nB-2\nB-3\nB-4\nME\nR-L\nCIDEr\nF1\nEM\nBART\n82.40f\n69.10\n58.20\n48.70\n31.30\n63.50\n1.38\n91.56i\n84.23\nMVP\n83.75\n70.89\n60.19\n50.94\n32.12\n65.04\n1.45\n93.45\n87.20\nMVP+S\n83.81\n71.07\n60.45\n51.20\n31.77\n64.76\n1.44\n93.45\n87.17\nSOTA\n83.20f\n70.50\n59.80\n50.60\n31.40\n63.80\n1.39\n96.22h\n91.26\n# B Fine-tuning and Evaluation Details\nIn this section, we introduce the details for finetuning and evaluating each downstream task. For the experiments in Section 4 (Tables 2 and 6), and Appendix C (Table 11), the fine-tuning details are introduced in Section 4, and the evaluation details are presented as follows: \u2022 For data-to-text generation tasks, we use BLEU(4), ROUGE-L, and METEOR for evaluation. We use the script provided by Chen et al. (2020b)4; \u2022 For open-ended dialogue system tasks, we use BLEU-1, BLEU-2, Distinct-1, and Distinct-2 for evaluation. For DSTC7-AVSD, we also utilize CIDEr (Vedantam et al., 2015). We employ NLTK 3.5 with smoothing function 7 to compute BLEU for PersonaChat and DailyDialog and utilize the script5 to evaluate DSTC7-AVSD; \u2022 For question answering tasks, we use Exact Match (EM) and Macro-averaged F1 score (F1) for evaluation. We use the provided script for CoQA6 and SQuAD7. 4https://github.com/wenhuchen/ Data-to-text-Evaluation-Metric 5https://github.com/lemuria-wchen/DialogVED/ blob/main/src/utils/evaluate.py 6https://github.com/PaddlePaddle/ERNIE/blob/ repro/ernie-gen/eval/tasks/coqa/eval.py 7https://github.com/allenai/bi-att-flow/blob/\nMethods\nDART\nE2E\nToTTo\nB-4\nR-2\nME\nB-4\nR-2\nME\nB-4\nR-2\nME\nT5.1.1\n34.31\n45.22\n36.30\n42.57\n46.60\n38.20\n39.79\n49.90\n36.80\nExT5\n36.62\n48.14\n37.60\n42.25\n46.70\n38.10\n40.14\n50.33\n36.90\nMVP\n39.13\n48.92\n38.53\n37.38\n47.96\n39.39\n50.58\n55.24\n41.27\nMVP+S\n38.83\n48.49\n38.41\n37.32\n47.40\n38.90\n50.69\n55.52\n41.29\nMethods\nWebNLG\nCommonGen\nSGD\nB-4\nR-2\nME\nB-4\nR-2\nME\nB-4\nR-2\nME\nT5.1.1\n31.67\n43.31\n34.40\n8.38\n17.01\n20.20\n33.15\n36.17\n32.40\nExT5\n35.03\n48.17\n36.50\n9.68\n19.04\n21.40\n34.74\n37.77\n33.00\nMVP\n47.03\n59.00\n42.34\n32.59\n37.71\n33.00\n45.63\n48.29\n38.48\nMVP+S\n47.03\n59.03\n42.28\n34.10\n37.87\n33.11\n45.24\n48.25\n38.47\nMethods\nWiA-A\nWiA-T\nWLE\nB-4\nR-2\nME\nB-4\nR-2\nME\nB-4\nR-2\nME\nT5.1.1\n29.30\n38.37\n30.10\n42.12\n50.52\n36.2\n15.55\n20.47\n19.60\nExT5\n29.23\n37.98\n30.00\n41.39\n50.38\n35.8\n16.64\n21.16\n20.40\nMVP\n71.55\n70.88\n48.19\n91.73\n83.46\n57.34\n18.80\n22.84\n21.95\nMVP+S\n70.37\n70.65\n47.70\n91.12\n83.59\n56.95\n18.52\n22.57\n22.02\nare the same as above. We use BLEU-4, ROUGE2, and METEOR for evaluation. We use the GEM evaluation scripts11. For the experiments in Section 4.3 (Tables 4 and 5), the fine-tuning and evaluation details are as follows:\n\u2022 For paraphrase generation tasks, we employ the fine-tuning and evaluation scripts provided by AESOP (Sun et al., 2021)12. The evaluation metrics are BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L, and METEOR. \u2022 For text style transfer tasks, we employ the finetuning and evaluation scripts provided by SC & BLEU (Lai et al., 2021)13. We conduct the informal-to-formal transfer and train the model on the data from both the E&M and F&R domains following Lai et al. (2021). The evaluation metrics are BLEU-4, accuracy, and HM. Accuracy is calculated by a pre-trained TextCNN to evaluate the style strength, and HM denotes the harmonic mean of BLEU-4 and style accuracy (Lai et al., 2021). \u2022 For GLUE tasks, we utilize the fine-tuning code provided by Hugging Face14. The hyper-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4ed6/4ed658fd-3741-4ea7-a14b-dc4b491cbdcd.png\" style=\"width: 50%;\"></div>\nparameters are consistent with the original BART (Lewis et al., 2020)15. The evaluation is computed by the official website16.\n# C Additional Results\nIn this section, we provide additional results of our MVP model and other baselines.\n# C.1 Results of Common Datasets\nWe also conduct experiments on eight common datasets under full tuning settings. Due to space limitations in Section 4, these results are shown in Table 11. We can see that these results share a similar trend to those in Section 4, and we achieve SOTA performances in 6 of 8 datasets.\n# C.2 Results on the GEM Benchmark\nTo better compare with ExT5 (Aribandi et al., 2022), we conduct experiments on the GEM benchmark (Gehrmann et al., 2021). For \u201cunseen\u201d commonsense generation and text simplification tasks, we utilize prompts of data-to-text generation and summarization, respectively. The results are presented in Table 12, and our MVP models outperform ExT5 in 26 out of 27 metrics.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1d7a/1d7aed15-6b42-4ad9-8a62-8c32f2644367.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Human evaluation guidelines.</div>\n# D Human Evaluation\nWe hired six English-proficient college students with TOEFL or IELTS scores greater than 110 or 7.0. We paid 0.2$ per judge for each instance, for a total budget of 320$ for 400 instances. The text instructions we provided for each judge are shown in Figure 2.\n# E Qualitative Examples\nIn this section, we showcase",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of natural language generation (NLG) and the limitations of existing pre-trained language models (PLMs) that primarily use unsupervised methods. Previous models have shown that supervised pre-training with labeled data can significantly enhance performance, motivating the development of a new method that leverages a large-scale labeled dataset for NLG tasks.",
        "problem": {
            "definition": "The problem is the inefficiency and noise incorporation in unsupervised pre-training of PLMs for NLG tasks, leading to suboptimal performance in downstream applications.",
            "key obstacle": "The main challenge is the discrepancy between unsupervised pre-training and supervised fine-tuning, which hinders the effective learning of task-specific characteristics."
        },
        "idea": {
            "intuition": "The idea stems from the observation that supervised pre-training can provide more relevant task-related instructions earlier in the training process, potentially leading to better performance.",
            "opinion": "The proposed method, Multi-task superVised Pre-training (MVP), aims to unify various labeled NLG datasets into a general text-to-text format to enhance the model's capability to generate text across diverse tasks.",
            "innovation": "The key innovation lies in the combination of multi-task supervised pre-training with task-specific prompts, which improves the model's performance and generalization across multiple NLG tasks."
        },
        "method": {
            "method name": "Multi-task superVised Pre-training (MVP)",
            "method abbreviation": "MVP",
            "method definition": "MVP is a framework that pre-trains language models using a large-scale labeled corpus, MVPCorpus, which encompasses various NLG tasks formatted into a unified text-to-text structure.",
            "method description": "MVP leverages multi-task learning and supervised pre-training to enhance the performance of PLMs in NLG tasks.",
            "method steps": [
                "Collect a large-scale labeled dataset (MVPCorpus) from diverse NLG tasks.",
                "Convert task-specific data into a unified text-to-text format.",
                "Pre-train the model using the collected dataset in a supervised manner.",
                "Implement task-specific soft prompts to enhance model performance."
            ],
            "principle": "The effectiveness of MVP is supported by the principle that supervised pre-training allows for explicit learning of task-specific features, reducing the noise from unsupervised methods."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using MVPCorpus, which includes 77 datasets across 11 NLG tasks, with comparisons made against existing models like BART and Flan-T5.",
            "evaluation method": "Performance was assessed using standard metrics such as BLEU, ROUGE, and METEOR across various NLG tasks, with both full tuning and parameter-efficient settings evaluated."
        },
        "conclusion": "The MVP model demonstrates significant improvements in performance over existing models, achieving state-of-the-art results on 13 out of 17 datasets and showcasing strong generality for unseen tasks.",
        "discussion": {
            "advantage": "MVP effectively combines supervised pre-training with multi-task learning, leading to superior performance and generalization capabilities compared to traditional unsupervised methods.",
            "limitation": "Despite its strengths, the model's performance is evaluated on a limited number of tasks, and its interpretability and robustness require further investigation.",
            "future work": "Future research will explore multilingual versions of MVP and investigate the relationships between different tasks within a unified semantic framework."
        },
        "other info": {
            "Acknowledgements": "This work was supported by various grants from the National Natural Science Foundation of China and the Beijing Outstanding Young Scientist Program.",
            "Limitations": "The study acknowledges the potential biases in pre-training data and the environmental impact of model training."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of natural language generation (NLG) and the limitations of existing pre-trained language models (PLMs) that primarily use unsupervised methods."
        },
        {
            "section number": "1.2",
            "key information": "The main challenge is the discrepancy between unsupervised pre-training and supervised fine-tuning, which hinders the effective learning of task-specific characteristics."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, Multi-task superVised Pre-training (MVP), aims to unify various labeled NLG datasets into a general text-to-text format to enhance the model's capability to generate text across diverse tasks."
        },
        {
            "section number": "2.1",
            "key information": "MVP leverages multi-task learning and supervised pre-training to enhance the performance of PLMs in NLG tasks."
        },
        {
            "section number": "2.2",
            "key information": "The effectiveness of MVP is supported by the principle that supervised pre-training allows for explicit learning of task-specific features, reducing the noise from unsupervised methods."
        },
        {
            "section number": "3.1",
            "key information": "The MVP model demonstrates significant improvements in performance over existing models, achieving state-of-the-art results on 13 out of 17 datasets."
        },
        {
            "section number": "4.1",
            "key information": "MVP effectively combines supervised pre-training with multi-task learning, leading to superior performance and generalization capabilities compared to traditional unsupervised methods."
        },
        {
            "section number": "6.1",
            "key information": "The study acknowledges the potential biases in pre-training data and the environmental impact of model training."
        },
        {
            "section number": "6.3",
            "key information": "Despite its strengths, the model's performance is evaluated on a limited number of tasks, and its interpretability and robustness require further investigation."
        },
        {
            "section number": "6.4",
            "key information": "Future research will explore multilingual versions of MVP and investigate the relationships between different tasks within a unified semantic framework."
        }
    ],
    "similarity_score": 0.6140254515854645,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-15-0239_large/papers/MVP_ Multi-task Supervised Pre-training for Natural Language Generation.json"
}