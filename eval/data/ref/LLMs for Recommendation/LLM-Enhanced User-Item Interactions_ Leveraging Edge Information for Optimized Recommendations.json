{
    "from": "google",
    "scholar_id": "95o5vaLOd6IJ",
    "detail_id": null,
    "title": "LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations",
    "abstract": "The extraordinary performance of large language models has not only reshaped the research landscape in the field of natural language processing but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cuttingedge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of graph neural networks for mining relationships in graph data. Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. By enhancing the understanding of graph relationships by LLMs, our framework provides more comprehensive, accurate, and personalized recommendations. Our evaluation on real-world datasets demonstrates the framework\u2019s ability to understand connectivity information in graph data and to improve the relevance and quality of recommendation results. Our code is released at: https://github.com/anord-wang/LLM4REC.git",
    "bib_name": "wang2024llm",
    "md_text": "# LLM-Enhanced User-Item Interactions: Leveraging Edge Information for Optimized Recommendations\nXinyuan Wang Arizona State University xwang735@asu.edu Liang Wu Linkedin liawu@linkedin.com\nLiang Wu Linkedin liawu@linkedin.com\nXinyuan Wang Arizona State University xwang735@asu.edu\nHao Liu HKUST (Guangzhou) liuh@ust.hk Yanjie Fu Arizona State University yanjie.fu@asu.edu\nHao Liu HKUST (Guangzhou) liuh@ust.hk\n# ABSTRACT\nThe extraordinary performance of large language models has not only reshaped the research landscape in the field of natural language processing but has also demonstrated its exceptional applicative potential in various domains. However, the potential of these models in mining relationships from graph data remains under-explored. Graph neural networks, as a popular research area in recent years, have numerous studies on relationship mining. Yet, current cuttingedge research in graph neural networks has not been effectively integrated with large language models, leading to limited efficiency and capability in graph relationship mining tasks. A primary challenge is the inability of LLMs to deeply exploit the edge information in graphs, which is critical for understanding complex node relationships. This gap limits the potential of LLMs to extract meaningful insights from graph structures, limiting their applicability in more complex graph-based analysis. We focus on how to utilize existing LLMs for mining and understanding relationships in graph data, applying these techniques to recommendation tasks. We propose an innovative framework that combines the strong contextual representation capabilities of LLMs with the relationship extraction and analysis functions of graph neural networks for mining relationships in graph data. Specifically, we design a new prompt construction framework that integrates relational information of graph data into natural language expressions, aiding LLMs in more intuitively grasping the connectivity information within graph data. Additionally, we introduce graph relationship understanding and analysis functions into LLMs to enhance their focus on connectivity information in graph data. By enhancing the understanding of graph relationships by LLMs, our framework provides more comprehensive, accurate, and personalized recommendations. Our evaluation on real-world datasets demonstrates the framework\u2019s ability to understand connectivity information in graph data and to improve the relevance and quality of recommendation results. Our code is released at: https://github.com/anord-wang/LLM4REC.git\n# 1 INTRODUCTION\nWe aim to explore new methods for applying large language models to recommendation systems, especially when dealing with data containing a large amount of edge information. Our goal is to develop a recommendation system that can effectively utilize this edge information while integrating the powerful text-processing capabilities of LLM. Through this approach, not only can we improve the relevance and accuracy of recommendations, but we can also provide a richer and more personalized experience.\nLiangjie Hong Linkedin liahong@linkedin.com\nYanjie Fu Arizona State University yanjie.fu@asu.edu\nIn recent years, significant progress has been made in the development of LLMs such as BERT and GPT. These models demonstrate excellent performance in natural language understanding and generation, showing enormous potential in applications in multiple fields [7] [21]. With the advancement of technology, LLM\u2019s ability to handle complex text data continues to enhance, providing new solutions for various tasks and opening a new chapter in intelligent system research [2]. In this context, combining LLM with recommendation systems has become a cutting-edge and revolutionary research field [16]. Traditional recommendation systems focus on analyzing users\u2019 behavior data, while when combined with LLM, recommendation systems can understand explicit feedback from users and mine deeper into their implicit needs and preferences [27]. This combination provides new possibilities for improving the accuracy and satisfaction of recommendation systems. A major challenge in current recommendation systems is that although data contain a large amount of edge information (such as the relationship between users and items), this information is not fully utilized in LLMs, especially in its key attention mechanism [31]. Even though people have used this edge information to construct a variety of prompts [35], existing methods of building recommendation systems using LLMs cannot consider edge information structurally. This raises a research question: how to effectively integrate graph information within the framework of LLMs to improve the performance of recommendation systems? To overcome the challenges faced by existing recommendation systems in processing data containing a large amount of edge information, we propose an innovative prompt mechanism that can convert the connection relationship between users and items, as well as the background information of items, into natural language text. Our method considers the direct relationship between users and items and enriches the model\u2019s understanding by constructing a second-order relationship between items, a complex association not directly present in traditional recommendation data. At the same time, we also introduce an improved attention mechanism. This mechanism calculates spatial information between nodes in the graph based on the connection relationship between users and items and directly integrates this information into the computation of the attention mechanism as edge encoding. This method enhances the model\u2019s ability to process graph-structured data and greatly improves the accuracy and efficiency of recommendation systems through a deep understanding of the multidimensional relationships between users and items within the model. Through\nthese innovations, we can effectively integrate complex graph structure information into the LLMs framework, greatly improving the model\u2019s comprehensive understanding of users\u2019 behavior and items\u2019 characteristics. This enhances the model\u2019s ability to simulate realworld user-item interactions and lays a solid foundation for generating more accurate and personalized recommendation results. In our model, each recommendation is not only a simple response based on users\u2019 historical behavior but also an intelligent decisionmaking process that deeply and comprehensively considers users\u2019 preferences and items\u2019 attributes. To validate the effectiveness of our method, we conduct a series of experiments on different recommendation datasets, including validation experiments on the novel prompt mechanism and improved injection attention mechanism. These experiments demonstrate the performance improvement of our method on recommendation tasks and reveal its potential in handling complex user-item relationships. Our contributions are as follows:\n\u2022 An innovative way of combining LLMs with recommendation systems: By creatively integrating LLMs\u2019 deep text understanding ability with users\u2019 behavior analysis of recommendation systems, our method can more comprehensively understand users\u2019 and items\u2019 information. Our method effectively utilizes the language processing capabilities of LLMs, bringing new dimensions and depth to traditional recommendation logic, thereby ensuring recommendation quality while also increasing the diversity and innovation. \u2022 A new prompt strategy: We introduce a new prompt mechanism that can transform the relationship between users and items, as well as the background information of items, into natural language form. In addition, by constructing secondorder relationships between items, we can uncover deeper correlations between items, thereby providing more comprehensive and detailed recommendations. \u2022 A novel fusion method for edge information: We propose a new approach that directly embeds the edge information of graph data into the attention mechanism of LLMs. This method effectively utilizes the connection information in the graph structure, enhancing the model\u2019s ability to handle complex user-item interactions.\n# 2 PRELIMINARIES\n# 2.1 Important Definitions\nGenerative Large Language Models. Generative Large Language Models (LLMs) are a type of model based on transformer encoders that generate natural language texts [26]. LLMs are trained on massive text corpora and can capture broad contextual relationships between words. LLMs generate a series of words (\ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc5b) by modeling the joint probability of word sequences \ud835\udc43(\ud835\udc641,\ud835\udc642, . . . ,\ud835\udc64\ud835\udc5b). Token and Embedding. In NLP, tokens are the smallest units for LLMs, such as words, sub-words, or characters [33]. Embedding is a dense vector representation of a token in a continuous space that encodes language attributes and semantic information [24]. In recommendations, we see users and items as unique tokens. Prompt. Prompts are designed to guide generative large language models to generate specific responses. They serve as guides for the model to generate text tokens in specific contexts or styles [19]\n# 2.2 Problem Statement\nConsider the existence of \ud835\udc3cusers and \ud835\udc3ditems, let \ud835\udc4b\ud835\udc56\ud835\udc57be the binary interaction (e.g., purchase) matrix between user \ud835\udc56and item \ud835\udc57. Besides, we collect user descriptions, item descriptions (e.g., prices, brand, category, title), user reviews for items, explanations of user purchase reasons. We denote \ud835\udc47\ud835\udc56as the descriptions of the user \ud835\udc56, \ud835\udc47\ud835\udc57 as the descriptions of the item \ud835\udc57, \ud835\udc47\ud835\udc56\ud835\udc57as the joint texts of the user \ud835\udc56and item \ud835\udc57, such as user reviews and purchase reasons for items. We unify all textual descriptions into \ud835\udc47that includes \ud835\udc41sequences, \ud835\udc58indexes the tokens in each sequence, and \ud835\udc47\ud835\udc5b\ud835\udc58is the \ud835\udc58\u2212\ud835\udc61\u210etoken in the \ud835\udc5b\u2212\ud835\udc61\u210esequence. Our goal is to leverage LLMs and graphs to develop a generative recommendation system that takes a prompt including a userID and a user\u2019s historical interaction records with items, and generate product recommendations to the user.\n# 3 LEVERAGING LLM AND GRAPHS FOR RECOMMENDER SYSTEMS 3.1 Framework Overview\nGiven user descriptions, item descriptions, user textual reviews for items, and the user-item interaction (e.g., purchase, rating) graph, we aim to leverage and connect generative LLM, textual generation, and user-item interaction graph to advance recommender systems [35]. Figure 1 shows the four major steps for building our recommender system: 1) graph knowledge guided attentive LLM recommendation model backbone; 2) pre-training of graph attentive LLM with crowd contextual prompts; 3) fine-tuning of graph attentive LLM with personalized predictive prompts; 4) using fine-tuned graph attentive LLM for item recommendation. In Step 1, considering the existence of multi-source textual information, including user descriptions, item descriptions, and user reviews for items, we propose to leverage LLM to learn the generation of these texts in order to model the representations of user preferences and item functionalities. Aside from texts, the user-item interaction graph can provide two types of signals: i) user nodes and item nodes as tokens and ii) graph structure information about the direct (first-order) connectivity between users and items and indirect (second-order) connectivity among items, users, or useritem pairs. To leverage the user and item tokens, we add the user and item tokens to enrich the texts; to leverage the graph structure information, we develop a graph knowledge guided attentive LLM backbone, particularly with a new neural attentive decoder structure, to model the first-order and second-order connectivity graph knowledge. Our graph attentive LLM backbone strategically reformulates the user-item recommendation problem into a probabilistic generative problem in response to prompts. In Step 2, we develop crowd contextual prompts to pre-train the graph attentive LLM by maximizing textual generation likelihood. Specifically, we first construct the text prompts of all users and items including not just the texts of user descriptions, item descriptions, and user reviews for items, but also the fact or event checking texts of whether a user interacts with (e.g., rates or purchases) an item. We merge all the description, review, fact and event-based texts together as crowd contextual prompts. We then utilize the crowd contextual prompts to pre-train the graph attentive LLM backbone. The optimization goal of Step 2 is to maximize text generative likelihood, so the LLM can learn the contextual knowledge of a recommendation world. In Step 3, we develop personalized predictive prompts to incentivize\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/93ac/93ac3065-2009-4111-b54c-293d33722e9f.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a348/a348588b-0f23-4767-a2f6-af83db46b550.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\nthe pre-trained graph attentive LLM backbone to advance generative recommendation accuracy. Specifically, we convert a user\u2019s interaction (e.g., rating, purchases) history with all items into past tense texts, combined with a future tense trigger (e.g., user \ud835\udc56will purchase ? \u2212\u2212\u2212), to motivate the graph attentive LLM to recommend items. The optimization goal of Step 3 is to minimize recommendation errors, not textual generation likelihood. In Step 4, given a test user with the corresponding personalized rating or purchase history and a predictive trigger, we leverage the fine-tuned graph attentive LLM backbone to recommend items to the test user.\n# 3.2 Graph Structured Attentive LLM based Generative Recommendation Backbone\n3.2.1 GPT2 as LLM Base Model. Our base model is the GPT-2 [26]. The original GPT-2 utilizes the Transformer architecture, pre-trained on vast text datasets to predict subsequent words in sequences. With a multi-layer structure containing attention heads, it scales up to billions of parameters for enhanced pattern recognition. The model supports conditional text generation and offers various sampling strategies for generating text. In GPT-2, the attention mechanism is to weigh the importance of different words in a sentence. It operates by computing attention scores for each word in the input sequence based on their relevance to each other. These scores determine how much attention the model should pay to each word when generating the next word in the sequence. By attending to relevant parts of the input text, GPT-2 can capture long-range dependencies and generate coherent and contextually relevant output.\n3.2.2 Integrating Two Structure Knowledge for Graph Attentive LLM. When performing generative recommendations, we obtain recommendation results in the form of text generation to connect items to users. As a result, users and items are usually regarded as tokens in a text sequence for pre-training. In the real world, users interact (e.g., rate, purchase) with items. Such interactions can be modeled as a graph where users or items are seen as nodes, and user-user, itemitem, user-item connectivity is seen as edge weights, representing\n<div style=\"text-align: center;\"></div>\na kind of graph-structured information propagation-driven edge knowledge. In other words, user and item tokens are not just simply independent entities in a sequence. The LLM should not just learn user and item embeddings by paying attention to their mutual relevance in a sequence. It is critical to leverage the graph-structured edge knowledge to improve LLM for recommendations. Firstly, we propose a graph structure knowledge attentive LLM method in order to integrate graph knowledge into recommendation systems. Specifically, we incorporate the edge connectivity between users and items into attention weight calculation [5, 37]. Our idea is to leverage Graph Neural Networks (GNNs) to describe the relationships between nodes by modeling their connectivity (direct relationship) and spatial information (indirect relationship) in the graph. Formally, the edge information, denoted by the \ud835\udc45-term is used to calculate the attention weights in the graph-structured attention mechanism, which is given by:\n(1)\n\u221a\ufe01 where \ud835\udc44, \ud835\udc3e, and \ud835\udc49are queries, keys, and values respectively, while \ud835\udc45represents the relationship encoding extracted from graph knowledge. The \u221a\ufe01 \ud835\udc51\ud835\udc58is the dimensionality (size) of the key vector to enforce a normalization effect. Secondly, we identify two kinds of important graph structural knowledge: 1) the direct (first-order) connectivity and 2) the indirect (high-order path) connectivity, among users and items. Correspondingly, the graph structured relational attention \ud835\udc45term is composed of two distinct parts of the graph topology:\n# \ud835\udc45= \ud835\udc45conn + \ud835\udc45path.\n(2)\nThe first part, \ud835\udc45conn, represents the direct connection relationships between nodes. Typically, we denote \ud835\udc45conn as a binary adjacency\n\ud835\udc45\ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc5b \ud835\udc56\ud835\udc57 = \ufffd 1, if there is a direct connection between node \ud835\udc56and \ud835\udc57 0, otherwise\n(3)\nIn this matrix, 1 indicates there is an edge between two nodes, and 0 indicates there is no edge between two nodes. The second part, \ud835\udc45path, represents a normalized shortest path score between nodes, which is computed based on the entire graph. The shortest path information is essential because it reflects the indirect relationships of node pairs and the degrees of separation or distance between nodes, which can be highly informative for understanding complex graph structures. The normalized shortest path score is calculated using the shortest path matrix \ud835\udc43, where each element \ud835\udc43\ud835\udc56\ud835\udc57is defined as the minimum path length among all possible paths from node \ud835\udc56to node \ud835\udc57. The \ud835\udc43\ud835\udc56\ud835\udc57is given by: \ud835\udc43\ud835\udc56\ud835\udc57= min{path length|all paths from node \ud835\udc56to node \ud835\udc57}. Later, we introduce a damping factor \ud835\udeffin order to adjust the influence of distant nodes. This is achieved by inverting and normalizing the path lengths in the matrix. The modified shortest path matrix \ud835\udc45path is defined as:\n(4)\n() In this formulation, \ud835\udeffis a value between 0 and 1, and max(\ud835\udc43) represents the maximum path length in the shortest path matrix \ud835\udc43. The normalization step ensures that \ud835\udc45path \ud835\udc56\ud835\udc57 remains within the range of 0 to 1. \ud835\udc45path \ud835\udc56\ud835\udc57 captures the proximity between nodes in the graph. Shorter paths (indicating closer connections) result in higher values. Integrating the direct connections and indirect relationships between nodes into a unified representation \ud835\udc45, the attention mechanism is empowered to model the inherent characteristics of individual nodes and their relative positions and interconnections within the overall graph.\n# 3.3 Pre-training Graph Attentive LLM with Crowd Contextual Prompts Pre-training is to initially train the of our graph attentive G\n# 3.3 Pre-training Graph Attentive LLM with\nPre-training is to initially train the of our graph attentive GPT-2 model on a larger corpus of recommendation text data, including data collection, tokenization, model architecture, pre-training objective, and optimization procedure.\n3.3.1 Data Collection. We first collect a large textual corpus from diverse sources: user descriptions, item descriptions, user reviews for items, historical events that users interact (e.g., rate or purchase) with items, in order to ensure that the graph attentive LLM model learns robust representations of languages.\nthat we define the unique structure of our crow contextual prompts for pre-training a LLM recommender. User/Item Tokens. Our prompts include two unique tokens: user\nembed rich semantic information about users, such as user profiles, demographics, reviews, and preferences, information propagated from items, and rich semantic information about items, such as item descriptions, item functionalities, item reviews, information propagated from users.\nIn this way, we aim to augment the prompt texts and enrich the contextual environment that simulates the preference, characteristics, functionality, categorization, opinion, and first-order and second-order social or network dimensions of a real recommender system in a language modality. These crowd contextual prompts are used as training data to pre-train our graph attentive LLM.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ffc/0ffc4fbc-5b00-46b0-bcac-55391a0a61f9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Crowd Contextual Prompts based on Recommendation Data.</div>\n3.3.3 The Optimization Objective and Procedures of Pre-training. Given the prepared crowd contextual prompts as pre-training data, we will train the graph attentive GPT-2 to predict the next token in the textual sequence. The optimization objective is to maximize the\ntoken generation likelihood of a textual sequence. The likelihood function is given by: \u2211\ufe01\n(5)\nwhere\ud835\udc47\ud835\udc56,\ud835\udc47\ud835\udc56\u22121, . . . ,\ud835\udc471 is the first\ud835\udc56tokens, and \u0398 indicates the weights of the LLM, the objective is to predict the \ud835\udc56+1 token\ud835\udc47\ud835\udc56+1. By solving the optimization objective, the graph attentive LLM can combine knowledge from different information sources to learn more comprehensive portraits of users and items. By integrating graph structured attention, the graph attentive LLM can model the first-order and second-order edge connectivity among users and items.\n# 3.4 Fine-tuning Graph Attentive LLM with Personalized Predictive Prompts After the pre-training step, the graph attentive LLM learn\nAfter the pre-training step, the graph attentive LLM learns the contextual knowledge of users, items, relationships of a recommender system\u2019s world. However, the optimization objective in the pre-training step is focusing on maximizing textual generation accuracy in a language sequence, instead of recommending personalized items. Therefore, in the fine-tuning step, we develop 1) personalized predictive prompts and 2) recommendation loss functions of fine-tuning to incentivize the graph attentive LLM to shift model focuses from text generation to generating accurate personalized item recommendations.\n3.4.1 Personalized Predictive Prompts. When fine-tuning the pretrained graph attentive LLM, we introduce the personalized predictive Prompt method. Our idea is to use the historical purchase events of users for items as prompts to guide the LLM to learn user preferences for items. Figure 3 shows that we convert a user\u2019s interaction (e.g., rating, purchases) history with all items into past tense texts, combined with a future tense trigger (e.g., user \ud835\udc56will purchase ?), to motivate the graph attentive LLM to generate item recommendations for a user.\n# Figure 3: A Personalized Predictive Prompt.\n3.4.2 The Optimization Objective and Training Procedure of Finetuning. Different from learning the world knowledge of a recommendation system, the fine-tuning stage is to adapt the pre-trained LLM to personalized item recommendations. During the fine-tuning process, we will integrate personalized predictive prompts into LLM so that the model can model the historical item purchase event of a user to generate a list of items \ud835\udc45\ud835\udc56to the specific user, based on the prompt. The generative recommended items are then compared with the actual purchase records of the specific user. The resulting loss function derived from this comparison is utilized as the optimization objective of fine-tuning. In particular, we define the generative probability that measures whether LLM recommendations are statistically close to historical user purchase records,\nwhich is given by Equation 6:\n\u2211\ufe01 where \ud835\udc43\ud835\udc5f\ud835\udc56is the prompt for fine-tuning stage, and \ud835\udc45\ud835\udc56is the final recommendation list, \u0398 denotes the LLM weights. In summary, fine-tuning is to optimize the recommendation loss.\n# 3.5 Graph Attentive LLM for Item\n# Recommendations in Deployment re-training and fine-tuning, given a testing user\nAfter pre-training and fine-tuning, given a testing user \ud835\udc56, we convert the user\u2019s interaction records with items into a personalized predictive prompt as the input of the graph attentive LLM. Therefore, the classic recommendation engine in production can be viewed as a wrapper, where the request is reconstructed as a prompt - the same as the prompt in the fine-tuning stage, and the graph attentive LLM will provide a recommendation score for each user-item pair. In order to reduce the serving latency in production and alleviate the peak load pressure, the proposed graph attentive LLM model can be deployed onto both offline and online GPU clusters. The offline pipeline focuses on the batch processing and calculates the relevance between a user and the candidate items to recommend, shown as LLM(\ud835\udc4b\ud835\udc56\ud835\udc57; \u0398), where \ud835\udc4b\ud835\udc56\ud835\udc57is the user-item interaction prompt of user \ud835\udc56and item \ud835\udc57, and \u0398 denotes the weights for the LLM backbone. The batch processing can directly be applied in offline recommendation scenarios such as promotional emails and notifications. The results can also be used for warming up online cache to minimize redundant computations. In a typical online recommendation scenario, the prompt containing user, item and the interaction information is sent to the graph attentive LLM model, and the top \ud835\udc40items with the highest scores are selected as recommendations by comparing the probability scores against each other, \ufffd\ufffd\n(7)\n\ufffd\ufffd In this case, the proposed method can easily be integrated into most common recommender systems in industry. The additional pressure caused by GPU serving can be handled by offline (batch) precomputation and online caching warm-up.\n# 4 EXPERIMENTAL RESULTS\nWe conduct empirical experiments to answer the following questions: 1) can our method generate more accurate recommendations? 2) what are the contributions of different technical components? 3) what are the contributions of second order relationship and item background information? 4) what are the impacts of different attention mechanisms? 5) parameter sensitivity and robustness.\n# 4.1 Experimental Setup\n4.1.1 Data Description. We used seven public recommendation datasets: Amazon (AM)-Beauty dataset, AM-Toys dataset, AMSports, AM-Luxury, AM-Scientific, AM-Instruments dataset [20]. We binarized the user-item interaction matrix by scores. If the score is greater than 3, there is a connection between a user and an item. For each user in the dataset, we randomly select 80% of interactions for training, 10% for validation, and 10% for testing, with at least one sample selected in both the validation and test sets. According\nto the prompt construction method in Section 3.3, we constructed the data for pretraining. Table 1 shows The main dataset statistics.\n<div style=\"text-align: center;\">Table 1: Dataset Statistics</div>\nDataset\nUser\nItem\nInteraction\nContent\nAM-Beauty\n10,553\n6,086\n94,148\n165,228\nAM-Toys\n11,268\n7,309\n95,420\n170,551\nAM-Sports\n22,686\n12,301\n185,718\n321,887\nAM-Luxury\n2,382\n1,047\n21,911\n15,834\nAM-Scientific\n6,875\n3,484\n50,985\n43,164\nAM-Instruments\n20,307\n7,917\n183,964\n143,113\nAM-Food\n95,421\n32,180\n834,514\n691,543\n4.1.2 Evaluation Metrics. We used three metrics: Recall@20, Recall@40, and NDCG@100 to evaluate algorithmic effectiveness. Recall@k [32] indicates the proportion of items that users are interested in among the top-\ud835\udc58recommended items:\n(8)\nNDCG@k is a position-sensitive indicator that measures the quality of recommendation lists:\n(9)\nwhere, DCG@k = \ufffd\ud835\udc58 \ud835\udc56=1 2\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc56\u22121 log2(\ud835\udc56+1) and IDCG@k = \ufffd|\ud835\udc45\ud835\udc38\ud835\udc3f| \ud835\udc56=1 2\ud835\udc5f\ud835\udc52\ud835\udc59\ud835\udc56\u22121 log2(\ud835\udc56+1)\n4.1.4 Hyper parameters and Settings. We conducted experiments using GPT-2 as the base model. We set the maximum input length to 1024, the token embedding dimension to 768, and the vocabulary length of natural language tokens to 50257. In the pre-training stage, we first trained 10 epochs using crowd contextual data to optimize LLM and then trained 100 rounds using user-item interaction data. In the fine-tuning stage, we used 50 epochs for recommendationoriented fine-tuning of LLM.\n4.1.5 Experimental Environment. All experiments were conducted on Ubuntu 22.04.3 LTS OS, Intel(R) Core(TM) i9-13900KF CPU, with the framework of Python 3.11.5 and PyTorch 2.0.1.\n# 4.2 Experimental Results\n4.2.1 Overall Comparison. This experiment aims to answer: Can our model really generate more accurate recommendation results through the natural language processing method? We compared our model with several baseline models on various Amazon datasets. The baseline models used for comparison include ID-based and Attention-based methods. Our model was tested on the same dataset as these baseline models to ensure fairness and accuracy in the comparison. The experimental results are shown in Table 2, and our model performs well in seven Amazon datasets. Recall@20, Recall@40, and NDCG@100 are superior to the baseline models. This indicates that LLMs have strong capabilities in understanding text, and capturing user preferences and needs, thereby promoting the accuracy of recommendations. Overall, the experimental results support our hypothesis that our model can generate more accurate recommendations through the graph attentive LLM. This discovery is important for research and the practical application of recommendation systems.\nWe pre-trained and fine-tuned each baseline model separately, and then compared it with our complete model. These pre-training and fine-tuning experimental settings are consistent and conducted on the same dataset to ensure comparability of results. Table 2 shows the specific contribution of each component to the overall performance of the model. For example, the performance of LLMNoPretrain is significantly lower than that of the complete model. This implies that using recommendation related graph data and\n<div style=\"text-align: center;\">Table 2: Comparison Between Our Model and Baselines on Three Amazon Review Datasets.</div>\nDataset\nMetric\nMulti-VAE\nMD-CVAE\nBERT4Rec\n\ud835\udc463Rec\nUniSRec\nFDSA\nSASRec\nGRU4Rec\nLLM-\nNoPretrain\nLLM-\nNoFineTune\nLLM-\nNoGKIA\nLLM-\nNoGHIP\nOurs\nAM-Beauty\nRecall@20\n0.1295\n0.1472\n0.1126\n0.1354\n0.1462\n0.1447\n0.1546\n0.0997\n0.0464\n0.0441\n0.1225\n0.1267\n0.1590\nRecall@40\n0.1720\n0.2058\n0.1677\n0.1789\n0.1898\n0.1875\n0.2071\n0.1528\n0.0709\n0.0691\n0.1665\n0.1799\n0.2177\nNDCG@100\n0.0835\n0.0835\n0.0781\n0.0867\n0.0907\n0.0834\n0.0949\n0.0749\n0.0339\n0.0323\n0.0790\n0.0827\n0.1029\nAM-Toys\nRecall@20\n0.1076\n0.1291\n0.0853\n0.1064\n0.1110\n0.0972\n0.0869\n0.0657\n0.0477\n0.0580\n0.0896\n0.0858\n0.1349\nRecall@40\n0.1558\n0.1804\n0.1375\n0.1524\n0.1457\n0.1268\n0.1146\n0.0917\n0.0689\n0.1003\n0.1272\n0.1179\n0.1873\nNDCG@100\n0.0781\n0.0844\n0.0532\n0.0665\n0.0638\n0.0662\n0.0525\n0.0439\n0.0330\n0.0481\n0.0612\n0.0594\n0.0876\nAM-Sports\nRecall@20\n0.0659\n0.0714\n0.0521\n0.0616\n0.0714\n0.0681\n0.0541\n0.0720\n0.0449\n0.0394\n0.0555\n0.0558\n0.0764\nRecall@40\n0.0975\n0.1180\n0.0701\n0.0813\n0.1143\n0.0866\n0.0739\n0.1086\n0.0719\n0.0613\n0.0846\n0.0830\n0.1240\nNDCG@100\n0.0446\n0.0514\n0.0305\n0.0438\n0.0504\n0.0475\n0.0361\n0.0498\n0.0322\n0.0278\n0.0391\n0.0379\n0.0535\nAM-Luxury\nRecall@20\n0.2306\n0.2771\n0.2076\n0.2241\n0.3091\n0.2759\n0.2550\n0.2126\n0.1872\n0.1885\n0.2474\n0.2679\n0.3066\nRecall@40\n0.2724\n0.3206\n0.2404\n0.2672\n0.3675\n0.3176\n0.3008\n0.2522\n0.2233\n0.2254\n0.2880\n0.3028\n0.3441\nNDCG@100\n0.1697\n0.2064\n0.1617\n0.1542\n0.2010\n0.2107\n0.1965\n0.1623\n0.1223\n0.1235\n0.1834\n0.2065\n0.2331\nAM-Scientific\nRecall@20\n0.1069\n0.1389\n0.0871\n0.1089\n0.1492\n0.1188\n0.1298\n0.0849\n0.0708\n0.0668\n0.1383\n0.1206\n0.1480\nRecall@40\n0.1483\n0.1842\n0.1160\n0.1541\n0.1954\n0.1547\n0.1776\n0.1204\n0.1037\n0.0960\n0.1822\n0.1575\n0.1908\nNDCG@100\n0.0766\n0.0872\n0.0606\n0.0715\n0.1056\n0.0846\n0.0864\n0.0594\n0.0568\n0.0465\n0.0940\n0.0810\n0.1072\nAM-Instruments\nRecall@20\n0.1096\n0.1398\n0.1183\n0.1352\n0.1684\n0.1382\n0.1483\n0.1271\n0.0766\n0.0727\n0.1387\n0.1426\n0.1698\nRecall@40\n0.1628\n0.1743\n0.1531\n0.1767\n0.2239\n0.1787\n0.1935\n0.1660\n0.1004\n0.0948\n0.1741\n0.1779\n0.2265\nNDCG@100\n0.0735\n0.1040\n0.0922\n0.0894\n0.1075\n0.1080\n0.0934\n0.0998\n0.0500\n0.0478\n0.1042\n0.1044\n0.1312\nAM-Food\nRecall@20\n0.1062\n0.1170\n0.1036\n0.1157\n0.1423\n0.1099\n0.1171\n0.1140\n0.0224\n0.0204\n0.1275\n0.1264\n0.1438\nRecall@40\n0.1317\n0.1431\n0.1284\n0.1456\n0.1661\n0.1317\n0.1404\n0.1389\n0.0299\n0.0274\n0.1559\n0.1487\n0.1673\nNDCG@100\n0.0727\n0.0863\n0.0835\n0.0926\n0.1024\n0.0904\n0.0942\n0.0910\n0.0153\n0.0141\n0.0898\n0.0963\n0.1119\nnatural language data for pre-training plays a crucial role in improving model performance. Similarly, the results of LLM-NoFineTune demonstrate the importance of fine-tuning. Subsequently, by comparing the performance of LLM-NoGKIA, and LLM-NoGHIP with that of the complete model, we find that the addition of graph connection information in attention calculation and complex prompts containing second-order relationships is crucial for improving the performance of recommendation systems.\nBy comparing the performances of these models, we quantified the impact of the second-order relationships and the background information of items on recommendation accuracy. Figure 4 shows that the model that uses prompt sentences of complete information (with the second-order relationship) performs best over all the performance indicators. The performances of the \"Without second-order relationship\" model are lower than that of the complete model. As can be seen, second-order relationship information is an essential component of graph connectivity. Similarly,\nthe \"Without Item\" model performs poorly, highlighting the importance of natural language background information in enhancing recommendation systems.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5334/5334225b-686f-4f12-acf1-e2faaedc4745.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Results of Different Prompt Structures.</div>\n4.2.4 Study on Different Attention Injection Ways. This experiment aims to answer: Is the connection information in the attention calculation process of the GPT-2 model really that important? To answer this question, we used the AM-Toys and AM-Beauty datasets. The experimental design included three different attention mechanisms: \u2022 Reasonable Injection: Injecting meaningful connection information into the attention mechanism. \u2022 Meaningless Injection: Set all connection information of the attention mechanism to 1, without considering actual connection strength or relationships. \u2022 Normal Attention: Maintain the normal attention mechanism of the GPT-2 model without any injection.\nFigure 5 shows that the model using our graph attentive LLM method exhibits the best performance. Our method not only considers the direct connections between nodes but also the spatial relationships (i.e., the shortest connected path) between nodes in the graph. We compared our method with regular attention mechanisms, and the experimental results clearly support this point.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d169/d169229e-94fe-432f-954c-c1f948b395e7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) AM-Beauty</div>\n# Figure 5: Results of Different Attention Injection Ways\n<div style=\"text-align: center;\">Figure 5: Results of Different Attention Injection Ways</div>\nTo avoid bias that may arise from adding input only between user/project tokens, we introduced a comparison with fixed additive attention. We found that simply adding fixed connection information to attention calculation for nodes in the graph is not effective. It is truly effective to include information that reflects the actual relationships between nodes.\n4.2.5 Study of Parameters. This experiment aims to answer: Can we ensure consistency between our pre-training and fine-tuning tasks? We conducted experiments on the AM-Toys dataset to analyze the performance alignment between the pre-training task and the fine-tuning task. We used the results of the first 10 pre-training epochs and the corresponding loss function. Then, we fine-tuned the pre-trained model to obtain evaluation metrics. We compared the 3 metrics, Recall@20, Recall@40, and NDCG@100 with the loss function. Figure 6 shows the trend of changes in the 3 metrics is consistent with the trend of changes in the loss functions. This indicates that our pre-training task and fine-tuning task are wellaligned, and our prompt construction method can provide rich information for subsequent recommendation tasks.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b4a/0b4a7e6d-3724-4853-a2c2-673cf4d76405.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Results of Different Training Epochs.</div>\nThe application of LLMs in recommendation systems mainly contains two types: deep representation of data using LLMs, and direct application of generative LLMs to construct recommendation logic. In deep representation, discriminative language models like BERT are widely used for fine-tuning and pre-training, integrating specific domain data features to enhance the performance of recommendation systems. For instance, U-BERT [25] leverages contentrich domain data to learn user representations, compensating for the scarcity of behavioral data. Similarly, UserBERT [34] includes two self-supervised tasks for pretraining on unlabeled behavior data. Additionally, BECR [36] combines deep contextual token interactions with traditional lexical word matching features. Notably, the \"pretrain-finetune\" mechanism plays a crucial role in sequence or session-based recommendation systems, like BERT4Rec [30] and RESETBERT4Rec [40]. UniSRec [9] develops a BERT fine-tuning framework that links item description texts. In content-based recommendations, especially in the news domain, models like NRMS [34],\nTiny-NewsRec [38], and PREC [18] enhance news recommendations by leveraging LLMs, addressing domain transfer issues or reducing transfer costs. Research by Penha and Hauff [22] shows that BERT, even without fine-tuning, effectively prioritizes relevant items in ranking processes, illustrating the potential of large language models in natural language understanding. In recent studies, generative LLMs have shown huge potential in recommendation systems through prompting and tuning methods. Notable works and advancements include: Liu et al. [17] conducted a comprehensive assessment of ChatGPT\u2019s performance in five key recommendation tasks. Sanner et al. [28] designed three different prompt templates to evaluate the enhancement effect of prompts, finding that zero-shot and few-shot strategies are particularly effective in preference-based recommendations using language. Sileo et al. [29] and Hou et al. [10] focused on designing effective prompt methods for specific recommendation tasks. Gao and team [6] developed ChatREC around ChatGPT, an interactive recommendation framework that understands user needs through multiple rounds of dialogue. Petrov and Macdonald [23] introduced GPTRec, a generative sequence recommendation model based on GPT-2. Kang and colleagues [13] explored formatting user historical interactions as prompts and assessed the performance of LLMs of different scales. Dai et al. [3] designed templates for various recommendation tasks using demonstration example templates. Bao et al. [1] developed TALLRec, which demonstrates the potential of LLMs in recommendation domains through two-stage fine-tuning training. Ji et al. [11] presented GenRec, a method that leverages the generative capabilities of LLMs to directly generate the target of recommendations. In specific scenarios like online recruitment, generative recommendation models such as GIRL [41] and reclm [4] demonstrated enhanced explainability and appropriateness in recommendations. Li et al. (2023e) [14] described user behaviors and designed prompts in news with PBNR. Wang et al. [22] proposed UniCRS, a design based on knowledge-enhanced rapid learning.\n# 6 CONCLUSION\nWe tackle a key issue in recommendation systems: how to integrate LLM and graph structures into recommendations. To this end, we propose a graph attentive LLM generative recommender system. By introducing new prompting methods and graph structured attention mechanisms, we can effectively integrate the complex relationships and background information between users and items into the model. We first design a natural language prompt that can reflect the relationship between users and items and embedded the 2-order relationship between items into it. Next, we improved the attention mechanism of LLM to model complex graph structure information. Through experiments, we validate the effectiveness of our method. The experimental results show that our model has significantly improved recommendation accuracy and personalization compared to traditional recommendation systems. Considering these innovations, our approach provides a new technological path for developing more efficient and intelligent recommendation systems. Meanwhile, these methods demonstrate new perspectives and ideas in applying LLM to recommendation systems and a wider range of fields. This promotes the development of recommendation systems and provides strong support and inspiration for using LLM in various complex application scenarios.\n# REFERENCES\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the integration of large language models (LLMs) with recommendation systems, focusing on the underutilization of edge information in graph data. Previous methods have not effectively combined LLMs with graph neural networks, limiting their efficiency in relationship mining tasks. This work proposes a new framework to leverage LLMs for mining relationships in graph data, particularly for enhancing recommendation tasks.",
        "problem": {
            "definition": "The problem is the ineffective utilization of edge information in recommendation systems powered by LLMs, which hinders their ability to understand complex user-item relationships.",
            "key obstacle": "The main challenge is that existing LLMs fail to exploit edge information in graphs, crucial for understanding the intricate relationships between nodes, thereby limiting their effectiveness in recommendation scenarios."
        },
        "idea": {
            "intuition": "The idea is inspired by the need to better understand user-item relationships through the integration of LLMs with graph structures, enhancing the recommendation process.",
            "opinion": "The proposed idea involves creating a new prompt construction framework that incorporates relational information from graph data into natural language, allowing LLMs to better grasp connectivity.",
            "innovation": "The innovation lies in the introduction of a novel prompt mechanism and an improved attention mechanism that embeds edge information directly into the LLM's processing, enhancing its capability to handle complex relationships."
        },
        "method": {
            "method name": "Graph Attentive LLM",
            "method abbreviation": "GALLM",
            "method definition": "GALLM is a generative recommendation system that combines LLMs with graph structures to enhance the accuracy and personalization of recommendations by incorporating edge information.",
            "method description": "The core of the method involves using a graph-structured attention mechanism within LLMs to process user-item interactions more effectively.",
            "method steps": [
                "Graph knowledge guided attentive LLM recommendation model backbone.",
                "Pre-training of graph attentive LLM with crowd contextual prompts.",
                "Fine-tuning of graph attentive LLM with personalized predictive prompts.",
                "Using fine-tuned graph attentive LLM for item recommendation."
            ],
            "principle": "This method is effective because it integrates complex graph structures into LLMs, allowing for a deeper understanding of user-item relationships and enhancing the quality of recommendations."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using seven public recommendation datasets, including Amazon datasets, with a focus on user-item interaction matrices and textual descriptions.",
            "evaluation method": "The performance was assessed using metrics such as Recall@20, Recall@40, and NDCG@100, comparing the proposed method against various baseline models."
        },
        "conclusion": "The proposed graph attentive LLM generative recommender system effectively integrates LLMs with graph structures, significantly improving recommendation accuracy and personalization. The innovations in prompting and attention mechanisms provide a new technological path for developing advanced recommendation systems.",
        "discussion": {
            "advantage": "The key advantages of the proposed approach include enhanced understanding of user-item relationships, improved recommendation accuracy, and the ability to incorporate complex relational information.",
            "limitation": "One limitation is the dependency on the quality and completeness of the graph data, which may affect the performance of the recommendation system.",
            "future work": "Future research directions may include exploring alternative graph structures, refining the attention mechanism, and applying the method to other domains beyond recommendations."
        },
        "other info": {
            "code repository": "https://github.com/anord-wang/LLM4REC.git"
        }
    },
    "mount_outline": [
        {
            "section number": "4.1",
            "key information": "The proposed method, Graph Attentive LLM (GALLM), integrates LLMs with graph structures to enhance the accuracy and personalization of recommendations by incorporating edge information."
        },
        {
            "section number": "4.2",
            "key information": "GALLM utilizes a graph-structured attention mechanism within LLMs to process user-item interactions more effectively, enhancing the integration of LLMs into recommendation systems."
        },
        {
            "section number": "3.2",
            "key information": "The proposed framework leverages LLMs for mining relationships in graph data, which is crucial for enhancing recommendation tasks through improved semantic understanding."
        },
        {
            "section number": "10.2",
            "key information": "Future research directions may include exploring alternative graph structures, refining the attention mechanism, and applying the method to other domains beyond recommendations."
        },
        {
            "section number": "1.1",
            "key information": "The integration of LLMs with graph structures significantly improves the understanding of complex user-item relationships, highlighting the importance of recommendation algorithms in modern applications."
        },
        {
            "section number": "2.3",
            "key information": "Previous methods have not effectively combined LLMs with graph neural networks, limiting their efficiency in relationship mining tasks."
        }
    ],
    "similarity_score": 0.7861560197312952,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/93ac/93ac3065-2009-4111-b54c-293d33722e9f.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a348/a348588b-0f23-4767-a2f6-af83db46b550.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0ffc/0ffc4fbc-5b00-46b0-bcac-55391a0a61f9.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5334/5334225b-686f-4f12-acf1-e2faaedc4745.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d169/d169229e-94fe-432f-954c-c1f948b395e7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0b4a/0b4a7e6d-3724-4853-a2c2-673cf4d76405.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-10-1935_recom/papers/LLM-Enhanced User-Item Interactions_ Leveraging Edge Information for Optimized Recommendations.json"
}