{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.13772",
    "title": "On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning",
    "abstract": "Following the standard supervised fine-tuning (SFT) paradigm, in-context learning (ICL) has become an efficient approach propelled by the recent advancements in large language models (LLMs), yielding promising performance across various tasks in few-shot data setups. However, both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration), especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration, as well as their interplay. Through extensive controlled experiments, we find that simultaneous gains for both task performance and calibration are difficult to achieve, and the problem of miscalibration exists across all learning methods in low-resource scenarios. To address this challenging trade-off between performance and calibration, we then investigate the potential of self-ensembling techniques applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies). We justify the feasibility of self-ensembling on SFT in addition to ICL, to make the predictions more calibrated and have comparable or even better performance. Our work sheds light on which learning paradigm to choose and how to enhance both task performance and calibration of LLMs.",
    "bib_name": "li2023taskperformancemodelcalibration",
    "md_text": "# On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning\nChengzu Li1, Han Zhou1, Goran Glava\u01612, Anna Korhonen1, Ivan Vuli\u00b4c1 1Language Technology Lab, University of Cambridge 2Center for Artificial Intelligence and Data Science, University of W\u00fcrzburg {cl917, hz416, iv250, alk23}@cam.ac.uk goran.glavas@uni-wuerzburg.de\n# Abstract\nFollowing the standard supervised fine-tuning (SFT) paradigm, in-context learning (ICL) has become an efficient approach propelled by the recent advancements in large language models (LLMs), yielding promising performance across various tasks in few-shot data setups. However, both paradigms are prone to suffer from the critical problem of overconfidence (i.e., miscalibration), especially in such limited data setups. In this work, we deliver an in-depth analysis of the behavior across different choices of learning methods from the perspective of both performance and calibration, as well as their interplay. Through extensive controlled experiments, we find that simultaneous gains for both task performance and calibration are difficult to achieve, and the problem of miscalibration exists across all learning methods in low-resource scenarios. To address this challenging trade-off between performance and calibration, we then investigate the potential of self-ensembling techniques applied at different modeling stages (e.g., variations of in-context examples or variations in prompts or different ensembling strategies). We justify the feasibility of self-ensembling on SFT in addition to ICL, to make the predictions more calibrated and have comparable or even better performance. Our work sheds light on which learning paradigm to choose and how to enhance both task performance and calibration of LLMs.\n  22 Dec 2023\n# 1 Introduction\nMachine learning and NLP have undergone a significant transformation recently, largely propelled by large language models (LLMs) (Radford et al., 2019; Brown et al., 2020; Chowdhery et al., 2022; OpenAI, 2023). Among different learning paradigms, Supervised Fine-Tuning (SFT) and InContext Learning (ICL) have emerged as predominant methodologies (Raffel et al., 2020; Dong et al., 2022), demonstrating commendable efficacy across\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f3f7/f3f7a7f8-d449-4d66-9ed8-7ea8995136b3.png\" style=\"width: 50%;\"></div>\nFigure 1: Illustration of the self-ensembled learning methods. We introduce different types of variations to the input and feed them to a single language model. After having the predictions, we run self-ensembling to obtain final predictions and their confidence. many tasks. SFT tunes the model\u2019s parameter and effectively specializes a (general-purpose) model to specific tasks by learning the knowledge in the training data and optimizing the objective. ICL, for each input, leverages the few-shot examples (i.e., the so-called demonstrations) to generate predictions without tuning model parameters and treating the model as a \u2018black box\u2019. Considering the different input format between training with SFT and inference with ICL, Min et al. (2022) and Chen et al. (2022) introduce the in-context examples into training phrase, which we call supervised in-context learning (SICL). However, when the demonstrations as a strong inductive bias get combined with SFT, it has been shown to become more likely to fall into the problem of overconfidence (Desai and Durrett, 2020; Jiang et al., 2021), and the predicted confidence distribution of ICL may be miscalibrated due to the bias in in-context examples (Fei et al., 2023). Through our extensive experiments, we observe that both paradigms, SFT and ICL, suffer from the problem of miscalibration in low-resource scenarios.\nThe important challenges of overconfidence and miscalibration, particularly in scenarios marked by limited data availability, underscore the need for a nuanced understanding of these paradigms. However, most of the previous work (Mosbach et al., 2023; Sun et al., 2023) only focus on comparing solely the performance of SFT and ICL on out-of-distribution (OOD) data, targeting generalpurpose LLMs. Here, we instead focus on studying task-specialized language models, where the behavior of different paradigms\u2019 in-task performance along with their calibration remains an open research question. In addition, considering the possible issue of overconfidence and miscalibration, we pose and study another crucial research question: is it possible to ensure both in-task performance and well-calibrated LM behavior at the same time? Satisfying both requirements is critical to the application of the model in real-life setups: an applied system should provide both accurate and calibrated (trustworthy) predictions to be responsible. To address the challenges above, in this paper we first investigate the performance and calibration of different model-tuning and ICL methods, along with their interplay, on 7 classification datasets in limited data setups. Our empirical investigations unveil a task-dependant phenomenon of in-task performance and calibration with regard to whether the corpus has been seen by the model before, which also gains increasing attention over the research community in data contamination (Zhu et al., 2023; Deng et al., 2023). We further observe that most of the results show relatively high calibration errors, which are in contrast with responsible LLM applications. In response to the possible miscalibration in lowresource settings, we introduce and explore the application of self-ensembling, inspired by the effect of ensembling of multiple independent models in improving the reliability of the final predictions (Ovadia et al., 2019). We incorporate diverse variations in in-context examples and prompts, tailor them to different learning methods, and find that self-ensembling fortifies the model\u2019s calibration without compromising performance. The overall flow of the self-ensembling method is illustrated in Figure 1. This strategic approach not only contributes to reducing the calibration error of model predictions by 43% on average but also streamlines resource utilization and time expenditures by focusing on modifications to a single model.\nContributions. 1) We deliver a comprehensive empirical analysis with different choices of learning methods across a variety of tasks in limited data scenarios. 2) We show the task-dependent relationship between in-task performance and calibration of LLMs and provide practical guidelines for the choice of learning paradigms (\u00a76.1). 3) We investigate and justify the feasibility of the self-ensembling method in enhancing both the performance and calibration of LLMs (\u00a76.2). We release the code at https://github.com/ cambridgeltl/ensembled-sicl.\n# 2 Related Work\nLearning Paradigms. Fine-tuning (FT) pretrained LMs has been used as an effective method to adapt them to specific tasks and datasets (Devlin et al., 2019; Raffel et al., 2020). With the generation of much larger and more powerful LMs (Brown et al., 2020; Chung et al., 2022), parameter-efficient finetuning (PEFT) (He et al., 2022; Zhou et al., 2023b) has been proposed, where the central idea is to tune only a fraction of the model parameters to reduce computation and memory costs. Without tuning the model, in-context learning (ICL) has shown great potential, achieving promising performance on various tasks with demonstration examples (Brown et al., 2020). Motivated by the positive results of ICL by concatenating multiple in-context examples to the LLM input at inference, Min et al. (2022) and Chen et al. (2022) introduce labeled in-context examples to the supervised training process (SICL). This has been further improved and utilized with pretraining (Gu et al., 2023; Shi et al., 2023) and other training strategies (Ye et al., 2023b,a; Wei et al., 2023). Sun et al. (2023) study task performance and stability with different PEFT methods in addition to ICL, including prompt tuning, and instruction tuning (IT) (Singhal et al., 2022). Duan et al. (2023) explore the relationship between ICL and IT, and interpret ICL as implicit IT. Mosbach et al. (2023) compare the generalization ability of ICL and FT on out-of-distribution (OOD) data. Zhou et al. (2023c) show the superior performance of hard prompt tuning over standard FT in low-data scenarios. However, previous work has not explored all the learning methods systematically within low-data scenarios and has not investigated them through the joint optics of in-task performance, confidence, and their trade-offs.\nCalibrating LLMs. A well-calibrated LM should be accurate in terms of performance while also producing reliable confidence estimates for their predictions. When concatenating different in-context examples with various labels, tokens, and example ordering, ICL would be influenced by the bias in the concatenated examples (Zhao et al., 2021), and various calibration methods have been proposed to mitigate this issue (Wang et al., 2023a; Zhou et al., 2023a). Moreover, the model itself also contains certain \u2018implicit\u2019 label biases due to its (pre)training corpus, which would have an effect on the confidence estimation as well (Fei et al., 2023). Furthermore, FT may suffer from miscalibration for both in-distribution and OOD data due to overparameterization when adapting to the specific data (Kong et al., 2020), and the miscalibration effect, as we investigate in this paper, might be even more pronounced in limited data scenarios. Ensembling Model Predictions has been used to mitigate the problem of overconfidence and improve the reliability of the final model predictions (Ovadia et al., 2019). A standard ensembling practice is to train the model with different hyper-parameters or different initialization (Wenzel et al., 2020; Lakshminarayanan et al., 2017). Concerning LLMs, Sun et al. (2022) ensemble fine-tuned LLMs to quantify the uncertainty with disagreement among different ensembling components. Gleave and Irving (2022) and Wang et al. (2023b) ensemble partially tuned LLMs, considering the computation resources for training and the storage for saving different LLM checkpoints. Although the proposed method achieves better and more reliable results, it takes a considerable amount of time, computation, and storage resources to train and save multiple models, which often makes them inapplicable to LLMs. In contrast to having several tuned models with supervised learning, Yao et al. (2023) demonstrate the feasibility of using self-ensembling in ICL without tuning the model. In this paper, we explore self-ensembling in the novel context of diverse learning paradigms and low-data setups and demonstrate that it is possible to improve model calibration without compromising performance.\n# 3 Background: Learning Paradigms\nIn this paper, we analyze and compare four different learning paradigms in low-resource scenar-\nios: zero-shot learning (ZSL),1 in-context learning (ICL), supervised fine-tuning (SFT) and supervised in-context learning (SICL). With classification tasks in focus, we briefly describe each paradigm in what follows. Zero-Shot Learning (ZSL). Given the input x and the prompting template fp, the prediction \u02c6y from the LM can be represented as \u02c6y = arg maxj P(yj|fp(x)), where the parameters of the underlying LM are fixed. The prompting template fp(x) includes the task instructions and special symbols which can be replaced by the input x. We attach the prompting templates for different classification tasks in Appendix D. In-Context Learning (ICL). Similar to ZSL, instead of only feeding the input x to the model, we first prepend M in-context examples (IC) (also called demonstrations) [fp(x1), y1; ...; fp(xM), yM] to the input x. The examples are retrieved from the pool of examples R following (random or non-random) selection strategy. The prediction is then defined as \u02c6y = arg maxj P(yj|[fp(xIC), yIC], fp(x)). Supervised Fine-Tuning (SFT). As mentioned, ZSL and ICL are inference-only paradigms treating the LM as a black box. On the other hand, FT first trains the model on the training set following the input format fp(x) from ZSL. Note that here we use SFT to refer to instruction-style finetuning with a prompting template. Inference with the tuned model P\u2032 is then conducted in the same way as with ZSL. Both during training and inference, we can use different prompting templates to create variations in the model input, which we further elaborate on in \u00a74.1. Supervised In-Context Learning (SICL). Based on the propositions from Min et al. (2022) and\n# Supervised In-Context Learning (SICL). Based\non the propositions from Min et al. (2022) and Chen et al. (2022), we can also fine-tune the model to directly optimize the in-context learning objective. For each training step, M in-context examples (x1, y1), ..., (xM, yM) are selected from the pool R. We then prepend the selected in-context examples to the input x as before with ICL and use the concatenation as the final model input, and train the model to generate y. Inference proceeds in the same way as with ICL, except that we now use the task-tuned model P\u2032.\n1In the context of ZSL and later ICL there is no actual \u2019learning\u2019 taking the place, and the model simply reacts to the provided prompt, but we have chosen the term ZSL for consistency with previous literature.\n# 4 Methodology\nThere are two points that create possible variations that can be used for ensembling: 1) variation in the selection of in-context examples (for ICL and SICL), and 2) variation in the chosen prompt (for all the paradigms). Previous work focuses on 1) selecting a better combination of in-context examples (Su et al., 2023) for the model or 2) generating an optimal prompting template (Zhou et al., 2023c). On the other side, how the variation of multiple demonstration combinations and prompting templates influences the model behavior is still unexplored. Furthermore, we can 3) \u2018self-ensemble\u2019 the model based on different ensembling strategies. We now introduce all these variants.\n# 4.1 Variation of Ensembling Components\nVariation of In-Context Examples (Var-IC). For ICL and SICL, IC examples and their ordering [fp(xIC), yIC] create variations in the model inputs with a fixed template fp, while not impacting the test pair fp(x) \u223cy. This allows us to create various in-context example combinations as different inputs to a single model and obtain different ensemble components. Variation of Prompting Templates (Var-Prompt). Different prompting templates have shown high variance in task performance (Mishra et al., 2022). By changing the wording in the templates fp, we can also create variations even with the same input to the model. For each input x, we randomly select a prompting template f\u2032 p from a set of available prompting template candidates. In ICL and SICL, the same template is also applied to the in-context examples, formatting the final input as [f\u2032 p(x1), y1; ...; f\u2032 p(xM), yM, f\u2032 p(x)]. This makes it applicable not only to ICL and SICL, but also to ZSL and SFT as well. Variation of Both (Var-Both). When we create a set of ensembling components, we can also combine these two variations.\n# 4.2 Self-Ensembling Strategy\nFor each variant, we obtain the predicted results \u02c6y and the confidence \u02c6p for each component. The next step involves ensembling the predictions over K different components. We experiment with three (self-) ensembling strategies to compare their impact on both performance and calibration. Majority Vote. We select the predicted results that have the highest accumulated probability across K\nvariants as the ensembling predictions. The accumulated probability Pacc for the predicted label li is defined as follows:\n(1)\nWe pick the variants that have the same prediction as the ensembling prediction and average the probability distribution of the selected components,\n(2)\nwhere K\u2032 is the number of selected variants. Mean Probability. We average the predicted probability distribution of K variants and use the prediction that has the largest probability in the averaged distribution as the ensemble result \u02c6 yens:\n(3) (4)\n(3)\n(4)\nMax Probability. For each possible value in the output space, we find the maximum probability of the predicted values across K variants and use this as the prediction\u2019s probability.\nBecause the probability is obtained from different components, the summation of these probabilities is not guaranteed to be 1. Therefore, we apply the normalization on the new probability distribution: Pens(y|x) = Norm(P\u2032(y|x)). The ensemble prediction is determined as the \u02c6y that has the highest probability after the ensembling step.\n# 4.3 Estimating Calibration\nBeyond task performance of all the possible variants, we estimate the calibration of the model\u2019s predictions (as a proxy towards model confidence) by using Expected Calibration Error (ECE) (Guo et al., 2017). It divides the n predicted results based on their confidence into M bins B1 to BM and then computes a weighted average over the absolute difference between the accuracy acc(Bm) and mean confidence conf(Bm) of the predictions within each bin. We set M to 10 in this work.\n(5)\nECE measures the difference between the model\u2019s empirical accuracy and its confidence (predicted probability). The smaller the ECE, the more confident the model prediction would be. We also report the negative log-likelihood (NLL) (Hastie et al., 2001) \u2212\ufffdn i=1 log(pi) and information entropy (IE) \u2212\ufffdn i=1 pilog(pi) as supplementary metrics of model\u2019s (lack of) confidence.\n# 5 Experimental Setup\nDatasets and Evaluation Metrics. We consider 7 classification datasets that cover a range of label numbers and scenarios: SST-2, SST-5 (Socher et al., 2013), RTE (Wang et al., 2019), ANLI (Nie et al., 2020), Measuring Hate Speech corpus (Sachdeva et al., 2022), Intent Detection from NLU++ (Casanueva et al., 2022) and Manifestos (Lehmann et al., 2023). In order to simulate low-data setups, we sub-sample smaller training data from the full data for each dataset. The details of the datasets along with their corresponding evaluation metrics are provided in Appendix A.1. Implementation Details. Unless noted otherwise, we use Flan-T5large (Chung et al., 2022) as the main model in the experiments. Detailed training environments and hyper-parameters are provided in Appendix A.2 and A.3. Further, we provide all the prompting templates in Appendix D.1 and D.2. Regarding the self-ensembling experiments, for variations of in-context examples, we generate 20 different combinations of IC examples per each original training example. For variations of prompting templates, we manually create 4 different templates for each task. For Var-Both, we experiment with the same 4 templates and generate 4 \u00d7 5 variants with different combinations of IC examples.\n# 6.1 Comparison between Learning Methods\nWe first present a comprehensive analysis of various learning methods in low-resource settings, detailing model performance and calibration errors in Table 1. For full experimental results, we refer the readers to Appendix B. Performance and calibration of learning methods are task-dependent. We find that learning methods perform differently depending on the datasets and we divide the tasks into different families depending on their observed behavior. ICL demonstrates comparable performance to SFT/SICL on SST-2\nand RTE. However, tuning on these datasets with SFT/SICL yields increased ECE along with higher NLL and lower IE scores as shown in Table 2, but no substantial in-task performance improvement. This indicates that the model does not recover the ground truth distribution in the test set while becoming more confident and certain about its predictions, which serves as a sign of miscalibration. Conversely, tasks such as intent detection (NLU++), Manifestos, and Hate speech, show noticeable performance enhancement and better calibration with lower ECE by using SFT/SICL. Nevertheless, despite these task-dependent variations, ECE remains relatively high across all methods except for intent detection, indicating the problem of miscalibration across all learning methods. ICL can achieve comparable performance with SFT on \u2018seen\u2019 data. We suspect the divergent behaviors are possibly due to data contamination of FLAN training corpus (Longpre et al., 2023) wherein ZSL and ICL exhibit similar performances with SFT and SICL on training datasets labeled as seen (e.g., SST-2, RTE)2. To further investigate the performance on seen datasets, we apply the Batch Calibration method (Zhou et al., 2023a), as shown in Table 3. Surprisingly, we find that ICL performs on par or even better than SICL with calibration across all the (possibly) seen data, which reveals the ability of LLMs that have been recovered by calibration techniques on these seen tasks. However, for unseen datasets (NLU++, Manifestos, etc.), the performance of ICL is not comparable to either SICL or SFT even with calibration. Different learning methods excel with different task families. Given the comparison of the performances and calibration on different datasets, we suggest that the choice of learning methods should be task-dependent. The experiments and analysis indicate that unseen datasets obtain better performance and more trustworthy results with supervised tuning methods. For the seen datasets, ICL combined with other \u2018tweaks\u2019 such as model calibration can be a better choice, since the supervised tuning methods are more likely to make the model over-confident and less trustworthy. Within supervised tuning methods, for SFT and SICL, we empirically observe that SICL shows marginally higher performance (\u21911.23) and lower 2We corroborate findings from other concurrent work on data contamination (Zhu et al., 2023; Deng et al., 2023) that also reports the unfair practice of evaluations on seen datasets.\n2We corroborate findings from other concurrent work on data contamination (Zhu et al., 2023; Deng et al., 2023) that also reports the unfair practice of evaluations on seen datasets.\nMetrics\nMethods\nSST-2\nRTE\nANLI\nSST-5\nNLU++\nManifestos\nHate Speech\nPerformance\nZSL\n94.67\n86.64\n52.30\n42.00\n29.20\n14.50\n37.08\nICL\n95.220.12\n88.450\n52.170.47\n37.590.23\n40.110.09\n13.010.19\n40.090.08\nSFT\n95.610.20\n88.810.29\n61.631.68\n46.272.09\n79.980.59\n35.761.23\n58.011.01\nSICL\n95.630.29\n88.570.45\n63.900.14\n47.121.93\n80.760.31\n37.551.61\n59.481.79\nECE\nZSL\n0.907\n0.809\n0.356\n0.142\n0.231\n0.432\n0.318\nICL\n0.9150.001\n0.8150.003\n0.3510.005\n0.1830.002\n0.1290.000\n0.4760.002\n0.2710.002\nSFT\n0.9410.011\n0.8420.002\n0.3160.023\n0.4030.032\n0.0110.001\n0.2010.072\n0.3540.036\nSICL\n0.9450.013\n0.8760.003\n0.2800.011\n0.3600.025\n0.0020.001\n0.2140.038\n0.1930.113\nTable 1: Results for different learning methods across all 7 datasets. We report the average of 3 independent runs with different random seeds; variance is reported in the subscript. Numbers in bold represent the best performance and calibration score per dataset. The datasets \u2018seen\u2019 by Flan-T5 at pretraining are labeled in italic.\nEvaluation Metrics\nSST-2\nSST-5\nICL\nSICL\nICL\nSICL\nCalibration\nECE\n0.915\n0.945\n0.183\n0.360\nNLL\n0.135\n0.271\n1.226\n2.339\nIE\n0.056\n0.015\n0.152\n0.049\nEvaluation Metrics\nNLU++\nManifestos\nICL\nSICL\nICL\nSICL\nCalibration\nECE\n0.129\n0.002\n0.476\n0.214\nNLL\n0.214\n0.084\n3.942\n2.026\nIE\n0.142\n0.002\n0.101\n0.145\nTable 2: Calibration errors and other uncertainty metrics of different learning methods across tasks (part of). We refer the readers to Appendix B.1 for full results.\nEvaluation Metrics\nSST-2\nSST5\nICL\nSICL\nICL\nSICL\nPerformance\nacc\n95.22\n95.63\n50.48\n54.25\nmacro f1\n-\n-\n37.59\n47.12\nacc\n95.95\n95.63\n50.89\n49.91\n+ calibrated\nmacro f1\n-\n-\n49.80\n49.89\nEvaluation Metrics\nManifestos\nHate speech\nICL\nSICL\nICL\nSICL\nPerformance\nmicro f1\n19.29\n38.12\n40.18\n61.33\nmacro f1\n13.01\n37.55\n40.09\n59.48\nmicro f1\n31.00\n38.33\n45.11\n59.58\n+ calibrated\nmacro f1\n29.15\n37.83\n42.36\n57.81\nTable 3: A selection of results after applying Batch Calibration. We refer the readers to Appendix B.1 for the full set of results with the calibration method.\nTable 3: A selection of results after applying Batch Calibration. We refer the readers to Appendix B.1 for the full set of results with the calibration method.\nECE (\u21930.05) than SFT on average across unseen datasets in Table 1. We believe this is possibly due to 1) the knowledge in the IC examples in addition to the training input-label pairs and 2) different combinations of in-context examples as a way of data augmentation in low-resource scenarios.\n# 6.2 Self-Ensembling Results\nSo far, we have observed the common miscalibration issues for all learning methods. We then investigate the feasibility of self-ensembling to improve calibration.\nSelf-ensembling works across learning methods and enhances calibration performance. In Table 4, with different learning methods combined with self-ensembling variations in designs, we find that by changing the in-context example combinations or prompting templates, the best performance of self-ensembling outperforms the baseline without any ensembling by 0.79. Even though the performance gains seem marginal, self-ensembling substantially enhances the calibration performance, reducing the mean ECE value by 43%. The empirical results also show that in addition to ICL, SFT, and SICL also benefit from self-ensembling in both performance and calibration scores. SFT and SICL exhibit a larger drop in ECE after selfensembling than ICL. We also notice that when self-ensembling over SFT and SICL, the model has lower ECE scores than ICL, but with much better performances. This indicates the efficiency of selfensembling in making the predictions more trustworthy while maintaining or even improving the performances. It also indicates that self-ensembling has the potential to mitigate the prominent problem of overconfidence in supervised tuning methods, as shown by Figure 2.\nDifferent Variations and Ensembling Strategies. Our results suggest that with ICL, Var-IC yields more improvements than Var-Prompt, while the latter shows its efficiency with SFT and SICL. This difference stems from inappropriate prompting templates, requiring model tuning to adhere to the prompt structure. We also find that combining both variations may not necessarily improve the performance but is helpful in enhancing the trustworthiness empirically. Regarding ensemble strategies, we notice that the majority vote improves the performances but struggles to reduce the calibration error. Ensembling with max probability consistently\nSystems\nManifestos\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n14.50\n\u21910.79\n0.432\n\u21930.170\n+ Var-Prompt\n14.50\n13.69\n12.93\n15.29\n\u21910.79\n0.432\n0.262\n0.335\n0.437\n\u21930.170\nICL\n13.01\n\u21910.68\n0.476\n\u21930.283\n+ Var-IC\n13.01\n13.50\n13.46\n13.69\n\u21910.68\n0.476\n0.415\n0.465\n0.469\n\u21930.061\n+ Var-Prompt\n13.01\n13.25\n11.67\n11.53\n\u21910.24\n0.476\n0.268\n0.366\n0.472\n\u21930.208\n+ Var-Both\n13.01\n11.19\n11.50\n11.21\n\u21931.51\n0.476\n0.193\n0.354\n0.480\n\u21930.283\nFT\n35.76\n\u21910.73\n0.201\n\u21930.134\n+ Var-Prompt\n35.39\n36.49\n35.66\n34.91\n\u21911.10\n0.144\n0.066\n0.105\n0.135\n\u21930.077\nSupICL\n37.55\n\u21910.06\n0.214\n\u21930.090\n+ Var-IC\n37.55\n36.57\n37.35\n37.61\n\u21910.06\n0.214\n0.179\n0.210\n0.215\n\u21930.035\n+ Var-Prompt\n37.04\n37.14\n37.25\n36.77\n\u21910.21\n0.229\n0.139\n0.191\n0.219\n\u21930.090\n+ Var-Both\n37.04\n36.67\n37.15\n37.50\n\u21910.46\n0.229\n0.124\n0.192\n0.230\n\u21930.105\nSystems\nHate Speech\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n37.08\n\u21930.13\n0.318\n\u21930.049\n+ Var-Prompt\n37.08\n36.54\n36.95\n36.95\n\u21930.13\n0.318\n0.269\n0.302\n0.320\n\u21930.049\nICL\n40.09\n\u21911.10\n0.271\n\u21930.111\n+ Var-IC\n40.09\n40.01\n39.98\n40.49\n\u21910.40\n0.271\n0.233\n0.267\n0.269\n\u21930.038\n+ Var-Prompt\n40.09\n41.03\n41.19\n41.05\n\u21911.10\n0.271\n0.194\n0.236\n0.275\n\u21930.077\n+ Var-Both\n40.09\n39.68\n40.30\n40.49\n\u21910.40\n0.271\n0.160\n0.237\n0.278\n\u21930.111\nFT\n58.01\n\u21930.82\n0.354\n\u21930.115\n+ Var-Prompt\n55.92\n57.16\n57.17\n57.19\n\u21911.27\n0.350\n0.239\n0.290\n0.345\n\u21930.111\nSupICL\n59.48\n\u21910.74\n0.193\n\u21930.078\n+ Var-IC\n59.48\n59.98\n59.82\n59.83\n\u21910.50\n0.193\n0.141\n0.179\n0.191\n\u21930.052\n+ Var-Prompt\n58.66\n59.96\n60.10\n59.86\n\u21911.44\n0.251\n0.165\n0.211\n0.246\n\u21930.086\n+ Var-Both\n58.66\n60.12\n60.22\n59.97\n\u21911.56\n0.251\n0.115\n0.206\n0.246\n\u21930.136\nSystems\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n37.08\n\u21930.13\n0.318\n\u21930.049\n+ Var-Prompt\n37.08\n36.54\n36.95\n36.95\n\u21930.13\n0.318\n0.269\n0.302\n0.320\n\u21930.049\nICL\n40.09\n\u21911.10\n0.271\n\u21930.111\n+ Var-IC\n40.09\n40.01\n39.98\n40.49\n\u21910.40\n0.271\n0.233\n0.267\n0.269\n\u21930.038\n+ Var-Prompt\n40.09\n41.03\n41.19\n41.05\n\u21911.10\n0.271\n0.194\n0.236\n0.275\n\u21930.077\n+ Var-Both\n40.09\n39.68\n40.30\n40.49\n\u21910.40\n0.271\n0.160\n0.237\n0.278\n\u21930.111\nFT\n58.01\n\u21930.82\n0.354\n\u21930.115\n+ Var-Prompt\n55.92\n57.16\n57.17\n57.19\n\u21911.27\n0.350\n0.239\n0.290\n0.345\n\u21930.111\nSupICL\n59.48\n\u21910.74\n0.193\n\u21930.078\n+ Var-IC\n59.48\n59.98\n59.82\n59.83\n\u21910.50\n0.193\n0.141\n0.179\n0.191\n\u21930.052\n+ Var-Prompt\n58.66\n59.96\n60.10\n59.86\n\u21911.44\n0.251\n0.165\n0.211\n0.246\n\u21930.086\n+ Var-Both\n58.66\n60.12\n60.22\n59.97\n\u21911.56\n0.251\n0.115\n0.206\n0.246\n\u21930.136\nTable 4: Results of self-ensembling with different variations (selection). We mark the cells of baseline system without self-ensembling and their results in grey. Numbers in bold represents the best values for each learnin method. \u2206calculates the difference of performance and calibration error between the original results (Ori.) and th best self-ensembled results, where green denotes better results and red denotes worse results. We refer the reader to Appendix B.2 for full self-ensembling results.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8084/8084cc93-8060-4539-9720-0a33d62aa9a4.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) E-SICL(Var-Both)</div>\nFigure 2: The confidence histograms and reliability diagrams of SICL and self-ensembling with max probability on SST5. E- means the self-ensembled systems. produces the most faithful predictions with promising performances. This can be explained by the\nFigure 2: The confidence histograms and reliability diagrams of SICL and self-ensembling with max probability on SST5. E- means the self-ensembled systems.\nproduces the most faithful predictions with promising performances. This can be explained by the\nidea that normalizing over the max probabilities in a way smooths the probability distribution, making the max probabilities less extreme and mitigating over-confidence issues.\n# 6.3 Ablation Studies\nMore ensembling components with Var-IC lead to better calibration. We explore how varying the number of ensembling components with different in-context examples affects performance and calibration. Figure 3 shows the performance and ECE scores with different components. We observe that although the performances remain comparable, the calibration is improved with more components in both ICL and SICL. This highlights the effectiveness of the self-ensembling method in making the predictions more trustworthy through increased input variations. Diversity in Var-Prompt influences self-ensembling.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a7dd/a7dd27f5-384a-4296-aedd-51b14006c8d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Performance and calibration errors of different number of components with variations in in-context examples on Manifestos.</div>\nFigure 3: Performance and calibration errors of different number of components with variations in in-context examples on Manifestos.\n<div style=\"text-align: center;\">Figure 3: Performance and calibration errors of different number of components with variations in in-context examples on Manifestos.</div>\nHate speech\nMetrics\nNum\nSFT\nE-SFT\nSICL\nE-SICL\nPerformance\n1\n58.01\n58.01\n59.48\n59.48\n4\n55.92\n57.16\n58.66\n59.96\n8\n56.00\n56.60\n58.37\n59.61\nECE\n1\n0.354\n0.354\n0.193\n0.193\n4\n0.350\n0.239\n0.251\n0.165\n8\n0.335\n0.208\n0.403\n0.200\nSST-5\nMetrics\nNum\nSFT\nE-SFT\nSICL\nE-SICL\nPerformance\n1\n46.27\n46.27\n47.12\n47.12\n4\n48.11\n47.43\n47.99\n47.70\n8\n47.55\n47.88\n45.75\n45.12\nECE\n1\n0.403\n0.403\n0.360\n0.360\n4\n0.396\n0.301\n0.271\n0.170\n8\n0.362\n0.244\n0.403\n0.272\nTable 5: Results with different numbers of prompting templates on Hate speech and SST-5.\nTable 5: Results with different numbers of prompting templates on Hate speech and SST-5.\nIn Table 5, we show that when tuning with more templates, SFT has lower calibration errors whereas those of SICL increase. Regarding self-ensembling results, we empirically find that by introducing more prompting templates, selfensembling yields lower calibration error with SFT, but shows worse calibration with SICL, meanwhile yielding similar performances. Nonetheless, selfensembling consistently improves the performance and calibration within each setting. The key findings are robust across different training data sizes. We experiment with larger training data and report the results of SFT and SICL on SST-5 in Figure 4. We observe that both SFT and SICL yield better performance and lower ECE with more training data, whereas ICL maintains similar performance. This indicates that supervised methods can better calibrate the model, and produce more calibrated predictions if provided with sufficient data. We also find that self-ensembling remains effective in improving the performance and mitigating the calibration error of the model on average. The findings hold across different model sizes. In order to assess the impact of model sizes, we fur-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1920/1920411d-0275-4c10-94b2-3362585983e6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Performance and calibration errors on SST-5 with different numbers of training data.</div>\nther conduct experiments with Flan-T5xl. For full detailed experiment results, we refer the readers to Table 12 and 13 in the Appendix. We find that a larger model achieves a better calibration score with SFT and SICL than ZSL and ICL. We also witness similar behavior of ICL, surpassing SFT and SICL with or without Batch Calibration on seen data, which aligns with previous findings. The results on SST-5 and Hate Speech show that by applying the self-ensembling method, Flan-T5xl achieves better performance and lower calibration scores, indicating that the model becomes more \u2018task-specialized\u2019. It is also worth noticing that our method is able to improve the performance and decrease the calibration scores on some tasks (Hate Speech) where traditional calibration currently fails. In general, this indicates the potential of self-ensembling in improving both task performance and calibration.\n# 7 Conclusion\nWe have provided a comprehensive analysis of the intricate relationship between in-task performance and calibration across various learning methods in low-resource scenarios. Our findings illuminate the nuances of in-task performance and calibration across different task families, meanwhile addressing the inherent miscalibration over all learning methods. We have also investigated effective strategies to enhance both aspects simultaneously, offering a viable solution through self-ensembling: it results in more calibrated predictions with comparable or superior task performance. We hope that this study will contribute valuable insights into the dynamic landscape of LLMs. These discoveries also offer practical guidance to practitioners, aiding them in choosing suitable learning paradigms and paving the way for the development of more reliable and high-performing LLMs across diverse applications.\n# Limitations\nOur experimental results are conducted with the Flan-T5 model family, which is an encoder-decoder architecture, where we have not investigated how the behaviours of other popular choices of decoderonly models, such as Llama (Touvron et al., 2023), would behave in low-resource scenarios with different learning methods and self-ensembling strategies. Secondly, limited by the training resources, our experiments only consider LLMs within 3B parameters. We will endeavor to scale our experiments to cover larger language models as part of future work. Moreover, there is a variety of sophisticated ensembling methods (Mohammed and Kora, 2023), where we have only studied the max, mean, and majority vote variants for self-ensembling. In future work, we aim to extend the analysis and the self-ensembling methods to more families of tasks, diverse types of models, and other ensembling techniques.\n# References\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901. Inigo Casanueva, Ivan Vuli\u00b4c, Georgios Spithourakis, and Pawe\u0142 Budzianowski. 2022. NLU++: A multilabel, slot-rich, generalisable dataset for natural language understanding in task-oriented dialogue. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 1998\u20132013, Seattle, United States. Association for Computational Linguistics. Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719\u2013730, Dublin, Ireland. Association for Computational Linguistics. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda,\nakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416. Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, and Arman Cohan. 2023. Investigating data contamination in modern benchmarks for large language models. arXiv preprint arXiv:2311.09783. Shrey Desai and Greg Durrett. 2020. Calibration of pre-trained transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295\u2013302, Online. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning. arXiv preprint arXiv:2301.00234. Hanyu Duan, Yixuan Tang, Yi Yang, Ahmed Abbasi, and Kar Yan Tam. 2023. Exploring the relationship between in-context learning and instruction tuning. arXiv preprint arXiv:2311.10367. Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023. Mitigating label biases for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14014\u201314031, Toronto, Canada. Association for Computational Linguistics. Adam Gleave and Geoffrey Irving. 2022. Uncertainty estimation for language reward models. arXiv preprint arXiv:2203.07472. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\nVedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113.\nAdam Gleave and Geoffrey Irving. 2022. Uncertainty estimation for language reward models. arXiv preprint arXiv:2203.07472.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Pre-training to learn in context. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR. Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York Inc., New York, NY, USA. Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. 2022. Towards a unified view of parameter-efficient transfer learning. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 2529, 2022. OpenReview.net. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. 2021. How can we know when language models know? on the calibration of language models for question answering. Transactions of the Association for Computational Linguistics, 9:962\u2013977. Lingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie Lyu, Tuo Zhao, and Chao Zhang. 2020. Calibrated language model fine-tuning for in- and outof-distribution data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1326\u20131340, Online. Association for Computational Linguistics. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30. Pola Lehmann, Simon Franzmann, Tobias Burst, Sven Regel, Felicia Riethm\u00fcller, Andrea Volkens, Bernhard We\u00dfels, and Lisa Zehnter. 2023. The manifesto data collection. manifesto project (mrg/cmp/marpor). version 2023a. Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 22631\u201322648. PMLR. Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of\nComputational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. 2022. Reframing instructional prompts to GPTk\u2019s language. In Findings of the Association for Computational Linguistics: ACL 2022, pages 589\u2013612, Dublin, Ireland. Association for Computational Linguistics. Ammar Mohammed and Rania Kora. 2023. A comprehensive review on ensemble deep learning: Opportunities and challenges. Journal of King Saud University - Computer and Information Sciences, 35(2):757\u2013 774. Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, Dietrich Klakow, and Yanai Elazar. 2023. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 12284\u2013 12314, Toronto, Canada. Association for Computational Linguistics. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020. Adversarial NLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4885\u20134901, Online. Association for Computational Linguistics. OpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. 2019. Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems, 32. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551. Evgeniia Razumovskaia, Goran Glava\u0161, Anna Korhonen, and Ivan Vuli\u00b4c. 2023. Sqatin: Supervised instruction tuning meets question answering for improved dialogue nlu. arXiv preprint arXiv:2311.09502. Pratik Sachdeva, Renata Barreto, Geoff Bacon, Alexander Sahn, Claudia von Vacano, and Chris Kennedy. 2022. The measuring hate speech corpus: Leveraging rasch measurement theory for data perspectivism. In Proceedings of the 1st Workshop on Perspectivist\nApproaches to NLP @LREC2022, pages 83\u201394, Marseille, France. European Language Resources Association.\nApproaches to NLP @LREC2022, pages 83\u201394, Marseille, France. European Language Resources Association.\nciation. Weijia Shi, Sewon Min, Maria Lomeli, Chunting Zhou, Margaret Li, Victoria Lin, Noah A Smith, Luke Zettlemoyer, Scott Yih, and Mike Lewis. 2023. Incontext pretraining: Language modeling beyond document boundaries. arXiv preprint arXiv:2310.10638. Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2022. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631\u20131642, Seattle, Washington, USA. Association for Computational Linguistics. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023. Selective annotation makes language models better few-shot learners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Meiqi Sun, Wilson Yan, Pieter Abbeel, and Igor Mordatch. 2022. Quantifying uncertainty in foundation models via ensembles. In NeurIPS 2022 Workshop on Robustness in Sequence Modeling. Simeng Sun, Yang Liu, Dan Iter, Chenguang Zhu, and Mohit Iyyer. 2023. How does in-context learning help prompt tuning? arXiv preprint arXiv:2302.11521. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In the Proceedings of ICLR. Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926. Xi Wang, Laurence Aitchison, and Maja Rudolph. 2023b. Lora ensembles for large language model fine-tuning. arXiv preprint arXiv:2310.00035.\nJerry Wei, Le Hou, Andrew Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc Le. 2023. Symbol tuning improves in-context learning in language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 968\u2013979, Singapore. Association for Computational Linguistics. Florian Wenzel, Jasper Snoek, Dustin Tran, and Rodolphe Jenatton. 2020. Hyperparameter ensembles for robustness and uncertainty quantification. Advances in Neural Information Processing Systems, 33:6514\u20136527. Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu, Jiachen Li, Shao Zhang, Sijia Liu, James Hendler, and Dakuo Wang. 2023. More samples or more prompt inputs? exploring effective in-context sampling for llm few-shot prompt engineering. arXiv preprint arXiv:2311.09782. Qinyuan Ye, Iz Beltagy, Matthew Peters, Xiang Ren, and Hannaneh Hajishirzi. 2023a. FiD-ICL: A fusionin-decoder approach for efficient in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8158\u20138185, Toronto, Canada. Association for Computational Linguistics. Seonghyeon Ye, Hyeonbin Hwang, Sohee Yang, Hyeongu Yun, Yireun Kim, and Minjoon Seo. 2023b. In-context instruction learning. arXiv preprint arXiv:2302.14691. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697\u201312706. PMLR. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. 2023a. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. arXiv preprint arXiv:2309.17249. Han Zhou, Xingchen Wan, Ivan Vuli\u00b4c, and Anna Korhonen. 2023b. Autopeft: Automatic configuration search for parameter-efficient fine-tuning. arXiv preprint arXiv:2301.12132. Han Zhou, Xingchen Wan, Ivan Vuli\u00b4c, and Anna Korhonen. 2023c. Survival of the most influential prompts: Efficient black-box prompt search via clustering and pruning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13064\u2013 13077, Singapore. Association for Computational Linguistics. Wenhong Zhu, Hongkun Hao, Zhiwei He, Yunze Song, Yumeng Zhang, Hanxu Hu, Yiran Wei, Rui Wang, and Hongyuan Lu. 2023. Clean-eval: Clean evaluation on contaminated large language models. arXiv preprint arXiv:2311.09154.\n# A Experiment Setup\n# A.1 Dataset Details\nSST-2. The SST-2 dataset, a widely-used benchmark in sentiment analysis, comprises sentences from movie reviews annotated with binary sentiment labels (positive or negative). We train the model with the data randomly sampled from the original training set and report the performance on the test set. We evaluate the model\u2019s performance based on accuracy. SST-5. SST-5, an extension of SST-2, enhances sentiment analysis with five classes: very negative, negative, neutral, positive, and very positive. Derived from movie reviews, this dataset provides a nuanced perspective on sentiment, allowing models to distinguish fine-grained emotional tones. With all other practices aligned with SST-2, the results are evaluated with micro f1 and macro f1 scores because it has more than 2 labels. RTE. Recognizing Textual Entailment is a benchmark dataset assessing the task of determining logical entailment between pairs of text snippets. Annotated with binary labels indicating entailment or not, RTE is crucial for evaluating models\u2019 logical reasoning abilities. We report the accuracy in accordance with other binary classification tasks. ANLI. Adversarial NLI is a benchmark dataset introducing adversarial examples to challenge models with nuanced reasoning and complex inferences. With labeled sentence pairs denoting entailment, contradiction, or neutrality, ANLI is crucial for assessing models\u2019 robustness and generalization in the face of diverse linguistic challenges. ANLI has three different rounds of contexts, with later rounds having a better base model, thus being more difficult for the model to distinguish. In this paper, we conduct the experiments mainly on the first round, which is easier than other rounds, in order to compare the performance with ICL. Since it is a multiclass classification task, we report the performance with micro and macro F1 scores. In this paper, we mainly use r1 level data for experiments. NLU++. NLU++ is a more challenging benchmark in task-oriented dialogue system with more finegrained domain ontologies and sentences with multiple intents. It has two tasks: intent detection and slot labeling, covering the banking and hotels two domains. In this work, we focus on the intent detection task, which is a multi-label classification task\nDataset\nLabel number\nMain Metric\nTrain size\nSST-2\n2\nacc\n50\nRTE\n2\nacc\n50\nANLI\n3\nacc\n50\nSST5\n5\nmacro f1\n50\nNLU++\n2\nmicro f1\n50\nManifestos\n8\nmacro f1\n800\nHate speech\n3\nmacro f1\n50\nTable 6: Summary of the datasets, main evaluation metric for performance and training data size used for experiment.\nand we follow the setting from recent work with state-of-the-art results (Razumovskaia et al., 2023), which formats it as a binary yes/no classification task. See the cited work for further details. Regarding the data split, 1,000 sentences from NLU++ were held out for testing and 50 sentences from the leftover 2k+ sentences were sub-sampled for training.\nand we follow the setting from recent work with state-of-the-art results (Razumovskaia et al., 2023), which formats it as a binary yes/no classification task. See the cited work for further details. Regarding the data split, 1,000 sentences from NLU++ were held out for testing and 50 sentences from the leftover 2k+ sentences were sub-sampled for training. Manifestos. Manifestos was originally created to collect the manifestos of parties from different countries. It also includes the analytical variables that indicates the respective categories of the quasisentences. The corpus have 8 domains overall, which are listed as follows: None (of the below) / Other, External Relations, Freedom and Democracy, Political System, Economy, Welfare and Quality of Life, Fabric of Society, Social Groups. In this paper, we use the sentences that only have one golden domain and exclude the ones with multiple labels. Measuring Hate Speech Corpus. Measuring Hate Speech Corpus, in short Hate speech, contains1 10 constituent ordinal labels and the continuous hate speech score to measure the extent of hate. We use the hate speech score as indicator of hate speech in this paper. We follow the original division of approximate hate speech provided by the authors, where > 0.5 is approximately hate speech, < -1 is counter or supportive speech, and -1 to 0.5 is neutral or ambiguous. We only experiment on the intent detection task in the NLU++ bank domain and for ANLI we mainly discuss r1 level data. We summarize the training data size, main performance evaluation metrics, and the number of labels for each dataset in Table 6. We also list the label verbalizers for all datasets in Table 7.\nTask\nLabel Verbalizer\nSST2\npostive, negative\nRTE\nyes, no\nANLI\nyes, maybe, no\nSST5\nterrible, bad, neutral, good, great\nNLU++\nyes,no\nManifestos\nother, external, democracy, political,\neconomy, welfare, fabric, group\nHate Speech\nsupport, neutral, hate\nTable 7: Label verbalizer for different tasks.\n<div style=\"text-align: center;\">Table 7: Label verbalizer for different tasks.</div>\n# A.2 Environment Setup\nWe mainly use Flan-T5large (783M parameters) as the task models for all the datasets. We also use Flan-T5xl (2.85B parameters) on some of the task to see whether the findings still hold on the larger model. For SFT and SICL, we use LoRA (Hu et al., 2022) to tune Flan-T5xl. Due to the computational limitations, we can\u2019t obtain the results on all the datasets with Flan-T5xl. All the experiments are conducted on Cambridge High-Performance Clusters with a single A100 (80G) and a 32-core vCPU. We release the code and the environment dependencies for reproducible purposes at https://github.com/ cambridgeltl/ensembled-sicl.\n# A.3 Hyperparameters\nIn order to evaluate the model\u2019s performance and trustworthiness in low-resource scenarios, we sample a subset of the training set and evaluate it on a fixed set of data as an evaluation and test set. For Manifestos, because it has 8 classes and is more expertise in specialized domains (politics, economics and etc.), we use a relatively larger training set to adapt the model to the task itself. For Hate Speech, we manually sample the training set and test set ourselves since the corpus didn\u2019t provide the split. We randomly sample 1500 data as the fixed test set and 500 examples as the fixed evaluation set. All the main experiments are conducted three times with 0, 21, 42 as the random seeds. We report the mean values of three runs in the main content. Across different learning paradigms (ICL and SICL), we concatenate 3 in-context examples in front of the input for the main experiments. For supervised fine-tuning methods, we attach the detailed hyperparameters in Table 8 for reproducibility. Because tuning the model in the lowresource setting is prone to over-confidence, in or-\nder to mitigate the problem, we apply the early stopping with the patience of 5. Regarding the configuration hyper-parameters of PEFT, they are listed in Table 9. Unlisted properties use the default values in PEFT implementation from huggingface3.\n# B Full Experiment Results\nTo solidify the empirical findings, in this section, we present full experiment results with more metrics in addition to the table in the main content for readers\u2019 reference.\n# B.1 Results of different learning methods\nTable 10 shows the full results on all 7 datasets. We report the accuracy, micro f1, and macro f1 as the performance metrics. We report ECE as the measurement of calibration. We also include NLL and IE as supplementary uncertainty metrics. We find that on SST-2 and RTE, the model achieves comparable or even better performance with ZSL and ICL than SFT and SICL. In the meantime, the predictions have a relatively high ECE, indicating that on these two datasets, the model has the issue of miscalibration. On ANLI, Manifestos, Hate speech, and NLU++, with SFT and SICL the model has lower calibration error than ICL and achieves better performance in both performance metrics. In addition to the original results, we include the Batch Calibration results across all the datasets. On SST-5 and ANLI, although ZSL and ICL achieve similar micro f1 scores, there is still a gap in the original macro f1 scores between ZSL/ICL and SFT/SICL. However, after applying Batch Calibration, we find that ZSL/ICL has a comparable macro f1 score to SFT/SICL. On Manifestos, Hate speech, and NLU++, we don\u2019t observe comparable performance between ZSL/ICL and SFT/SICL either with or without Batch Calibration.\n# B.2 Results of self-ensembling\nTable 11 shows the self-ensembling results across 4 datasets. We exclude the seen datasets (SST-2 and RTE) for fair evaluation, as well as NLU++ since it\u2019s almost well-calibrated with supervised tuning. We still include ANLI for comparison even though it is included during pre-training. We report the mean values of the results with 3 different random seeds (0, 21, 42).\n3https://huggingface.co/docs/peft/index\nHyperparameters\nICL\nFT\nSupICL\nSST2\ntrain batch size\n-\n8\n8\neval batch size\n64\n32\n32\ngrad accumulation\n-\n1\n1\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\naccuracy\naccuracy\nRTE\ntrain batch size\n-\n8\n4\neval batch size\n64\n32\n32\ngrad accumulation\n-\n1\n2\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\naccuracy\naccuracy\nANLI\ntrain batch size\n-\n8\n4\neval batch size\n64\n32\n32\ngrad accumulation\n-\n1\n2\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\naccuracy\naccuracy\nSST5\ntrain batch size\n-\n8\n8\neval batch size\n64\n32\n32\ngrad accumulation\n-\n1\n1\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\nmacro f1\nmacro f1\nNLU++\ntrain batch size\n-\n16\n16\neval batch size\n64\n32\n32\ngrad accumulation\n-\n2\n2\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n500\n500\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\nmicro f1\nmicro f1\nManifestos\ntrain batch size\n-\n8\n4\neval batch size\n32\n32\n32\ngrad accumulation\n-\n1\n2\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\nmacro f1\nmacro f1\nHate speech\ntrain batch size\n-\n8\n4\neval batch size\n32\n32\n32\ngrad accumulation\n-\n1\n2\nlearning rate\n-\n5e-5\n5e-5\nevaluation per steps\n-\n10\n10\nmax training epochs\n-\n200\n200\nearly stopping patience\n-\n5\n5\nearly stopping metric\n-\nmacro f1\nmacro f1\nTable 8: Hyper-parameters for each dataset when com-\nparing different learning methods.\nTable 8: Hyper-parameters for each dataset when comparing different learning methods.\nHyperparameters\nValues\nr\n8\nlora alpha\n32\nlora dropout\n0.05\ntarget modules\nq, v\nTable 9: Hyper-parameters for PEFT with FlanT5-xl.\nFrom the perspective of performance, we find that on SST-5, Manifestos, and Hate speech, selfensembled results achieve slightly better performances on average and show positive improvements with each learning method. On ANLI, we observe no significant improvement in the accuracy of self-ensembled results and the decreases in performance are trivial as well. However, from the perspective of calibration, we find that selfensembling with max probability consistently decreases the calibration over all settings, as shown in Figure 5. Introducing variations in both in-context examples and prompting templates yields the lowest calibration error in all experiments. Among different ensembling methods, we find that majority vote can achieve better performances sometimes but it doesn\u2019t help to reduce the calibration error or even make it worse. Mean probability and max probability are able to improve the performance meanwhile reducing the calibration error. The empirical experiment results suggest that although majority vote as a widely used ensemble method achieves better performance, it is worth noting that it may deliver unfaithful predictions, which is not preferred in real application.\n# C Supplementary Results for Ablations\n# C Supplementary Results for Ablations\n# C.1 How about larger models?\nTable 12 shows the results on SST-5 and Hate speech with different learning methods using FlanT5xl. With ZSL and ICL, we observe that xl version model has larger calibration errors than Flan-T5large model on possibly seen datasets (SST-2 and SST5), whereas on unseen datasets (Hate speech and Manifestos) it shows lower ECE. Regarding the performances, the xl model shows better performances on unseen datasets than the large version model but doesn\u2019t guarantee better performances on seen datasets. After tuning the model with SFT or SICL, we find that the calibration errors are reduced across all tasks, which is different from Flan-T5large. Due to the computation constraint, we leave the discrepancy in the behaviors of different-\nEvaluation Metrics\nSST2\nRTE\nZSL\nICL\nSFT\nSICL\nZSL\nICL\nSFT\nSICL\nPerformance\nacc\n94.67\n95.220.12\n95.610.20\n95.630.29\n86.64\n88.450\n88.810.29\n88.570.45\nmacro f1\n-\n-\n-\n-\n-\n-\n-\n-\nacc\n95.50\n95.950.19\n95.610.20\n95.630.29\n89.53\n90.250.51\n89.170.29\n87.970.45\n+ calibrated\nmacro f1\n-\n-\n-\n-\n-\n-\n-\n-\nTrustworthiness\nECE\n0.9069\n0.91490.0014\n0.94080.0113\n0.94490.0129\n0.8092\n0.81500.0030\n0.84180.0224\n0.87590.0025\nNLL\n0.1465\n0.13470.0006\n0.21460.0962\n0.27100.1396\n0.3438\n0.29100.0037\n0.44210.2133\n1.12340.1133\nIE\n0.0615\n0.05990.0003\n0.02150.0149\n0.01540.0124\n0.0977\n0.09750.0002\n0.06300.0304\n0.01530.0030\n+ calibrated\nECE\n0.7877\n0.79610.0016\n0.80400.0056\n0.80640.0096\n0.6831\n0.69910.0054\n0.70220.0061\n0.71110.0045\nEvaluation Metrics\nSST5\nANLI\nZSL\nICL\nSFT\nSICL\nZSL\nICL\nSFT\nSICL\nPerformance\nmicro f1\n52.58\n50.480.15\n50.591.38\n54.250.46\n52.30\n52.170.47\n61.631.68\n63.900.14\nmacro f1\n42.00\n37.590.23\n46.272.09\n47.120.0193\n42.07\n42.060.39\n61.171.76\n63.530.43\nmicro f1\n50.05\n50.890.86\n50.350.50\n49.910.21\n62.30\n61.270.82\n62.471.60\n64.500.41\n+ calibrated\nmacro f1\n48.98\n49.800.91\n50.430.68\n49.890.26\n61.98\n61.150.83\n62.361.67\n64.460.40\nTrustworthiness\nECE\n0.1416\n0.18330.0020\n0.40300.0321\n0.36020.0250\n0.3555\n0.35110.0050\n0.31610.0230\n0.28030.0108\nNLL\n1.1762\n1.22610.0021\n3.19861.4010\n2.33900.2827\n4.7484\n3.89610.0233\n2.25990.3255\n1.92260.2912\nIE\n0.1599\n0.15220.0001\n0.04540.0209\n0.04890.0107\n0.0945\n0.09870.0001\n0.05870.0070\n0.06630.0113\n+ calibrated\nECE\n0.1010\n0.10270.0084\n0.07200.0129\n0.04560.0035\n0.0709\n0.04300.0079\n0.06260.0145\n0.04950.0085\nEvaluation Metrics\nManifestos\nHate speech\nZSL\nICL\nSFT\nSICL\nZSL\nICL\nSFT\nSICL\nPerformance\nmicro f1\n20.87\n19.290.16\n37.541.10\n38.122.01\n39.67\n40.180.14\n59.670.47\n61.332.05\nmacro f1\n14.50\n13.010.19\n35.761.23\n37.551.61\n37.08\n40.090.08\n58.011.01\n59.481.79\nmicro f1\n33.63\n31.000.37\n38.580.46\n38.330.33\n43.87\n45.110.46\n59.890.17\n59.583.26\n+ calibrated\nmacro f1\n30.86\n29.150.53\n37.571.05\n37.830.41\n40.86\n42.360.42\n58.060.27\n57.813.03\nTrustworthiness\nECE\n0.4319\n0.47600.0018\n0.20050.0723\n0.21380.0376\n0.3175\n0.27080.0024\n0.35410.0364\n0.19280.1131\nNLL\n3.7344\n3.94230.0091\n2.07250.1637\n2.02640.0751\n1.3836\n1.21360.0057\n4.47202.4332\n2.08271.6186\nIE\n0.1141\n0.10060.0001\n0.14690.0182\n0.14460.0104\n1.0112\n0.24260.0007\n0.03910.0286\n0.14470.0963\n+ calibrated\nECE\n0.0356\n0.07210.0020\n0.04870.0120\n0.06540.0043\n0.1107\n0.11730.0045\n0.06460.0274\n0.04530.0114\nEvaluation Metrics\nNLU++\nZSL\nICL\nSFT\nSICL\nPerformance\nmicro f1\n29.2\n40.110.09\n79.980.59\n80.760.31\nmacro f1\n40.26\n51.960.04\n80.581.00\n80.490.12\nmicro f1\n11.18\n12.250.02\n16.450.72\n21.004.78\n+ calibrated\nmacro f1\n11.35\n12.560.05\n21.272.05\n27.073.58\nTrustworthiness\nECE\n0.2311\n0.12910.0001\n0.01120.0007\n0.00200.0007\nNLL\n0.3435\n0.21400.0001\n0.13050.0348\n0.08390.0127\nIE\n0.2268\n0.14190.0001\n0.00140.0007\n0.00200.0007\n+ calibrated\nECE\n0.4679\n0.45590.0001\n0.41830.0038\n0.40980.0049\nTable 10: Full experiment results across 7 datasets with different learning methods. We report the mean value for 3 runs with different random seeds and list the variance in the subscripts. We color the Batch Calibration results in grey.\nTable 10: Full experiment results across 7 datasets with different learning methods. We report the mean value for 3 runs with different random seeds and list the variance in the subscripts. We color the Batch Calibration results in\nSystems\nSST5\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n42.00\n\u21914.79\n0.1416\n\u21930.0634\n+ Var-Prompt\n42.00\n46.79\n45.70\n44.82\n\u21914.79\n0.1416\n0.0782\n0.1132\n0.1374\n\u21930.0634\nICL\n37.59\n\u21910.16\n0.1833\n\u21930.0911\n+ Var-IC\n37.59\n37.75\n37.33\n37.26\n\u21910.16\n0.1833\n0.1198\n0.1749\n0.1857\n\u21930.0635\n+ Var-Prompt\n37.59\n37.13\n36.52\n36.83\n\u21930.46\n0.1833\n0.1363\n0.1838\n0.2025\n\u21930.0470\n+ Var-Both\n37.59\n33.82\n35.33\n35.78\n\u21931.81\n0.1833\n0.0955\n0.1832\n0.2107\n\u21930.0911\nFT\n46.27\n\u21912.08\n0.4030\n\u21930.1022\n+ Var-Prompt\n48.11\n47.43\n48.35\n48.33\n\u21910.24\n0.3960\n0.3008\n0.3466\n0.3973\n\u21930.0952\nSupICL\n47.12\n\u21910.79\n0.3602\n\u21930.2402\n+ Var-IC\n47.12\n47.30\n47.37\n47.31\n\u21910.25\n0.3602\n0.2755\n0.3428\n0.3615\n\u21930.0847\n+ Var-Prompt\n47.99\n47.70\n47.88\n47.91\n\u21930.08\n0.2714\n0.1698\n0.2342\n0.2801\n\u21930.1016\n+ Var-Both\n47.99\n47.18\n47.56\n47.54\n\u21930.43\n0.2714\n0.1200\n0.2296\n0.2791\n\u21930.1514\nSystems\nManifestos\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n14.50\n\u21910.79\n0.4319\n\u21930.1699\n+ Var-Prompt\n14.50\n13.69\n12.93\n15.29\n\u21910.79\n0.4319\n0.2620\n0.3349\n0.4374\n\u21930.1699\nICL\n13.01\n\u21910.68\n0.4760\n\u21930.2828\n+ Var-IC\n13.01\n13.50\n13.46\n13.69\n\u21910.68\n0.4760\n0.4146\n0.4645\n0.4689\n\u21930.0614\n+ Var-Prompt\n13.01\n13.25\n11.67\n11.53\n\u21910.24\n0.4760\n0.2682\n0.3661\n0.4718\n\u21930.2078\n+ Var-Both\n13.01\n11.19\n11.50\n11.21\n\u21931.51\n0.4760\n0.1932\n0.3537\n0.4796\n\u21930.2828\nFT\n35.76\n\u21910.73\n0.2005\n\u21930.1343\n+ Var-Prompt\n35.39\n36.49\n35.66\n34.91\n\u21911.10\n0.1435\n0.0662\n0.1049\n0.1352\n\u21930.0773\nSupICL\n37.55\n\u21910.06\n0.2138\n\u21930.0902\n+ Var-IC\n37.55\n36.57\n37.35\n37.61\n\u21910.06\n0.2138\n0.1789\n0.2096\n0.2152\n\u21930.0349\n+ Var-Prompt\n37.04\n37.14\n37.25\n36.77\n\u21910.21\n0.2285\n0.1388\n0.1912\n0.2190\n\u21930.0897\n+ Var-Both\n37.04\n36.67\n37.15\n37.50\n\u21910.46\n0.2285\n0.1236\n0.1918\n0.2303\n\u21930.1049\nSystems\nHate Speech\nMacro F1\nECE\nOri.\nMax\nMean\nMajority\n\u2206\nOri.\nMax\nMean\nMajority\n\u2206\nZSL\n37.08\n\u21930.13\n0.3175\n\u21930.0489\n+ Var-Prompt\n37.08\n36.54\n36.95\n36.95\n\u21930.13\n0.3175\n0.2686\n0.3024\n0.3200\n\u21930.0489\nICL\n40.09\n\u21911.10\n0.2708\n\u21930.1112\n+ Var-IC\n40.09\n40.01\n39.98\n40.49\n\u21910.40\n0.2708\n0.2332\n0.2668\n0.2694\n\u21930.0376\n+ Var-Prompt\n40.09\n41.03\n41.19\n41.05\n\u21911.10\n0.2708\n0.1944\n0.2359\n0.2745\n\u21930.0764\n+ Var-Both\n40.09\n39.68\n40.30\n40.49\n\u21910.40\n0.2708\n0.1596\n0.2366\n0.2776\n\u2193",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of miscalibration in large language models (LLMs) during task performance, particularly in low-resource scenarios. Existing methods, such as Supervised Fine-Tuning (SFT) and In-Context Learning (ICL), have shown promising results, but both suffer from overconfidence and miscalibration. This necessitates a new approach to improve both performance and calibration.",
        "problem": {
            "definition": "The problem focuses on the miscalibration of predictions made by LLMs, which leads to overconfidence in low-resource settings, affecting the reliability of the model's outputs.",
            "key obstacle": "The main challenge is the trade-off between achieving high task performance and maintaining well-calibrated predictions, as existing methods often exacerbate miscalibration."
        },
        "idea": {
            "intuition": "The idea arose from the observation that self-ensembling techniques, which combine predictions from various model inputs, could potentially improve calibration without sacrificing performance.",
            "opinion": "The proposed approach involves leveraging self-ensembling in conjunction with SFT and ICL to enhance both calibration and task performance in LLMs.",
            "innovation": "This method differs from existing approaches by systematically applying self-ensembling across multiple learning paradigms and demonstrating its effectiveness in improving calibration while maintaining or enhancing performance."
        },
        "method": {
            "method name": "Self-Ensembled Supervised In-Context Learning (E-SICL)",
            "method abbreviation": "E-SICL",
            "method definition": "E-SICL is a technique that combines self-ensembling strategies with supervised in-context learning to enhance the calibration of predictions made by LLMs.",
            "method description": "The core of the method involves generating multiple variations of in-context examples and prompts, which are then ensembled to produce more reliable predictions.",
            "method steps": [
                "Generate variations of in-context examples.",
                "Create different prompting templates.",
                "Combine predictions from multiple model inputs using ensembling strategies (majority vote, mean probability, max probability).",
                "Evaluate the final predictions for performance and calibration."
            ],
            "principle": "This method is effective in solving the problem because it reduces the extreme confidence levels typically associated with miscalibrated predictions, leading to more trustworthy outputs."
        },
        "experiments": {
            "evaluation setting": "The experiments were conducted using seven classification datasets under low-resource conditions, comparing the performance and calibration of E-SICL against baseline methods like SFT and ICL.",
            "evaluation method": "The evaluation involved measuring accuracy, Expected Calibration Error (ECE), Negative Log-Likelihood (NLL), and Information Entropy (IE) across different datasets."
        },
        "conclusion": "The study concludes that E-SICL significantly improves both task performance and calibration, reducing average calibration errors by 43% while maintaining competitive performance levels across various datasets.",
        "discussion": {
            "advantage": "The key advantage of E-SICL is its ability to enhance model calibration without compromising task performance, making it a reliable choice for practical applications.",
            "limitation": "A limitation of the method is its reliance on the Flan-T5 model family, which may not generalize across other architectures or larger models due to computational constraints.",
            "future work": "Future research will explore the applicability of self-ensembling techniques to other model architectures and larger language models, as well as investigate more sophisticated ensembling methods."
        },
        "other info": {
            "code repository": "The code for implementing E-SICL is available at https://github.com/cambridgeltl/ensembled-sicl.",
            "datasets used": "The datasets include SST-2, SST-5, RTE, ANLI, NLU++, Manifestos, and Hate Speech, each evaluated for its unique characteristics and challenges."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of miscalibration in large language models (LLMs) during task performance, particularly in low-resource scenarios."
        },
        {
            "section number": "1.2",
            "key information": "Miscalibration leads to overconfidence in low-resource settings, affecting the reliability of LLM outputs."
        },
        {
            "section number": "3.1",
            "key information": "The proposed method, Self-Ensembled Supervised In-Context Learning (E-SICL), enhances model calibration without compromising task performance."
        },
        {
            "section number": "3.4",
            "key information": "E-SICL generates multiple variations of in-context examples and prompts, which are ensembled to produce more reliable predictions."
        },
        {
            "section number": "4.1",
            "key information": "The design of prompts in E-SICL involves creating different prompting templates that contribute to improved calibration and performance."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of the method is its reliance on the Flan-T5 model family, which may not generalize across other architectures or larger models."
        },
        {
            "section number": "7",
            "key information": "The study concludes that E-SICL significantly improves both task performance and calibration, reducing average calibration errors by 43%."
        }
    ],
    "similarity_score": 0.7203760786639823,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning.json"
}