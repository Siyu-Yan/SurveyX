{
    "from": "google",
    "scholar_id": "op_JB6KFyXgJ",
    "detail_id": null,
    "title": "Many-shot in-context learning",
    "abstract": " Abstractive Summarization\n\nTo investigate how scaling ICL examples can impact the comprehension ability of LLMs, we now consider abstractive news summarization using XSum dataset from the GEM benchmark [1]. Using XSum dev set examples containing news articles and summaries, we also evaluate how many-shot ICL generalizes to XLSum [20]. We report performance on 150 test articles using ROUGE-L [35], which measures the longest common subsequence between reference and generated summaries.\nAs depicted in Figure 4, peak performance with many-shot ICL is remarkably close to specialized models fine-tuned on XSum and XLSum. However, XSum performance declines with more than 50 in-context examples. Surprisingly, we observed the many-shot prompted model occasionally generating summaries with fabricated dates and times (\u00a7 A.8), despite the absence of such data in the in-context summaries. Nonetheless, performance on XLSum monotonically improves with more shots, demonstrating positive transfer from many-shot learning to a related task.\n\n# 2.3. Planning: Logistics Domain\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de9e/de9eda04-b9f3-4749-8df6-e7b74b59f2aa.png\" style=\"width: 50%;\"></div>\nPlanning: Logistics Domain (600 instances)\n\nRecent work has highlighted shortcomings in planning abilities of LLMs [59]. To this end, we evaluate whether many-shot ICL can improve their ability to generate simple plans on the Logistics domain, a widely used benchmark. The objective in this domain is to transport packages within cities via trucks, and between cities via airplanes. We generate a set of planning problems with 2-3 cities, 1-2 packages, 1 truck and airplane per city using a formal planning language (PDDL) generator [54], resulting in 1.3K problems for learning and 600 for evaluation. To compute optimal solutions for each problem, we use the FastDownward planner [21].\n\nFigure 5 | In-context Planning. A recent version of 1.5 Pro ",
    "bib_name": "agarwal2024many",
    "md_text": "# Many-Shot In-Context Learning\n\nRishabh Agarwal *, Avi Singh *, Lei M. Zhang \u2020, Bernd Bohnet \u2020, Luis Rosias \u2020, Stephanie C.Y. Chan \u2020, Biao Zhang \u2020, Ankesh Anand , Zaheer Abbas , Azade Nova , John D. Co-Reyes , Eric Chu , Feryal Behbahani , Aleksandra Faust and Hugo Larochelle * Contributed equally, \u2020 Key contribution\n\nLarge language models (LLMs) excel at few-shot in-context learning (ICL) \u2013 learning from a few inputoutput examples (\u201cshots\u201d) provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples \u2013 the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated outputs. To mitigate this limitation, we explore two settings: (1) \u201cReinforced ICL\u201d that uses model-generated chain-of-thought rationales in place of human rationales, and (2) \u201cUnsupervised ICL\u201d where we remove rationales altogether, and prompt the model only with domain-specific inputs. We find that both Reinforced and Unsupervised ICL can be effective in the many-shot regime, particularly on complex reasoning tasks. Furthermore, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to fine-tuning. Finally, we reveal the limitations of next-token prediction loss as an indicator of ICL performance.\n\n# 1. Introduction\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1421/14210be3-bbe1-45d0-88cf-eb38e289c1ca.png\" style=\"width: 50%;\"></div>\nFigure 1 | Many-shot vs Few-Shot In-Context Learning (ICL) across several tasks. Many-shot ICL consistently outperforms few-shot ICL, particularly on difficult non-natural language tasks. Optimal number of shots for many-shot ICL are shown inside the bar for each task. For few-shot ICL, we either use typical number of shots used on a benchmark, for example, 4-shot for MATH, or the longest prompt among the ones we tested with less than the GPT-3 context length of 2048 tokens. Reasoning-oriented tasks, namely MATH, GSM8K, BBH, and GPQA use chain-of-thought rationales. For translation, we report performance on English to Bemba, summarization uses XLSum, MATH corresponds to the MATH500 test set, and sentiment analysis results are reported with semantically-unrelated labels. See \u00a7 2, \u00a7 3, and \u00a7 4 for more details.\nA limiting factor for in-context learning (ICL) in LLMs is the context window, restricting prior research to the few-shot ICL regime. Many-shot learning \u2013 ICL with a large number of shots, for example, hundreds or thousands \u2013 allows for better task specification, can reduce the need for finetuning, and potentially make LLMs more versatile and adaptable. Exploring many-shot ICL is now\n\nccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1eb/d1eb1327-ebf3-4148-8873-6060a4b7e366.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">| Context Length for best-performing and the maximum number of shots tested for each task. The horizontal ne shows the context length of GPT-3 (2048 tokens), which is representative of typical few-shot prompts tested in literature. For several tasks, we observed the best-performing shots correspond to the maximum number of shots d, which was often limited by the number of available examples for in-context learning. On some tasks (e.g., code planning), we did observe slight performance deterioration beyond a certain number of shots.\n</div>\nfeasible, given the recent increase in context windows of publicly available LLMs by at least 100 \u00d7: from only a few thousand tokens in GPT-3 [8] and Llama 2 [57] to 1M tokens in Gemini 1.5 Pro [16].\nIn this paper, we investigate how scaling the number of shots affects ICL performance on a wide variety of tasks (\u00a7 2): problem solving using MATH [23] and GSM8K [10], question-answering [GPQA, 52], summarization using XSum [43] and XLSum [20], algorithmic reasoning [BBH, 56], reward modeling [Code Verifier, 24], low-resource machine translation [FLORES, 18], planning [Logistics, 54], and sentiment analysis [FP, 40]. Compared to few-shot ICL, many-shot learning performs significant better across these tasks, using several hundreds or thousands of shots (Figure 1). Furthermore, maximum performance is often achieved only once the number of shots reaches up to hundreds of thousands of tokens (Figure 2). Concurrent to our work, recent works explore many-shot ICL to jailbreak LLMs [2] (up to 256 shots) and tackle NLP classification tasks [6] (up to 80K tokens). In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots), and much longer context lengths (up to 1M tokens). See \u00a7 5 for a detailed discussion of related work.\nWhile many-shot ICL holds significant promise, it can be constrained by the need for high-quality, human-generated outputs. To overcome this, we introduce reinforced ICL and unsupervised ICL (\u00a7 3). Inspired by the efficacy of model-generated solutions for fine-tuning [55], Reinforced ICL involves replacing human-written rationales with model-generated ones, filtered via answer correctness, for in-context learning. Inspired by task-recognition view of ICL [66], we also introduce Unsupervised ICL where we prompt the model with only problems instead of problem-solution pairs. On problem-solving tasks such as MATH, GPQA and Big-Bench Hard, we find that both reinforced and unsupervised ICL with many-shots can be more effective than few-shot ICL with human-generated rationales, with reinforced ICL being more broadly effective.\nFinally, we empirically study how the learning dynamics of in-context learning changes from few-shot to the many-shot regime (\u00a7 4). We find that with sufficient examples, ICL can overcome pretraining biases, perform comparably to full fine-tuning, and solve high-dimensional prediction tasks with numerical inputs, namely sequential parity prediction and linear classification. This suggests the potential of many-shot ICL to adapt to unseen tasks and domains that might be misaligned with an LLM\u2019s training data. Surprisingly, the order of examples can influence many-shot performance (\u00a7 4.7) Finally, we demonstrate that long-context scaling laws [2, 68, 27] based on next-token prediction loss may not reliably predict ICL performance on problem-solving and reasoning tasks.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n\u2022 Scaling ICL (\u00a7 2): We systematically evaluate ICL performance at different scales of in-context examples for a wide range of tasks with Gemini 1.5 Pro. Our results indicate large performance jumps when transitioning from few-shot to many-shot regime.\n\u2022 Reinforced and Unsupervised ICL (\u00a7 3): We find that using model-generated rationales or only problems can reduce the dependence of many-shot ICL on human-generated data.\n\u2022 Analysing ICL (\u00a7 4): We show that many-shot ICL can overcome pre-training biases, perform comparably to fine-tuning, and learn non-NLP prediction tasks, where few-shot ICL struggles. We also reveal that next-token prediction loss may not be a good predictor of ICL performance.\n\n# 2. Scaling In-Context Learning\n\nDuring in-context learning (ICL), the LLM receives a prompt containing a set of input-output examples, also called shots, that illustrate the desired task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens auto-regressively. Recent increase in context windows of LLMs allow using many more shots for ICL than typically used. Exposure to many more shots can lead to better generalization, handle more complex problems than what is possible with few-shot ICL, make fine-tuning less essential, and greater control over model outputs, potentially reducing biases stemming from pre-training.\n\nEvaluation We evaluate the many-shot performance of Gemini 1.5 Pro 1 [16] model with 1 million token context length, the largest publicly available so far. Unless specified otherwise, we use greedy decoding. For reliable results, we randomly sample in-context examples for each \ud835\udc3e-shot prompt multiple times using different random seeds and report average performance, along with some visualization for performance on individual seeds. To ensure that using more shots provides additional information, any \ud835\udc3e-shot prompt in our setup includes all in-context examples from prompts with less than \ud835\udc3e examples. To reduce the inference cost, we use KV caching [49]. Next, we study many-shot ICL on typical LLM use-cases (also see \u00a7 2.4).\n\n# 2.1. Machine Translation\n\nWe consider translation from English to a low-resource target language, where many-shot ICL can complement the existing knowledge within the LLM. We use the target languages with the largest gap reported between LLMs and state-of-the-art systems [53], namely Bemba and Kurdish, from FLORES-200 benchmark [45]. We modify the default 1-shot MT prompt from Gemini Team [15] to include multiple translation pairs as shots from the FLORES dev split (containing 997 examples). We evaluate performance on the first 150 sentences from the test set using chrF2++ [50], a standard metric based on character and word \ud835\udc5b-gram overlap between generated and reference translation.\nSee Figure 3 for results. Similar to Robinson et al. [53], we observed small gains in the few-shot regime from 1-shot to 10-shot, particularly on Kurdish. However, when using the entire dev set for many-shot ICL, we observe improvements of 15.3% on Bemba and 4.5% on Kurdish, relative to the 1-shot Gemini prompt. Overall, these results establish the new-state-of-art for these language pairs.\n\n1 This corresponds to original version in the Gemini 1.5 Tech Report, released in February 2024. We note that th 1.5 Pro API now serves a newer version starting from April 2024.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\">Many-shot ICL: Machine Translation\n</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78a5/78a5ae0b-0759-46d4-a3af-fcb2b8856ef0.png\" style=\"width: 50%;\"></div>\nFigure 3 | Machine Translation  (MT). Test Performance improves monotonically as we increase the number of MT pairs provided as in-context examples during inference. Notably, many-shot ICL outperforms state-of-the-art chRF2++ scores of 35% (NLLB) on Bemba and 40% (Google Translate) on Kurdish [53]. We note that 997-shot prompt corresponds to around 85K tokens. See an example prompt in Figure A.1.\n\n# 2.2. Abstractive Summarization\n\nTo investigate how scaling ICL examples can impact the comprehension ability of LLMs, we now consider abstractive news summarization using XSum dataset from the GEM benchmark [1]. Using XSum dev set examples containing news articles and summaries, we also evaluate how many-shot ICL generalizes to XLSum [20]. We report performance on 150 test articles using ROUGE-L [35], which measures the longest common subsequence between reference and generated summaries.\nAs depicted in Figure 4, peak performance with many-shot ICL is remarkably close to specialized models fine-tuned on XSum and XLSum. However, XSum performance declines with more than 50 in-context examples. Surprisingly, we observed the many-shot prompted model occasionally generating summaries with fabricated dates and times (\u00a7 A.8), despite the absence of such data in the in-context summaries. Nonetheless, performance on XLSum monotonically improves with more shots, demonstrating positive transfer from many-shot learning to a related task.\n\n# 2.3. Planning: Logistics Domain\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de9e/de9eda04-b9f3-4749-8df6-e7b74b59f2aa.png\" style=\"width: 50%;\"></div>\nPlanning: Logistics Domain (600 instances)\n\nRecent work has highlighted shortcomings in planning abilities of LLMs [59]. To this end, we evaluate whether many-shot ICL can improve their ability to generate simple plans on the Logistics domain, a widely used benchmark. The objective in this domain is to transport packages within cities via trucks, and between cities via airplanes. We generate a set of planning problems with 2-3 cities, 1-2 packages, 1 truck and airplane per city using a formal planning language (PDDL) generator [54], resulting in 1.3K problems for learning and 600 for evaluation. To compute optimal solutions for each problem, we use the FastDownward planner [21].\n\nFigure 5 | In-context Planning. A recent version of 1.5 Pro starts from a high few-shot performance, and its many-shot performance scales uniformly from 42% to 62%. For an older version, success rate quickly improves with up to 10 shots (37K tokens), followed by saturation. As a reference, we report 1-shot GPT-4 results [59].\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd23/dd23b87f-f372-4d79-9a99-835cb7c57947.png\" style=\"width: 50%;\"></div>\nFigure 4 | Summarization. As we increase the number of shots from XSum dev set, XSum test performance improves up to 50 shots and then deteriorates. In contrast, XLSum performance typically improves with more shots from XSum. The 500-shot prompt corresponds to 205K tokens. PEGASUS [71] and mT5 [20] are specialized models fine-tuned for summarization. See an example prompt in Figure A.2.\n\n<div style=\"text-align: center;\">2 0 2 1 2 2 2 3 2 4 2 5 2 6 2 7 2 8 2 9 2 10\nNumber of Shots (K)\n</div>\nAs shown in Figure 5, we observe significant improvement in success rate with increasing numbers of ICL shots. While far from state-of-the-art planning approaches (e.g., Fast-Downward), our results demonstrate the potential of many-shot ICL to improve the commonsense planning abilities of LLMs.\n2.4. Reward Modelling with Many-Shot ICL: Learning Code Verifiers\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e7f/4e7f4032-c2ef-41af-a2b8-ea2f517f6792.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">1 2 4 8 16 32 64 128 256 512\nNumber of Shots (K)\n</div>\nFigure 6 | Learning Verifiers In-Context for checking correctness of GSM8K code solutions. Error bars denotes standard error of mean over 3 seeds. See Figure A.5 for a 2-shot prompt. Best-of-N accuracy. (Left) Average accuracy of top-ranked code solutions (among 4 solutions) based on the verifier score on 200 GSM8K test problems. Best-of-4 selection with 128-shot bridges the gap between Pass@1 accuracy of 77.25% and Pass@4 accuracy of 90% with Gemini 1.0 Pro model. Verifier Confidence. (Right) Conditional Probabilities of the Yes token IP (\ud835\udc4c\ud835\udc52\ud835\udc60) from the verifier, averaged over all correct and incorrect solutions on test problems.\nA standard approach to improve LLM reasoning is to use test-time verification [10, 44, 24]. Specifically, an LLM generates multiple candidate solutions for a given problem and a verifier, also known as an outcome reward model, ranks these solutions and selects the best one. Here, we focus on learning such verifiers in-context for code verification.\nTo create in-context verification examples, we utilize correct and incorrect code solutions in Python generated using Gemini 1.0 Pro [15] on the GSM8K train set. In the prompt, each (problem, solution) pair is appended with the question \u201cIs the solution correct?\u201d followed by the Yes or No token according to ground truth correctness. At inference, we modify each test (problem, solution) pair in the same way and record the logit of the Yes and No tokens (denoted by \ud835\udc3f \ud835\udc4c\ud835\udc52\ud835\udc60, \ud835\udc3f \ud835\udc41\ud835\udc5c). To compute the verifier score, we use the normalized probability of the Yes token: IP (\ud835\udc4c\ud835\udc52\ud835\udc60) = exp (\ud835\udc3f \ud835\udc4c\ud835\udc52\ud835\udc60)/ \ufffd exp (\ud835\udc3f \ud835\udc4c\ud835\udc52\ud835\udc60) + exp (\ud835\udc3f \ud835\udc41\ud835\udc5c) \ufffd. We evaluate verifier performance using best-of-4 selection based on the verifier score on 200 problems from GSM8K test set with Gemini 1.0 solutions.\nAs shown in Figure 6 (left), best-of-4 accuracy with the few-shot prompted verifier significantly improves above pass@1 accuracy with 16 or more in-context examples. Along with an accuracy improvement, the probabilities of the Yes token conditioned on ground-truth correct and incorrect solutions separate with increasing the number of shots up to 256, as shown in Figure 6 (right). Overall, these results show a proof-of-concept that the Gemini model becomes better at verifying correctness of solutions with many-shot ICL.\n\n# 3. Many-shot Learning without Human-Written Rationales\n\nMany-shot ICL could potentially be limited by the availability of high-quality human-generated rationales or demonstrations. This is particularly challenging for complex reasoning tasks, such as GPQA [52], where human-generated rationales require significant resources and expert knowledge. In this work, we explore two simple approaches for addressing this issue.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nReinforced ICL Recent work [55] proposed a simplified version of Reinforced Self-Training [19], demonstrating that fine-tuning using model-generated rationales can be more effective than humangenerated rationales for problem-solving tasks. Inspired by their work, we introduce Reinforced ICL, where we use model-generated rationales for in-context learning. To do so, we use a zero-shot or few-shot chain-of-thought [62] prompt as a starting point to sample multiple rationales for each training problem. Then, we select rationales that obtain the correct final answer (we assume access to ground truth final answers or correctness checks), and arrange them into in-context examples containing (problem, rationale) pairs.\nOne potential issue with model-generated rationales is that of false positives: it is possible for an incorrect reasoning chain to lead to the correct final answer, and fine-tuning or prompting using such a reasoning chain would typically harm performance. Nevertheless, as we discuss in later sections, we often find model-generated rationales to be at least as effective human-written rationales.\n\nReinforced ICL Recent work [55] proposed a simplified version of Reinforced Self-Training [19], demonstrating that fine-tuning using model-generated rationales can be more effective than humangenerated rationales for problem-solving tasks. Inspired by their work, we introduce Reinforced ICL, where we use model-generated rationales for in-context learning. To do so, we use a zero-shot or few-shot chain-of-thought [62] prompt as a starting point to sample multiple rationales for each training problem. Then, we select rationales that obtain the correct final answer (we assume access to ground truth final answers or correctness checks), and arrange them into in-context examples containing (problem, rationale) pairs.\nOne potential issue with model-generated rationales is that of false positives: it is possible for an incorrect reasoning chain to lead to the correct final answer, and fine-tuning or prompting using such a reasoning chain would typically harm performance. Nevertheless, as we discuss in later sections, we often find model-generated rationales to be at least as effective human-written rationales.\nUnsupervised ICL We now go one step further than Reinforced ICL: what if we removed rationales from the many-shot prompt altogether, and prompt the model only with inputs? Specifically, the Unsupervised ICL prompt consists of: 1) a preamble, such as, \u201cYou will be provided questions similar to the ones below:\u201d, 2) a list of unsolved inputs or problems, and 3) a zero-shot instruction or a few-shot prompt with outputs for the desired output format. See \u00a7 A.2 for the exact prompts we use.\nOne hypothesis for how many-shot unsupervised ICL might surpass few-shot learning with human demonstrations is that, when the LLM already possesses the required knowledge to solve a task, any information inserted in the prompt that can narrow down what knowledge is needed for the task\n\nUnsupervised ICL We now go one step further than Reinforced ICL: what if we removed rationales from the many-shot prompt altogether, and prompt the model only with inputs? Specifically, the Unsupervised ICL prompt consists of: 1) a preamble, such as, \u201cYou will be provided questions similar to the ones below:\u201d, 2) a list of unsolved inputs or problems, and 3) a zero-shot instruction or a few-shot prompt with outputs for the desired output format. See \u00a7 A.2 for the exact prompts we use.\nOne hypothesis for how many-shot unsupervised ICL might surpass few-shot learning with human demonstrations is that, when the LLM already possesses the required knowledge to solve a task, any information inserted in the prompt that can narrow down what knowledge is needed for the task becomes helpful. This would be consistent with the view that ICL simply \u201clocates\u201d latent concepts (e.g., math problem-solving) the LLM acquired during pre-training [66, 22, 61]. As such, any of the prompt components \u2013 inputs, outputs, and their mapping \u2013 can help locate such concepts. While Unsupervised ICL is broadly applicable, it may not perform well, for example, when outputs are critical for specifying the task (Figure 9 and A.11).\n\n# 3.1. Problem-solving: Hendrycks MATH & GSM8K\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40e8/40e8b673-68f4-40df-9465-b68837a17b1d.png\" style=\"width: 50%;\"></div>\nFigure 7 | Many-shot Reinforced and Unsupervised ICL for problem-solving generally outperform ICL with ground-truth MATH solutions. MATH. (Left) The bar plots depict the average performance across five random seeds on the MATH500 test set. Each random seed (denoted by the dots) corresponds to a different subset of problems along with ground truth or model-generated solutions (if any) in the prompt. Transfer to GSM8K. (Right) We see that the prompt obtained from MATH transfers well to the GSM8K test split containing 500 problems. Our results with many-shot ICL outperform the 4-shot Minerva prompt, which obtains a test accuracy of 55.7% on MATH500 and 90.6% on GSM8K.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nWe evaluate Reinforced and Unsupervised ICL on Hendrycks MATH [23], which consists of challenging high school competition-level mathematics problems. We use the MATH500 test set from Lightman et al. [33] to report performance, and our 4-shot MATH prompt for data generation can be found in Figure A.6. For Unsupervised ICL, we append this 4-shot prompt after the unsolved problems (see Figure A.8). For comparison, we also evaluate ICL with human-written solutions (ground-truth) from the MATH training set, with the same problems used for many-shot prompts.\nOur results are shown in the Figure 7 (left). On MATH500, both Reinforced and Unsupervised ICL outperforms ICL with ground-truth solutions in both the few-shot and many-shot regime. For ICL, we observe that the performance improves with more examples in the prompt up to a point, and then declines (with the peak being at about 125 examples). Performance for Reinforced ICL also improves with the number of examples, and reaches a plateau at around 25 examples (while being about 5% higher than ICL), and unlike ICL, we don\u2019t see a significant drop in performance even for a very large number of examples in the context. Notably, many-shot ICL achieves comparable or superior performance when using only problems compared to using problems with solutions. This suggests solutions may be redundant for eliciting problem-solving via in-context learning on this domain, potentially due to extensive math-related data seen during pretraining.\n\nchallenging high school competition-level mathematics problems. We use the MATH500 test set from Lightman et al. [33] to report performance, and our 4-shot MATH prompt for data generation can be found in Figure A.6. For Unsupervised ICL, we append this 4-shot prompt after the unsolved problems (see Figure A.8). For comparison, we also evaluate ICL with human-written solutions (ground-truth) from the MATH training set, with the same problems used for many-shot prompts.\nOur results are shown in the Figure 7 (left). On MATH500, both Reinforced and Unsupervised ICL outperforms ICL with ground-truth solutions in both the few-shot and many-shot regime. For ICL, we observe that the performance improves with more examples in the prompt up to a point, and then declines (with the peak being at about 125 examples). Performance for Reinforced ICL also improves with the number of examples, and reaches a plateau at around 25 examples (while being about 5% higher than ICL), and unlike ICL, we don\u2019t see a significant drop in performance even for a very large number of examples in the context. Notably, many-shot ICL achieves comparable or superior performance when using only problems compared to using problems with solutions. This suggests solutions may be redundant for eliciting problem-solving via in-context learning on this domain, potentially due to extensive math-related data seen during pretraining.\nCan many-shot ICL enable out-of-distribution generalization? Singh et al. [55]  found that finetuning a model on model-generated solutions from MATH resulted in improved test performance on GSM8K [10], which has a different distribution of problems than MATH. Here, we investigate whether many-shot ICL also improves transfer performance on GSM8K, indicating an improvement in general problem-solving abilities from in-context learning. Our results in Figure 7 (right) show that this is indeed the case \u2013 Reinforced ICL with MATH prompts excels on GSM8K, outperforming ICL with ground truth MATH solutions as well as Unsupervised ICL in the many-shot setting with at least 25 shots. This indicates that model-generated solutions can enable better generalization than just using problems or combining them with ground-truth solutions for ICL.\n\nCan many-shot ICL enable out-of-distribution generalization? Singh et al. [55]  found that finetuning a model on model-generated solutions from MATH resulted in improved test performance on GSM8K [10], which has a different distribution of problems than MATH. Here, we investigate whether many-shot ICL also improves transfer performance on GSM8K, indicating an improvement in general problem-solving abilities from in-context learning. Our results in Figure 7 (right) show that this is indeed the case \u2013 Reinforced ICL with MATH prompts excels on GSM8K, outperforming ICL with ground truth MATH solutions as well as Unsupervised ICL in the many-shot setting with at least 25 shots. This indicates that model-generated solutions can enable better generalization than just using problems or combining them with ground-truth solutions for ICL.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf8a/cf8aa52c-a480-4715-b954-82a1c77df4da.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">| Many-shot Reinforced and Unsupervised ICL for GPQA. The baseline zero-shot prompt, which is used for ng rationales for Reinforced ICL and appended to the prompt for Unsupervised ICL, obtains a performance of The average test accuracy with 125-shot prompt with both ground-truth or model-generated rationales surpass the btained by Claude-3 Sonnet. As we vary the number of shots, while Unsupervised ICL matches or outperforms the t prompt, Reinforced ICL consistently outperforms it.\n</div>\nGPQA [52] is a multiple-choice QA benchmark, with difficult questions focused on graduate-level reasoning in biology, physics, and chemistry. Following Claude-3 [3], we use the diamond split (198 problems) for evaluation. This split focuses on questions where domain experts agree but experts in other domains struggle despite extended effort and internet access. Remaining 250 questions in non\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n0-Shot Prompt ICL (Ground-Truth) Unsupervised ICL Reinforced ICL\n\ndiamond split are used for many-shot ICL with and without human-written rationales. For Reinforced ICL, we use a zero-shot prompt (Figure A.4) to generate multiple rationales on the non-diamond split, solving 129 problems. We also append this zero-shot prompt after the GPQA problems for specifying output format for Unsupervised ICL.\nAs shown in Figure 8, average test accuracy with ground-truth rationales improves substantially from 5 shots to 125 shots, with the best-performing 125-shot prompt nearly matching the accuracy of the state-of-the-art Claude-3 Opus. However, we do observe a performance degradation with 250 shots. Moreover, Reinforced ICL results indicate that model-generated rationales on GPQA seem to be better than ground-truth rationales up to 25 shots, while resulting in similar performance with more shots. Additionally, Unsupervised ICL does not follow any systematic trend: it sometimes performs better ICL with ground-truth rationales depending on the number of shots, but generally underperforms Reinforced ICL. As noted in Anthropic [3], GPQA is a small evaluation dataset and has an inherent higher variance across different runs, which might explain the non-systematic trends.\n\ndiamond split are used for many-shot ICL with and without human-written rationales. For Reinforced ICL, we use a zero-shot prompt (Figure A.4) to generate multiple rationales on the non-diamond split, solving 129 problems. We also append this zero-shot prompt after the GPQA problems for specifying output format for Unsupervised ICL.\n\nAs shown in Figure 8, average test accuracy with ground-truth rationales improves substantially from 5 shots to 125 shots, with the best-performing 125-shot prompt nearly matching the accuracy of the state-of-the-art Claude-3 Opus. However, we do observe a performance degradation with 250 shots. Moreover, Reinforced ICL results indicate that model-generated rationales on GPQA seem to be better than ground-truth rationales up to 25 shots, while resulting in similar performance with more shots. Additionally, Unsupervised ICL does not follow any systematic trend: it sometimes performs better ICL with ground-truth rationales depending on the number of shots, but generally underperforms Reinforced ICL. As noted in Anthropic [3], GPQA is a small evaluation dataset and has an inherent higher variance across different runs, which might explain the non-systematic trends.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b34/1b343089-9a82-429d-915b-5511a8602096.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c14f/c14f2d3c-f89f-4cdb-9a12-e82029a090fe.png\" style=\"width: 50%;\"></div>\nFigure 9 | BIG-Bench Hard. Reinforced and Unsupervised ICL with varying number of shots, averaged across five random seeds. We evaluate test performance on a held-out set of 100 problems. The error bars denote standard deviation. Reinforced ICL outperforms Unsupervised ICL for all tasks, which in turns outperforms the human-written chain-of-thought (CoT) prompt. Averaged across tasks, CoT prompting using human-written rationales gets a success rate of 72.1%, Unsupervised ICL obtains 77.1%, while Reinforced ICL gets 83%.\n\nWe now evaluate Reinforced ICL and Unsupervised ICL on BIG-Bench Hard [56], a suite of challenging algorithmic reasoning tasks. To reduce the impact of false positives, we select 8 tasks out of 23 in BIG-Bench Hard for which the likelihood of getting a false positive is low: either the answer string is long, or the number of options for each question is large (at least 6). For Reinforced ICL, we use the standard 3-shot CoT prompt from Suzgun et al. [56] to sample 10 rationales per problem from a training set of 150 problem at a temperature of 1.0. We filter the rationales based on final answer correctness and arrange them into prompts containing 3 to 100 (problem, rationale) pairs.\nAs shown in Figure 9, Reinforced ICL strongly outperforms Unsupervised ICL for almost all tasks, which in turn outperforms the standard 3-shot CoT prompt. Performance for Reinforced ICL generally improves monotonically with the number of prompts for 7 out of 8 tasks. These results indicate the Reinforced ICL is a more robust technique than Unsupervised ICL, especially for tasks in which the demonstrations contain crucial information about the task. For a few tasks, Reinforced ICL outperforms the human-written 3-shot prompt even in the 3-shot setting. This result suggests that model-generated rationales can s ometimes outperform human-written rationales even when controlling for the amount of data, mirroring the results reported by Singh et al. [55] for fine-tuning.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n# 4. Analyzing Many-Shot ICL\n\n4.1. Overcoming Pre-training Biases with Many-Shot ICL\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55a0/55a030c4-2665-412f-b91a-45613a338212.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d83e/d83ea8d2-1d68-4a5e-93a2-91589fcb9ac5.png\" style=\"width: 50%;\"></div>\nFigure 10 | Overcoming Pre-Training Bias with Many-Shot ICL. (Left) Many-shot ICL overcomes label flips: Test accuracy for sentiment analysis typically improves with more training shots. Flipped and abstract labels eventually approaching the performance of default labels. (Right) Confidence shift in overcoming bias. For flipped and abstract labels, model confidence in its predicted sentiment labels initially drops, then sharply increases with more training shots to similar value, suggesting a period of overcoming pre-training bias.\nWhile LLMs demonstrate in-context learning of novel tasks, Kossen et al. [30] suggest that ICL may have difficulty unlearning biases derived from pre-training data. Their experiments, however, focused mainly on few-shot ICL due to LLM context length limitations. Here, we revisit their study using many-shot ICL on the Financial PhraseBank (FP) sentiment analysis dataset [40]. Like Kossen et al. [30], we study label relationships that affect pre-training biases:\n\u2022 Flipped Labels: Default labels are rotated, that is, [\u2018negative\u2019, \u2018neutral\u2019, \u2018positive\u2019] becomes [\u2018neutral\u2019, \u2018positive\u2019, \u2018negative\u2019]. This conflicts with sentiment biases an LLM might have learned.\n\u2022 Abstract Labels: We use [\u2018A\u2019, \u2018B\u2019, \u2018C\u2019], removing any pre-existing sentiment association [63].\n\nFor ICL shots, we sample examples from the validation set (with replaced labels) to exhibit the input-label relationship and report the results in Figure 10. With few shots, test accuracy with replacement labels is much lower than with default labels. This suggests that with few-shot ICL, the model struggles to overcome its pre-existing biases from pre-training. However, as the number of shots increases, performance on flipped and abstract labels dramatically improves, approaching that of default labels. For default labels, confidence in predicted labels steadily increases with more shots, as shown in Figure 10 (right). In contrast, for flipped labels, confidence initially drops then sharply increases before reaching a plateau, suggesting a period of overcoming pre-training bias.\nWe posit that the initial drop in performance and confidence in the few-shot regime may be attributed to the \u201cearly ascent\u201d phenomenon [47, 36]: a small number of shots may lead to the retrieval of an incorrect skill, which eventually diminishes as task learning takes effect in the manyshot regime. Overall, these results indicate that many-shot ICL can overcome pre-training biases.\n\n# 4.2. Learning Non-Natural Language Tasks: High-Dimensional Functions\n\nWe now test many-shot ICL\u2019s ability to learn abstract mathematical functions with numerical inputs, which let us stress test its generality and applicability to possibly unseen tasks.\nBinary Linear Classification in High Dimensions Following the setup from Wei et al. [63], we create datasets with \ud835\udc41-dimensional inputs vectors and their binary class labels, where each dimension\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d3b/7d3bfb0d-904f-4d5f-b21d-9973c55e4ca1.png\" style=\"width: 50%;\"></div>\nFigure 11 | In-Context Classification. Test accuracy for 16, 32 and 64 dimensional linear classification problems, averaged across 5 randomly-generated datasets with 25 points per class for each dataset (250 evaluation points total). As we increase the number of shots, the accuracy improves and approximately tracks the performance of the nearest-neighbor baseline trained from scratch on the same data. We use the default implementation of \ud835\udc58-nearest neighbours (with \ud835\udc58 = 5) from scikit-learn [48]. See Figure A.7 for an example prompt.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dad/7dad78f6-d40a-4929-871d-a7d8d27d41db.png\" style=\"width: 50%;\"></div>\nis a random integer in [1, 1000]. See more details in \u00a7 A.5. While Wei et al. [63] used only 16 shots per class, we scale ICL up to 2048 shots per class. As shown in Figure 11, while 2048 shots per class perform best when \ud835\udc41 = 16, we observe slight accuracy decrease beyond 512 shots for higher values of \ud835\udc41 (Figure 11 C, R). Moreover, many-shot ICL substantially outperforms random-chance accuracy and nearly matches the accuracy of a strong baseline, namely \ud835\udc58-nearest neighbors, indicating that many-shot ICL can implement nearest-neighbour search over inputs. This is reminiscent of induction heads that implement prefix matching over sequences [46], a plausible mechanism for ICL abilities.\nSequential Parity. Parity is a fundamental Boolean function that determines if a binary input sequence contains an even or odd number of 1s. Despite their power, transformers trained specifically for in-context learning, struggle to learn the Parity function over 20-digit sequences [7]. In this work, we evaluate how well many-shot ICL performs with a pretrained LLM to learn the sequential parity function \ud835\udc53 (\ud835\udc65) = [\ud835\udc53 1 (\ud835\udc65), \ud835\udc53 2 (\ud835\udc65), \u00b7 \u00b7 \u00b7, \ud835\udc53 \ud835\udc5b (\ud835\udc65)], where \ud835\udc65 \u2208{0, 1} \ud835\udc5b and \ud835\udc53 \ud835\udc56 (\ud835\udc65) = \ud835\udc65 1 \u2295 \ud835\udc65 2 \u00b7 \u00b7 \u00b7 \u2295 \ud835\udc65 \ud835\udc56 \u2200 \ud835\udc56 \u2208[1, \ud835\udc5b]. We report the results in Figure 12. We see consistent improvement in test accuracy as we increase the number of shots to 8192. Performance surpasses a GPT-2 Medium sized transformer [51] trained from scratch on 20 \u00d7 more input-output examples (with no repeated examples; \u00a7 A.6). This result indicates many-shot ICL can implement computations analogous to gradient descent [60].\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8365/8365c4bd-b4a1-430a-837c-d6cabee258d7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">3 | Comparing SFT with Many-Shot ICL on low-resource translation. We plot mean performance across 3 seed dard deviation is between 0.1% to 0.5%. Base model corresponds to 1-shot performance of Gemini 1.5 Pro.\n</div>\nMany-shot ICL could make task-specific fine-tuning less essential or, in some cases, even unnecessary, allowing LLMs to tackle a wider range of tasks without specialization. While supervised fine-tuning (SFT) is the dominant LLM paradigm when making use of hundreds or thousands of examples, it is computationally expensive in terms of training. In contrast, many-shot ICL does not require any training, however it has a larger inference cost, which can be substantially reduced with KV caching, which might be available off-the-shelf with context caching [12].\nHere, we compare many-shot ICL to full fine-tuning for machine translation (\u00a7 2.1). We run two sets of experiments: one using 250 examples, and another using the entire dev set (997 examples). Our results in Figure 13 show that SFT and ICL performance is quite close for Bemba, while SFT has a slight edge for Kurdish. Overall, these results demonstrate that many-shot ICL can be a viable alternative to SFT for some tasks.\n\n4.4. Inference Cost of Many-Shot ICL\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/125a/125aeaba-bd5b-4c89-a19d-bd0bd2c668e0.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">4 | Per-Output runtime as we increase shots, averaged across the test set and multiple seeds, on (left)  sumon and (right) parity prediction. When computing the next token, we still have to attend to the fixed many-shot even if KV is cached. We hypothesize that up to a token length of 32K, you can fit the entire KV cache into TPU hich roughly means that you compute next tokens in O(1) memory load.\n</div>\nFigure 14 | Per-Output runtime as we increase shots, averaged across the test set and multiple seeds, on (left)  summarization and (right) parity prediction. When computing the next token, we still have to attend to the fixed many-shot prompt, even if KV is cached. We hypothesize that up to a token length of 32K, you can fit the entire KV cache into TPU HBM, which roughly means that you compute next tokens in O(1) memory load.\nWhile many-shot ICL increases inference computation time, it can allow for quick prototyping and experimentation using just an inference API. Moreover, being able to spend additional inference-time compute to obtain better performance is a useful feature to have. With KV caching [49, 64] enabled (default for long-context servers), as shown in Figure 14, runtime increases linearly with a large\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\">Sequential Parity\n(8192-shot = 540K tokens)\n</div>\nnumber of shots, as opposed to quadratically for self-attention: doubling the number of shots nearly doubles the runtime. However, for a small number of shots, runtime is nearly constant. When the number of generated tokens is much smaller than many-shot prompts, each new token is still linear, which explains our observed runtime for a large number of shots.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cbb1/cbb19736-e293-4cee-893a-601f4c11875d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">e 15 | Many-shot ICL with GPT-4-Turbo and Claude-3-Opus [3] on low-resource machine translation (\u00a7 2.1).\n</div>\nThe strong many-shot results with Gemini 1.5 Pro raises the question of whether other long-context frontier LLMs also benefit from many-shot ICL. To do so, we evaluate GPT-4-Turbo (128K context length) and Claude-3-Opus [3] (200K context length) on the low-resource translation (\u00a7 2.1). For both these models, many-shot ICL scales favorably on Bemba but do not exhibit much improvement on Kurdish. Notably, 1.5 Pro starts lower than Claude-3 on Bemba but improves more rapidly, achieving much higher performance at 997 shots. It also outperforms GPT-4 in few-shot learning and improves further with more examples. Overall, these results indicate that frontier LLMs exhibit varying degree of many-shot ICL capability.\nTo understand the role of model size, we also evaluated the many-shot performance of Gemini 1.5 Flash, a smaller long-context LLM than Gemini 1.5 Pro. On the English \u2192Bemba task, we find that 1.5 Flash matches Claude-3-Opus and outperforms GPT-4 with 997-shots, despite having much worse few-shot performance than Claude and GPT. On English \u2192Tamil MT, 1.5 Flash outperforms Claude-3 in terms of many-shot performance, while lags behind 1.5 Pro. These results suggest that even smaller LLMs can benefit from many-shot ICL and outperform LLMs with stronger few-shot performance with enough shots.\n\n# 4.6. Where Do Gains in Many-Shot ICL Stem From?\n\nIncluding more examples has two effects: (1) increasing information when using distinct samples, (2) but another of increasing the context length. To separate these two effects, we ran an experiment on low-resource MT by repeating 25 examples several times to create many-shot prompts with up to 1000 examples (shuffled ordering) and added the results in Figure 16. The performance with repeated examples stays nearly the same and significantly lags behind many-shot performance with distinct examples. On this task, the benefit of many-shot ICL mainly stems from adding new information as opposed to increasing context length.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c235/c23501c5-3405-4848-ad4f-e419e891b0ab.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c307/c3070503-9ebd-4a3f-a866-593c12c15822.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 16 |  Many-shot performance with distinct examples vs repeating the same 25 examples \ud835\udc41 times on low-resource MT. Bars show avg. perf with std across 3 seeds. Most of the benefit of many-shot ICL stems from adding new information.\n</div>\n<div style=\"text-align: center;\">Figure 17 | Many-Shot Sensitivity To Example Ordering. Each colored data point represents a different random ordering of 50 in-context examples provided to Gemini 1.5 Pro.\n</div>\n# s Many-Shot ICL Sensitive to Example Orderi\n\nIn few-shot in-context learning (ICL), the order of examples within the prompt can significantly impact model performance [38, 65]. Here, we investigate whether such sensitivity to prompt ordering observed in few-shot ICL persists in many-shot scenarios, which remains largely unexplored. Specifically, we evaluate ten different random orderings of fixed 50 in-context examples from MATH training split and evaluate performance on the held-out 500 examples from MATH test set.\nAs Figure 17 reveals, performance varies significantly across different splits in MATH. Strikingly, an ordering that that excels in one subarea may perform poorly in another, for example, the best ordering for \u2018Split 1\u2019 yields weak results on \u2018Split 2\u2019. This fluctuation results in a smaller variation in average performance compared to individual subareas. One interesting extension would be to optimize many-shot prompts using frameworks like DSPy [28] that has been successfully applied for optimizing few-shot prompts based a given metric. Overall, these findings highlight a key challenge in ensuring reliable results with many-shot ICL for long-context models.\n\n# 4.8. Long-context scaling laws may not predict ICL performance\n\nPrior works [68, 2, 27] have found that the negative log-likelihood (NLL) for ground-truth test outputs decreases predictably as the context length increases. We confirm this finding for GPQA, Hendrycks MATH and GSM8K with many-shot ICL, and report our results in Figure 18. However, we note that NLL trends are not a strong predictor for downstream task performance. For example, the success rate for both MATH and GPQA with ICL decreases after 125 shots (Figure 7, 8), but we do not observe a corresponding increase in the NLL in Figure 18.\nWe also plot NLL curves for Reinforced and Unsupervised ICL, and find them to generally have a smaller slope when compared to supervised ICL. Interestingly, NLL curves for ICL with ground-truth outputs is lower than with model-generated outputs, even though the latter often performs better. In the GSM8K transfer setting (using MATH problems and solutions to score GSM8K solutions), the change in NLL is close to nil. However, this doesn\u2019t reflect transfer performance on GSM8K, which continues to improve with more examples (Figure 7).\nOverall, our results demonstrate that NLL is not a reliable proxy when attempting to predict\n\nccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nNegative Log-Likelihood on Ground-Truth Solutions\nMATH\n\nNegative Log-Likelihood on Ground-Truth Solutions\n\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c19/7c1925a6-93b7-45ac-9fe0-5f69f2171952.png\" style=\"width: 50%;\"></div>\nFigure 18 | Negative Log-Likelihood (NLL) as a function of number of shots. We plot NLL on ground truth test set solutions for GPQA, MATH and GSM8K. For GPQA and MATH, questions for Reinforced ICL and Unsupervised ICL comes from the training splits of those datasets. We study GSM8K in the transfer setting, i.e. questions for Reinforced and Unsupervised ICL come from MATH. The absolute NLL for ICL and Reinforced ICL are not directly comparable to Unsupervised ICL, since they use different prompt formats.\n\nICL performance for problem-solving domains. This makes intuitive sense: for any given problem, there are a large number of potentially correct CoT solutions that the model can generate, and calculating the log-likelihood on only one such solution may not provide a clear picture for overall model capability. We also explore computing NLL on a diverse set of model-generated outputs on MATH, and our findings are presented in \u00a7 A.7.\n\n# 5. Related Work\n\nScaling in-context learning Brown et al. [8] reported improved performance as you increase the number of examples (up to 64) for in-context learning in LLMs , and later works corroborated this finding [39]. However, very few works have explored using a large number of examples (1000 or above) in the prompt. This is likely due to the fact the context lengths in large language models have been quite limited until recently [16, 3]. One closely related work to ours is from Li et al. [31], who scale the number of examples for in-context learning to 2000. However, Li et al. [31] use a custom model architecture [74] to achieve long context lengths, and only evaluate models of up to 1.3B parameters, which is several orders of magnitude smaller than state-of-the-art language models, and are ineffective for complex tasks, such as GPQA [52].\nConcurrently to our work, Anil et al. [2] used many-shot prompting (upto 256 shots) to jailbreak language models. In our work, we focus on a much wider range of tasks, use a lot more examples (up to 8192 shots) and use models with much longer context lengths (up to 1M tokens). Also, we explore mitigations for needing many human-generated examples with many-shot ICL. Furthermore, while Anil et al. [2] use many-shot learning to override preferences learned during RLHF phase to elicit the biases stemming from pretraining, our results in \u00a7 4.1 demonstrate that we can also override pre-training biases themselves. Bertsch et al. [6] also concurrently shows benefits of scaling up in-context learning to many demonstrations on several classification datasets with up to 151 labels, albeit also using smaller context windows of up to 80k tokens (using Llama2-80k [13]).\nLong-context scaling laws Prior works [68, 2, 27, 16] have reported smaller next-token prediction\n\nLong-context scaling laws Prior works [68, 2, 27, 16] have reported smaller next-token prediction loss with longer contexts, which Jeon et al. [25] also show using theoretical analysis. Our findings confirm this trend for even longer context lengths, but our analysis reveals some of the limitations of using next-token prediction loss as a metric for evaluating long-context performance, as next-token prediction loss continues to go down even as overall performance plateaus.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nLearning from self-generated data Numerous recent works [19, 70, 55] propose fine-tuning language models on self-generated data to improve performance. Their approach consists of (1) generate samples from the model and filter them using binary feedback, (2) fine-tune the model on these samples, and (3) repeat this process a few times. In this work, we extend this idea to in-context learning, and study the efficacy of Reinforced ICL in reasoning and problem-solving domains.\n\nSelf-generated data and in-context learning Kim et al. [29] propose using self-generated data for few-shot ICL on classification problems, where they generate demonstrations using the LLM conditioned on the test input for each possible class label, and including these demonstrations in the context when performing the final prediction. Li et al. [32] extend this approach to reasoning and language understanding tasks, where they also generate demonstrations conditioned on the test input. Consistent with our findings, these works show that model-generated demonstrations can outperform human-generated demonstrations in the few-shot regime. Another related approach is AutoCoT [73] that uses a zero-shot CoT prompt to produce model-generated demonstrations for few-shot ICL. To do so, AutoCoT samples diverse questions one-by-one based on embedding-based clustering followed by heuristics-based post-processing for selecting demonstrations.\nDifferent from above approaches, Reinforced ICL generates demonstrations using the same procedure as Singh et al. [55], does not require clustering, post-processing heuristics, or access to the test inputs for generating demonstrations, and can be applied to any problem for which we can obtain reliable reward signals. Moreover, our work mainly focuses on the utility of randomly-sampled model-generated demonstrations for many-shot ICL.\n\nLearning Input-Output Relationships with ICL Numerous works [41, 30, 69, 36] have investigated whether LLMs truly learn input-output relationships during in-context learning. Min et al. [41] found that replacing the ground truth labels in in-context examples with random labels barely effected final performance. Further investigations by Yoo et al. [69] and Kossen et al. [30] found that this finding does not necessarily hold across tasks and model sizes. In particular, Kossen et al. [30], Lin and Lee [36] showed that LLMs can indeed learn input-output relationships via in-context learning, but require more examples in order to do so well. In our work, we extrapolate the trend found in those works to much longer context lengths, showing that pre-training biases can be mostly overcome given enough training examples.\n\nLearning Mathematical Functions with LLMs Several prior works investigate whether mathematical functions can be learned with transformers [14, 72, 67, 7]. All these works train transformers specifically to perform in-context learning for such functions. In contrast, we demonstrate that manyshot ICL can learn high-dimensional functions even with pre-trained LLMs. Concurrent to our work, Vacareanu et al. [58] demonstrate that pretrained LLMs are able to perform regression tasks, with performance rivaling that of traditional supervised methods with 500 in-context examples. Our work complement their findings to other synthetic tasks with a much larger number of in-context examples. Dinh et al. [11] fine-tuned GPT-3 on synthetic classification tasks and observed similarities in the decision boundaries learned by the fine-tuned model and kNNs. Our results in Figure 11 show that many-shot ICL also performs comparably to kNNs on high-dimensional classification tasks.\n\nComparing ICL with fine-tuning Contrary to task-specific fine-tuning, ICL does not require optimizing any model weights, allowing LLMs to perform a variety of tasks at inference. As such, several prior works compare fine-tuning with ICL but in few-shot regime. Liu et al. [37] proposed a\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nparameter-efficient few-shot fine-tuning (FT) approach for T0 that outperforms few-shot ICL with GPT-3. However, Awadalla et al. [5] argue that few-shot ICL is more robust to distribution shifts than fine-tuning for question answering tasks. Similarly, Asai et al. [4] show better transfer with ICL compared to fine-tuning on some tasks. Mosbach et al. [42] fairly compare ICL with FT by using the same model for both approaches and show that full fine-tuning (FT) generally outperforms ICL in the few-shot regime with 16 examples. More recently, Lin et al. [34] show that few-shot ICL can outperform fine-tuning based approaches for aligning LLMs.\nComplementary to prior works, we compare full fine-tuning with many-shot ICL with the same number of examples for low-resource translation. Notably, we find that many-shot ICL performs comparably to FT. Aligned with our findings, Bertsch et al. [6] concurrently show that many-shot ICL generally outperforms parameter-efficient fine-tuning (LoRA) on classification tasks. Overall, many-shot ICL and FT can exhibit comparable behaviors, which we leave for further investigation.\nExemplar vs. Rule-based ICL generalization Chan et al. [9] indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention, Bertsch et al. [6] also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw the most improvement, whereas it should be a task that benefits least from seeing similar examples \u2013 after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit). Chan et al. [9] do show that a transformer\u2019s inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based \u2013 perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.\n\nExemplar vs. Rule-based ICL generalization Chan et al. [9] indicate that ICL tends to generalize in a more exemplar-based way, compared to rule-based generalization during in-weights learning. Using a clever experiment with blocked attention, Bertsch et al. [6] also argue that the benefits of many in-context demonstrations arise from having access to more similar examples. While our results on in-context linear classification agree with this conclusion, our sequential parity results seem to contradict it. Strikingly, sequential parity was the task on which we saw the most improvement, whereas it should be a task that benefits least from seeing similar examples \u2013 after all, the nearest neighbor is always going to give the wrong answer (off by 1 bit). Chan et al. [9] do show that a transformer\u2019s inductive biases towards exemplar-based generalization can be shifted both by the training data and the model size, with larger models being less exemplar-based \u2013 perhaps this explains the contradictory findings, given that our work used a larger and much more capable model, though this remains an open question.\n\n# 6. Discussion, Limitations and Future\n\nWe found significant gains in performance when going from few-shot to many-shot ICL on a wide range of tasks, including translation, summarization, planning, reward modeling, mathematical problem solving, question-answering, algorithmic reasoning, and sentiment analysis. To overcome the challenges of obtaining a large number of high-quality human-written rationales for many-shot ICL, we introduced two regimes: Reinforced ICL and Unsupervised ICL. Moreover, we demonstrate that, unlike few-shot ICL, many-shot ICL is effective at overriding pretraining biases, can learn high-dimensional functions with numerical inputs, and performs comparably to SFT.\nOne limitation of our work is that it mainly evaluates many-shot ICL with Gemini 1.5 Pro. That said, concurrent works [2, 6] as well as our preliminary results with GPT-4-Turbo, Gemini 1.5 Flash, and Claude-3-Opus (\u00a7 4.5) indicate that other LLMs can also benefit from many-shot ICL. Future work should focus on evaluating the many-shot abilities of a wide range of long context models, as they become available. Furthermore, many-shot performance can likely serve as a valuable metric for evaluating the quality of long-context models, going beyond the prevalent needle-in-a-haystack test [26].\nAnother limitation of our work is that we don\u2019t completely understand why performance can sometimes degrades with more examples in the prompt (for example, for MATH). Our analysis found that negative log-likelihood trends are insufficient to explain this degradation, and future work should investigate new directions to shed light on the matter and improving many-shot ICL capabilities. Overall, we hope that this work lays a foundation for understanding and optimizing the use of long-context models for ICL, opening up a new frontier of LLM capabilities.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n# Acknowledgements\n\nWe would like to thank Gheorghe Comanici for reviewing an early draft of this work. We are also grateful to Doina Precup, Aviral Kumar, Dale Schuurmans, Ankit Anand, Ross Goroshin, Urvashi Singh, Jannik Kossen, Charline Le Lan, and Daniel Toyoma for helpful discussions.\n\n# Contribution Statement\n\nRA initiated and led the project, ran majority of the many-shot experiments and analysis, came up with reinforced ICL, on-boarded collaborators, handled the NeurIPS rebuttal, wrote the initial draft and the revision. AS contributed initial infra for experiments on MATH and GSM8K, ran BBH experiments, co-led the fine-tuning experiments, conducted NLL analysis on problem-solving tasks, and wrote several sections.\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped with GPQA, SC contributed the baseline for parity task and both helped edit the paper. AF and HL provided feedback on an early draft. HL also suggested the unsupervised ICL experiments. Others were involved in project discussions and minor edits to the paper.\n\nRA initiated and led the project, ran majority of the many-shot experiments and analysis, came up with reinforced ICL, on-boarded collaborators, handled the NeurIPS rebuttal, wrote the initial draft and the revision. AS contributed initial infra for experiments on MATH and GSM8K, ran BBH experiments, co-led the fine-tuning experiments, conducted NLL analysis on problem-solving tasks, and wrote several sections.\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped\n\nand wrote several sections.\nLZ contributed results for in-context verifier. BB contributed the planning logistics task. LR led the fine-tuning experiments. BZ contributed the many-shot results for GPT-4 and Claude-3. AA helped with GPQA, SC contributed the baseline for parity task and both helped edit the paper. AF and HL provided feedback on an early draft. HL also suggested the unsupervised ICL experiments. Others were involved in project discussions and minor edits to the paper.\n\n# References\n\n[1] S. N. Akter, Z. Yu, A. Muhamed, T. Ou, A. B\u00e4uerle, \u00c1. A. Cabrera, K. Dholakia, C. Xiong, G. Neubig. An in-depth look at gemini\u2019s language abilities. arXiv preprint arXiv:2312.11 2023.\n\n[2] C. Anil, E. Durmus, M. Sharma, J. Benton, S. Kundu, J. Batson, N. Rimsky, M. Tong, J. Mu, D. Ford, F. Mosconi, R. Agrawal, R. Schaeffer, N. Bashkansky, S. Svenningsen, M. Lambert, A. Radhakrishnan, C. Denison, E. J. Hubinger, Y. Bai, T. Bricken, T. Maxwell, N. Schiefer, J. Sully, A. Tamkin, T. Lanham, K. Nguyen, T. Korbak, J. Kaplan, D. Ganguli, S. R. Bowman, E. Perez, R. Grosse, and D. Duvenaud. Many-shot jailbreaking. Technical report, Anthropic, 2024.\n[3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. Technical Report, 2024.\n[4] A. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and H. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.\n[5] A. Awadalla, M. Wortsman, G. Ilharco, S. Min, I. Magnusson, H. Hajishirzi, and L. Schmidt. Exploring the landscape of distributional robustness for question answering models. arXiv preprint arXiv:2210.12517, 2022.\n[6] A. Bertsch, M. Ivgi, U. Alon, J. Berant, M. R. Gormley, and G. Neubig. In-Context Learning with Long-Context Models: An In-Depth Exploration, Apr. 2024. URL http://arxiv.org/abs/ 2405.00200. arXiv:2405.00200 [cs].\n[7] S. Bhattamishra, A. Patel, P. Blunsom, and V. Kanade. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023.\n[8] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n[9] S. C. Y. Chan, I. Dasgupta, J. Kim, D. Kumaran, A. K. Lampinen, and F. Hill. Transformers generalize differently from information stored in context vs in weights, Oct. 2022. URL http: //arxiv.org/abs/2210.05675. arXiv:2210.05675 [cs].\n[10] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n[11] T. Dinh, Y. Zeng, R. Zhang, Z. Lin, M. Gira, S. Rajput, J.-y. Sohn, D. Papailiopoulos, and K. Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. Advances in Neural Information Processing Systems, 35:11763\u201311784, 2022.\n[12] G. A. for Developers. Context caching guide, 2024. URL https://ai.google.dev/ gemini-api/docs/caching.\n[13] Y. Fu, R. Panda, X. Niu, X. Yue, H. Hajishirzi, Y. Kim, and H. Peng. Data Engineering for Scaling Language Models to 128K Context, Feb. 2024. URL http://arxiv.org/abs/2402.10171. arXiv:2402.10171 [cs].\n[14] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35: 30583\u201330598, 2022.\n[15] G. Gemini Team. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[16] G. Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arxiv:2403.05530, 2024.\n[17] M. Ghallab, A. Howe, C. Knoblock, D. Mcdermott, A. Ram, M. Veloso, D. Weld, and D. Wilkins. PDDL\u2014The Planning Domain Definition Language, 1998.\n[18] N. Goyal, C. Gao, V. Chaudhary, P. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzm\u00e1n, and A. Fan. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Trans. Assoc. Comput. Linguistics, 10:522\u2013538, 2022.\n[19] C. Gulcehre, T. L. Paine, S. Srinivasan, K. Konyushkova, L. Weerts, A. Sharma, A. Siddhant, A. Ahern, M. Wang, C. Gu, et al. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\n[20] T. Hasan, A. Bhattacharjee, M. S. Islam, K. S. Mubasshir, Y. Li, Y. Kang, M. S. Rahman, and R. Shahriyar. Xl-sum: Large-scale multilingual abstractive summarization for 44 languages. In C. Zong, F. Xia, W. Li, and R. Navigli, editors, Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6, 2021, volume ACL/IJCNLP 2021 of Findings of ACL, pages 4693\u20134703. Association for Computational Linguistics, 2021.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n[21] M. Helmert. The fast downward planning system. Journal of Artificial Intelligence Research, 26: 191\u2013246, July 2006. ISSN 1076-9757. doi: 10.1613/jair.1705. URL http://dx.doi.org/ 10.1613/jair.1705.\n[22] R. Hendel, M. Geva, and A. Globerson. In-context learning creates task vectors. arXiv preprint arXiv:2310.15916, 2023.\n[23]  D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.\n[24] A. Hosseini, X. Yuan, N. Malkin, A. Courville, A. Sordoni, and R. Agarwal. V-star: Training verifiers for self-taught reasoners. arXiv preprint arXiv:2402.06457, 2024.\n[25] H. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context learning. arXiv preprint arXiv:2401.15530, 2024.\n[26] G. Kamradt. LLMTest_NeedleInAHaystack. https://github.com/gkamradt/LLMTest_ NeedleInAHaystack, 2023. Accessed: 2024-04-16.\n[27] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\n[28] O. Khattab, A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. V. A, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. DSPy: Compiling declarative language model calls into state-of-the-art pipelines. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=sY5N0zY5Od.\n[29]  H. J. Kim, H. Cho, J. Kim, T. Kim, K. M. Yoo, and S. Lee. Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator. CoRR, abs/2206.08082, 2022. doi: 10.48550/ARXIV.2206.08082. URL https://doi.org/10.48550/arXiv.2206. 08082.\n[30] J. Kossen, Y. Gal, and T. Rainforth. In-context learning learns label relationships but is not conventional learning. In The Twelfth International Conference on Learning Representations, 2023.\n[31] M. Li, S. Gong, J. Feng, Y. Xu, J. Zhang, Z. Wu, and L. Kong. In-context learning with many demonstration examples. CoRR, abs/2302.04931, 2023. doi: 10.48550/ARXIV.2302.04931. URL https://doi.org/10.48550/arXiv.2302.04931.\n[32] R. Li, G. Wang, and J. Li. Are human-generated demonstrations necessary for in-context learning? In The Twelfth International Conference on Learning Representations, 2024. URL\nhttps://openreview.net/forum?id=frRDT6EOhg.\n[33] H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe. Let\u2019s verify step by step. CoRR, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050. URL https://doi.org/10.48550/arXiv.2305.20050.\n[34] B. Y. Lin, A. Ravichander, X. Lu, N. Dziri, M. Sclar, K. Chandu, C. Bhagavatula, and Y. Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. arXiv preprint arXiv:2312.01552, 2023.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n[35] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In  Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\n[36] Z. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.\n[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950\u20131965, 2022.\n[38] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\n[39] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086\u20138098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-LONG.556. URL https://doi.org/10.18653/v1/2022.acl-long.556.\n[40] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782\u2013796, 2014.\n[41] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048\u201311064. Association for Computational Linguistics, 2022.\n[42] M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\n[43]  S. Narayan, S. B. Cohen, and M. Lapata. Don\u2019t give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797\u20131807. Association for Computational Linguistics, 2018.\n[44] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pages 26106\u201326128. PMLR, 2023.\n[45] M. A. NLLB Team. No language left behind: Scaling human-centered machine translation. arXiv preprint, 2022.\n[46] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n\n[35] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In  Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.\n[36] Z. Lin and K. Lee. Dual operating modes of in-context learning. arXiv preprint arXiv:2402.18819, 2024.\n[37] H. Liu, D. Tam, M. Muqeeth, J. Mohta, T. Huang, M. Bansal, and C. A. Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950\u20131965, 2022.\n[38] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\n[39] Y. Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086\u20138098. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022. ACL-LONG.556. URL https://doi.org/10.18653/v1/2022.acl-long.556.\n[40] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65(4):782\u2013796, 2014.\n[41] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Y. Goldberg, Z. Kozareva, and Y. Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 11048\u201311064. Association for Computational Linguistics, 2022.\n[42] M. Mosbach, T. Pimentel, S. Ravfogel, D. Klakow, and Y. Elazar. Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. arXiv preprint arXiv:2305.16938, 2023.\n[43]  S. Narayan, S. B. Cohen, and M. Lapata. Don\u2019t give me the details, just the summary! topicaware convolutional neural networks for extreme summarization. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 1797\u20131807. Association for Computational Linguistics, 2018.\n[44] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify language-to-code generation with execution. In International Conference on Machine Learning, pages 26106\u201326128. PMLR, 2023.\n[45] M. A. NLLB Team. No language left behind: Scaling human-centered machine translation. arXiv preprint, 2022.\n[46] C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, S. Johnston, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n\nAccepted at Neural Information Processing Systems (NeurIPS) 2024 as a Spotlight Presentation.\n\n[47] J. Pan. What in-context learning \u201clearns\u201d in-context: Disentangling task recognition and task learning. PhD thesis, Princeton University, 2023.\n[48]  F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, et al. Scikit-learn: Machine learning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.\n[49] R. Pope, S. Douglas, A. Chowdhery, J. Devlin, J. Bradbury, J. Heek, K. Xiao, S. Agrawal, and J. Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.\n[50] M. Popovi\u0107. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612\u2013",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "The paper addresses the limitations of few-shot in-context learning (ICL) in large language models (LLMs) due to restricted context windows, which previously limited the number of examples (shots) that could be used during inference. As context windows have expanded significantly, the authors investigate the many-shot regime, which allows for hundreds or thousands of examples, leading to substantial performance improvements across various tasks.",
            "purpose of benchmark": "The benchmark aims to evaluate the performance of many-shot ICL across a wide range of tasks, comparing it to few-shot ICL and assessing the effectiveness of new methods like Reinforced ICL and Unsupervised ICL."
        },
        "problem": {
            "definition": "The benchmark addresses the problem of enhancing in-context learning capabilities in LLMs by utilizing a larger number of examples (shots) during inference, which can improve performance on complex tasks.",
            "key obstacle": "The main challenge is the reliance on high-quality, human-generated outputs, which can be scarce and resource-intensive to obtain, limiting the effectiveness of many-shot ICL."
        },
        "idea": {
            "intuition": "The authors were inspired by the potential of many-shot ICL to significantly improve model performance by increasing the number of examples used during inference, thus allowing the model to better generalize and adapt to tasks.",
            "opinion": "The authors believe that the benchmark is crucial for demonstrating the capabilities of LLMs in many-shot ICL, which can provide insights into their adaptability and effectiveness in various domains.",
            "innovation": "The benchmark introduces novel methodologies such as Reinforced ICL and Unsupervised ICL, which utilize model-generated rationales and domain-specific inputs to reduce dependence on human-generated data while still achieving high performance.",
            "benchmark abbreviation": "MICL"
        },
        "dataset": {
            "source": "The dataset was created using a combination of real-world data and model-generated examples, particularly for tasks requiring high-quality outputs.",
            "desc": "The dataset comprises a diverse collection of tasks including mathematical problem-solving, translation, summarization, and sentiment analysis, with varying numbers of shots used for each task.",
            "content": "The dataset includes text data, specifically input-output pairs for tasks like translation and summarization, as well as problem-solution pairs for mathematical and reasoning tasks.",
            "size": "1,000,000",
            "domain": "Mathematics",
            "task format": "Question Answering"
        },
        "metrics": {
            "metric name": "Accuracy, ROUGE-L",
            "aspect": "Model performance in terms of correctness and quality of generated outputs.",
            "principle": "The chosen metrics are based on their ability to effectively measure task-specific performance and generalization capabilities of the models across different tasks.",
            "procedure": "Model performance is evaluated by comparing predicted outputs against ground truth using the specified metrics, with multiple trials conducted to ensure reliability."
        },
        "experiments": {
            "model": "The experiments utilized state-of-the-art models such as Gemini 1.5 Pro and compared their performance across different tasks and shot configurations.",
            "procedure": "Models were trained and evaluated using varying numbers of shots, with systematic sampling and performance metrics recorded for each configuration.",
            "result": "The results indicated significant improvements in performance when transitioning from few-shot to many-shot ICL, with many-shot configurations often outperforming traditional methods.",
            "variability": "Variability was accounted for by conducting multiple trials with different random seeds and evaluating performance across various subsets of the dataset."
        },
        "conclusion": "The experiments demonstrated that many-shot ICL can effectively enhance model performance across a variety of tasks, overcoming limitations associated with few-shot learning and reliance on human-generated data.",
        "discussion": {
            "advantage": "The benchmark highlights the strengths of many-shot ICL in improving task performance and generalization abilities of LLMs, providing a framework for future research in this area.",
            "limitation": "One limitation is the potential for performance degradation with excessive examples in the prompt, indicating a need for further investigation into optimal shot configurations.",
            "future work": "Future research should focus on evaluating many-shot ICL across a broader range of models and tasks, as well as understanding the underlying mechanisms that influence performance trends."
        },
        "other info": []
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the limitations of few-shot in-context learning (ICL) in large language models (LLMs) due to restricted context windows, which previously limited the number of examples (shots) that could be used during inference."
        },
        {
            "section number": "1.2",
            "key information": "As context windows have expanded significantly, the authors investigate the many-shot regime, which allows for hundreds or thousands of examples, leading to substantial performance improvements across various tasks."
        },
        {
            "section number": "1.3",
            "key information": "The benchmark aims to evaluate the performance of many-shot ICL across a wide range of tasks, comparing it to few-shot ICL and assessing the effectiveness of new methods like Reinforced ICL and Unsupervised ICL."
        },
        {
            "section number": "3.1",
            "key information": "The main challenge is the reliance on high-quality, human-generated outputs, which can be scarce and resource-intensive to obtain, limiting the effectiveness of many-shot ICL."
        },
        {
            "section number": "4.1",
            "key information": "The authors believe that the benchmark is crucial for demonstrating the capabilities of LLMs in many-shot ICL, which can provide insights into their adaptability and effectiveness in various domains."
        },
        {
            "section number": "5.2",
            "key information": "The dataset comprises a diverse collection of tasks including mathematical problem-solving, translation, summarization, and sentiment analysis, with varying numbers of shots used for each task."
        },
        {
            "section number": "6.1",
            "key information": "One limitation is the potential for performance degradation with excessive examples in the prompt, indicating a need for further investigation into optimal shot configurations."
        }
    ],
    "similarity_score": 0.6973456759995715,
    "image": [
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1421/14210be3-bbe1-45d0-88cf-eb38e289c1ca.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d1eb/d1eb1327-ebf3-4148-8873-6060a4b7e366.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/78a5/78a5ae0b-0759-46d4-a3af-fcb2b8856ef0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/de9e/de9eda04-b9f3-4749-8df6-e7b74b59f2aa.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/dd23/dd23b87f-f372-4d79-9a99-835cb7c57947.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4e7f/4e7f4032-c2ef-41af-a2b8-ea2f517f6792.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/40e8/40e8b673-68f4-40df-9465-b68837a17b1d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cf8a/cf8aa52c-a480-4715-b954-82a1c77df4da.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1b34/1b343089-9a82-429d-915b-5511a8602096.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c14f/c14f2d3c-f89f-4cdb-9a12-e82029a090fe.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55a0/55a030c4-2665-412f-b91a-45613a338212.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d83e/d83ea8d2-1d68-4a5e-93a2-91589fcb9ac5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7d3b/7d3bfb0d-904f-4d5f-b21d-9973c55e4ca1.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7dad/7dad78f6-d40a-4929-871d-a7d8d27d41db.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8365/8365c4bd-b4a1-430a-837c-d6cabee258d7.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/125a/125aeaba-bd5b-4c89-a19d-bd0bd2c668e0.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/cbb1/cbb19736-e293-4cee-893a-601f4c11875d.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c235/c23501c5-3405-4848-ad4f-e419e891b0ab.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c307/c3070503-9ebd-4a3f-a866-593c12c15822.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7c19/7c1925a6-93b7-45ac-9fe0-5f69f2171952.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1253/1253eea5-54a8-4994-9f58-d4a7d688e5bf.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6d51/6d51526d-60ff-4f3f-846c-ddf203f0b384.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4182/418233e8-fc0f-4370-a011-baf2d126f9ba.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2cd3/2cd39c6e-5713-4778-9123-42c6f3e781d2.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/57a9/57a95d25-5872-48e4-a39d-4f5415a9e3b5.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b390/b390bf41-ed52-4b64-8afe-ee8fdf20fa9c.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c8bd/c8bdb73e-044f-4182-9879-6c9109e25961.png",
        "https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3f02/3f022b26-4683-45d4-9690-355524aaf81b.png"
    ],
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Many-shot in-context learning.json"
}