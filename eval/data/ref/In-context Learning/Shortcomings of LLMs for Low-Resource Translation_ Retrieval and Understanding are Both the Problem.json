{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2406.15625",
    "title": "Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem",
    "abstract": "This work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of context retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of context type, retrieval method, model type, and language-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world's 7,000+ languages and their speakers.",
    "bib_name": "court2024shortcomingsllmslowresourcetranslation",
    "md_text": "# Shortcomings of LLMs for Low-Resource Translation: Retrieval and Understanding are Both the Problem\nSara Court The Ohio State University court.22@osu.edu\nSara Court The Ohio State University court.22@osu.edu Micha Elsner The Ohio State University elsner.14@osu.edu\nAbstract\nThis work investigates the in-context learning abilities of pretrained large language models (LLMs) when instructed to translate text from a low-resource language into a high-resource language as part of an automated machine translation pipeline. We conduct a set of experiments translating Southern Quechua to Spanish and examine the informativity of various types of context retrieved from a constrained database of digitized pedagogical materials (dictionaries and grammar lessons) and parallel corpora. Using both automatic and human evaluation of model output, we conduct ablation studies that manipulate (1) context type (morpheme translations, grammar descriptions, and corpus examples), (2) retrieval methods (automated vs. manual), and (3) model type. Our results suggest that even relatively small LLMs are capable of utilizing prompt context for zero-shot low-resource translation when provided a minimally sufficient amount of relevant linguistic information. However, the variable effects of context type, retrieval method, model type, and language-specific factors highlight the limitations of using even the best LLMs as translation systems for the majority of the world\u2019s 7,000+ languages and their speakers.\n 24 Oct 2024\narXiv:2406.15625v3 \n# 1 Introduction\nDespite great progress in the quality of today\u2019s state of the art machine translation (MT) systems, constraints on the amount and kinds of data available in the majority of the world\u2019s 7,000+ languages have led to yet another disparity in access and support for speakers of these languages: low-resource MT continues to be a major challenge (Hendy et al., 2023; Nicholas and Bhatia, 2023; Robinson et al., 2023; Stap and Araabi, 2023). Although many languages lack the kinds of large, standardized corpora necessary for traditional MT methods, recent work suggests it may be possible to leverage a smaller amount of existing resources, for example\nMicha Elsner The Ohio State University elsner.14@osu.edu\npedagogical materials used for language instruction, to develop MT systems with Large Language Models (LLMs), albeit with varying results (Elsner and Needle, 2023; Tanzer et al., 2024; Zhang et al., 2024). These materials are often the result of community-driven or government-led initiatives to support language revitalization, reclamation, and mother-tongue education (Schreiner et al., 2020; Liu et al., 2022; Riestenberg et al., 2024). Such discrepancies in the needs and priorities of academic, commercial, and community-led efforts to develop digital resources and language technologies is what Gessler (2022) terms the \u201cNLP Gap\u201d. In this study, we investigate one way to lessen the NLP Gap, comparing LLMs\u2019 in-context learning abilities when translating from a low-resource language (a Peruvian variety of Southern Quechua) to a high-resource language (Spanish) using information retrieved from a database of pedagogical materials. We replicate results of earlier studies on a new language pair by comparing the effects of morpheme translations, sentences from a parallel corpus, and passages from a grammar instruction document on translation quality. We then conduct a more focused analysis by annotating translation outputs by hand using a modified MQM error typology (Burchardt, 2013). Finally, we conduct an ablation study on the effects of automated retrieval by manually constructing prompts using the same set of materials. Our results suggest that while, unsurprisingly, translation quality improves with model size, such improvements seem to primarily be the result of previous exposure to the low-resource language during model pretraining, rather than an improved ability for the model to utilize prompt context, as evidenced by high scores in response to baseline (zero-shot) translation prompts. However, we also find evidence that in-context learning abilities may be inconsistent across different models of similar size. As found in previous studies, prompts contain-\ning morpheme and word-level translations reliably improve model outputs, but information from the grammar and corpus have a null or even negative effect on results. Human evaluation on a selection of outputs from two models \u2013 GPT-3.5 Turbo and GPT-4o \u2013 align with the quantitative measures we obtain using BLEURT (Sellam et al., 2020) as an automatic metric. Quantitative results also show an effect of automated retrieval on translation quality that is most evident in prompts containing morpheme translations and for models with lower baseline scores. Finally, we highlight a number of ethical concerns and limitations that arise from the proposed methods that are supported by our findings, and discuss the potential risks and challenges LLM-based methods for low-resource MT face moving forward.\n# 2 LLMs for Machine Translation\nModern LLMs are now capable of translating many high-resource languages, but lack sufficient coverage of even modestly resourced languages to achieve comparable results without additional support (Kocmi et al., 2023). Retrieval-augmented generation (Rubin et al., 2022) may provide such support in the form of parallel sentences (Agrawal et al., 2022), dictionary definitions (Ghazvininejad et al., 2023; Lu et al., 2023) or other linguistic meta-knowledge such as a grammatical description. Retrieval-augmented methods offer exciting possibilities for low-resource translation, since the LLM might (in principle) be able to \u201cteach itself\u201d the language from learner-oriented resources produced by community members or language specialists. Studies to date (Elsner and Needle, 2023; Reid et al., 2024; Zhang et al., 2024) experiment with four dimensions of variability: source language, LLM, type(s) of information retrieved, and retrieval method. Since the source languages in these studies have relatively little presence in public corpora or on the web, differing results across LLMs can tentatively be attributed to differences in their incontext learning and instruction-following abilities. All studies find that word-level translations are helpful additions to prompts. Zhang et al. (2024) and Tanzer et al. (2024) also add sentence pairs from a parallel corpus, while Elsner and Needle (2023) add usage examples from a dictionary. Each improves results, although to a lesser degree. Elsner and Needle (2023) and Zhang et al. (2024) experiment with small fixed \u201cgrammar lesson\u201d pas-\nsages to provide explicit syntactic instruction, but find these ineffective. Tanzer et al. (2024) uses passages retrieved from a grammar book, also with relatively disappointing results. Reid et al. (2024) use the entire grammar book and a very long-context model to obtain better translations, but without exploring the role explicit grammar instruction actually plays in doing so. Zhang et al. (2024) find that sentences from the corpus retrieved using BM25 embeddings (Robertson et al., 2009) work better than random ones. Tanzer et al. (2024), however, report that retrieval with longest common substring (LCS) matching outperforms embedding-based retrieval. Overall, the question of how to best retrieve relevant passages containing grammar material or sentences in a low-resource language is still open. This also complicates the interpretation of the mostlynegative results found for grammar passages. It is not clear whether these stem from poor retrieval, from the LLM\u2019s inability to process the retrieved content, or both. Moreover, although Reid et al. (2024) conducts human evaluation of the results for quality, to the best of our knowledge no study to date systematically investigates specific grammatical errors in the output. Finally, each of these studies finds a significant decrease in LLMs\u2019 abilities to translate from a highresource language into a low-resource language relative to experiments in the opposite direction. This is in line with McCoy et al. (2023), who find that while the accuracy of an LLM\u2019s output highly depends on the probability of both the input and the output text, output probability has a greater influence on model performance. We therefore focus this study on a single translation direction, instructing LLMs to output translations from a lowresource language into the language with which they are likely to have had more exposure during training, i.e, from Southern Quechua into Spanish, and leave the reverse direction for future work.\n# 3 Quechuan Languages\nQuechua is a family of languages Indigenous to the Andes in South America. This study focuses on varieties of Southern Quechua (S. Quechua, also known as urin quechua or quechua sure\u00f1o) spoken in parts of Peru.1 While previous studies investigated language-LLM pairs for which the baseline\n1Unless noted otherwise, we use Quechua in this study to refer Southern Quechua and related varieties.\nLLM lacked any pretrained knowledge, we find that newer LLMs can translate some S. Quechua sentences in a zero-shot setting. We expect this to be typical of many low-resource languages which, while often endangered, still may have some presence on the web. Quechuan languages have by far the largest representation of all Indigenous Latin American languages in NLP research (Tonja et al., 2024) and are often included in ACL-affiliated workshops, datasets, and shared tasks (Cotterell et al., 2020; Ebrahimi et al., 2022, 2023). S. Quechua has a robust language toolkit (Rios, 2015), including the morphological parser we use in our pipeline. It has also been the subject of numerous studies on MT for both text and speech, developed in conjunction with monolingual and parallel corpora (Rios, 2015; Cardenas et al., 2018; Ortega et al., 2020; Zevallos et al., 2022). Nonetheless, such tools continue to face challenges, and Quechuan languages continue to lack the resources necessary to develop most of today\u2019s state of the art models. Since Quechua is primarily spoken in South America, the majority of available digital resources, including all materials used in this study, use Spanish as the language of translation, explanation, and/or instruction. We therefore also use Spanish, rather than English or any other high-resource language, as the language of translation and prompting when testing our system.\n# 3.1 Language-Specific Factors\nWhile the proposed methods are general enough to be applied to any language pair, model outputs may reflect certain language-specific characteristics of the source and target languages, respectively. In this section, we provide a brief description of selected language-specific factors in S. Quechua as they relate to their translated Spanish counterparts. For a discussion of their potential effects on our results, please see Section 6.1.\n# 3.1.1 Morphological Segmentation\nS. Quechua is primarily agglutinating, i.e., much of the morphology may be described in terms of isomorphic form-meaning relationships, morphemes generally maintain a consistent form regardless of their phonological environment, and morpheme boundaries tend to be transparent. In contrast, morpheme segmentation in Spanish may be rendered opaque due to its fusional morphology and widespread use of conditioned allomorphy.\nWhile LLMs are trained to process text via tokenbased rather than morpheme-based segmentation, it is possible that a lack of direct correspondence between the expression of morphosyntactic categories in S. Quechua and Spanish may affect a model\u2019s ability to leverage the information we provide as prompt context in our experiments. Correspondences in form and meaning across parallel usage examples may be particularly obscured, limiting the use of corpora designed for traditional MT methods. It may be possible to mitigate such issues with more advanced retrieval or prompting techniques, for example by explicitly instructing an LLM to conduct morphological analysis as part of the translation process, but we leave this for future work.\n# 3.1.2 Syncretism and Polysemy\nAlthough the language is primarily agglutinating, a number of morphemes in S. Quechua are syncretic, such that a given form may be used to express more than one grammatical category. For example, the 1SG.POSS marker, -y, shares the same form as both the 2SG.IMP marker and the infinitival marker, as illustrated in the following examples:\n(1)\n(2)\n(3)\nmikhu-y-ta eat-INF-ACC muna-ni want-1SG \u2018I want to eat\u2019\nSimilarly, words in S. Quechua may be polysemous, with the potential to express more than one meaning depending on their use in context. For example, the S. Quechua word miski (misk\u2019i) may be translated as either dulce \u2018sweet\u2019 or rico/a \u2018delicious\u2019, and both dulce and rico/a are themselves polysemous in Spanish. Dulce may be used as an adjective, i.e., \u2018sweet\u2019, or a noun, i.e.,\u2018candy\u2019, and rico/a may describe either richness in flavor, i.e., \u2018delicious\u2019, or in monetary wealth, i.e., \u2018rich\u2019. The exact forms displaying syncretism or polysemy must be identified on a language-specific basis, but the ambiguity they present poses a clear problem for our proposed methods in general, with\n# potential effects on both retrieval and generation. We discuss this issue further in Section 6.1.\npotential effects on both retrieval and generation. We discuss this issue further in Section 6.1.\n# 3.1.3 Variation\nBoth S. Quechua and Spanish are characterized by extensive regional and dialectal variation. In S. Quechua, this includes differences in orthographic and/or phonological conventions as well as the specific lexical items and expression of morphosyntactic content. For example, the S. Quechua word for \u2018dog\u2019 may be rendered orthographically as alqo, allqo, allku, allqu, or ashko, and the additive suffix may be expressed as either -pas or -pis, depending on the community. Variation in the attested usage of specific lexical items and morphemes across communities is also common in S. Quechua. For example, the evidential marker -mi /-m is frequently attested in the Peruvian variety of S. Quechua used in this study, but essentially absent in many Bolivian varieties. Variation across Spanish-speaking communities may also affect models\u2019 abilities to produce translations that are both accurate and appropriate. The Andean Spanish reference translations used in this study do not appear to affect the results of our automatic evaluation. However, were the proposed methods to be applied in a realistic setting, it would be especially important to assess the degree of alignment between any prescriptive linguistic standards that have been implicitly acquired by the LLM and the usage conventions of the language community or communities of interest.\n# 4 Methods\n# 4.1 Data\nWe conduct experiments on a collection of 50 pairs of S. Quechua - Spanish sentences sourced from one of the author\u2019s personal notes. These were selected to highlight a range of specific grammatical phenomena at multiple levels of difficulty\u2014 they include simple clauses and tenses (Example (4)), as well as more advanced constructions such as those involving past participles (Example (5)) and simultaneous events (Example (6).\n(4) qam you allin-ta good-ACC tusu-nki dance-2SG tu bailas bien \u2018you dance well\u2019\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a044/a0446acc-1d29-4779-a0fb-4064243bc68a.png\" style=\"width: 50%;\"></div>\nFigure 1: Example BASELINE prompt. English: [TASK] Translate the following sentence from Quechua to Spanish. Respond only with the translation: Quechua: kay wasiqa turiypam; Spanish:\n(5)\n) awa-sqa-y knit-PP-1SG wali-qa skirt-TOP sumaq-mi great-ASSERTIVE la falda que tej\u00ed es linda \u2018the skirt that I knit is pretty\u2019 ) qam-qa you-TOP taki-ta song-ACC uyari-spa listen-SUBR wasi-yki-ta house-2.POSS-ACC picha-chka-nki clean-PROG-2SG t\u00fa est\u00e1s limpiando tu casa escuchand m\u00fasica \u2018you\u2019re cleaning your house listening to m sic\u2019\n(6)\nThe first author, a foreign-language student of S. Quechua, received permission from her instructor to use notes from their lessons for the study. All sentence pairs were inspected by the instructor, a native bilingual speaker of both S. Quechua and Peruvian Spanish, to eliminate any errors and confirm the accuracy of all reference translations.\n# 4.2 Prompt Construction\nAs a baseline, each sentence is inserted into a prompt template that instructs the model in Spanish to translate the S. Quechua sentence into Spanish and respond only with the translation (Figure 1). We automate a process for building on this template and compare the effects of adding information from three different sources to the prompt context.\n# 4.2.1 Morpheme Translations (MORPH)\nWe use a morphological parser (Rios, 2015) to segment each word of the source segment into morphemes, each with gloss symbols and a Spanish translation.2 Some morphemes have multiple candidate meanings, all of which are retrieved. As an example, the word rantikuq is segmented as ranti-ku-q and glossed as \u201ccomprar.DB.VRootDB.VDeriv.+RflxInt+Ag.NS.\u201d\n2We set aside valid concerns regarding the theoretical status of the morpheme for this study and define a morph(eme) loosely as a recognizable form-meaning pair that recurs in a language.\nWhile numerous orthographic standards have been developed and promoted across Quechuanspeaking communities in South America, considerable variation in orthographic conventions may be found even within a particular community or variety (Rios and Castro Mamani, 2014). We discuss the implications of this for our results in Section 4.2.5. We supplement the output from the parser using a Quechua-Spanish bilingual dictionary (Qheswa Simi Hamut\u2019ana Kurak Suntur, 2005). We retrieve any dictionary entry whose headword exactly matches a morpheme in our segmentation. By default, we include all senses and any usage examples or contextual information in the dictionary entry as part of the prompt. We then concatenate the output of the parser with the retrieved dictionary entries and include this MORPH information as prompt context preceding the source sentence and baseline translation prompt.\n# 4.2.2 Grammar Descriptions (GRAMMAR)\nWe also experiment with the inclusion of grammar lessons found in student-facing pedagogical materials, retrieving grammatical explanations relevant to each source sentence from a PDF document developed for students and teachers of S. Quechua (Pinto Tapia et al., 2005). The document is organized into short sections (1-3 sentences, plus paradigm tables or usage examples) that describe the particular grammatical concept associated with an affix in Quechua. For each source sentence, we retrieve sections associated with any affix listed in the document that is an exact match of a morpheme and include this in prompts using contextual information from the grammar. This improves on the methods described in Tanzer et al. (2024), who use LCS-based retrieval over an entire textbook, and Elsner and Needle 2023 and Zhang et al. 2024, whose grammatical description remains consistent across prompts regardless of the source text being translated.\n# 4.2.3 Parallel Usage Examples (CORPUS)\nFinally, we experiment with sentence-level examples from a S. Quechua-Spanish parallel corpus designed for traditional NLP tasks. We combine data made available via the AmericasNLP 2021 Shared Task on Open Machine Translation and the 2023 IWSLT shared task on low-resource SLT (Tiedemann, 2012; Agi\u00b4c and Vuli\u00b4c, 2019; Ortega et al., 2020; Mager et al., 2021; Agarwal et al.,\n2023). For each source sentence, we retrieve the three best matches from the corpus using a LCS search against the full source sentence.\n# 4.2.4 Combined Prompt Types\nCombinations of information from all three sources yields 8 total conditions, including the baseline. An example prompt from each information source is given in Appendix E.\n# 4.2.5 Manually Revised Prompts\nTo compute a soft upper bound on the improvements possible with better retrieval, we conduct an additional set of experiments using manually revised prompts. We first examine the content retrieved from the morphological parser, dictionary, and grammar document and remove all instances of ambiguity and irrelevant or misleading information from the prompt context.3 For example, many S. Quechua speakers use the term runasimi (lit: \u2018people mouth\u2019, \u2018the people\u2019s language\u2019), as an endonym for the language. The parser, however, returns only the literal decomposition (runa \u2018ser humanos\u2019/\u2018people\u2019 and simi \u2018boca\u2019/\u2018mouth\u2019), and the dictionary does not list runasimi as a headword but rather as one of eight different senses of simi. We thus remove all such irrelevant examples and translations from the prompt and retain only the content indicating a translation of runasimi in the linguistic sense. We also manually retrieve content from the dictionary and grammar documents that were overlooked by the automated retriever. For example, the verb yanuy \u2018to cook\u2019 does not appear as a headword in the dictionary, but rather as a regional variant of wayk\u2019uy \u2018to cook\u2019. We also eliminate content from the grammar that was retrieved because of syncretism, or mistakes that cascaded from the morphological parser to result in irrelevant retrievals. We manually parse each source sentence to only retrieve and include relevant information in the prompt context. All content in the revised prompts is sourced from the same material available to the automated retriever systems, and we do not add any additional information or use supplemental materials of any sort to create the revised prompts.\n3We do not experiment with retrieval methods for corpus examples, which were retrieved using LCS match in both conditions. Improving on LCS-based retrieval remains an open question in low-resource LLM-MT, and we leave this for future work.\n# 4.3 Models\nWe experiment with three proprietary models, GPT3.5 Turbo (gpt-3.5-turbo-0125, Brown et al., 2020), GPT-4o (gpt-4o, Achiam et al., 2023), and Gemini 1.5 Pro (gemini-1.5-pro, Reid et al., 2024), and one open-source model, Llama 3 (llama-3-8b-instruct, AI@Meta, 2024). We use the pretrained models with their default settings, and do not adjust hyperparameters or conduct any finetuning as part of our experiments.\n# 4.4 Evaluation\nWe conduct both automatic and human evaluations to identify trends in model errors and outputs in the various experimental conditions. We calculate BLEURT and BLEU scores as automatic metrics, and report mean BLEURT scores across items as the primary quantitative measure of translation quality for each of the conditions and models. We also use an adapted MQM schema to conduct qualitative human evaluation of the outputs of GPT-3.5 and GPT-4o for all prompts using automatic retrieval. Each item selected for human evaluation is annotated by at least one of the authors by comparing the model\u2019s output to the source text and reference translation. We refer to the complete MQM typology to design our own four-dimensional framework of commonly attested errors in LLM-MT, each with a defined set of specific subtypes. Precise definitions and examples for all error categories and subtypes may be found in Appendix D. Many of the categories in our schema are defined as in the core MQM framework. However, to capture some of the key behaviors reported in previous studies on LLM-MT and to evaluate the effects of prompt type on model outputs, we make the following adjustments. First, we utilize the Addition and Omission errors defined as Accuracy subtypes in the original MQM typology, but distinguish these from three additional subtypes: Substitution - Incorrect Subject, Substitution - Incorrect Tense/Aspect/Modality (TAM), and Substitution Other. This is intended to capture LLM translations that differ from the source in terms of discrete lexical material or case, person, number, and/or TAM markings while otherwise maintaining the lexical and structural content needed to appropriately translate the source text. Although they are not the only grammatical phenomena that may be similarly misrendered, we select subject and TAM\nmarkers for analysis as they are straightforward to identify and give a good indication of how well the LLMs cope with more abstract information about the meanings of functional morphemes. Rather than including Mistranslation and MT Hallucination as Accuracy Error subtypes as in the original MQM typology, we define a separate NonTranslation category with three possible subtypes: Complete Mistranslation, Mistranslation with Lexical Correspondences, and Refusal. The third dimension of our typology, Model Error, was ultimately not used to classify any output in this study, but characterizes more generic model \u201cmisbehavior\u201d such as failing to follow instructions, producing garbled text, or inappropriately generating content in the source language. Finally, Target Errors identify outputs that are ungrammatical, stylistically inappropriate, or semantically incoherent in the target language, regardless of their accuracy. Detailed annotation guidelines were drafted and agreed upon to encourage consistency across annotators and experimental items. Annotators are instructed to identify and tag up to three specific errors for each translation output, with the exception of Target Errors, which do not count towards the three-error maximum. Each model output is also tagged for quality along a four-point scale as defined in Table 5. Before proceeding with annotation over the larger dataset, both annotators also completed a test evaluation of the same 12 experimental items (96 sentences total) to assess inter-annotator agreement. Statistical measures (\u03ba = 0.72 for quality judgments, \u03b1 = 0.55 for error categories) indicated some discrepancies in annotator judgments, especially for categories, since determining the three most important errors is especially subjective. These were identified and discussed, and agreement was ultimately deemed sufficient to proceed.\n# 5 Results\n# 5.1 Quality Metrics\nWe present BLEURT scores for prompts generated using automated retrieval in Table 1 and summarize human quality judgments for GPT-3.5 and GPT4o with automated retrieval in Table 2. The complete distribution of BLEURT, BLEU, and humanannotated quality ratings for all of our experiments is provided in Appendix F. We find clear effects of LLM, prompt type, and retrieval method, as well as interactions among all three factors.\n<div style=\"text-align: center;\">GPT3.5 GPT4o</div>\n<div style=\"text-align: center;\">GPT3.5 GPT4o Gem. L</div>\nGPT3.5\nGPT4o\nGem.\nLla3\nBASE\n0.19\n0.66\n0.56\n0.15\nCORPUS\n0.27\n0.59\n0.49\n0.19\nGRAM\n0.23\n0.56\n0.55\n0.17\nMORPH\n0.44\n0.54\n0.61\n0.39\nC+G\n0.26\n0.59\n0.54\n0.21\nC+M\n0.44\n0.59\n0.59\n0.36\nG+M\n0.41\n0.53\n0.61\n0.39\nC+G+M\n0.43\n0.57\n0.61\n0.15\nTable 1: Mean BLEURT scores by LLM and prompt type. Shaded rows include morpheme contexts.\n<div style=\"text-align: center;\">GPT-3.5 GPT-4o</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b1a0/b1a03a8c-bd15-4871-b646-b13948b7d127.png\" style=\"width: 50%;\"></div>\nLLM\nGPT-3.5\nGPT-4o\nBASELINE\n21\n108\nCORPUS\n43\n101\nGRAMMAR\n33\n99\nMORPH\n79\n102\nC+G\n41\n101\nC+M\n75\n110\nG+M\n68\n100\nC+G+M\n77\n109\nTable 2: Human-annotated quality ratings summarized as 3 \u00d7 high + 2 \u00d7 med + low. Shaded rows include morpheme contexts.\n<div style=\"text-align: center;\">GPT3.5 GPT4 Gem. Lla3</div>\nG-AUTO\n0.23\n0.56\n0.55\n0.17\nG-MAN\n0.24\n0.58\n0.54\n0.15\nM-AUTO\n0.44\n0.54\n0.61\n0.39\nM-MAN\n0.56\n0.63\n0.66\n0.49\nCGM-AUTO\n0.43\n0.57\n0.61\n0.15\nCGM-MAN\n0.54\n0.63\n0.63\n0.26\nTable 3: Comparison of mean BLEURT scores for automatic versus manual retrieval of material in GRAMMAR, MORPH, and CORPUS-GRAMMAR-MORPH prompts.\nComparing across models, we find that Gemini and GPT-4o outperform Llama 3 and GPT-3.5 for every prompt type. This gap is highest for the least informative prompts, indicating that the Llama 3 and GPT-3.5 base models have relatively poor coverage of S. Quechua, while GPT-4o and Gemini have much better coverage. The effect is evident in both automatic and human quality evaluations. Effects of prompt type are mediated by the quality of the pretrained model. Llama 3 and GPT-3.5 show a clear improvement in quality when MORPH information is included in the prompt. Gemini also improves when this information is added, but to a lesser extent. GPT-4o, on the other hand, performs best in response to the BASELINE (zero-shot) prompts, which attain the highest BLEURT scores across all models, prompt types, and retrieval methods evaluated in this study. In other words, providing additional information in the prompt\u2019s context actually degrades GPT-4o\u2019s ability to translate from S. Quechua to Spanish in all experimental conditions.\n# 5.2 Effects of Automated Retrieval\nTo highlight the effects of automated retrieval on model output, we present BLEURT scores for a selection of prompt types and all four models in Table 3 (full scores may be found in Appendix F). The effect of manual retrieval for MORPH information is positive for all models, although this gap is smallest for Gemini (probably because its performance for these prompts is already highest). The effect for GRAMMAR prompts is either minor or negative.\n# 5.3 Human Analysis of Translation Errors\nThe most common error type identified by the annotators is Substitution - Other, which includes a diverse assortment of lexical and phrasal incongruencies of varying degrees of severity. These are largely item-specific and therefore hard to characterize as a group. Using the error categories described in Section 4.4, we instead identify three more clearly interpretable phenomena and provide a detailed discussion of each in the following sections. We present counts for selected prompt types in Table 4, with examples in Appendix A and counts for all errors in Appendix G.\n# 5.4 Mistranslations\nOutright mistranslations are most common for GPT3.5, making up 30 of the 50 responses in the BASE-\nMistranslation: complete +\nlexical correspondence\nGPT-3.5\n45\n11\n12\nGPT-4o\n4\n6\n4\nTarget Fluency: grammar +\ncoherence + style\nGPT-3.5\n0\n14\n10\nGPT-4o\n3\n13\n9\nGrammatical Divergence:\nsubject + TAM\nGPT-3.5\n0\n24\n31\nGPT-4o\n17\n13\n11\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1aeb/1aeb9f77-cd1d-4b4d-8824-425b5a018b80.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Table 4: Counts of human-annotated error types (per 50 sentences) by LLM and prompt type.</div>\nLINE condition. We also consider outputs that retain only minimal traces of the source content, which we label as Mistranslations with Lexical Correspondence. Approximately 1/3 of the 637 total errors tagged across all prompt types for GPT3.5 are mistranslations of either type, roughly split between complete mistranslations and those with lexical correspondence (15.07% and 18.37%, respectively, of all errors tagged for GPT-3.5). As reported in previous work, adding morphemeand word-level translations to the prompt greatly reduces the rate of this kind of response. GPT4o also produces drastically fewer mistranslations compared to its predecessor. However, it is notable that both models produce at least one mistranslation for each prompt type. In general, complete mistranslations are in fluent Spanish and contain no overt indications that something has been misrepresented. We return to the ethical implications of these errors in the Discussion. We also note that many of the items tagged as Mistranslation with Lexical Correspondence show correspondence only for words that were already in Spanish in the source text. For example, some sentences contain Spanish loan names for the days of the week. While some of these errors are produced in deceptively fluent Spanish, we find many to be accompanied by semantic incoherence or ungrammaticality in the output. We discuss such target language fluency errors in the following section.\n# 5.5 Target Fluency\nTarget Fluency errors occur when the output is not grammatical, coherent, or stylistically appropriate \u2013 for instance, if an output contains a nonsensical repetition or a verb with missing arguments. Outputs of this type bear a strong similarity to human \u201ctranslationese\u201d in that structural features of the source language may surface in the translation at the expense of naturalness (Koppel and Ordan, 2011; Freitag et al., 2019). Both GPT-3.5 and GPT-4o tend\n<div style=\"text-align: center;\">BASE MORPH C+G+M</div>\nto produce more such outputs when the prompt is more informative \u2013 10 to 20% of the time (510 instances per 50) in prompts with morpheme translations.\n# 5.6 Grammatical Divergence\nWe group misrendered verbal subjects and tense/aspect/morphology (TAM) markers together as Grammatical Divergence errors. Such errors are distinct from the Target Fluency errors described in the previous section\u2014 the Spanish output is grammatical, but fails to accurately reflect the content from the source. TAM divergences are much more prevalent than divergences in subject; for instance, only one of GPT-4o\u2019s 13 Grammatical Divergence errors in the MORPH condition misrender the subject marker. Grammatical Divergence errors are annotated only for sentences that are not mistranslated outright, so GPT-3.5 produces none of these in the BASELINE condition. For more informative prompts, it is clear that GPT-4o is better than GPT3.5 at translating both functional and lexical meanings. However, a relatively large number of sentences (over 20%) still contain such an error even with the highest performing model and prompt type. The relatively small drop in error between different prompt types for GPT-4o suggests that neither the corpus-based usage examples nor example paradigms and descriptions from the grammar document can fully prevent this type of error.\n# 6 Discussion\nWe observe large differences between LLMs, both in terms of the overall quality of their generated translations as well as the effects of prompt type on their outputs. GPT-4o and Gemini, which have the highest baseline scores, benefit least from additional information\u2014 BLEURT scores actually decrease when CORPUS and GRAMMAR information is included. This occurs even with manually\ncurated prompts, suggesting it is not an effect of including irrelevant material. Nonetheless, the baseline results do not represent a ceiling on quality, since both models still produce errors in the BASELINE condition (GPT-4o produces 10 LOW-quality translations in our set of 50). These results suggest that even relevant grammar explanations, when written in prose with examples, do little to help the newest generation of LLMs to translate a lowresource language such as Southern Quechua. Although GPT-4o and Gemini results are similar in many ways, we do find evidence for differences in their in-context learning abilities. Baseline prompts and the GPT-4o model produce the highest BLEURT scores across the dataset, but these outputs still show a number of errors characteristic of LLMs, particularly lexical substitution errors that are not necessarily corrected with the inclusion of more context. In contrast, Gemini, which has near-comparable performance across prompt types, shows an increase in scores when prompts include MORPH information, regardless of retrieval type, suggesting a greater ability to identify and utilize relevant word- and morph-level translations in the prompt\u2019s context. Previous work suggests that newer builds of GPT-4 are less capable of following instructions (Chen et al., 2023); such differences may be masked by the effects of pretraining when automatically evaluating translations. This suggests that researchers should continue to carefully select and compare among different LLMs when experimenting with retrieval-based translation.\n# 6.1 Language-Specific Effects\nWe identify a number of translation errors of varying types that appear to be due to language-specific factors such as those discussed in Section 3.1. For example, we find an effect of polysemous lexical items for all prompt types in the outputs of both models on which we conduct human evaluation. In the most straightforward cases, the model incorrectly generates an alternate sense of the word that is inappropriate given the content of the source sentence. We also find a number of instances in which the presence of such ambiguity has a cascading effect on lexical selection in the rest of the model\u2019s output. For example, when co-occuring with miski in the source text, the word lawa \u2018soup\u2019 is translated at times as mazamorra, a sweet porridge or pudding, crema \u2018cream\u2019, miel \u2018honey\u2019, golosina \u2018candy\u2019, or dulces, the nominal form of dulce meaning \u2018candy\u2019 or \u2018sweets.\u2019\nIt may be possible to moderate such effects with additional refinement of the database structure and retrieval methods, which we leave for future work.\n# 6.2 Ethical Concerns\nBoth our work and much of the previous work in this paradigm is motivated by the desire to close the \u201cNLP Gap\u201d among researchers, community members, and software developers interested in lowresource language technologies. Machine translation is listed as a welcome topic of research by some (though not all) members of American Indigenous communities (Mager et al., 2023), and is potentially an important tool for language learners (Jolley and Maimone, 2022). Even an imperfect translation system might be a useful tool for users with a clear understanding of its limitations. However, the systems evaluated in this work have two problematic tendencies that limit their potential for deployment in real community settings. First, unfaithful translations often tend to be highly fluent (Section 5.4). While fluency ratings for older MT systems correlate well with accuracy scores, and have even been used as a proxy for overall translation quality (Gamon et al., 2005; Estrella et al., 2007), this correlation is reversed for our systems. LLMs are well-known for making false statements that seem plausible and authoritative (Bickmore et al., 2018; Dinan et al., 2021); this could be particularly problematic when they project illusions of expertise at the expense of an already marginalized group. Second, some mistranslations identified in our study appear to draw on stereotypes of Indigenous groups (Appendix B). These are most apparent for the BASELINE system and GPT-3.5, but also (less frequently) occur with more informative prompts and better LLMs. Stereotypical sentences can involve flowery language with an emphasis on tradition or connectedness to nature (Erhart and Hall, 2019), as well as the unprompted addition of Indigenous Andean cultural customs and products (e.g., traditional medicine and chicha) to translations that are otherwise faithful to the source text. The overall effect is to exoticize Southern Quechua speakers and writers in ways that the original sentences do not. Similar stereotypes have also been noted in LLM-generated responses to open-ended prompts (Cheng et al., 2023; Delgado Solorzano and Toxtli, 2023; Shieh et al., 2024). While we prompt models to output only the translation for evaluation purposes, models may have\nsome capacity to explain or qualify their translations and give reminders for responsible use of the technology. Should a retrieval-based translation system ever be deployed in a real-world setting for language learning, its developers should maximize transparency by presenting the content of any retrieved information and its source to the user along with the translation, reminding users directly of potential inaccuracies, and offering vetted resources for additional fact-checking when available.\n# 7 Conclusion\nOur results suggest a number of key limitations and concerns regarding the use of LLMs in a lowresource MT context, and have greater implications for our understanding of the seemingly \u201chumanlike\u201d conceptual, analytical, and in-context learning abilities of LLMs. For the majority of the world\u2019s languages and their speakers, powering and supplying LLMs with enough pretraining data to overcome their limitations is not feasible. We therefore offer the following suggestions to those looking to develop lowresource LLM-MT: (1) improve data structures and methods for interacting with a language-specific database for retrieval-aided generation, (2) continue analysis of the mechanisms driving in-context learning in LLMs, for example by comparing ICL to the effects of finetuning (Dai et al., 2023), and (3) experiment with prompt structures and techniques, for example by altering the order of information (Liu et al., 2024) or by iteratively prompting the model to guide its reasoning towards a suitable translation (Wang et al., 2022). Finally, we wish to emphasize the continued risks of prematurely deploying this or similar methods in any low-resource language community, particularly given the vulnerability and disproportionate lack of resources many such communities face in domains where these technologies would likely be used. As AI research continues to rapidly develop, we urge those conducting it to increase community engagement, amplify the voices of those traditionally at a disadvantage, and collaboratively develop research infrastructures that may lessen the NLP Gap (Brinklow, 2021). While there\u2019s still much to be done before low-resource LLM-MT may be safely implemented, we believe such a tool has the potential to empower speakers of any variety, including nonstandard varieties of highresource languages such as English, to develop\ntechnologies that reflect their preferences and serve their unique needs.\n# 8 Limitations\nLimitations on the scope and replicability of this work may be attributed to one or more characteristics of the data and models used in this study, in addition to limitations inherent to the respective identities of its authors. First, the automatic metrics (i.e., BLEURT and BLEU scores) that we report are limited in their statistical validity. We have conducted some constrained tests to explore potential variance in scores, but expenses associated with text generation using proprietary models such as those developed by OpenAI and Google on a larger dataset may be prohibitive. This is compounded by the widely-acknowledged \u201cblack box\u201d nature of the models powering both LLMs and BLEURT, as well as an increasing opacity with respect to the exact content and methods used to pretrain modern state of the art LLMs. For this reason, we focus our discussion on those results that show clear trends in both the quantitative and human evaluations we conduct. There are also some constraints on our study and its methodology that are largely tied to linguistic factors, such as variation in orthography (and the need for digitized text-based resources as a prerequisite) as well as the lexical and grammatical variation that may be found in all languages, particularly the low-resource varieties we wish to support. We discuss some of these factors in Sections 3.1 and 6.1. Our results suggest it may be possible to guide the outputs of LLMs towards the specific usage conventions of a given community, but this is itself limited by the content of the materials used to develop the database from which prompt contexts are retrieved. Neither of the authors is a native speaker of any Quechua or Spanish varieties, and only one is a student of these languages and has relationships to Quechua speakers and communities. While we have strived to be consistent in the Quechua and Spanish varieties used in our study (both the dictionary and grammar materials were provided by the same instructor who shared and proofread the 50 sentence pairs we use, and we select a morphological parser and corpora intended for use with Southern Quechua), variation is widespread among and within Quechua-speaking communities, and we do not have access to a dictionary, grammar,\nmorphological parser, and corpus developed by a unified and consistent set of authors. Future work should continue to explore ways to faithfully represent the diversity of linguistic conventions employed by communities interested in developing such technologies. We acknowledge, as well, limitations that arise from the size of our dataset and database and the methods used to curate them. The 50 sentence pairs we use were selected to highlight a range of specific grammatical phenomena, not all of which were well represented in our database, and differ in their structural complexity. We are grateful for the guidance provided by the Quechua instructor whose lessons were a source for such examples and proofread the sentences before their inclusion in our experiments, but are limited by our status as non-native speakers. Human evaluation of model outputs was partially conducted using machine-translated English texts as references, but all annotations were inspected by the Spanish- and Quechua-speaking author who removed a small number of evaluations that reflected linguistic discrepancies between Quechua, Spanish, and English or inaccuracies in the machine-translated English.\n# 9 Ethics Statement\nWe consulted the first author\u2019s Quechua instructor, Prof. Carmen Cazorla Zen, who gave us permission to use the sentences from the notes in this project and verified their accuracy. We cite the Quechua dictionary and grammar materials used to provide prompt information, and believe that our use of these materials is consonant with their original purpose. However, we do not distribute machine-readable versions of them as a contribution of this project, since this would violate the rights of the publisher. These materials were developed for use as pedagogical resources by institutions affiliated with the governments of Cuzco, Peru and Apur\u00edmac, Peru, respectively. Their authors were not contacted or consulted as part of the project. We wish to acknowledge the delicate issue of academic extractiveness and its harmful impact on Indigenous and minority language communities and speakers. We are also aware of some of the controversial ideologies and policies associated with Qheswa Simi Hamut\u2019ana Kuraq Suntur, the government-afilliated institution who published the dictionary we use in this study, and the potentially\nnegative effects of government-sponsored linguistic standardization more broadly (see, e.g., Coronel Molina (2008) for an analysis of the effects of the institution\u2019s ideologies on revitalization efforts in Peru). We do not endorse such policies, and have sought to avoid representing the diversity of Southern Quechua-speaking communities as a monolith. Instead, we hope our continued efforts to improve methods for low-resource translation will empower speakers of Southern Quechua and other Indigenous and minority languages to develop language technologies capable of representing their own community\u2019s unique language variety to serve the unique needs of its speakers. There are numerous ethical issues related to the training and use of LLMs, such as labor issues and energy costs. While these issues are inextricable from the methods used in this project, we believe the potential impact of making low-resource translation viable and accessible to minority language communities who want them (our primary goal in this line of research) outweighs the problems inherent in using LLMs at all. We discuss the potential risks of deploying systems like the ones described here further in Section 6.2 of the main text.\n# Acknowledgments\nWe thank Prof. Carmen Cazorla Zen, Professor of Quechua, for her help curating the data used in this study and for deepening our understanding of Southern Quechua and its speakers. We also thank Prof. Elvia And\u00eda Gr\u00e1geda, Professor of Quechua, for her instruction and advice, and the OSU Linguistics department for their feedback on a preliminary presentation of the work.\n# References\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. See also: https://openai.com/index/hello-gpt-4o/. Milind Agarwal, Sweta Agrawal, Antonios Anastasopoulos, Luisa Bentivogli, Ond\u02c7rej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, et al. 2023. FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN. In Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023), pages 1\u201361, Toronto, Canada (in-person and online). Association for Computational Linguistics.\n\u017deljko Agi\u00b4c and Ivan Vuli\u00b4c. 2019. JW300: A widecoverage parallel corpus for low-resource languages. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3204\u20133210, Florence, Italy. Association for Computational Linguistics. Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad. 2022. Incontext examples selection for machine translation. Preprint, arXiv:2212.02437.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3204\u20133210, Florence, Italy. Association for Com-\nputational Linguistics.\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2022. In-\ncontext examples selection for machine translation.\nPreprint, arXiv:2212.02437.\nAI@Meta. 2024. Llama 3 model card.\nTimothy W Bickmore, Ha Trinh, Stefan Olafsson,\nTeresa K O\u2019Leary, Reza Asadi, Nathaniel M Rick-\nles, and Ricardo Cruz. 2018. Patient and consumer\nsafety risks when using conversational assistants for\nmedical information: An observational study of Siri,\nAlexa, and Google Assistant. J Med Internet Res,\n20(9):e11510.\nNathan Thanyeht\u00e9nhas Brinklow. 2021. Indigenous lan-\nguage technologies: Anti-colonial oases in a coloniz-\ning (digital) world. WINHEC: International Journal\nof Indigenous Education Scholarship, (1):239\u2013266.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell,\net al. 2020.\nLanguage models are\nfew-shot learners.\nPreprint, arXiv:2005.14165.\nSee also: https://openai.com/index/new-embedding-\nmodels-and-api-updates/.\nAljoscha Burchardt. 2013.\nMultidimensional qual-\nity metrics: a flexible system for assessing trans-\nlation quality. In Proceedings of Translating and the\nComputer 35, London, UK. Aslib.\nRonald Cardenas, Rodolfo Zevallos, Reynaldo Ba-\nquerizo, and Luis Camacho. 2018. Siminchik: A\nspeech corpus for preservation of Southern Quechua.\nISI-NLP, 2:21.\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nAnalyzing chatgpt\u2019s behavior shifts over time. In\nR0-FoMo: Robustness of Few-shot and Zero-shot\nLearning in Large Foundation Models.\nMyra Cheng, Esin Durmus, and Dan Jurafsky. 2023.\nMarked personas: Using natural language prompts\nto measure stereotypes in language models.\nIn\nProceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1504\u20131532, Toronto, Canada.\nAssociation for Computational Linguistics.\nSeraf\u00edn M. Coronel Molina. 2008. Language Ideologies\nof the High Academy of the Quechua Language in\nCuzco, Peru. Latin American and Caribbean Ethnic\nStudies, 3(3):319\u2013340.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nG\u00e9raldine Walther, Ekaterina Vylomova, Arya D. Mc-\nCarthy, Katharina Kann, Sabrina J. Mielke, Garrett\nAI@Meta. 2024. Llama 3 model card.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/aa86/aa8684d0-19c4-46a7-bef7-3115920c6104.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b95b/b95b3d41-8fbf-407e-8f72-4c190c83e257.png\" style=\"width: 50%;\"></div>\nAbteen Ebrahimi, Manuel Mager, Shruti Rijhwani,\nEnora Rice, Arturo Oncevay, Claudia Baltazar, Mar\u00eda\nCort\u00e9s, Cynthia Monta\u00f1o, John E. Ortega, Rolando\nCoto-solano, Hilaria Cruz, Alexis Palmer, and Katha-\nrina Kann. 2023.\nFindings of the AmericasNLP\n2023 shared task on machine translation into Indige-\nnous languages. In Proceedings of the Workshop\non Natural Language Processing for Indigenous\nLanguages of the Americas (AmericasNLP), pages\n206\u2013219, Toronto, Canada. Association for Compu-\ntational Linguistics.\nMicha Elsner and Jordan Needle. 2023.\nTranslat-\ning a low-resource language using GPT-3 and a\nhuman-readable dictionary. In Proceedings of the\n20th SIGMORPHON workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 1\u201313, Toronto, Canada. Association for Com-\nputational Linguistics.\nRyan S Erhart and Deborah L Hall. 2019. A descriptive\nand comparative analysis of the content of stereo-\ntypes about Native Americans.\nRace and Social\nProblems, 11:225\u2013242.\nPaula Estrella, Andrei Popescu-Belis, and Maghi King.\n2007. A new method for the study of correlations be-\ntween MT evaluation metrics. In Proceedings of the\n11th Conference on Theoretical and Methodological\nIssues in Machine Translation of Natural Languages:\nPapers, Sk\u00f6vde, Sweden.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\nAPE at scale and its implications on MT evaluation\nbiases. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 1: Research Papers),\npages 34\u201344, Florence, Italy. Association for Com-\nputational Linguistics.\nMichael Gamon, Anthony Aue, and Martine Smets.\n2005.\nSentence-level MT evaluation without ref-\nerence translations:\nBeyond language modeling.\nIn Proceedings of the 10th EAMT Conference:\nPractical applications of machine translation.\nLuke Gessler. 2022. Closing the NLP gap: Documen-\ntary linguistics and NLP need a shared software in-\nfrastructure. In Proceedings of the Fifth Workshop\non the Use of Computational Methods in the Study\nof Endangered Languages, pages 119\u2013126, Dublin,\nIreland. Association for Computational Linguistics.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based phrase-level prompt-\ning of large language models for machine translation.\nPreprint, arXiv:2302.07856.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Has-\nsan Awadalla. 2023. How good are GPT models\nat machine translation? A comprehensive evaluation.\nPreprint, arXiv:2302.09210.\nJason R Jolley and Luciane Maimone. 2022. Thirty\nyears of machine translation in language teaching\nand learning: A review of the literature. L2 Journal,\n14(1):26\u201344.\nTom Kocmi, Eleftherios Avramidis, Rachel Bawden,\nOnd\u02c7rej Bojar, Anton Dvorkovich, Christian Feder-\nmann, Mark Fishel, Markus Freitag, Thamme Gowda,\nRoman Grundkiewicz, Barry Haddow, et al. 2023.\nFindings of the 2023 Conference on Machine Trans-\nlation (WMT23): LLMs are here but not quite there\nyet.\nIn Proceedings of the Eighth Conference on\nMachine Translation, pages 1\u201342, Singapore. Asso-\nciation for Computational Linguistics.\nMoshe Koppel and Noam Ordan. 2011. Translationese\nand its dialects. In Proceedings of the 49th Annual\nMeeting of the Association for Computational\nLinguistics: Human Language Technologies, pages\n1318\u20131326, Portland, Oregon, USA. Association for\nComputational Linguistics.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2024. Lost in the Middle: How Language\nModels Use Long Contexts.\nTransactions of the\nAssociation for Computational Linguistics, 12:157\u2013\n173.\nZoey Liu, Crystal Richardson, Richard Hatcher, and\nEmily Prud\u2019hommeaux. 2022. Not always about\nyou: Prioritizing community needs when developing\nendangered language technology. In Proceedings\nof the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong\nPapers), pages 3933\u20133944, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nHongyuan Lu, Haoyang Huang, Dongdong Zhang, Hao-\nran Yang, Wai Lam, and Furu Wei. 2023. Chain-\nof-dictionary prompting elicits translation in large\nlanguage models. Preprint, arXiv:2305.06575.\nManuel Mager, Elisabeth Mager, Katharina Kann, and\nNgoc Thang Vu. 2023.\nEthical considerations\nfor machine translation of Indigenous languages:\nGiving a voice to the speakers.\nIn Proceedings\nof the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1:\nLong\nPapers), pages 4871\u20134897, Toronto, Canada. Associ-\nation for Computational Linguistics.\nManuel Mager, Arturo Oncevay, Abteen Ebrahimi, John\nOrtega, Annette Rios, Angela Fan, Ximena Gutierrez-\nVasques, Luis Chiruzzo, Gustavo Gim\u00e9nez-Lugo, Ri-\ncardo Ramos, et al. 2021. Findings of the Amer-\nicasNLP 2021 shared task on open machine trans-\nlation for Indigenous languages of the Americas.\nIn Proceedings of the First Workshop on Natural\nLanguage Processing for Indigenous Languages of\nthe Americas, pages 202\u2013217, Online. Association\nfor Computational Linguistics.\nR Thomas McCoy, Shunyu Yao, Dan Friedman,\nMatthew Hardy, and Thomas L Griffiths. 2023. Em-\nbers of autoregression: Understanding large language\nmodels through the problem they are trained to solve.\narXiv preprint arXiv:2309.13638.\nGabriel Nicholas and Aliya Bhatia. 2023. Lost in trans-\nlation: Large language models in non-english content\nanalysis. Preprint, arXiv:2306.07377.\nJohn E Ortega, Richard Castro Mamani, and Kyunghyun\nCho. 2020.\nNeural machine translation with a\npolysynthetic low resource language.\nMachine\nTranslation, 34(4):325\u2013346.\nMiguel \u00c1ngel Pinto Tapia, Luis Quispe Z\u00fa\u00f1iga, et al.\n2005. Did\u00e1ctica quechua i. Documento de trabajo,\nDirecci\u00f3n Regional de Educaci\u00f3n Apur\u00edmac | Direc-\nci\u00f3n Gesti\u00f3n Pedag\u00f3gica.\nQheswa\nSimi\nHamut\u2019ana\nKurak\nSuntur.\n2005.\nDiccionario Quechua - Espa\u00f1ol - Quechua Qheswa -\nEspa\u00f1ol - Qheswa Simi Taqe, 2 edition. Multiservi-\ncios e Imprenta Edmundo Pantigozo EIRL, Cusco,\nPeru.\nMachel Reid, Nikolay Savinov, Denis Teplyashin,\nDmitry Lepikhin, Timothy Lillicrap, Jean-Baptiste\nAlayrac, Radu Soricut, Angeliki Lazaridou, Orhan Fi-\nrat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Un-\nlocking multimodal understanding across millions of\ntokens of context. arXiv preprint arXiv:2403.05530.\nKatherine\nJ.\nRiestenberg,\nAlly\nFreemond,\nBrook Danielle Lillehaugen,\nand Jonathan N.\nWashington. 2024. Prioritizing Community Partners\u2019\nGoals in Projects to Support Indigenous Language\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/8d1f/8d1f6493-e2d7-4cc6-bedb-b12fe42f8f64.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Gabriel Nicholas and Aliya Bhatia. 2023. Lost in translation: Large language models in non-english content analysis. Preprint, arXiv:2306.07377.</div>\nUniversity Press.\nAnnette Rios. 2015.\nA basic language technology\ntoolkit for Quechua.\nPh.D. thesis, University of\nZurich.\nAnnette Rios and Richard Castro Mamani. 2014. Mor-\nphological disambiguation and text normalization for\nsouthern Quechua varieties.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond.\nFoundations and Trends in Information\nRetrieval, 3(4):333\u2013389.\nNathaniel Robinson, Perez Ogayo, David R. Mortensen,\nand Graham Neubig. 2023. ChatGPT MT: Competi-\ntive for high- (but not low-) resource languages. In\nProceedings of the Eighth Conference on Machine\nTranslation, pages 392\u2013418, Singapore. Association\nfor Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning.\nIn Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2655\u20132671, Seattle, United\nStates. Association for Computational Linguistics.\nSylvia L.R. Schreiner, Lane Schwartz, Benjamin\nHunt, and Emily Chen. 2020.\nMultidirectional\nleveraging for computational morphology and lan-\nguage documentation and revitalization. Language\ndocumentation and conservation, 14.\nThibault Sellam, Dipanjan Das, and Ankur P. Parikh.\n2020. BLEURT: Learning robust metrics for text\ngeneration. Preprint, arXiv:2004.04696.\nEvan Shieh, Faye-Marie Vassel, Cassidy Sugimoto, and\nThema Monroe-White. 2024. Laissez-faire harms:\nAlgorithmic biases in generative language models.\nPreprint, arXiv:2404.07475.\nDavid Stap and Ali Araabi. 2023.\nChatGPT\nis\nnot\na\ngood\nIndigenous\ntranslator.\nIn\nProceedings of the Workshop on Natural Language\nProcessing\nfor\nIndigenous\nLanguages\nof\nthe\nAmericas (AmericasNLP), pages 163\u2013167, Toronto,\nCanada. Association for Computational Linguistics.\nGarrett Tanzer, Mirac Suzgun, Eline Visser, Dan Juraf-\nsky, and Luke Melas-Kyriazi. 2024. A benchmark\nfor learning to translate a new language from one\ngrammar book. Preprint, arXiv:2309.16575.\nJ\u00f6rg Tiedemann. 2012.\nParallel data, tools and in-\nterfaces in OPUS.\nIn Proceedings of the Eighth\nInternational Conference on Language Resources\nand Evaluation (LREC\u201912), pages 2214\u20132218, Istan-\nbul, Turkey. European Language Resources Associa-\ntion (ELRA).\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0921/0921e310-6842-40b3-9e52-0027c1636cb5.png\" style=\"width: 50%;\"></div>\nThe following section provides examples of errors analyzed in Section 5.3, one error per type.\nThe following section provides examples of errors\nanalyzed in Section 5.3, one error per type.\nMistranslation: Complete Mistranslation\nModel: GPT-3.5 - BASELINE - AUTO\nSource:\nqamqa taytaykipa munasqan lawata\nyanurqanki\nGloss:\nqam-qa\nyou-TOP\ntayta-yki-pa\nfather-2.POSS-GEN\nmuna-sqa-n\nlike-SUBR-3SG\nlawa-ta\nsoup-ACC\nyanu-rqa-nki\ncook-PST-2SG\nReference: t\u00fa cocinaste la sopita que le gusta a tu\npap\u00e1\nyou cooked the soup that your dad likes\nOutput: yo te quiero mucho y mi amor por ti\nnunca se acabar\u00e1\nI love you very much and my love for you will\nnever end\nMistranslation: Lexical Correspondence\nModel: GPT-4o - C+G+M - AUTO\nSource:\n\u00f1uqaqa illani runakunawan rimayta\nmunaptiy\nGloss:\n\u00f1uqa-qa\nI-TOP\nilla-ni\ntravel-1SG\nruna-kuna-wan\nperson-PL-with\nrima-y-ta\nspeak-ACC\nmuna-pti-y\nlike-SUBR-1SG\nReference: yo viajo porque me gustar hablar con\nlas personas\nI travel because I like to talk to people\nOutput: yo quiero hablar con las personas cuando\namanece\nI want to talk to people when the sun rises\nTarget Fluency: Coherence\nUnresolvable pronoun\nSource: qamqa yachayta tukuptiyki, pu\u00f1unki\nGloss:\nqam-qa\nyou-TOP\nyacha-y-ta\nstudy-INF-ACC\ntuku-pti-yki,\nfinish-SUBR-2SG,\npu\u00f1u-nki\nsleep-2SG\nReference:\ncuando termines de estudiar, tu\nduermes\nwhen you finish studying, you sleep\nOutput: acabar\u00e9 de aprender lo que t\u00fa duermes\nI will finish learning what you sleep\nTarget Fluency: Grammar\nMissing causative and possessive\nModel: GPT-4o - CORPUS - AUTO\nSource: \u00f1uqaqa \u00f1a\u00f1ayman wasita pichachini\nGloss:\n\u00f1uqa-qa\nI-TOP\n\u00f1a\u00f1a-y-man\nsister-1SG.POSS-ILL\nwasi-ta\nhouse-ACC\npicha-chi-ni\nclean-CAUS-1SG\nReference:\nyo le hago limpiar la casa a mi\nhermana\nI make my sister clean the house\nOutput: yo hermana casa limpio\nI sister house clean\nGrammatical Divergence: TAM\nTense\nModel: GPT-3.5 - C+G+M - AUTO\nSource: qam allinta tusunki\nGloss:\nqam\nyou\nallin-ta\ngood-ACC\ntusu-nki\ndance-2SG\nReference: tu bailas bien\nyou dance well\nOutput: t\u00fa bailar\u00e1s bien\nyou will dance well\nGrammatical Divergence: Subject\n\u2018you\u2019 to \u2018I\u2019\nModel: GPT-3.5 - MORPH - AUTO\nSource: qamqa mamaykipa wasinta yanuq rinki\nGloss:\nqam-qa\nyou-TOP\nmama-yki-pa\nmother-2.POSS-GEN\nwasi-n-ta\nhouse-3SG.POSS-ACC\nyanu-q\ncook-AGT\nri-nki\ngo-2SG\nReference: t\u00fa ir\u00e1s a cocinar a la casa de tu mam\u00e1\nyou will go to cook at your mother\u2019s house\nOutput: voy a cocinar en la casa de tu madre\nI\u2019m going to cook at your mother\u2019s house\nGrammatical Divergence: Subject\n\u2018you\u2019 to \u2018the man\u2019\nModel: GPT-4o - C+G+M - AUTO\nSource:\nqamqa misita qawaspa qari warata\nawachkarqanki\nGloss:\nqam-qa\nyou-TOP\nmisi-ta\ncat-ACC\nqawa-spa\nwatch-SUBR\nqari\nmale\nwara-ta\npants-ACC\nawa-chka-rqa-nki\nknit-PROG-PST-2SG\nReference: t\u00fa estabas tejiendo un pantal\u00f3n de\nvar\u00f3n mirando al gato\nyou were knitting a pair of men\u2019s pants looking at\nthe cat\nOutput: t\u00fa mirabas al gato mientras el hombre\ntej\u00eda el pantal\u00f3n\nyou looked at the cat while the man knitted the\npants\nGrammatical Divergence: TAM\nMissing conditional\nModel: GPT-4o - MORPH - AUTO\nModel: GPT-4o - MORPH - AUTO\nqam you runa-simi-ta people-mouth-ACC yacha-paya-pti-yki, learn-REP-COND-2SG, allin-ta-m good-ACC-ASRT rima-nki speak-2SG\nReference: si practicas el quechua lo hablar\u00e1s bien if you practice Quechua you will speak it well\nOutput: t\u00fa ya sab\u00edas el runasimi, hablas muy bien you already knew runasimi, you speak very well\n# B Unprompted Stereotypes\nThe following section provides examples of errors involving cultural stereotypes as discussed in Section 6.2.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6af6/6af6e5cb-ee8d-4eb9-857c-21d24d12efdc.png\" style=\"width: 50%;\"></div>\nThe following section provides examples of errors\ninvolving cultural stereotypes as discussed in\nSection 6.2.\nModel: GPT-3.5 - BASELINE - AUTO\nSource: payqa qatupi \u00f1a\u00f1anpa rantisqan apita\nrurarqan\nGloss:\npay-qa\n3-TOP\nqatu-pi\nmarket-LOC\n\u00f1a\u00f1a-n-pa\nsister-3SG.POSS-GEN\nranti-sqa-n\nbuy-SUBR-3SG\napi-ta\nmazamorra-ACC\nrura-rqa-n\nmake-PST-3SG\nReference:\nella prepar\u00f3 una mazamorra que\ncompr\u00f3 su hermana en el mercado\nshe prepared a mazamorra [pudding] that her\nsister bought at the market\nOutput: el sol brilla en nuestra tierra, calienta y\nnos da vida\nthe sun shines on our land, warms us and gives us\nlife\nModel: GPT-3.5 - GRAMMAR\nSource:\npaykunaqa yachachiqninkupa rakta\nmedicina liwru apasqanta likanku\nGloss:\npay-kuna-qa\n3-PL-TOP\nyacha-chi-q-ni-nku-pa\nlearn-CAUS-AGT-NI-3PL.POSS-GEN\nrakta\nthick\nmedicina\nmedicine\nliwru\nbook\napa-sqa-n-ta\ncarry-SUBR-3SG-ACC\nlika-nku\nread-3PL\nReference: ellos leer\u00e1n el libro grueso de medicina\nque trajo su profesor\nthey will read the thick medical book that their\nteacher brought\nOutput:\nlos m\u00e9dicos de la comunidad nos\nense\u00f1aron a utilizar plantas medicinales\nthe doctors from the community taught us how to\nuse medicinal plants\nOutput: el sol brilla en nuestra tierra, calienta y nos da vida the sun shines on our land, warms us and gives us life\nSource: paykunaqa yachachiqninkupa rakta medicina liwru apasqanta likanku\nSource: qamkunapa taytamamaykichikta watukunaykichik, paykunata purikuq apanaykichik\n<div style=\"text-align: center;\">C Quality Descriptions</div>\nQuality\nDescription\nHigh\nOutput is an accurate and/or acceptable translation of the source content.\nMed\nOutput contains errors that prevent it from being an acceptable translation, but is generally\nhigh in quality otherwise.\nLow\nOutput contains errors that prevent it from being an acceptable translation, with minor\ncorrespondences that vaguely identify it as relevant to the source.\nNone\nOutput does not appear to be relevant to the source.\n<div style=\"text-align: center;\">D Annotation Error Typology</div>\nDimension\nError\nDescription\nAccuracy\nAddition\nTranslation includes information not present in the source, but does\nnot result in the displacement of source content.\nAccuracy\nOmission\nTranslation is missing content from the source.\nAccuracy\nSubstitution - Subject\nThe translated segment contains content identified as relevant to the\nsource in other spans, but substitutes novel subject markers for those\npresent in the source in the highlighted span; Classify an error as a\n\u201csubstitution\u201d when the error appears to result in both Addition and\nOmission errors that cannot be distinguished into two distinct spans.\nAccuracy\nSubstitution - TAM\nThe translated segment contains content identified as relevant to the\nsource in other spans, but substitutes novel TAM for those present in\nthe source in the highlighted span; Classify an error as a \u201csubstitution\u201d\nwhen the error appears to result in both Addition and Omission errors\nthat cannot be distinguished into two distinct spans.\nAccuracy\nSubstitution - Other\nSubstitution errors that do not involve mistranslated subject markers or\nTAM. See above.\nAccuracy\nOvertranslation\nError occurring in the target content that is inappropriately more\nspecific than the source content.\nAccuracy\nUndertranslation\nError occurring in the target content that is inappropriately less spe-\ncific than the source content.\nTarget Error\nGrammar\nOther spans in the translated segment may be identified as relevant to\nthe source, but the highlighted span is not grammatical in the target\nlanguage.\nTarget Error\nCoherence\nOther spans in the translated segment may be identified as relevant to\nthe source, but the highlighted span is unnatural or incoherent in the\ntarget language.\nTarget Error\nStyle/Register\nOther spans in the translated segment may be identified as relevant to\nthe source, but the highlighted span is produced in a style or register\nthat is inappropriate given the content.\nNon-Translation\nComplete Mistransla-\ntion\nThe entire segment is coherent in the target language but the core\npredicate shows no immediate connection to the reference translation.\nNon-Translation\nMistranslation - Lexical\nCorrespondence\nThe entire segment is coherent in the target language but only minor\ncorrespondences to the reference translation may be identified.\nNon-Translation\nRefusal\nModel does not attempt to translate into the target language, e.g.,\nbecause it \"does not understand\".\nModel error\nGarbled\nOutput does not contain coherent text in the target language.\nModel error\nChattyGPT\nOutput contains translated content, but is wordy, over-explanatory,\nand/or abruptly truncated.\n<div style=\"text-align: center;\">Table 6: Adapted MQM typology for human error annotation</div>\n# E Example Prompts\nThe following are examples of prompts generated used automated retrieval from the database. English is included in italics for the reader, but was not provided to the models as part of the prompt.\nBASELINE\n[TAREA] Traduce la siguiente frase del quechua al\nespa\u00f1ol. Responde s\u00f3lo con la traducci\u00f3n:\nquechua: qam allinta tusunki\nespa\u00f1ol:\n[TASK] Translate the following sentence from\nQuechua to Spanish.\nRespond only with the\ntranslation:\nQuechua: You dance well\nSpanish:\n[CONTEXTO] qam: [PrnPers+2sg] allin: bueno [ \u02c6DB][NRoot] ta: [+Acc][Cas] tusu: bailar [VRoot][ \u02c6DB] nki: [+2sg.Subj][VPers]\nallin. adj. Bueno (t\u00e9rmino de aprobaci\u00f3n). SIN\u00d3N: kusa. EJEM: allin p\u2019unchay, buenos d\u00edas: allin tuta, buenas noches; allin tutamanta, buena ma\u00f1ana, buenos d\u00edas; allin inti chinkay, buenas tardes; allin i\u00f1iyniyoq, de buena fe, fiel, justo, \u00edntegro: allin nunayoq, de esp\u00edritu bueno; allin puriq, de comportamiento bueno; allin puriy, comportamiento bueno; allin rikuy, tratamiento bueno; allin rikuq, el que trata bien; allin ruway, obrar bien, beneficiar; lo que se hace bien, beneficioso; allin ruwaq, el que hace bien; allin yuyay, pensar bien; pensamiento bueno; allin qolqeyoq, poseedor de plata fina; adinerado. ta. s. Gram. Sufijo que desempe\u00f1a los papeles de art\u00edculo y preposici\u00f3n. EJEM: llamata qatiy, arrea la llama; Urkusmanta hamuni, vengo de Urcos.\n# [TAREA] Traduce la siguiente frase . . .\n[TAREA] Traduce la siguiente frase . . .\n[CONTEXT] qam: [PrnPers+2sg] allin: bueno [ \u02c6DB][NRoot] ta: [+Acc][Cas]\ntusu: bailar [VRoot][ \u02c6DB] nki: [+2sg.Subj][VPers] allin. adj. Good (term of approval). SYN: kusa. EX: allin p\u2019unchay, good day: allin tuta, good evening; allin tutamanta, good morning, good day; allin inti chinkay, good afternoon; allin i\u00f1iyniyoq, good faith, faithful, just, upright: allin nunayoq, in good spirits; allin puriq, well behaved; allin puriy, good behavior; allin rikuy, good treatment; allin rikuq, one who treats others well; allin ruway, to do good, to benefit; one who does good, beneficial; allin ruwaq, one who does good; allin yuyay, think well; good thought; allin qolqeyoq, possessor of fine silver; wealthy. ta. s. Gram. Suffix that plays the roles of article and preposition. EX: llamata qatiy, herd the llama; Urkusmanta hamuni, I come from Urcos.\n[CONTEXTO] ta: CASO ACUSATIVO. Su marca es \u2013ta, esta es una marca de objeto directo con los verbos que no son de movimiento (quietud). Ejemplo: Quyllur\u2013ta qhawani Veo una estrella T\u2019anta\u2013ta apay Lleva pan \u00d1uqa quylluyta qhawani Pedrucha t\u2019antata rantin En cambio con los verbos de movimiento \u2013ta indica (hacia) que es igual a meta. Ejemplos: Punu\u2013ta rini Voy a Puno Llaqta-ta risaq Ir\u00e9 al pueblo Hamawt\u2019anchis Punuta rinqa Llanta umalliq llaqtata richkan nki: FLEXI\u00d3N DE TIEMPO. TIEMPO FUTURO. TIEMPO FUTURO. Los sufijos para cada una de las personas gramaticales son: saq, nki, nqa, sun, saqku, nkichis, nqaku; en singular y plural respectivamente. Ejemplos: Puklla-saq jugar\u00e9 Puklla-nki jugar\u00e1s Puklla-nqa jugar\u00e1 Puklla-sun jugaremos Puklla-saqku jugaremos Puklla-nkichis Uds. jugar\u00e1n Puklla-nqaku ellos jugar\u00e1n\n[TAREA] Traduce la siguiente frase . . .\n[CONTEXT] ta: ACCUSATIVE CASE. Marked by \u2013ta, this is a direct object marker with verbs that don\u2019t indicate movement. Example: Quyllur\u2013ta qhawani I see a star T\u2019anta\u2013ta apay Bring bread \u00d1uqa quylluyta qhawani Pedrucha t\u2019antata rantin On the other hand, with verbs of motion -ta indicates (towards) the same goal Examples: Punu\u2013ta rini I go to Puno Llaqta-ta risaq I will go to town Hamawt\u2019anchis Punuta rinqa Llanta umalliq llaqtata richkan nki: TENSE INFLECTION. FUTURE TENSE. FUTURE TENSE. The suffixes for each of the grammatical persons are: saq, nki, nqa, sun, saqku, nkichis, nqaku; in singular and plural respectively. Ejemplos: Puklla-saq jugar\u00e9 Puklla-nki jugar\u00e1s Puklla-nqa jugar\u00e1 Puklla-sun jugaremos Puklla-saqku jugaremos Puklla-nkichis Uds. jugar\u00e1n Puklla-nqaku ellos jugar\u00e1n\nCORPUS-ONLY\n[CONTEXTO]\nquechua: rimanakunapaq wawakunapa rimasqan\nsimi\naswan\nallinta\ntakyachinaraq\npiwanpas\nmaywanpas\nmana\nmanchakuspa\nrimananpaq\nchaymi qillqanapaqpas \u00f1awichanapaqpas aswan\nallin kanqa\nespa\u00f1ol: para este di\u00e1logo saber la lengua que\ndominan los ni\u00f1os ser\u00eda importante para que ellos\nse expresen sin miedo de ah\u00ed ser\u00e1 que la escritura y\nla lectura salga de manera \u00f3ptima\nquechua:\nkay tiqsipi sumaq rimanakunapaqa\nkawsayninchikmi allinta kallpachawanchik runaku-\nnahina allinta tiyanapaq chaymi \u00f1uqanchikkqa\nallinta \u00f1awichayta qillqayta yachananchik \u00f1awpa\nayllunchikkuna rurasqankuta maytukunapi tukuy\npuy\u00f1ukunapi tiqsi muyu qhawarisqankuta\nespa\u00f1ol:\npara vivir en armon\u00eda tenemos que\nconocer bien nuestra forma de vivir y luego\nescribir leer tambien a valorar lo que nos dejaron\n[CONTEXTO] quechua: rimanakunapaq wawakunapa rimasqan simi aswan allinta takyachinaraq piwanpas maywanpas mana manchakuspa rimananpaq chaymi qillqanapaqpas \u00f1awichanapaqpas aswan allin kanqa espa\u00f1ol: para este di\u00e1logo saber la lengua que dominan los ni\u00f1os ser\u00eda importante para que ellos se expresen sin miedo de ah\u00ed ser\u00e1 que la escritura y la lectura salga de manera \u00f3ptima quechua: kay tiqsipi sumaq rimanakunapaqa kawsayninchikmi allinta kallpachawanchik runakunahina allinta tiyanapaq chaymi \u00f1uqanchikkqa allinta \u00f1awichayta qillqayta yachananchik \u00f1awpa ayllunchikkuna rurasqankuta maytukunapi tukuy puy\u00f1ukunapi tiqsi muyu qhawarisqankuta espa\u00f1ol: para vivir en armon\u00eda tenemos que conocer bien nuestra forma de vivir y luego escribir leer tambien a valorar lo que nos dejaron\n[CONTEXTO] quechua: rimanakunapaq wawakunapa rimasqan simi aswan allinta takyachinaraq piwanpas maywanpas mana manchakuspa rimananpaq chaymi qillqanapaqpas \u00f1awichanapaqpas aswan allin kanqa espa\u00f1ol: para este di\u00e1logo saber la lengua que dominan los ni\u00f1os ser\u00eda importante para que ellos se expresen sin miedo de ah\u00ed ser\u00e1 que la escritura y la lectura salga de manera \u00f3ptima quechua: kay tiqsipi sumaq rimanakunapaqa kawsayninchikmi allinta kallpachawanchik runakunahina allinta tiyanapaq chaymi \u00f1uqanchikkqa allinta \u00f1awichayta qillqayta yachananchik \u00f1awpa ayllunchikkuna rurasqankuta maytukunapi tukuy puy\u00f1ukunapi tiqsi muyu qhawarisqankuta espa\u00f1ol: para vivir en armon\u00eda tenemos que conocer bien nuestra forma de vivir y luego escribir leer tambien a valorar lo que nos dejaron\nnuestros antecesores en cada visi\u00f3n sobre el mundo quechua: winsislawcha chayarqamuptinsi tuparquspanku allinta qatunakusqanku suwakuypi purinankupaq espa\u00f1ol: cuando hab\u00eda llegado wenseslau y a su encuentro se hab\u00edan reforzar\u00f3n para andar a robar\n# [TAREA] Traduce la siguiente frase . . .\n[TAREA] Traduce la siguiente frase . . .\n[CONTEXT] quechua: rimanakunapaq wawakunapa rimasqan simi aswan allinta takyachinaraq piwanpas maywanpas mana manchakuspa rimananpaq chaymi qillqanapaqpas \u00f1awichanapaqpas aswan allin kanqa Spanish: For this dialogue, knowing the language that the children speak would be important for them to express themselves without fear, and that is why writing and reading will be optimal. quechua: kay tiqsipi sumaq rimanakunapaqa kawsayninchikmi allinta kallpachawanchik runakunahina allinta tiyanapaq chaymi \u00f1uqanchikkqa allinta \u00f1awichayta qillqayta yachananchik \u00f1awpa ayllunchikkuna rurasqankuta maytukunapi tukuy puy\u00f1ukunapi tiqsi muyu qhawarisqankuta Spanish:To live in harmony we have to know our way of life well and then write and read to also value what our ancestors left us in each vision of the world. quechua: winsislawcha chayarqamuptinsi tuparquspanku allinta qatunakusqanku suwakuypi purinankupaq espa\u00f1ol: cuando hab\u00eda llegado wenseslau y a su encuentro se hab\u00edan reforzar\u00f3n para andar a robar\n[TASK] Translate the following sentence . . .\n# F Full Quality Scores\nThis section contains tables showing all automatic and human-annotated quality scores for each of our experiments. Table 7 contains the full set of BLEURT scores summarized in Tables 1 and 3 of the main text. Table 8 shows the corresponding BLEU scores for the same experiments. Table 9 and Table 10 contain the full set of the human-annotated scores summarized in Table 3.\n<div style=\"text-align: center;\">GPT-3.5</div>\nGPT-3.5\nGPT-4o\nGemini-1.5\nLlama 3\nauto\nmanual\nauto\nmanual\nauto\nmanual\nauto\nmanual\nBASELINE\n0.19\n0.22\n0.66\n0.66\n0.56\n0.57\n0.15\n0.16\nCORPUS-ONLY\n0.27\n0.29\n0.59\n0.61\n0.49\n0.47\n0.19\n0.18\nGRAMMAR-ONLY\n0.23\n0.24\n0.56\n0.58\n0.55\n0.54\n0.17\n0.15\nMORPH-ONLY\n0.44\n0.56\n0.54\n0.63\n0.61\n0.66\n0.39\n0.49\nCORPUS-GRAMMAR\n0.26\n0.28\n0.59\n0.59\n0.54\n0.53\n0.21\n0.21\nCORPUS-MORPH\n0.44\n0.52\n0.59\n0.64\n0.59\n0.64\n0.36\n0.38\nGRAMMAR-MORPH\n0.41\n0.54\n0.53\n0.61\n0.61\n0.64\n0.39\n0.37\nCORPUS-GRAMMAR-MORPH\n0.43\n0.54\n0.57\n0.63\n0.61\n0.63\n0.15\n0.26\nTable 7: BLEURT scores for all LLMs and prompt types.\nGPT-3.5\nGPT-4o\nGemini-1.5 Pro\nLlama 3 8B\nauto\nmanual\nauto\nmanual\nauto\nmanual\nauto\nmanual\nBASELINE\n0.01\n0.02\n0.19\n0.18\n0.12\n0.14\n0.00\n0.00\nCORPUS-ONLY\n0.02\n0.02\n0.16\n0.22\n0.14\n0.13\n0.02\n0.01\nGRAMMAR-ONLY\n0.01\n0.03\n0.14\n0.12\n0.18\n0.17\n0.01\n0.01\nMORPHS-ONLY\n0.06\n0.08\n0.12\n0.13\n0.15\n0.18\n0.03\n0.05\nCORPUS-GRAMMAR\n0.01\n0.01\n0.14\n0.17\n0.12\n0.08\n0.01\n0.01\nCORPUS-MORPHS\n0.05\n0.08\n0.19\n0.18\n0.17\n0.17\n0.02\n0.04\nGRAMMAR-MORPHS\n0.03\n0.04\n0.11\n0.10\n0.15\n0.16\n0.02\n0.01\nCORPUS-GRAMMAR-MORPHS\n0.04\n0.04\n0.16\n0.16\n0.17\n0.20\n0.00\n0.01\n<div style=\"text-align: center;\">Table 8: BLEU scores for all LLMs and prompt types.</div>\n<div style=\"text-align: center;\">GPT-3.5 Turbo</div>\nNone\nLow\nMed\nHigh\nBASELINE\n31\n17\n2\n0\nCORPUS-ONLY\n18\n23\n8\n1\nGRAMMAR-ONLY\n20\n27\n2\n1\nMORPHS-ONLY\n3\n22\n16\n9\nCORPUS-GRAMMAR\n18\n23\n9\n0\nCORPUS-MORPH\n2\n28\n12\n8\nGRAMMAR-MORPH\n3\n29\n13\n5\nCORPUS-GRAMMAR-MORPH\n2\n27\n12\n9\nTable 9: Human quality annotation of GPT-3.5 outputs with automated retrieval (raw counts out of 50) by promp\n<div style=\"text-align: center;\">GPT-4o</div>\nNone\nLow\nMed\nHigh\nBASELINE\n0\n10\n20\n20\nCORPUS-ONLY\n1\n16\n13\n20\nGRAMMAR-ONLY\n0\n17\n16\n",
    "paper_type": "benchmark",
    "attri": {
        "background": {
            "problem background": "Low-resource machine translation (MT) presents significant challenges due to the scarcity of data for the majority of the world\u2019s languages, leading to disparities in access to language technologies.",
            "purpose of benchmark": "The benchmark is intended for evaluating the in-context learning abilities of LLMs when translating between low-resource and high-resource languages."
        },
        "problem": {
            "definition": "The benchmark addresses the challenge of translating Southern Quechua to Spanish, focusing on the effectiveness of LLMs with limited resources.",
            "key obstacle": "Existing benchmarks do not adequately evaluate the unique challenges faced in low-resource language translation, such as the lack of standardized corpora."
        },
        "idea": {
            "intuition": "The authors were inspired by the potential of leveraging small amounts of pedagogical materials to improve translation outcomes for low-resource languages.",
            "opinion": "The authors emphasize the importance of this benchmark in highlighting the capabilities and limitations of LLMs in low-resource translation contexts.",
            "innovation": "This benchmark introduces a novel assessment framework that incorporates various context types and retrieval methods to evaluate translation performance.",
            "benchmark abbreviation": "LLM-MT"
        },
        "dataset": {
            "source": "The dataset was created from a collection of 50 pairs of Southern Quechua and Spanish sentences sourced from pedagogical materials.",
            "desc": "The dataset includes a diverse range of sentences that represent various grammatical phenomena and levels of difficulty.",
            "content": "The dataset contains text data in both Southern Quechua and Spanish, focusing on translation tasks.",
            "size": "50",
            "domain": "Machine Translation",
            "task format": "Translation"
        },
        "metrics": {
            "metric name": "BLEURT, BLEU",
            "aspect": "Translation quality",
            "principle": "Metrics were selected based on their ability to reflect the accuracy and fluency of translations.",
            "procedure": "Models were evaluated using both automatic metrics and human annotations to assess translation quality."
        },
        "experiments": {
            "model": "GPT-3.5 Turbo, GPT-4o, Gemini 1.5 Pro, Llama 3",
            "procedure": "Models were tested using different prompt types that included morpheme translations, grammar descriptions, and parallel usage examples.",
            "result": "Results indicated that translation quality varied significantly across models and prompt types, with some models showing better performance with specific context types.",
            "variability": "Variability was accounted for through multiple trials and the use of different models and retrieval methods."
        },
        "conclusion": "The experiments reveal key limitations in LLMs for low-resource translation, underscoring the need for improved methods and data structures to enhance translation quality.",
        "discussion": {
            "advantage": "The benchmark provides a structured approach to evaluate LLMs in low-resource contexts, contributing valuable insights to the field.",
            "limitation": "The benchmark's effectiveness may be limited by the small dataset size and the inherent challenges of low-resource languages.",
            "future work": "Future research should focus on expanding the dataset and exploring additional retrieval methods to improve translation outcomes."
        },
        "other info": {
            "ethical concerns": "The authors highlight the ethical implications of deploying LLMs in low-resource contexts and the importance of community engagement in language technology development."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The benchmark addresses the challenge of translating Southern Quechua to Spanish, focusing on the effectiveness of LLMs with limited resources."
        },
        {
            "section number": "1.2",
            "key information": "Low-resource machine translation (MT) presents significant challenges due to the scarcity of data for the majority of the world\u2019s languages, leading to disparities in access to language technologies."
        },
        {
            "section number": "3.1",
            "key information": "Results indicated that translation quality varied significantly across models and prompt types, with some models showing better performance with specific context types."
        },
        {
            "section number": "4.1",
            "key information": "Models were tested using different prompt types that included morpheme translations, grammar descriptions, and parallel usage examples."
        },
        {
            "section number": "6.1",
            "key information": "The benchmark's effectiveness may be limited by the small dataset size and the inherent challenges of low-resource languages."
        },
        {
            "section number": "6.4",
            "key information": "Future research should focus on expanding the dataset and exploring additional retrieval methods to improve translation outcomes."
        }
    ],
    "similarity_score": 0.7011007523301336,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Shortcomings of LLMs for Low-Resource Translation_ Retrieval and Understanding are Both the Problem.json"
}