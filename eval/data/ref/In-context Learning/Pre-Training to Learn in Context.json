{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2305.09137",
    "title": "Pre-Training to Learn in Context",
    "abstract": "In-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models\u2019 in-context learning ability by pre-training the model on a large collection of \u201cintrinsic tasks\u201d in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the SUPERNATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github. com/thu-coai/PICL.",
    "bib_name": "gu2023pretraininglearncontext",
    "md_text": "# Pre-Training to Learn in Context\nYuxian Gu1,2,\u2217, Li Dong2, Furu Wei2, Minlie Huang1,\u2020\n1The CoAI Group, DCST, Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing 100084, China 2 Microsoft Research guyx21@mails.tsinghua.edu.cn, {lidong1,fuwei}@microsoft.com aihuang@tsinghua.edu.cn\n# Abstract\nIn-context learning, where pre-trained language models learn to perform tasks from task examples and instructions in their contexts, has attracted much attention in the NLP community. However, the ability of in-context learning is not fully exploited because language models are not explicitly trained to learn in context. To this end, we propose PICL (Pre-training for In-Context Learning), a framework to enhance the language models\u2019 in-context learning ability by pre-training the model on a large collection of \u201cintrinsic tasks\u201d in the general plain-text corpus using the simple language modeling objective. PICL encourages the model to infer and perform tasks by conditioning on the contexts while maintaining task generalization of pre-trained models. We evaluate the in-context learning performance of the model trained with PICL on seven widely-used text classification datasets and the SUPERNATURALINSTRCTIONS benchmark, which contains 100+ NLP tasks formulated to text generation. Our experiments show that PICL is more effective and task-generalizable than a range of baselines, outperforming larger language models with nearly 4x parameters. The code is publicly available at https://github. com/thu-coai/PICL.\narXiv:2305.09137v1\n# 1 Introduction\nPre-trained language models (PLMs; Han et al., 2021; Qiu et al., 2020) have shown strong abilities of learning and performing unseen tasks conditioning on several task examples or instructions in its context, which is called in-context learning (ICL; Brown et al., 2020). Compared to conventional fine-tuning methods, ICL adapts PLMs to downstream tasks only through inference, without parameter updates, which is computationally cheaper in practice and is closer to general AI.\nHowever, PLMs trained on massive corpora to predict the next word given previous words are not explicitly taught to learn in the context. This makes ICL a surprising emergent ability but also indicates that the ICL ability of PLMs is not fully exploited. Garg et al. (2022) has shown that by directly training to do ICL in a meta-learning paradigm, models show strong performance on learning simple function classes in the context. In practical NLP scenarios, previous works (Min et al., 2022b; Chen et al., 2022b) also enhance the ICL performance by metafine-tuning PLMs on a large collection of downstream tasks and evaluating them on unseen tasks. However, the low diversity of human-annotated downstream tasks restricts the performance of the meta-tuned model. Direct training on downstream tasks also brings undesired bias on specific input formats, label spaces, or domains, which hurts the generalization of PLMs. To enhance the ICL ability while maintaining generalization, we propose PICL (Pre-training for In-Context Learning), a framework that exploits the PLM\u2019s ICL ability by pre-training models on data automatically constructed from the general plain-text corpus. Our framework is based on a simple observation that many paragraphs in the text documents contain \u201cintrinsic tasks\u201d. As shown in the left part of Figure 1, each paragraph in the document contains an intrinsic task. When doing language modeling on each paragraph, models implicitly perform the corresponding intrinsic tasks simultaneously. This shares a similar idea with the prompt-learning paradigm (Liu et al., 2021), where downstream data examples from NLP tasks are transformed into text sequences, and the model learns to perform the original tasks when trained on the text sequences with language modeling. Different from the downstream data, text paragraphs contain more diverse intrinsic tasks and have little bias on input formats, label spaces, or domains because they are free-form texts from the large-\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e5a0/e5a031b1-0320-492a-ba6a-a8c2907afb7d.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a321/a32115c1-7fc5-4073-9fc8-43a5b4d2e539.png\" style=\"width: 50%;\"></div>\nFigure 1: Left: An example of intrinsic tasks found in a document from the OpenWebText (Gokaslan et al., 2019) corpus. Right: The overall framework of PICL. For each paragraph z0 in the corpus C, we retrieve k paragraphs that share the same intrinsic task (Sentiment Analysis) as demonstrations and then concatenate them with z0 to construct a pre-training instance. We compute the language modeling loss on the whole instance to train the model.\nscale general corpus. By gathering and concatenating paragraphs with the same intrinsic tasks (right part of Figure 1), we can construct a meta-training dataset to pre-train the model to perform the intrinsic task conditioning on paragraphs in the context, and thereby improve the ICL ability. We adopt a retrieval-based approach to gather paragraphs sharing the same intrinsic tasks from a general corpus. We first train an encoder to represent each paragraph in a vector space where paragraphs with the same intrinsic task have close embeddings. The encoder is trained with contrastive learning (Khosla et al., 2020) on a collection of downstream datasets by taking examples from the same tasks as positive pairs and those from different tasks as negative pairs. Then, treating any paragraph in the corpus as a query, we retrieve the paragraphs with close representations to the query, namely, sharing the same intrinsic task with the query. Finally, we concatenate the query and the retrieved paragraphs to get a pre-training instance. Note that although we use downstream datasets, the model is trained on instances constructed from the general corpus, which ensures its generalization. We evaluate the ICL performance of the model pre-trained with PICL on seven widelyused text classification datasets and SUPERNATURALINSTRUCTIONS (Wang et al., 2022), a benchmark whose test split contains more than 100 tasks formulated into text generation. Empirical results show the effectiveness of PICL, enabling the model to reach or even outperform larger models with nearly 4x parameters. Besides, we find that\nthe PICL-trained model is more generalizable on various tasks than previous meta-fine-tuning methods. We also conduct extensive experiments to analyze several key factors of PICL.\n# 2 Method\nWe first present an overview of PICL and then describe the details in the following sections. As shown in the right part of Figure 1, we construct the pre-training instances from a corpus C consisting of paragraphs split from full documents by \u201c\\n\u201d. For each paragraph z0 in C, we first use a retriever R to find k paragraphs {z1, z2, \u00b7 \u00b7 \u00b7 , zk} sharing the same intrinsic task (Sentiment Analysis) with z0. Then the retrieved paragraphs are treated as demonstrations and concatenated with z0 to form a pre-training instance: zk \u2295zk\u22121 \u2295\u00b7 \u00b7 \u00b7 \u2295z1 \u2295z0. Finally, we adopt a language modeling objective to pre-train the model on the constructed instances. In this way, the pre-training stage can be regarded as a meta-training process, where the model learns to solve the intrinsic task in z0 conditioning on its context zk \u2295zk\u22121 \u2295\u00b7 \u00b7 \u00b7 \u2295z1. Since C is a large-scale general corpus, it contains a variety of intrinsic tasks and little domain bias, which ensures the generalization of the pre-trained model.\n# 2.1 Retriever\nThe main component of the retriever R is a tasksemantics encoder E that represents a text paragraph as a d-dimensional vector in a space V , where paragraphs with the same intrinsic tasks have similar representations. We define the similarity\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/9eb3/9eb31ce8-bce2-4307-a1e2-74a0180bceec.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: An example of how we construct the positive and negative pairs to train the task-semantics encoder E. The solid line means positive pairs, and the dashed lines mean negative pairs.</div>\n# We employ the FAISS library (Johnson et al., 2019) for efficient searching.\nlearning (Khosla et al., 2020; Karpukhin et al., 2020) to train the task-semantics encoder E. As shown in Figure 2, we take two paragraphs with the same intrinsic task as positive pairs and those from different tasks as negative pairs. However, the annotation of a paragraph\u2019s intrinsic task is usually unavailable. To this end, we use a collection of downstream NLP datasets from various tasks whose examples are converted into text sequences with human-written prompts to train E. In this way, treating each text sequence as a paragraph, we can regard the corresponding downstream task as the intrinsic task annotation. We assume that the instances from all downstream tasks form a dataset D. For each z0 \u2208D, we have a positive instance\nz+ sharing the same task with z0 and a set N(z0) consisting of negative instances with different tasks than z0, the loss function takes the form:\n(2)\nPositive and Negative Instances For each z0 \u2208 D, we randomly sample a positive instance z+ belonging to the same task with z0 from D\\{z0}. As shown in Figure 2, N(z0) contains two kinds of negative instances: (1) Easy Negatives z\u2212 easy sampled from D and belonging to different tasks than z0. (2) Hard Negatives z+ hard sharing the same prompt with z0 but containing mismatched tasks. For instance, in Figure 2, we apply the prompt from the sentiment task to the summarization task to create the hard negative instance z\u2212 hard. This prevents the model from hacking the contrastive objective using prompts like \u201cGuess the sentiment\u201d and learning a trivial pattern matching but forces the model to extract task semantics from the whole paragraph.\n# 2.2 Data Construction\nFor each paragraph z0 \u2208C, we concatenate the retrieved paragraphs {z1, z2, \u00b7 \u00b7 \u00b7 , zk} = R(z0) with z0 to get a pre-training instance zk \u2295zk\u22121 \u2295\u00b7 \u00b7 \u00b7 \u2295 z1 \u2295z0. To improve the quality of the constructed data, we derive an approach to filter out instances that are less informative to ICL. We consider the following score to measure the informativeness of an instance based on the perplexity difference of the paragraphs in the instance before and after they are concatenated as a sequence:\ns = \u2212\ufffdk i=0 log P(zi) + log P(zk \u2295zk\u22121 \u2295\u00b7 \u00b7 \u00b7 \u2295z0) |zk \u2295zk\u22121 \u2295\u00b7 \u00b7 \u00b7 \u2295z0| , (\nwhere | \u00b7 | is the length of a sequence and P(\u00b7) is the language modeling probability based on any uni-direct PLMs. Given a manually set threshold \u03b4, we retain the instances that satisfy s > \u03b4. This criterion leverages the original ICL ability of the PLM. If concatenating the paragraphs results in lower perplexity, they are more correlated and may be more informative for ICL. We finally construct a pre-training corpus containing N instances Cpre-train = {zi k \u2295zi k\u22121\u2295, \u00b7 \u00b7 \u00b7 , \u2295zi 1 \u2295zi 0}N i=1.\n# 2.3 Pre-Training\nWe pre-train the model with auto-regressive language modeling on Cpre-train. Unlike previous\nworks (Min et al., 2022b; Chen et al., 2022b), which only compute the language modeling loss on the label tokens, we compute the loss on the whole sequence. There are two reasons for this choice. First, the intrinsic tasks are already in the natural language format, and it is unnecessary to split the input and the label. Second, we argue that computing loss on the whole sequence ensures a large token number in a forward batch, which is critical to maintaining the basic in-weights ability (Chan et al., 2022). Therefore, the loss function is:\nwhere \u03b8 is the parameters of the model. In addition, we find that adding a language modeling loss LLM(\u03b8) on the original full documents before being split into paragraphs benefits the performance. Therefore, the final optimization objective is:\n(5)\nwhere we set \u03b1 = 0.5 in our main experiments.\n# 3 Experimental Setup\n# 3 Experimental Setup 3.1 Pre-training Data\n# 3.1 Pre-training Data\nWe merge OPENWEBTEXT (Gokaslan et al., 2019), WIKICORPUS (Foundation, 2022), and BOOKCORPUS (Zhu et al., 2015) to construct the pre-training data, where full documents are split into paragraphs by \u201c\\n\u201d. The corpus C consists of 80M paragraphs, totaling about 30GB. For each paragraph, we search for k = 20 demonstrations and concatenate them until 1024 tokens, the maximum input length constraint of the language model we used. This ensures that the model sees various demonstration numbers during pre-training. We use GPT2-Large (Radford et al., 2019) to compute P(\u00b7) in Equation 3 and set \u03b4 = 0.0 for filtering. More details of data processing and statistics are shown in Appendix A.\n# 3.2 Baselines\n# We consider four baselines in our experiments:\n\u2022 VanillaICL directly prompts a PLM with the concatenation of training examples to do ICL. \u2022 ExtraLM further pre-trains the PLM on the original full documents before being split into paragraphs with the language modeling objective. \u2022 Self-Sup (Chen et al., 2022a) designs four self-\nsupervised pre-training objectives, including Next Sentence Generation, Masked Word Prediction, Last Phrase Prediction, and Classification, to enhance the ICL performance. We conduct the self-supervised pre-training on our merged corpus for a fair comparison. \u2022 MetaICL (Min et al., 2022b) meta-trains the model on a large collection of downstream human-annotated datasets for learning to learn in context. The meta-training instances are constructed by concatenating several training examples in each dataset to a single text sequence. We replicate the method on the training set of our task-semantics encoder for a fair comparison.\n# 3.3 Evaluation\nWe evaluate the model trained with PICL on two kinds of downstream tasks.\nFew-Shot Text Classification We consider seven widely-used text classification datasets, including SST-2 (Socher et al., 2013), SST5 (Socher et al., 2013), Subj (Pang and Lee, 2004), MR (Pang and Lee, 2005), RTE (Dagan et al., 2006), CB (De Marneffe et al., 2019), and AGNews (Zhang et al., 2015) to evaluate the few-shot ICL performance of the trained models (see Appendix B.1 for more details). Note that these tasks are not included in the training set of the tasksemantics encoder. We randomly sample 4 or 8 demonstrations from the official training sets of each dataset. Effects of other demonstration numbers can be found in Section 4.3. We compute the average accuracy scores on at most 1000 samples from the validation split of each dataset across five random seeds for selecting demonstrations.\nInstruction Following To test the generalization of PICL, we also evaluate the trained model on a larger range of tasks with more free-form inputs, including both human instructions and fewshot examples. We use the test split of SUPERNATURALINSTRUCTIONS (Wang et al., 2022) as the benchmark and exclude the tasks that appear in the training set of the task-semantics encoder, resulting in 105 evaluation tasks (see Appendix B.2 for a full list of tasks). Each task is specified with a human-written instruction and two or three demonstrations. We follow Wang et al. (2022) to formulate all tasks to the text generation format and score the outputs with ROUGE-L (Lin, 2004).\nShot\nMethod\nParam.\nSST2\nSUBJ\nMR\nRTE\nAgNews\nCB\nSST5\nAverage\n4-shot\nVanillaICL\n770M\n67.59.2\n57.77.8\n50.30.3\n50.81.7\n67.52.3\n68.12.4\n24.45.4\n55.20.5\nVanillaICL\n1.5B\n74.99.7\n65.210.0\n61.96.5\n50.40.4\n65.64.8\n67.85.6\n32.44.6\n59.72.5\nVanillaICL\n2.7B\n75.07.5\n65.42.9\n71.413.3\n49.81.8\n65.62.8\n60.02.1\n32.15.4\n59.91.1\nExtraLM\n770M\n68.911.3\n63.96.4\n60.36.4\n51.21.7\n64.51.5\n63.75.3\n27.85.1\n57.22.1\nSelf-Sup\n770M\n55.07.4\n50.30.6\n59.73.5\n52.22.0\n50.37.0\n63.47.1\n28.83.3\n51.42.2\nMetaICL\n770M\n69.84.0\n63.54.6\n65.67.5\n57.62.3\n66.32.4\n65.23.0\n31.72.1\n60.01.5\nPICL\n770M\n79.78.6\n66.87.4\n81.01.3\n54.51.8\n67.73.4\n69.64.3\n34.84.0\n64.41.6\n8-shot\nVanillaICL\n770M\n68.76.0\n66.69.8\n60.25.5\n51.81.6\n60.25.6\n68.83.2\n31.43.8\n58.22.9\nVanillaICL\n1.5B\n72.112.6\n63.46.5\n63.35.4\n52.72.8\n54.28.4\n70.45.7\n33.53.3\n58.62.5\nVanillaICL\n2.7B\n71.011.6\n65.24.0\n70.46.3\n51.32.0\n63.12.4\n69.64.0\n34.12.8\n60.63.2\nExtraLM\n770M\n69.73.4\n65.26.5\n63.66.0\n52.61.6\n58.97.0\n69.63.8\n32.24.7\n58.81.6\nSelf-Sup\n770M\n61.46.5\n54.34.5\n73.88.1\n53.02.4\n52.13.8\n63.06.9\n33.71.8\n55.92.1\nMetaICL\n770M\n73.66.2\n67.28.8\n70.15.6\n53.62.1\n56.10.7\n65.84.1\n33.74.7\n60.02.2\nPICL\n770M\n78.010.6\n69.39.5\n77.55.0\n53.01.6\n64.74.4\n70.42.1\n34.13.8\n63.91.3\nTable 1: Main results of few-shot text classification. We report the average accuracy scores and the standard deviations across 5 random seeds for selecting demonstrations. We use GPT2-Large (770M), GPT2-xLarge (1.5B), and GPT-Neo (2.7B) for VanillaICL. The best scores on each dataset under 4 or 8 evaluation shots are in boldface.\n# 3.4 Settings\nRetriever We train the task-semantics encoder on 37 tasks (see Appendix C) using up to 10K examples per task. To enhance generalization, we apply multiple prompts from PromptSource (Bach et al., 2022) to one example and use 320 prompts in all. We use the in-batch negative trick (Chen et al., 2020) to compute the contrastive loss. We set the learning rate to 5 \u00d7 10\u22125, the batch size to 64, and construct 4 hard negatives for each instance. The encoder is trained from RoBERTaBase for 1 epoch. Language Model We test PICL based on the 770M GPT2-Large (Radford et al., 2019) unless otherwise specified. Results on larger models can be found in Appendix E.1. To save computational resources, we train the model from its pretrained checkpoints. We also test the VanillaICL performance of larger models, including GPT2xLarge (Radford et al., 2019) (1.5B) and GPTNeo (Black et al., 2021) (2.7B) for reference. Pre-Training We set the maximum learning rate to 1\u00d710\u22126 and use the \u201cinverse square root\u201d scheduler (Vaswani et al., 2017) with 1000 steps warmup. The model sees 131K tokens in a step and is pretrained for 100K steps. It takes less than a day to finish pre-training on 64 V100 32G GPUs.\n# 4 Results\n# 4.1 Few-Shot Text Classification\nTable 1 shows the results of few-shot text classification, from which we have 3 observations.\nFirst, among the baselines with 770M parameters, simply further training the model on our corpus with language modeling improves the performance (ExtraLM). This is likely due to the higher domain diversity of our corpus. MetaICL is helpful on most datasets, which verifies the effectiveness of meta-training for ICL. Self-Sup fails to bring benefits on most datasets against VanillaICL, probably because the constrained label space of the Classification training task (only contains \u201cTrue\u201d and \u201cFalse\u201d) brings bias to the model\u2019s output. This emphasizes the importance of using training objectives with little bias. Second, we observe that the PICL-trained model outperforms the baselines with the same model sizes by a large margin on most datasets across different shots, verifying the effectiveness of PICL. An exception is RTE, where MetaICL performs the best. We speculate the reason is that some training tasks of MetaICL share the same label space with RTE (\u201cYes\u201d/\u201cNo\u201d), such as paraphrase identification. Min et al. (2022c) has shown that the label space plays a vital role in ICL, which explains the good performance of MetaICL on RTE. Thrid, comparing models across different sizes, we find that increasing the model parameters boosts the performance, but PICL enables the 770M model to beat a 2.7B counterpart. This indicates that the ICL ability can be enhanced not only through scaling up the parameters. Improving the structure of the pre-training data is also beneficial. In Appendix E.1, we can see that PICL is also effective when applied to a 1.5B model.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/55f5/55f53bf7-e836-4d52-8b92-7929760e763b.png\" style=\"width: 50%;\"></div>\nFigure 3: Comparison between PICL and MetaICL on SUPER-NATURALINSTRUCTIONS (Wang et al., 2022) Each bar represents an evaluation task. The y-axis means the ROUGE-L score difference between the two methods\nModel\nParam.\nROUGE-L\nVanillaICL\n770M\n34.3\nVanillaICL\n1.5B\n34.9\nVanillaICL\n2.7B\n37.3\nExtraLM\n770M\n34.6\nSelf-Sup\n770M\n30.5\nMetaICL\n770M\n35.3\nPICL\n770M\n37.6\nTable 2: Results of instruction following on SUPERNATURALINSTRUCTIONS. We report the average ROUGE-L score across all 105 evaluation tasks.\n# 4.2 Instruction Following\nThe results on SUPER-NATURALINSTRUCTIONS are shown in Table 2. We can see that PICL achieves higher overall instruction following performance than the baselines, outperforming a larger model with about 4x parameters. In Figure 3, we compare the per-task performance of PICL and MetaICL because they share the most similar setting where human-annotated downstream datasets are used. We observe that PICL outperforms MetaICL on about 3/4 of evaluation tasks, indicating that compared to fine-tuning directly on downstream tasks, pre-training on intrinsic tasks constructed from the general plain-text corpus brings better ICL ability and ensures higher generalization performance across a broad range of tasks (see Appendix E.2 for more details). Most tasks where MetaICL beats PICL belong to text classification whose output spaces are \u201cYes/No\u201d or \u201cTrue/False\u201d. This matches the second observation in Section 4.1, where MetaICL predicts \u201cYes/No\u201d well because of training on tasks that share the same label spaces. On the other hand, PICL performs much better on text generation, or tasks whose output spaces share the same semantics with \u201cYes/No\u201d but use label words not in the training tasks of MetaICL (e.g., \u201cCorrect/Wrong\u201d).\nThis indicates that direct training on downstream datasets causes overfitting to specific labels. There are also tasks where PICL performs similarly to MetaICL, such as reasoning and word analogy. We notice that the improvements of PICL and MetaICL on these tasks are also marginal against VanillaICL probably because these tasks rely more on the \u201cinweights learning\u201d ability (Chan et al., 2022), rather than in-context learning.\n# 4.3 Analysis\nEffect of Retriever We compare different approaches to retrieve paragraphs and test the final model performance. We try randomly selecting paragraphs (Random), retrieving using the nonparametric approach (BM25), encoding each paragraph with the original pre-trained encoder as it is (RoBERTa), or using the encoder for sentence similarity (Reimers and Gurevych, 2019) (SRoBERTa). We also study different numbers of hard negatives (0, 1, 4) and downstream tasks (7, 24, 37) to train the task-semantics encoder in PICL. From the results in Table 3, we can see that all retrieval methods except Random bring improvements against VanillaICL on both text classification and instruction following settings, indicating that improving the coherence of the paragraphs in the pre-training data benefits ICL. Using the task-semantics encoder in PICL achieves the best performance, showing the importance of retrieving paragraphs based on task semantics rather than word overlap or sentence meanings. Comparing different settings to train the task-semantics encoder, we observe that increasing the number of hard negatives and training tasks improves the final performance. This is in line with previous works (Karpukhin et al., 2020; Chen et al., 2020; He et al., 2020) that more challenging hard negatives benefit contrastive learning. Effect of Demonstration Numbers Training with PICL brings two benefits: (1) PLMs learn\nRetriever\nnHardNeg.\nnTasks\nCLS\nSUP-NI\nAccuracy\nROUGE-L\nVanillaICL\n-\n-\n55.2\n34.3\nRandom\n-\n-\n56.7\n29.3\nBM25\n-\n-\n59.2\n34.5\nRoBERTa\n-\n-\n58.7\n34.6\nSRoBERTa\n-\n-\n59.0\n35.0\nPICL\n0\n37\n62.2\n36.4\n1\n37\n63.1\n36.5\n4\n7\n61.6\n35.4\n4\n24\n63.4\n36.6\n4\n37\n64.4\n37.6\nTable 3: Comparison of different retrievers. nHardNeg. and nTasks means the number of hard negatives and downstream tasks to train the task-semantics encoder in PICL. \u201cCLS Accuracy\u201d means the average accuracy scores on text classification tasks. \u201cSUP-NI ROUGEL\u201d means the average ROUGE-L scores across the tasks in SUPER-NATURALINSTRUCTIONS.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0830/0830ed4a-c4c4-4bd4-92f4-efe27fbfc355.png\" style=\"width: 50%;\"></div>\nFigure 4: Average text classification accuracy when the pre-training instances contain different demonstration numbers in PICL (the number in the brackets). \u201cPICLdefault\u201d means using a mixture of demonstration numbers as in previous experiments.\na format where demonstrations from the same task are concatenated as the prefix, which is beneficial when the model is evaluated under the same number of demonstrations. (2) PLMs learn a better ability to infer and perform tasks from the context, even when the demonstration numbers in evaluation and pre-training do not match. To differentiate these effects, we conduct pre-training on instances containing only 4, 8, or 16 demonstrations and test the trained models under different text classification shots. Results in Figure 4 show that when pre-trained with different demonstration numbers, the models generalize well to unseen demonstration numbers in evaluation, achieving similar performance with the default setup where the model sees various demonstration numbers in pre-training (PICL-default). This indicates that the models learn more than the input formats in PICL.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/0f5a/0f5af843-af64-403b-92cd-7f4b02cbd187.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c533/c5337d41-506d-453a-99c4-14cd1c1e982e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e9ad/e9ad0814-7808-449b-93e3-20faf01fd471.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Effect of \u03b1</div>\nEffect of Full Documents In Figure 5(b), we report the model performance on text classification tasks when using different choices of \u03b1, which controls the proportion of the full-document data. We find that balancing the constructed and fulldocument data performs the best (\u03b1 = 0.5). When \u03b1 is too large, the model is trained mostly on our constructed data and overfits its bias inevitably introduced by the task-semantics encoder in the data construction process. When \u03b1 is too small, our method degenerates into ExtraLM.\nEffect of Data Amount We study the size effect of the corpus used to construct the pre-training\n<div style=\"text-align: center;\">(a) Effect of Data Amount</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/7578/7578675b-752e-409a-8b7c-b1bc4d9a9b9a.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Data Comparison</div>\nFigure 6: Data analysis. (a): the average 4-shot text classification accuracies when constructing data using different proportions of the original corpus. (b): perplexity of full-document data (Full Doc), random retrieved data (Rand \u2295z0) and PICL data (R(z0) \u2295z0) based on GPT-J (6B) and the corresponding model performance.\ndata in PICL and report the performance on text classification tasks in Figure 6(a). We conduct the data construction on 0.01%, 0.1%, 1%, and 10% of the original 80M paragraphs (100%) and pre-train models for at most 100K steps until the validation losses begin to increase. From the results, we conclude that when the corpus is small, pre-training with the constructed data hurts the performance because the search library is too small to find paragraphs sharing the same intrinsic tasks. Training on small data for multiple epochs also causes overfitting. When the corpus contains more than 80K paragraphs (0.1%), adding more data constantly improves the performance, which is consistent with the scaling law (Kaplan et al., 2020).\ntraining data: original full documents before being split into paragraphs (Full Doc), concatenation of randomly selected paragraphs (Rand \u2295z0), and the concatenated same-intrinsic-task paragraphs gathered using the retrieval method in PICL before filtering (R(z0) \u2295z0). We can see that the data constructed by retrieval has much lower perplexity and correspondingly yields higher accuracy scores, which verifies its usefulness. In Appendix F, we present several examples of the retrieved paragraphs and the corresponding intrinsic tasks.\n# 5 Related Work\nIn-Context Learning Recently, in-context learning (ICL), where models perform tasks simply conditioning on instructions or the concatenation of examples in the context (Brown et al., 2020), has been found promising for using PLMs in various application scenarios. To this end, there emerge many works to improve the ICL performance by calibrating the model predictions (Zhao et al., 2021; Han et al., 2022; Holtzman et al., 2021; Min et al., 2022a), selecting or reordering demonstrations (Rubin et al., 2022; Liu et al., 2022; Lu et al., 2022), designing pre-training tasks (Chen et al., 2022a), and breaking the context length limits (Hao et al., 2022). However, the underlying mechanism of ICL is poorly understood (Min et al., 2022c). Therefore, some works propose mathematical frameworks to reveal how ICL works (Xie et al., 2021; Olsson et al., 2022; Elhage et al., 2021), or investigate the pre-training data to explain ICL\u2019s good performance (Chan et al., 2022; Shin et al., 2022). Multi-Task Fine-tuning for Cross-Task Generalization Fine-tuning PLMs on a large collection of downstream tasks enables generalization to unseen tasks under zero-shot (Wei et al., 2022; Sanh et al., 2022; Ouyang et al., 2022; Chung et al., 2022) and few-shot (Min et al., 2022b; Chen et al., 2022b; Mishra et al., 2022; Garg et al., 2022) scenarios. However, the performance of multi-task fine-tuning is largely restricted by the diversity of the annotated training tasks (Gu et al., 2022b), which requires massive human efforts to scale up. In addition, direct training on downstream tasks easily brings undesired bias. In this work, we propose to meta-train the model with the intrinsic tasks automatically collected from the large-scale general corpus, which is easier to scale up and introduces little bias.\nPre-training Data Programming The conventional pre-training paradigm trains the model on plain-text corpora with the language modeling objective (Radford et al., 2018, 2019; Brown et al., 2020). Recently works have found that carefully designed pre-training instances can further boost specific abilities like prompt adaption (Gu et al., 2022a), reasoning (Razeghi et al., 2022), or sentence representation (Levine et al., 2021). Our work studies constructing pre-training instances to improve the PLM\u2019s ICL ability while still maintaining its generalization on various NLP tasks.\nThis paper presents PICL, a framework that exploits the in-context learning ability of PLMs by pre-training models on concatenations of text paragraphs sharing the same \u201cintrinsic tasks\u201d gathered from the large-scale general corpus. In PICL, models learn to perform various intrinsic tasks conditioning on their context while preserving their generalization due to the little bias of the pre-training data. Extensive experiments show that PICL improves the ICL performance on various datasets against several baselines, enabling a 770 M model to outperform a larger model with about 4x parameters while maintaining good generalization across a wide range of tasks. For future work, we would like to consider adding human instructions to our pre-training framework to enhance more abilities of PLMs like zero-shot instruction following.\n# Limitations\nOne limitation of our paper is that the exact distribution of the intrinsic tasks in the original corpus and the constructed data is still unknown. Knowing the distribution can offer a better interpretation of the effectiveness of PICL, even of the strong performance of large language models. Besides, although we can find many constructed instances that share obvious intrinsic tasks (see Appendix F), there still exist some instances where the intrinsic tasks are hard to identify. How to better evaluate the contribution of these instances to the ICL ability or designing better filtering approaches to select more informative data for ICL is worth studying. Our task-semantics encoder inevitably contains some bias because it is trained on downstream datasets, although we have tried to ensure a large number and diversity of the dataset collection. However, the final language model is pre-trained on\nthe general corpus, and we add the full document loss, which eliminates the bias to some extent. Regarding computing power, we acknowledge that our framework takes relatively large training resources in the retrieval and pre-training process. Therefore, we did not conduct experiments based on extra-large language models.\n# Acknowledgements\nThis work was supported by the NSFC projects (Key project with No. 61936010 ). This work was also supported by the Guoqiang Institute of Tsinghua University, with Grant No. 2020GQG0005.\n# References\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a944/a94451b7-0cfd-429e-aa54-fde2bdd333b7.png\" style=\"width: 50%;\"></div>\n# A Details of the Pre-training Corpus\nThis section presents details of the data processing of the pre-training corpus and its statistics.\nData Processing Our pre-training corpus is a merge of OPENWEBTEXT (Gokaslan et al., 2019), WIKICORPUS (Foundation, 2022), and BOOKCORPUS (Zhu et al., 2015), downloaded from the HuggingFace datasets repository1. We first split each document in the corpus into paragraphs with \u201c\\n\u201d. To avoid training with too short paragraphs, we concatenate a paragraph with previous paragraphs if the token number after concatenation is lower than 128. We also exclude paragraphs longer than 500 tokens because they are not likely to fit into an instance with more than 1 paragraph. The filtering process in Section 2.3 drops about 24% instances. The licenses of all corpora allow for scientific research. Statistics We plot the distribution of the mean paragraph length per instance in Figure 7(a) and the distribution of the paragraph number per instance in Figure 7(b). The average paragraph length is 150.0, and the average paragraph number in an instance is 11.7. We can see that the model sees various demonstration numbers in PICL pre-training.\n# B Details of the Evaluation Data\n# B.1 Few-shot Text Classification\nThe details of each text classification dataset and the corresponding prompt in evaluation are listed in Table 6. All datasets are downloaded from the HuggingFace datasets repository1. We simplify the evaluation prompts as much as possible to reduce the effect of prompt engineering. Following previous works (Brown et al., 2020; Sanh et al., 2022), the model is evaluated by the ranking score strategy, where we compare the perplexity of each classification label under the model and choose the label with the lowest perplexity. The licenses of all datasets allow for scientific research.\n# B.2 Instruction Following\nThe original test split of the benchmark SUPERNATURALINSTRUCTIONS (Wang et al., 2022) contains 119 tasks. We exclude tasks that appear in the training tasks of the task-semantics encoder or whose input length is too long to fit in the context of our model. Our final evaluation includes 105 tasks. A full list of the tasks is shown in Table 8.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6f0a/6f0aaa8a-a490-4704-9fb9-4ae1f80adf28.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/bf67/bf679218-e826-48eb-9a24-f3c3aa0218cc.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Paragraph Number</div>\n<div style=\"text-align: center;\">(a) Paragraph Length</div>\nFigure 7: Pre-training data statistics. (a): the distribution of the average paragraph length per instance. (b): the distribution of the paragraph number per instance.\nStage\nName\nValues\nRetrieve\nlearning rate\n1e-4, 5e-5, 1e-5\nbatch size\n16, 32, 64\nhard negatives\n0, 1, 4\nPre-train\nlearning rate\n1e-5, 5e-6, 1e-6, 2e-7\nbatch size\n64, 128, 256, 512\nwarmup\n0, 1000, 5000\nTable 4: Searching intervals of hyper-parameters.\nWe use the same template to combine few-shot examples with task instructions as Wang et al. (2022). The license of this benchmark is Apache License 2.0.\n# C Details of the Downstream Training Data\nThe downstream datasets we use to train the tasksemantics encoder are a merge of the training data used in (Sanh et al., 2022) and the HR\u2192LR setting in (Min et al., 2022b). All datasets are downloaded from the HuggingFace datasets repository 1 and all prompts come from the PromptSource library (Bach et al., 2022)2. We exclude datasets from the sentiment classification task, the topic classification task, and the natural language inference task because they are included in our text classification evaluation. We finally get a collection of 37 datasets, as listed in Table 5. The licenses of all datasets allow for scientific research.\n# D More Experimental Details\nAll model checkpoints we used come from the HuggingFace models repository3. The searching interval of each hyper-parameter is listed in Table 4.\n# E More Results\n# E.1 Results on Larger Base Model\nWe test PICL based on the GPT2-xLarge (Radford et al., 2019) with 1.5B parameters. From the results in Figure 7, we can see that PICL is also applicable to larger models, outperforming the baselines based on the same-sized model on most datasets.\n# E.2 Instruction Following\nWe present the performance comparison between PICL and MetaICL per evaluation task in Figure 8. PICL outperforms MetaICL on 77 / 105 tasks, indicating that PICL ensures the better generalization of the trained model. The name of each task is also listed in Figure 8. We can see that the top three tasks where MetaICL performs the best are:\n\u2022 doqa_movies_isanswerable, \u2022 glue_entailment_classification, \u2022 tweetqa_classification,\n\u2022 winogrande_question_modification_object, \u2022 plausible_result_generation, \u2022 winowhy_reason_plausibility_detection ,\nwhich are text generation, text generation, and \u201cCorrect/Wrong\u201d classification tasks respectively. The four tasks where PICL and MetaICL have the\n\u2022 bard_analogical_reasoning_containers, \u2022 copa_commonsense_cause_effect, \u2022 winogrande_answer_generation, \u2022 bard_analogical_reasoning_trash_or_treasure, which belong to commonsense reasoning and word analogy tasks.\n# F Case Studies\nIn Table 9 and 10, we present several cases of the retrieved paragraphs and the corresponding intrinsic tasks. We can see that there exists a large range of intrinsic tasks in the constructed data and many of them do not appear in the training data of the tasksemantics encoder, which shows the generalization of the encoder.\n10\n0\n10\n20\n30\n ROUGE-L (PICL - MetaICL)\ntask242_tweetqa_classification\ntask1344_glue_entailment_classification\ntask1442_doqa_movies_isanswerable\ntask623_ohsumed_yes_no_answer_generation\ntask642_esnli_classification\ntask329_gap_classification\ntask1612_sick_label_classification\ntask1195_disflqa_disfluent_to_fluent_conversion\ntask1624_disfl_qa_question_yesno_classification\ntask1728_web_nlg_data_to_text\ntask199_mnli_classification\ntask743_eurlex_summarization\ntask640_esnli_classification\ntask520_aquamuse_answer_given_in_passage\ntask1409_dart_text_generation\ntask1516_imppres_naturallanguageinference\ntask970_sherliic_causal_relationship\ntask738_perspectrum_classification\ntask304_numeric_fused_head_resolution\ntask641_esnli_classification\ntask1154_bard_analogical_reasoning_travel\ntask1622_disfl_qa_text_modication\ntask1156_bard_analogical_reasoning_tools\ntask1157_bard_analogical_reasoning_rooms_for_containers\ntask1155_bard_analogical_reasoning_trash_or_treasure\ntask033_winogrande_answer_generation\ntask828_copa_commonsense_cause_effect\ntask1159_bard_analogical_reasoning_containers\ntask827_copa_commonsense_reasoning\ntask1534_daily_dialog_question_classification\ntask1158_bard_analogical_reasoning_manipulating_items\ntask290_tellmewhy_question_answerability\ntask392_inverse_causal_relationship\ntask891_gap_coreference_resolution\ntask677_ollie_sentence_answer_generation\ntask1152_bard_analogical_reasoning_causation\ntask249_enhanced_wsc_pronoun_disambiguation\ntask288_gigaword_summarization\ntask1586_scifact_title_generation\ntask769_qed_summarization\ntask201_mnli_neutral_classification\ntask937_defeasible_nli_social_classification\ntask879_schema_guided_dstc8_classification\ntask1342_amazon_us_reviews_title\ntask202_mnli_contradiction_classification\ntask1161_coda19_title_generation\ntask1640_aqa1.0_answerable_unanswerable_question_classification\ntask1390_wscfixed_coreference\ntask220_rocstories_title_classification\ntask418_persent_title_generation\ntask1391_winogrande_easy_answer_generation\ntask200_mnli_entailment_classification\ntask020_mctaco_span_based_question\ntask619_ohsumed_abstract_title_generation\ntask1529_scitail1.1_classification\ntask620_ohsumed_medical_subject_headings_answer_generation\ntask035_winogrande_question_modification_person\ntask281_points_of_correspondence\ntask1540_parsed_pdfs_summarization\ntask1394_meta_woz_task_classification\ntask1439_doqa_cooking_isanswerable\ntask233_iirc_link_exists_classification\ntask510_reddit_tifu_title_summarization\ntask226_english_language_answer_relevance_classification\ntask349_squad2.0_answerable_unanswerable_question_classification\ntask050_multirc_answerability\ntask1153_bard_analogical_reasoning_affordance\ntask036_qasc_topic_word_to_generate_related_fact\ntask1388_cb_entailment\ntask1407_dart_question_generation\ntask391_causal_relationship\ntask890_gcwd_classification\ntask957_e2e_nlg_text_generation_generate\ntask613_politifact_text_generation\ntask219_rocstories_title_answer_generation\ntask362_spolin_yesand_prompt_response_sub_classification\ntask880_schema_guided_dstc8_classification\ntask569_recipe_nlg_text_generation\ntask935_defeasible_nli_atomic_classification\ntask1531_daily_dialog_type_classification\ntask442_com_qa_paraphrase_question_generation\ntask1533_daily_dialog_formal_classification\ntask614_glucose_cause_event_detection\ntask1554_scitail_classification\ntask039_qasc_find_overlapping_words\ntask648_answer_generation\ntask936_defeasible_nli_snli_classification\ntask1659_title_generation\ntask500_scruples_anecdotes_title_generation\ntask602_wikitext-103_answer_generation\ntask401_numeric_fused_head_reference\ntask893_gap_fill_the_blank_coreference_resolution\ntask1385_anli_r1_entailment\ntask892_gap_reverse_coreference_resolution\ntask1387_anli_r3_entailment\ntask1631_openpi_answer_generation\ntask1615_sick_tclassify_b_relation_a\ntask330_gap_answer_generation\ntask190_snli_classification\ntask1386_anli_r2_entailment\ntask1393_superglue_copa_text_completion\ntask1664_winobias_text_generation\ntask133_winowhy_reason_plausibility_detection\ntask393_plausible_result_generation\ntask034_winogrande_question_modification_object\nPICL > MetaICL\nMetaICL > PICL\n<div style=\"text-align: center;\">Figure 8: Per-task results of the comparison between PICL and MetaICL.</div>\nCOS-E (Aggarwal et al., 2021)\nDREAM (Sun et al., 2019)\nQuAIL (Rogers et al., 2020)\nQuaRTz (Tafjord et al., 2019b)\nSocial-IQA (Sap et al., 2019)\nWiQA (Tandon et al., 2019)\nCosmosQA (Huang et al., 2019)\nQASC (Khot et al., 2020)\nQUAREL (Tafjord et al., 2019a)\nSciQ (Welbl et al., 2017)\nWiki-Hop (Welbl et al., 2018)\nAdversarial-QA (Ren et al., 2018)\nQuoref (Dasigi et al., 2019)\nROPES (Lin et al., 2019)\nDuoRC (Saha et al., 2018)\nHotpot-QA (Yang et al., 2018)\nWiki-QA (Yang et al., 2015)\nCommon-Gen (Lin et al., 2020)\nWiki-Bio (Lebret et al., 2016)\nSAMSum (Gliwa et al., 2019)\nXSum (Narayan et al., 2018)\nMRPC (Dolan and Brockett, 2005)\nPAWS (Zhang et al., 2019)\nQQP (Sharma et al., 2019)\nart (Bhagavatula et al., 2019)\ncirca (Louis et al., 2020)\ndiscovery (Sileo et al., 2019)\nFreebase_QA (Jiang et al., 2019)\ngoogle_wellformed_query (Faruqui and Das, 2018)\nHellaSwag (Zellers et al., 2019)\nliar (Wang, 2017)\npiqa (Bisk et al., 2020)\nscitail (Khot et al., 2018)\nswag (Zellers et al., 2018)\ntab_fact (Chen et al., 2019)\nyahoo_answer_topics4\nDBpedia (Lehmann et al., 2014)\nDataset\nTask\nPrompt\nLabel Space\nSST2\nSent. CLS\nSentence: {sentence} Label: {label}\nNegative / Positive\nSST5\nSent. CLS\nSentence: {sentence} Label: {label}\nTerrible / Bad / Neutral / Good\n/ Great\nMR\nSent. CLS\nSentence: {sentence} Label: {label}\nNegative / Positive\nRTE\nNLI\nPassage: {premise} Question: {hypothesis} Answer: {label}\nYes / No\nCB\nNLI\nPassage: {premise} Question: {hypothesis} Answer: {label}\nYes / No / Maybe\nSUBJ\nSubj. CLS\nInput: {text} Type: {label}\nObjective / Subjective\nAgNews\nTopic CLS\nSentence: {text} Label: {label}\nWorld politics / Sports / Busi-\nness / Science and technology\nTable 6: Details of the text classification datasets. \u201cSent. CLS\u201d stands for \u201cSentiment Classification\u201d. \u201cNLI\u201d stands for \u201cNatural Language Inference\u201d. \u201cSubj. CLS\u201d stands for \u201cSubjectivity Classification\u201d. \u201cTopic CLS\u201d stands for\nTable 6: Details of the text classification datasets. \u201cSent. CLS\u201d stands for \u201cSentiment Classification\u201d. \u201cNLI\u201d stands for \u201cNatural Language Inference\u201d. \u201cSubj. CLS\u201d stands for \u201cSubjectivity Classification\u201d. \u201cTopic CLS\u201d stands for \u201cTopic Classification\u201d.\nShot\nMethod\nSST2\nSUBJ\nMR\nRTE\nAgNews\nCB\nSST5\nAverage\nGPT-xlarge\nVanillaICL\n74.99.7\n65.210.0\n61.96.5\n50.40.4\n65.64.8\n67.85.6\n32.44.6\n59.72.4\nMetaICL\n71.12.0\n64.97.6\n66.86.3\n60.02.8\n66.25.4\n64.41.6\n34.63.7\n61.21.3\nPICL\n86.92.8\n72.57.3\n76.24.6\n54.02.7\n67.16.0\n70.04.6\n38.04.2\n66.41.6\nGPT-Neo\nVanillaICL\n75.07.5\n65.42.9\n71.413.3\n49.81.8\n65.62.8\n60.02.1\n32.15.4\n59.91.2\nMetaICL\n80.15.8\n55.69.5\n73.19.0\n57.53.9\n64.23.4\n65.56.4\n32.84.7\n61.31.4\nPICL\n86.41.0\n68.65.7\n83.62.4\n50.20.7\n67.51.2\n63.13.7\n35.73.6\n65.01.1\nEvaluation Tasks (105)\nCoreference Resolution\ntask893_gap_fill_the_blank_coreference_resolution\ntask1664_winobias_text_generation\ntask648_answer_generation\ntask304_numeric_fused_head_resolution\ntask891_gap_coreference_resolution\ntask033_winogrande_answer_generation\ntask892_gap_reverse_coreference_resolution\ntask401_numeric_fused_head_reference\ntask1390_wscfixed_coreference\ntask133_winowhy_reason_plausibility_detection\ntask330_gap_answer_generation\ntask329_gap_classification\ntask249_enhanced_wsc_pronoun_disambiguation\ntask1391_winogrande_easy_answer_generation\nTextual Entailment\ntask641_esnli_classification\ntask1529_scitail1.1_classification\ntask202_mnli_contradiction_classification\ntask1344_glue_entailment_classification\ntask1387_anli_r3_entailment\ntask738_perspectrum_classification\ntask890_gcwd_classification\ntask1612_sick_label_classification\ntask936_defeasible_nli_snli_classification\ntask1386_anli_r2_entailment\ntask201_mnli_neutral_classification\ntask1385_anli_r1_entailment\ntask1516_imppres_naturallanguageinference\ntask1615_sick_tclassify_b_relation_a\ntask970_sherliic_causal_relationship\ntask199_mnli_classification\ntask935_defeasible_nli_atomic_classification\ntask937_defeasible_nli_social_classification\ntask1388_cb_entailment\ntask1554_scitail_classification\ntask190_snli_classification\ntask200_mnli_entailment_classification\ntask640_esnli_classification\ntask642_esnli_classification\nCause Effect Classification\ntask1393_superglue_copa_text_completion\ntask391_causal_relationship\ntask828_copa_commonsense_cause_effect\ntask614_glucose_cause_event_detection\ntask827_copa_commonsense_reasoning\ntask393_plausible_result_generation\ntask392_inverse_causal_relationship\nTitle Generation\ntask288_gigaword_summarization\ntask1161_coda19_title_generation\ntask619_ohsumed_abstract_title_generation\ntask500_scruples_anecdotes_title_generation\ntask569_recipe_nlg_text_generation\ntask1586_scifact_title_generation\ntask602_wikitext-103_answer_generation\ntask769_qed_summarization\ntask510_reddit_tifu_title_summarization\ntask743_eurlex_summarization\ntask1342_amazon_us_reviews_title\ntask418_persent_title_generation\ntask220_rocstories_title_classification\ntask1659_title_generation\ntask219_rocstories_title_answer_generation\ntask1540_parsed_pdfs_summarization\nDialogue Act Recognition\ntask880_schema_guided_dstc8_classification\ntask1531_daily_dialog_type_classification\ntask1394_meta_woz_task_classification\ntask362_spolin_yesand_prompt_response_sub_classification\ntask1533_daily_dialog_formal_classification\ntask879_schema_guided_dstc8_classification\ntask1534_daily_dialog_question_classification\nAnswerability Classification\ntask1439_doqa_cooking_isanswerable\ntask1640_aqa1.0_answerable_unanswerable_question_classification\ntask242_tweetqa_classification\ntask1442_doqa_movies_isanswerable\ntask233_iirc_link_exists_classification\ntask290_tellmewhy_question_answerability\ntask520_aquamuse_answer_given_in_passage\ntask226_english_language_answer_relevance_classification\ntask050_multirc_answerability\ntask349_squad2.0_answerable_unanswerable_question_classification\ntask1624_disfl_qa_question_yesno_classification\ntask020_mctaco_span_based_question\nData to Text\ntask1728_web_nlg_data_to_text\ntask1409_dart_text_generation\ntask1407_dart_question_generation\ntask957_e2e_nlg_text_generation_generate\ntask677_ollie_sentence_answer_generation\ntask1631_openpi_answer_generation\nKeyword Tagging\ntask036_qasc_topic_word_to_generate_related_fact\ntask620_ohsumed_medical_subject_headings_answer_generation\ntask613_politifact_text_generation\ntask623_ohsumed_yes_no_answer_generation\nWord Analogy\ntask1159_bard_analogical_reasoning_containers\ntask1154_bard_analogical_reasoning_travel\ntask1152_bard_analogical_reasoning_causation\ntask1155_bard_analogical_reasoning_trash_or_treasure\ntask1156_bard_analogical_reasoning_tools\ntask1157_bard_analogical_reasoning_rooms_for_containers\ntask1153_bard_analogical_reasoning_affordance\ntask1158_bard_analogical_reasoning_manipulating_items\nOverlap Extraction\ntask039_qasc_find_overlapping_words\ntask281_points_of_correspondence\nQuestion Rewriting\ntask035_winogrande_question_modification_person\ntask1195_disflqa_disfluent_to_fluent_conversion\ntask034_winogrande_question_modification_object\ntask442_com_qa_paraphrase_question_generation\ntask1622_disfl_qa_text_modication\nExcluded Tasks (14)\ntask1356_xlsum_title_generation\ntask670_ambigqa_question_generation\ntask645_summarization\ntask760_msr_sqa_long_text_generation\ntask402_grailqa_paraphrase_generation\ntask1598_nyc_long_text_generation\ntask671_ambigqa_text_generation\ntask121_zest_text_modification\ntask1345_glue_qqp_question_paraprashing\ntask1557_jfleg_answer_generation\ntask232_iirc_link_number_classification\ntask1358_xlsum_title_generation\ntask1562_zest_text_modification\ntask102_commongen_sentence_generation\n<div style=\"text-align: center;\">List of Tasks</div>\n1\n\u2022 Marko Jovanovski Marko Jovanovski (born 24 July 1988) is a Macedonian professional\nfootballer who plays as a goalkeeper for Akademija Pandev.\n\u2022 Andreas Paraskevas Andreas Paraskevas (; born 15 September 1998) is a Cypriot footballer\nwho plays as a goalkeeper for Doxa Katokopias.\n\u2022 Evripidis Giakos Evripidis Giakos (; born 9 April 1991) is a Greek professional footballer\nwho plays as an attacking midfielder for Super League 2 club AEL.\nWorld Knowledge Com-\npletion\n2\n\u2022 The Hive scouting teams had been infiltrating our space for several weeks, sending three-\nand six-man teams in. In short, they were making me look bad on the home world.\n\u2022 And for good measure, Walker ordered the Wisconsin National Guard to prepare to\nintervene in case of any strike action by unions. In a word, Walker wants the destruction\nof organized labor in Wisconsin.\n\u2022 \"Scientists began examining him... he was covered in tattoos consisting of lines and dots,...\n80 percent of the points correspond to those used in acupuncture today.\" This means the\nPrince of Wales ought to start listening to scientists.\nIntent Identification\n3\n\u2022 ln(x) \u2248\u03c0 2 M (1,2\u02c62 - m / x ) - m ln(2). {\\displaystyle \\ln(x)\\approx {\\frac {\\pi\n}{2M(1,2\u02c6{2-m}/x)}}-m\\ln(2).}\n\u2022 sin( x ) + 1 3 sin ( 3 x ) + 1 5 sin ( 5 x ) + \u00b7 \u00b7 \u00b7 .\n{\\displaystyle \\sin(x)+{\\frac\n{1}{3}}\\sin(3x)+{\\frac {1}{5}}\\sin(5x)+\\dotsb.}\n\u2022 c q ( n ) = \u03a3 d | q \u00b5 ( q d ) \u03b7 d ( n ). {\\displaystyle c_{q}(n)=\\sum_{d\\mid q}\\mu\n\\left({\\frac{q}{d}}\\right)\\eta_{d}(n).}\nLatex Equation Transla-\ntion\n4\n\u2022 How did Japan stumble on for another nine years, borrowing trillions of yen and squan-\ndering those trillions on make-work bridges to nowhere and lavish social spending?\nAnswer: its citizens self-funded its deficits by saving trillions and investing those trillions\nin government debt.\n\u2022 How did our country thrive without income taxes for 126 years? Answer: federal spending\nwas significantly lower than it is today. In the early 1900s, government spending accounted\nfor roughly 7% of our GDP; today, federal spending accounts for around 35% of our GDP.\n\u2022 What was Trump\u2019s biggest persuasion problem in the election? Answer: His opponents\ndid a great job of framing him as some kind of Hitler.\nQuestion Answering\n5\n\u2022 An isopycnal is a line of constant density. An isoheight or isohypse is a line of constant\ngeopotential height on a constant pressure surface chart. Isohypse and isoheight are simply\nknown as lines showing equal pressure on a map. Temperature and related subjects\n\u2022 Once theory is applied to a mechanical design, physical testing is often performed to verify\ncalculated results. Structural analysis may be used in an office when designing parts, in\nthe field to analyze failed parts, or in laboratories where parts might undergo controlled\nfailure tests. Thermodynamics and thermo-science\n\u2022 Complex numbers often generalize concepts originally conceived in the real numbers. For\nexample, the conjugate transpose generalizes the transpose, hermitian matrices generalize\nsymmetric matrices, and unitary matrices generalize orthogonal matrices. In applied\nmathematics Control theory\nTopic Classification\n6\n\u2022 (speaking to Elder Fortie): Is this something you always wanted to do? ELDER FORTIE:\nNope. It\u2019s not. SEVERSON: So why are you here? ELDER FORTIE: Because the idea of\nhaving an empty seat in heaven troubles me. SEVERSON: Sister Waymith is from Sweet,\nIdaho.\n\u2022 (speaking to Steve Allen): Are there any countries in particular that you\u2019re really zeroing in\non, you\u2019d really like to make some inroads? ALLEN: Yeah, the United States of America,\nNorth America. We\u2019d like to make more inroads here. SEVERSON: Inroads like the\nchurch has made south of the border. Mexico, in particular, has been fertile ground for\nMormon missionaries.\n\u2022 (to Elder Russell): Why are you learning Mandarin if you\u2019re going to Canada? ELDER\nRUSSELL: I guess there\u2019s a sizable population up there. I mean, everyone deserves to\nhear our message, so we\u2019ll go worldwide wherever they are. SEVERSON: This group is\nleaving soon for Ukraine. First, they had to be considered worthy of serving a mission.\nDialogue in a Script\n7\n\u2022 Now we can log into our Twilio account and set the Message Request URL to our sms\nroute via ngrok: Try the app out by texting into your new Twilio number and you\u2019ll get the\nresponse back. Displaying Our Messages We\u2019re now passing our message to the arduino.\nThe next step is to write the code that examines that message and displays it on our LCD.\nLet\u2019s lay the foundation for our app: # include < Wire. h > # include \"rgb_lcd.h\" rgb_lcd\nlcd ; void setup () { Serial. begin ( 9600 ); // set up the LCD\u2019s number of columns and\nrows: lcd. begin ( 16, 2 ); lcd. setCursor ( 0, 1 ); // Print a message to the LCD. lcd. print (\n\"Ricky\u2019s Pager\" ); delay ( 1000 ); } void loop () { }\n\u2022 Now we\u2019re ready to track allocations. The first step is to \u201chijack\u201d our 3 memory functions\nwe defined in the first part (lines 4, 11 and 17): void* _Malloc(tU32 Size, tU32 Alloc-\nType, const tChar* Desc, const tChar* File, tU32 Line) { void* Result = malloc(Size);\nRegisterAlloc(Result, Size, AllocType, Desc, File, Line); return Result; } void* _Re-\nalloc(void* Ptr, tU32 Size, const tChar* File, tU32 Line) { void* Result = realloc(Ptr,\nSize); UpdateAlloc(Ptr, Result, Size, File, Line); return Result; } void _Free(void* Ptr) {\nUnregisterAlloc(Ptr); return free(Ptr); }\n\u2022 Here we use the gulp.src API to specify our input files. One thing to note is that we\nneed to specify a reporter for JSHint. I\u2019m using the default reporter, which should be fine\nfor most people. More on this can be found on the JSHint website. Compress Images\nNext, we\u2019ll set up image compression: gulp. task ( \u2019images\u2019, function () { return gulp.\nsrc (\u2019src/images/**/*\u2019 ). pipe ( imagemin ({ optimizationLevel : 3, progressive : true,\ninterlaced : true })). pipe ( gulp. dest ( \u2019dist/assets/img\u2019 )). pipe ( notify ({ message :\n\u2019Images task complete\u2019 })); });\nCode Generation\n8\n\u2022 Note: GP = Games played; W = Wins; L = Losses; T = Ties; OTL = Overtime loss;\n\u2022 SOL = Shootout loss; GF = Goals for; GA = Goals against; Pts = Points National\nConference\n\u2022 ERA = Earned run average; SO = Strikeouts; +/- = Plus/Minus; PIM = Penalty minutes;\nGS = Games Started;\nWord Abbriviation\n9\n\u2022 He strode toward her, barely slowly when he reached her. One arm slid around her waist\nand the other along her shoulders. She\u2019d barely registered his touch before his mouth\ndescended upon hers.\n\u2022 He lifted his head and stared at her. His face paled, and for the first time she noticed a\nspattering of orange freckles on his nose and across his cheekbones. He didn\u2019t speak. Just\nstared.\n\u2022 He continued rocking her gently, steadily. Her body\u2019s tremors calmed and her sobs\nquietened. He removed his white handkerchief from his trouser pocket, wiping her face.\nShe didn\u2019t look at him and kept her eyes lowered.\nVivid Description\n10\n\u2022 Even the sugary cereals, they said, are of nutritional value because they contain vitamins\nand minerals. Research shows that 40 percent of U.S. children consume their milk via\ncereal, said Sutherland of Kellogg. General Mills cites data from the Journal of the\nAmerican Dietetic Association that says people who frequently eat cereal, including kids\nwho eat sweetened ones, tend to have healthier body weights than those that don\u2019t.\n\u2022 Lead\u2019s toxicity has long been known, and most of the uses that led to human exposure, like\nthe manufacture of lead paint, have been banned for decades. Lead ammunition consumed\nonly about 3 percent of the 6.4 million tons of lead used worldwide in 2000, according to\na 2003 report by the Nordic Council of Ministers.\n\u2022 One reason Tesla has pushed the technology so aggressively is that its battery packs store\nmore than three times the energy of its competitors\u2019 electric-car batteries. As a result, they\nrequire more power to charge quickly, says Arindam Maitra, a senior project manager at\nthe Electric Power Research Institute.\nScientific\nEvidence\nGeneration\n",
    "paper_type": "method",
    "attri": {
        "background": "In-context learning (ICL) has gained attention in the NLP community, but the potential of language models in this area is not fully utilized due to a lack of explicit training for ICL. Existing methods have limitations related to bias and diversity in task examples, necessitating a new approach.",
        "problem": {
            "definition": "The core issue is that pre-trained language models (PLMs) are not explicitly trained to learn in context, limiting their performance on unseen tasks.",
            "key obstacle": "The main difficulty lies in the low diversity of human-annotated tasks and the biases introduced by direct training on specific downstream tasks."
        },
        "idea": {
            "intuition": "The idea stems from the observation that many paragraphs in text documents contain intrinsic tasks, which can be used to enhance ICL capabilities.",
            "opinion": "The proposed method, PICL (Pre-training for In-Context Learning), aims to improve the ICL ability of language models by pre-training them on a diverse set of intrinsic tasks derived from a general corpus.",
            "innovation": "PICL differs from existing methods by utilizing intrinsic tasks from a large-scale general corpus, minimizing bias and enhancing task generalization."
        },
        "method": {
            "method name": "Pre-training for In-Context Learning",
            "method abbreviation": "PICL",
            "method definition": "PICL is a framework that pre-trains language models on automatically constructed data containing intrinsic tasks from a general corpus.",
            "method description": "The method involves gathering paragraphs with similar intrinsic tasks and concatenating them to form pre-training instances for the model.",
            "method steps": [
                "Use a retriever to find paragraphs sharing the same intrinsic task.",
                "Concatenate the retrieved paragraphs with the target paragraph.",
                "Train the model using a language modeling objective on the constructed instances."
            ],
            "principle": "The effectiveness of PICL is supported by the rationale that training on diverse intrinsic tasks enhances the model's ability to generalize and learn from context."
        },
        "experiments": {
            "evaluation setting": "The model's performance was evaluated on seven widely-used text classification datasets and the SUPERNATURALINSTRUCTIONS benchmark, which includes over 100 NLP tasks.",
            "evaluation method": "Performance was assessed by comparing accuracy scores across different tasks and measuring ROUGE-L scores for text generation tasks."
        },
        "conclusion": "The experiments demonstrate that PICL significantly improves ICL performance, allowing a smaller model to outperform larger models while maintaining generalization across various tasks.",
        "discussion": {
            "advantage": "PICL's key advantages include enhanced task generalization and the ability to outperform larger models with fewer parameters.",
            "limitation": "A limitation of PICL is the unknown distribution of intrinsic tasks in the constructed data, which can impact the interpretability of its effectiveness.",
            "future work": "Future directions include exploring the integration of human instructions into the pre-training framework to further enhance model capabilities."
        },
        "other info": {
            "code availability": "The code for PICL is publicly available at https://github.com/thu-coai/PICL.",
            "data sources": "The pre-training corpus is constructed from OPENWEBTEXT, WIKICORPUS, and BOOKCORPUS."
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-context learning (ICL) has gained attention in the NLP community, but the potential of language models in this area is not fully utilized due to a lack of explicit training for ICL."
        },
        {
            "section number": "1.2",
            "key information": "The core issue is that pre-trained language models (PLMs) are not explicitly trained to learn in context, limiting their performance on unseen tasks."
        },
        {
            "section number": "1.3",
            "key information": "The proposed method, PICL (Pre-training for In-Context Learning), aims to improve the ICL ability of language models by pre-training them on a diverse set of intrinsic tasks derived from a general corpus."
        },
        {
            "section number": "1.4",
            "key information": "PICL differs from existing methods by utilizing intrinsic tasks from a large-scale general corpus, minimizing bias and enhancing task generalization."
        },
        {
            "section number": "3.1",
            "key information": "The effectiveness of PICL is supported by the rationale that training on diverse intrinsic tasks enhances the model's ability to generalize and learn from context."
        },
        {
            "section number": "5.1",
            "key information": "The model's performance was evaluated on seven widely-used text classification datasets and the SUPERNATURALINSTRUCTIONS benchmark, which includes over 100 NLP tasks."
        },
        {
            "section number": "6.1",
            "key information": "A limitation of PICL is the unknown distribution of intrinsic tasks in the constructed data, which can impact the interpretability of its effectiveness."
        }
    ],
    "similarity_score": 0.7381874515700012,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Pre-Training to Learn in Context.json"
}