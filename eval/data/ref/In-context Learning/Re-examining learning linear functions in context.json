{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2411.11465",
    "title": "Re-examining learning linear functions in context",
    "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for easily adapting Large Language Models (LLMs) to various tasks. However, our understanding of how ICL works remains limited. We explore a simple model of ICL in a controlled setup with synthetic training data to investigate ICL of univariate linear functions. We experiment with a range of GPT-2-like transformer models trained from scratch. Our findings challenge the prevailing narrative that transformers adopt algorithmic approaches like linear regression to learn a linear function in-context. These models fail to generalize beyond their training distribution, highlighting fundamental limitations in their capacity to infer abstract task structures. Our experiments lead us to propose a mathematically precise hypothesis of what the model might be learning.",
    "bib_name": "naim2024reexamininglearninglinearfunctions",
    "md_text": "# RE-EXAMINING LEARNING LINEAR FUNCTIONS IN CONTEXT\nr Naim - France ersit\u00b4e Toulouse 3 r.naim.docs@gmail.com Guilhem Fouill\u00b4e IRIT - France Universit\u00b4e Toulouse 3 guilhem.fouilhe-lafforgue@univ-tlse3.fr\nGuilhem Fouill\u00b4e IRIT - France Universit\u00b4e Toulouse 3 guilhem.fouilhe-lafforgue@univ-tlse3.fr\nOmar Naim IRIT - France Universit\u00b4e Toulouse 3 omar.naim.docs@gmail.com\nNicholas Asher CNRS - IRIT - France nicholas.asher@irit.fr\n18 Nov 2024\n# ABSTRACT\nIn context learning (ICL) is an attractive method of solving a wide range of problems. Inspired by Garg et al. (2022), we look closely at ICL in a variety of train and test settings for several transformer models of different sizes trained from scratch. Our study complements prior work by pointing out several systematic failures of these models to generalize to data not in the training distribution, thereby showing some limitations of ICL. We find that models adopt a strategy for this task that is very different from standard solutions.\nIn-context learning (ICL) Brown et al. (2020) promises to make interacting with LLMs easy and accessible. ICL enables the model to learn a task from a prompt with instructions and a few examples at inference time, without any adjustment of the model\u2019s parameters from pretraining. While there have been theoretical reconstructions of ICL, there have been few studies on exactly how ICL works in practice. However, ICL depends on a model\u2019s pretraining as Garg et al. (2022) have shown; so doing an in depth analysis of this feature of LLMs is difficult. Hence, most of analysis done on how ICL works are done on small models and simple tasks. Garg et al. (2022) makes the problem mathematically precise: the model learns a task/function given in-context examples at inference time in a next-token-prediction format Brown et al. (2020); given a prompt containing a task input-output examples (x1, f(x1), .., xn, ?), the model is asked to generate a value approximating f(xn). Inspired by Garg et al. (2022), we investigated whether smaller LLMs with transformer architectures ICL the class L of linear functions. While Garg et al. (2022) answer \u201cyes\u201d, we provide a more nuanced answer based on a deeper analysis. We have studied the 1 dimensional case with functions for over 30 models, from transformer architectures with 1 attention head (AH) and 1 MLP layer up 12 MLP layers and 8 AH. We also studied small attention-only models Olsson et al. (2022). Since we are interested in whether transformer models can ICL and if so how, even small transformer models are relevant, indeed essential since such an investigation requires training from scratch. Our main findings are these.\n1. Several recent papers claim that Transformer based models trained from scratch can ICL linear functions with performances close to algorithms such as Least Squares, and that a transformer can implement this algorithm. We show that the models we tested do not do this; they also failed to generalize and to provide robust predictions beyond their training data. In particular, all our transformer models failed to ICL the concept of a strictly increasing or strictly decreasing linear function, even over larger intervals in R. We replicated this behavior for several training and test distributions. We trained transformers on different distributions various Gaussian, Bimodal and Uniform distributions.\n2. Our experiments show that all our models trained from scratch have \u2018boundary values\u201d (B, \u2212B) for prompts xi; when f(xi) > B or < \u2212B, model performance degrades substantially. Training on uniform distributions makes this particularly clear. 3. All our transformer models solve task of ICL linear function by learning a projection from \u201cnearby\u201d sequences of points in the training data; Section 5 provides a mathematical formulation of what we think the models are doing. The exact projection depends upon the training distribution. The models do not implement linear regression or an algorithm like linear interpolation.\n2. Our experiments show that all our models trained from scratch have \u2018boundary values\u201d (B, \u2212B) for prompts xi; when f(xi) > B or < \u2212B, model performance degrades substantially. Training on uniform distributions makes this particularly clear. 3. All our transformer models solve task of ICL linear function by learning a projection from \u201cnearby\u201d sequences of points in the training data; Section 5 provides a mathematical formulation of what we think the models are doing. The exact projection depends upon the training distribution. The models do not implement linear regression or an algorithm like linear interpolation.\n# 2 BACKGROUND\nStatistical learning examines the application of a learned function over a test domain and the expected loss over novel applications. The ability to bring the error over test to that over the training set is typically taken to indicate an ability to generalize. Neyshabur et al. (2017), Villa et al. (2013) define learnability in statistical learning theory via the notion of uniform consistency. Let \u00b5 be a distribution over H and \u00b5n the update of \u00b5 after n training samples zi = (xi, yi). Let Azn be an algorithm for picking out a hypothesis from H based on n training samples. inf H is the hypothesis in H with the lowest possible error (Shalev-Shwartz et al., 2010; Kawaguchi et al., 2017).\nIn our example, the best hypothesis inf H is a prediction \u02c6f of some target function f. The best hypothesis is when \u02c6f = f with f, which yields 0 expected error. There is of course an algorithm that gives exactly the target function, linear interpolation, given two data points. Moreover linear regression is an algorithm that converges to the target function on any data set in our set up.\nDefinition 2 A class of hypotheses H is uniformly learnable just in case there exists a uniformly consistent algorithm for H.\nThe class of linear functions L is clearly uniformly learnable. What is left open here is the choice of distribution of the data both for train and test and the sampling method (since our class is uncountably large). Garg et al. (2022) take a definition of learning where average expected error goes to 0 when data in train and test are sampled both from the same normal distribution. So for instance this means that the model will see linear functions with a, b \u2208[\u22121, 1] around 70% of the time. This can make a big difference on the error reported for learning a class of mathematical functions like L, whose definition does not in any way depend on a particular distribution or sampling. Nevertheless, we would hope the model has found an algorithm such that \u02c6f = f given a test set of linear functions with a, b \u0338\u2208[\u22121, 1]. We also hope the algorithm will transfer to different distributions. This is what we investigate below.\n# 3 RELATED WORK\nSince Brown et al. (2020) introduced ICL, there has been considerable research indicating that ICL is possible because of a sort of gradient \u201cascent\u201d Aky\u00a8urek et al. (2022); Von Oswald et al. (2023). Dong et al. (2022) provides an important survey of successes and challenges in ICL and that so far, only simple problems for ICL have been analyzed, eg the case of linear or simple Boolean functions. Garg et al. (2022) offered an important advance showing that a Transformer trained from scratch (GPT-2 with an embedding size of 256) performed in-context learning of n-dimensional linear functions given identical train and test distributions N(0, 1). Further research then offered several theoretical reconstructions for how ICL for linear functions might work in Transformers. Von Oswald et al. (2023); Ahn et al. (2023); Mahankali et al. (2023) provided a construction to show transformers ICL from their doing gradient descent during ICL.\nFu et al. (2023) showed that Transformers could ICL in virtue of using higher-order optimization techniques. Xie et al. (2021); Wu et al. (2023); Zhang et al. (2023); Panwar et al. (2023) argued that ICL follows on a Bayesian point of view. Bai et al. (2024) show that transformers can under certain assumptions implement many algorithms with near-optimal predictive power on various incontext data distributions. Giannou et al. (2024); Zhang et al. (2024) modify transformers with linear attention and Zhang et al. (2024) introduce a new training regime to show that modified transformers can learn linear functions. Given P\u00b4erez et al. (2021)\u2019s result that full transformers with linear attention are Turing complete, however, these theoretical demonstrations are perhaps not surprising. Xie et al. (2021); Zhang et al. (2024) examine how ICL works despite differences between training and inference distributions. Unlike this prior research, we examine how ICL works in practice under different training and testing distributions in order to establish what transformers actually do in ICL 1 dimensional linear functions, whereas most prior research has concentrated on transformer models can or could do on this task. Even for this simplest case, we show transformers ICL in a different way from any of these proposed methods. Bhattamishra et al. (2023) trained small GPT-2 models from scratch to show that Transformers can ICL simple boolean functions, while their performance deteriorates on more complex tasks. Wu et al. (2023) studied ICL by pretraining a linearly parameterized single-layer linear attention model for linear regression with a Gaussian prior proving that the pretrained model closely matches the Bayes optimal algorithm. Ravent\u00b4os et al. (2024) investigated whether models with ICL can solve new tasks very different from those seen during pretraining. Olsson et al. (2022) offer an in depth analysis of ICL across tasks using a general evaluation measure on prompt length. They propose that a learned copying and comparison mechanism known as an induction head is at the heart of ICL.\n# 4 EXPERIMENTS\nIn this section, we show that: (i) models do not implement linear regression; (ii) this performance holds across different types of distributions; (iii) these distributions all show the presence of boundary values beyond which the models do not perform well; (iv) models with attention layers (AL) (models with at least two AL only or 1 AL+MLP layer) are needed to give an ICL effect (v) ordering and restricting the order of prompts can improve performance. In the last subsection, we put all of these observations together. Following Garg et al. (2022) we trained several small decoder only transformer models from scratch to perform in-context learning of linear functions.1 We set the number of layers (L) from 1 to 6, and attention heads (AH) from 1 to 4. We also trained a 9L6AH model and the 12L8AH GPT2 with an embedding size of 256. The task of the model is to predict the next value for f(xi) through a prompt of type (x1, f(x1), ..., xi). We refer to that prediction as \u02c6f(xi). To train the model L to ICL, we looked for a \u03b8\u2217that optimizes the following auto-regressive objective: \u03b8\u2217= arg min \u03b8 Exi\u2208DI,f\u2208DF \ufffdk \ufffd i=0 l (f (xi+1) , L\u03b8 ((x1, f(x1), ..., f(xi), xi+1))) \ufffd where L\u03b8 is a learner, l : (y, \u02c6y) \u2192||y \u2212\u02c6y||2 is squared error and f : x \u2192ax + b is a linear function with a, b chosen at random according to some training distribution for functions DF and samples xi picked randomly according to a training distribution for points DI. To simplify, we will note that f \u2208DF , x \u2208DI. We choose at random a function f \u2208DF and then a sequence of points xi \u2208DI at random, random prompts, from a distribution DI at each training step. We update the model through a gradient update. We use a batch size of 64 and train for 500k steps. The models saw over 1.3 billion training examples for each distribution we studied. For DF and DI we used several distributions: the normal distribution N(0, 1), \u201crectangle\u201d or uniform distributions over given intervals and bimodal distributions. In comparing how model performance evolves with parameters like the number of layers of the model or number of attention heads, we tested the models on a variety of test distributions for both 1Our code can be found in https://anonymous.4open.science/r/incontext-learning-556D/\nIn this section, we show that: (i) models do not implement linear regression; (ii) this performance holds across different types of distributions; (iii) these distributions all show the presence of boundary values beyond which the models do not perform well; (iv) models with attention layers (AL) (models with at least two AL only or 1 AL+MLP layer) are needed to give an ICL effect (v) ordering and restricting the order of prompts can improve performance. In the last subsection, we put all of these observations together. Following Garg et al. (2022) we trained several small decoder only transformer models from scratch to perform in-context learning of linear functions.1 We set the number of layers (L) from 1 to 6, and attention heads (AH) from 1 to 4. We also trained a 9L6AH model and the 12L8AH GPT2 with an embedding size of 256. The task of the model is to predict the next value for f(xi) through a prompt of type (x1, f(x1), ..., xi). We refer to that prediction as \u02c6f(xi). To train the model L to ICL, we looked for a \u03b8\u2217that optimizes the following auto-regressive objective:\nfunctions Dt F and data points or prompts Dt I. But while in train we always take the same distribution (DF = DI), in test, we sometimes take Dt F \u0338= Dt I. To see how the model performs in ICL relative to (Dt I, Dt F ), we generate a set of N = 100 functions in Dt F ; and our data samples for test are composed of Nb = 64 batches, each containing Np = 41 points in Dt I. In each batch b, for all points, we predict for each xb k, k \u22652, f(xb k) given the prompt (xb 1, f(xb 1), ..., xb k\u22121, f(xb k\u22121), xb k). We calculate for each function the mean average over all the points Np of all batches Nb, then do a mean average over all functions. Formally this is:\nIn all our error calculations, we exclude the first two predictions of each batch from the squared error calculation, since we need at least two points to be able to find a linear function and the first two predictions by the model are hence almost always wrong.\n# 4.1 MODELS DO NOT IMPLEMENT LINEAR REGRESSION\nWhen trained on DF = DI = N(0, 1) and the target functions had values in [-1, 1], even small models were able to converge to a 0 average error. The error was not always identical to 0 at least in some batches but rather similar to Liu et al.\u2019s finding on MSE estimation by transformers. On the other hand, all the models had systematic and non 0 average error once we chose the target f \u2208Dt F = N(0, \u03c3) for \u03c3 > 2. Figure 1 shows that the error rate increases substantially and non-linearly as Dt F = N(0, \u03c3) and \u03c3 increases. To ensure that comparisons between models are meaningful, for each N(0, \u03c3), we set a seed when generating the 100 random linear functions, ensuring that each model sees the same randomly chosen functions and the same set of prompting points xi. The table 2 in the Appendix contains the full figures for average error.\nOn the other hand, all the models had systematic and non 0 average error once we chose the target f \u2208Dt F = N(0, \u03c3) for \u03c3 > 2. Figure 1 shows that the error rate increases substantially and non-linearly as Dt F = N(0, \u03c3) and \u03c3 increases. To ensure that comparisons between models are meaningful, for each N(0, \u03c3), we set a seed when generating the 100 random linear functions, ensuring that each model sees the same randomly chosen functions and the same set of prompting points xi. The table 2 in the Appendix contains the full figures for average error.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/3efb/3efb5b31-b94b-43b2-a36f-47fc4be4920b.png\" style=\"width: 50%;\"></div>\nFigure 1: Evolution of error rates for various models with DF , DI = Dt I = N(0, 1) and Dt F for various N(0, \u03c3). The black curve illustrates a model that predicts f(xn) = 0, \u2200f and \u2200xn. The cyan line LS represents linear or ridge regression, which is trivially a perfect estimator given our totally clean input data. The results in Figure 1 and Table 2 (Table 2 is in Appendix B) confirm that at least the larger models are able to generalize somewhat to unseen examples, given that all the curves in Figure 1 have lower error rates than the baseline that predicts \u02c6f(xn) = 0 everywhere. But their generalizing ability was far from perfect; and contrary to what Aky\u00a8urek et al. (2022); Von Oswald et al. (2023) have suggested, the models did not use linear regression to ICL the target function. If they had, we would not see the error patterns we do.\n<div style=\"text-align: center;\">Figure 1: Evolution of error rates for various models with DF , DI = Dt I = N(0, 1) and Dt F for various N(0, \u03c3). The black curve illustrates a model that predicts f(xn) = 0, \u2200f and \u2200xn. The cyan line LS represents linear or ridge regression, which is trivially a perfect estimator given our totally clean input data.</div>\nThe results in Figure 1 and Table 2 (Table 2 is in Appendix B) confirm that at least the larger models are able to generalize somewhat to unseen examples, given that all the curves in Figure 1 have lower error rates than the baseline that predicts \u02c6f(xn) = 0 everywhere. But their generalizing ability was far from perfect; and contrary to what Aky\u00a8urek et al. (2022); Von Oswald et al. (2023) have suggested, the models did not use linear regression to ICL the target function. If they had, we would not see the error patterns we do.\nOur models\u2019 performance depends on how often it has seen examples \u201csimilar\u201d to the target function value it is trying to predict. When DF = N(0, \u03c3) there is an over 68% chance that a function chosen for train f(a, b) will have a, b \u2208[\u2212\u03c3, \u03c3] and over a 95% chance it will have a, b \u2208[\u22122\u03c3, 2\u03c3]. So a model with DF = DI = N(0, 1) has seen sequences of values for f(a, b) with a, b \u2208[\u22122, 2] more than 95% of the time. Given a pretraining with over a billion examples, models will have seen prompts for functions with a, b \u0338\u2208[\u22122, 2], just not many of them. As the models are tested with Dt F = N(0, \u03c3) and so confronted with more sequences representing functions f(a, b) for a, b \u0338\u2208[\u22122, 2], all the models do less and less well.\nWe\u2019ve just seen that when the distribution of training data follows a simple Gaussian N(0, 1), the models, for any number of layers and attention head, give good results when Dt F = Dt I = N(0, 1), but offer degraded performance as we test on distributions N(0, \u03c3) for larger \u03c3. We now show that same sort of behavior with different training distributions but all tested on N(0, \u03c3). Training on bimodal distributions We tested how our models fared with a bimodal distribution of training data. Our strategy was to increase the values the model can see during training, by extending the distribution seen during training. We trained our GPT2 on the bimodal distribution 0.5N(\u22121, 1)+0.5N(1, 1). With this training, the model was more likely to see wider values during training and therefore work better on larger values. Most of the models we tested had more robust performance with a bimodal distribution for DF = 0.5N(\u22121, 1) + 0.5N(1, 1) than they did with DF = N(0, 1) at least with Dt F = Dt I = N(0, \u03c3) and n \u22656. The best models had almost equally good performance on Dt F = N(0, \u03c3) for \u03c3 \u22643 and superior performance with Dt F = N(0, \u03c3) for \u03c3 \u22653, as can be seen from Table 1. For the values of the table, we took Dt I = U(\u22121, 1), the uniform distribution over [\u22121, 1], but the results remain similar when taking Dt I = N(0, 1). The fact that performance varies with the distribution should not happen, if the models were using gradient descent to compute linear regression in ICL. Training on uniform distributions To have more control on the notion of maximum and minimum values the models saw, we next trained our models on uniform distributions. We illustrate with U(\u22125, 5). Given the observations of Section 4.1 concerning the errors our models made on functions with large coefficients, we wanted to study whether these errors arose because the models hadn\u2019t encountered functions with such large coefficients in pretraining. By keeping DF , DI normal or bimodal, we can\u2019t control \u201cthe largest value the model could see\u201d, because it\u2019s always possible that it could have generated a large value during training. By training on a uniform distribution, however, we know exactly what the smallest and largest values that the model could have seen in its training. For example, setting DF , DI to U(\u22125, 5), the largest value the model could have seen is 30 = 5\u22175+5 and the smallest value it could have seen is \u221230. Most likely it saw values significantly > \u221230 and < 30. Training with U(\u22125, 5) gave good results for Dt F = Dt I = U(\u22121, 1). Models were able to find target functions with coefficients in [-1,1] from only 2 points (see leftmost plot of Figure 6 in Appendix C); and all our models work well when DF , DI, Dt F , Dt I use the same distribution. The models trained on a uniform distribution sometimes do even better than models trained on N(0,1) or a bimodal distribution\u2013up to three times better for Dt F = Dt I = N(0, 9) as Table 1 shows. Learning was at times very efficient, requiring just two prompts, as in Figure 6 (Appendix B).\nWe\u2019ve just seen that when the distribution of training data follows a simple Gaussian N(0, 1), the models, for any number of layers and attention head, give good results when Dt F = Dt I = N(0, 1), but offer degraded performance as we test on distributions N(0, \u03c3) for larger \u03c3. We now show that same sort of behavior with different training distributions but all tested on N(0, \u03c3). Training on bimodal distributions We tested how our models fared with a bimodal distribution of training data. Our strategy was to increase the values the model can see during training, by extending the distribution seen during training. We trained our GPT2 on the bimodal distribution 0.5N(\u22121, 1)+0.5N(1, 1). With this training, the model was more likely to see wider values during training and therefore work better on larger values. Most of the models we tested had more robust performance with a bimodal distribution for DF = 0.5N(\u22121, 1) + 0.5N(1, 1) than they did with DF = N(0, 1) at least with Dt F = Dt I = N(0, \u03c3) and n \u22656. The best models had almost equally good performance on Dt F = N(0, \u03c3) for \u03c3 \u22643 and superior performance with Dt F = N(0, \u03c3) for \u03c3 \u22653, as can be seen from Table 1. For the values of the table, we took Dt I = U(\u22121, 1), the uniform distribution over [\u22121, 1], but the results remain similar when taking Dt I = N(0, 1). The fact that performance varies with the distribution should not happen, if the models were using gradient descent to compute linear regression in ICL.\nTraining on uniform distributions To have more control on the notion of maximum and minimum values the models saw, we next trained our models on uniform distributions. We illustrate with U(\u22125, 5). Given the observations of Section 4.1 concerning the errors our models made on functions with large coefficients, we wanted to study whether these errors arose because the models hadn\u2019t encountered functions with such large coefficients in pretraining. By keeping DF , DI normal or bimodal, we can\u2019t control \u201cthe largest value the model could see\u201d, because it\u2019s always possible that it could have generated a large value during training. By training on a uniform distribution, however, we know exactly what the smallest and largest values that the model could have seen in its training. For example, setting DF , DI to U(\u22125, 5), the largest value the model could have seen is 30 = 5\u22175+5 and the smallest value it could have seen is \u221230. Most likely it saw values significantly > \u221230 and < 30. Training with U(\u22125, 5) gave good results for Dt F = Dt I = U(\u22121, 1). Models were able to find target functions with coefficients in [-1,1] from only 2 points (see leftmost plot of Figure 6 in Appendix C); and all our models work well when DF , DI, Dt F , Dt I use the same distribution. The models trained on a uniform distribution sometimes do even better than models trained on N(0,1) or a bimodal distribution\u2013up to three times better for Dt F = Dt I = N(0, 9) as Table 1 shows. Learning was at times very efficient, requiring just two prompts, as in Figure 6 (Appendix B).\nWe wanted to look at what sorts of errors our models made for f(a, b) \u2208L for a, b \u0338\u2208[\u22122, 2]\u2013 i.e. the values of \u02c6f(x) outside the interval that includes the vast majority they have seen. Our models exhibit problematic behavior of 2 kinds. Even our best models, for a, b \u0338\u2208[\u22122, 2] but reasonably close, say in [\u22129, 9], predict \u02c6f(x) to a sigmoid-like function with correct estimates for the target function within a certain interval.2 Consider Figure 2 for an illustrative f(x) = 8x + 9. While the\n2Giannou et al. (2024) tested a different type of model, with linear attention only,  behavior.\nmodels / \u03c3\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n3L4AHN, demb = 64\n0.0\n0.0\n0.22\n0.4\n1.73\n6.56\n8.56\n20.44\n39.73\n53.93\n3L4AHB, demb = 64\n0.03\n0.15\n0.53\n1.32\n2.74\n3.91\n5.52\n10.22\n13.86\n22.72\n3L4AHU, demb = 64\n0.02\n0.03\n0.13\n0.36\n0.84\n1.79\n2.54\n7.06\n11.38\n17.75\n6L4AHN, demb = 64\n0.0\n0.0\n0.2\n0.38\n1.58\n5.72\n7.99\n15.53\n32.96\n50.35\n6L4AHB, demb = 64\n0.01\n0.04\n0.23\n0.44\n1.19\n2.15\n3.08\n4.8\n9.98\n18.01\n6L4AHU, demb = 64\n0.02\n0.04\n0.11\n0.24\n0.57\n1.36\n1.82\n4.62\n10.23\n15.07\n12L8AHN, demb = 256\n0.0\n0.0\n0.32\n1.34\n3.14\n8.8\n12.13\n30.14\n49.37\n73.93\nsorted 12L8AHN\n0.0\n0.01\n0.32\n1.63\n3.69\n8.39\n10.06\n27.11\n43.23\n58.56\n12L8AHB, demb = 256\n0.0\n0.01\n0.08\n0.29\n0.78\n2.23\n3.66\n9.04\n18.68\n30.23\nsorted 12L8AHB\n0.01\n0.03\n0.18\n0.25\n0.74\n2.27\n2.62\n6.87\n13.73\n20.8\n12L8AHU, demb = 256\n0.0\n0.01\n0.13\n0.71\n1.92\n6.78\n10.92\n27.91\n38.75\n64.39\nsorted 12L8AHU\n0.01\n0.01\n0.13\n0.75\n2.12\n6.18\n10.5\n26.8\n36.3\n53.48\nREFDt\nF ,Dt\nI : y=0\n1.52\n4.43\n13.55\n19.94\n30.81\n44.75\n52.71\n76.11\n105.43\n128.52\nTable 1: Comparison showing the evolution of squared errors for models trained on different distributions; index N: DF = N(0, 1), B DF = 0.5N(\u22121, 1) + 0.5N(1, 1) and DF = U(\u22125, 5). We show error rates for models prompted without and with the natural ordering on the prompts [sorted], for the large model size. Dt i = U(\u22121, 1) and Dt F = N(0, \u03c3)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f19b/f19be241-d22d-4967-8e58-491a5ec26907.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 2: Plots of predictions of the 12L8AH model trained on N(0, 1) and error evolution over number of prompts for f(x) = 8x + 9</div>\nleft plot of Figure 2 shows that the model\u2019s prediction \u02c6f(x) diverges dramatically from f(x) outside of a certain interval, the right plot of Figure 2 shows the model has learned something with ICL about the function from the prompt and approximates it at least within a certain interval. For equations with coefficients highly unlikely to be sampled in N(0, 1) (for example f(x) = 30x+ 30 in Figure 3), however, the results are catastrophic. Figure 3 shows that the model\u2019s prediction \u02c6f doesn\u2019t converge to any stable prediction with ICL. This happens across a range of models. The same pattern holds when DF , DI = U(\u22125, 5). Consider again as an illustrative example the target function, f(x) = 9x for our largest trained model. The model approximates f(x) well within a certain range, but it predicts \u02c6f(x) to be a constant function for intervals outside of that range. As shown in the left plot in Figure 7 in Appendix D, \u02c6f +(v) \u224830 for values v for which the ground truth target function f is such that 30 \u2264f(v), and the model predicts an approximally constant function \u02c6f \u2212(v) \u2248\u221230 for values v on which f(v) \u2264\u221230. We call values like -30 and 30 where the model starts to predict constant functions \u02c6f(v) and \u02c6f \u2212(v) boundary values. For models trained on U(\u22125, 5), 30 and -30, with 30 = 5 \u22175 + 5 and \u221230 = \u22125 \u22175 \u22125, are respectively the biggest and smallest values the model could have seen during training. If such a model hasn\u2019t seen a value above 30 or below -30, it won\u2019t infer one.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/54b8/54b840c5-2566-4550-9473-e279ce416f82.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 3: Plots on first line of predictions for the 12L8AH model trained on N(0, 1) and error evolution over number of prompts for f(x) = 30x + 30. On second line Plots for f(x) = x and f(x) = 15x for models 2L attention only with 32AH and dembedding = 256</div>\nThis behavior just noted for our largest model remains true for other models M values \u02c6f(v) < B\u2212\u03b1 or \u02c6f(v) > B + \u03b1, where \u03b1 is a constant determined by M. However for functions and data samples when the values of f(x) in the prompt sequence are such that f(x) > B + \u03b1 or < \u2212B \u2212\u03b1\u2014an example is f(x) = 40x + 40\u2014the model starts to make a mess of things, assigning \u02c6f(v) random values for f(v) far away from B (i.e > B + \u03b1 or < \u2212B \u2212\u03b1, as seen in the rightmost plot in Figure 7. The model loses its capacity to model the target function anywhere. All our models trained on U(\u22125, 5) estimate the target function more or less well for x with f(x) \u2208[\u2212B, B] with boundary values \u2212B, B; but once we are outside [\u2212B, B], the estimations become constant functions or chaotic. Different models trained on different uniform distributions give different boundary values; for instance see Figures 3 or 6 Appendix F. Boundary values for models on normal and bimodal distributions will vary, as the largest values in the sequences the model has seen and the number of times it has seen those values will vary depending on the sampling. We have found that larger models will have slightly larger boundary values |B| than smaller ones. For instance, Figure 4 shows the plots for the predictions of two models (12L8AH, and 6L4AH) with both DF , DI = N(0, 1) for the equation f(x) = 10x. The larger model has boundary values \u2248-13.7, 13.7, the smaller one boundary values \u2248-12, 12. Constraints from boundary values hold regardless of model size (for plots see Appendix D and Figure 8) and also hold for attention only models (See Appendix E, Figure 9). Larger models trained on the same distribution and the same number of data are able to ICL L functions over a slightly larger number of intermediate values than smaller models, as Figure 1 suggests.\n# 4 PREDICTIONS FOR MODELS WITH ONLY ATTENTION LAYERS OR WITH ONLY MLP\n4.4 PREDICTIONS FOR MODELS WITH ONLY ATTENTION LAYERS OR WITH ONLY MLP\nTo understand better which components in the transformer architecture are responsible for ICL, we tested various components. We found that attention layers (AL) were the important components for ICL but ICL only worked reasonably well when the model had 2 AL (see also figure 3). Beyond\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/763b/763b012b-7f99-4db4-a82e-2c0ed9224199.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 4: Plots for f(x) = 10x by a 12l8ah model and by a 6l4ah model.</div>\n2 AL what mattered most was the number of attention heads (whether they are summed over all layers or counted within a layer). A single AL model had only a very limited ICL generalization capability beyond testing on Dt F = N(0, 1), but it did better than a 12 layer MLP, which showed no ICL capability, probably because the method of training on the predict next token format is not suitable for models without attention heads. The details of various AL models are found in Figure 5 and Table 3.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/91bc/91bcb267-4695-47f0-bf9e-b3e0696cfbf7.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Evolution of error rates models with attention layers only. We give figures for a model with only 1 attention layer/1AH (1AL1AH) two 2-attention layer only models (2AL8AH, 2AL32AH) and two 3 attention layer only model (3AL4AH,3AL8AH). DI = DF = U(\u22121, 1), Dt i = U(\u22121, 1) and Dt F = N(0, \u03c3). All models have embeddings of size 64, except 2AL32AH has size 256.</div>\nOur experiments with distributions also showed that a model performance improves when the sequence of prompts for the xi follows or are \u201csorted\u201d to follow the natural order on R, especially for bigger models. Error rates were comparable to the original models without sorting for small test values of \u03c3 with Dt F = N(0, \u03c3) and substantially lowered error rate, by up to a third depending on the training distribution, from the unsorted models. The details concerning the sorted models are in Table 1. While at least 2 points are needed to find a linear function, we noticed that model performance degrades when the size of the prompt during inference is greater than the maximal size of prompts the model saw during training, as the rightmost plot in Figure 6 shows (Appendix C). This held over all models and training distributions. This means that the model takes into account the whole sequence in its calculations, not just the last two or three data points. Had the model only looked at a small fixed subsequence, large size prompts in inference would not have affected model behavior.\nTo ICL L, we expected a transformer model given (x1, f(x1), ..., xn), ?) to perform a linear regression. The hypothesis and theoretical constructions of Aky\u00a8urek et al. (2022); Von Oswald et al. (2023) shows that transformer models can perform linear regression with 1 step of gradient descent. If that were the case, the models should generalize without difficulty. But this is clearly not what we observed. Error rates depend on training distributions DF , DI and on the distance of the target function\u2019s values from the majority of the data points in the model\u2019s training. We\u2019ve also demonstrated model sensitivity to the entire sequence of ICL prompts. This shows that the models did not learn to use linear regression to solve this task. All our models failed to learn the concept of a strictly monotone increasing or decreasing linear function in L over arbitrarily large or at least many large intervals of R.3 The lack of generalizability might suggest our models overfit the data. However, the pretraining data has no noise, and it\u2019s too large to be memorized by our models (our largest models with 256 size embeddings have < 107 parameters; each parameter would have to encode on average sequences for over 100 different functions). Moreover, our models performed similarly on several different training distributions for DF and DI and tested on N(0, \u03c3) for \u03c3 \u2208{1, 2}. Given that 100 samplings with Dt F = N(0, 1) nets on average 20 functions with coefficients the model with DF = DI = U(\u22121, 1) has not seen in training, we would expect the model\u2019s performance to degrade more substantially than it did. This implies that the models didn\u2019t overfit to their training regimes. Our error analysis has shown us the existence of boundary values, values for which models do well on the interval defined by the boundary values but degrade outside of them. These boundary values fluctuate depending on model training distributions. This is further evidence that the model\u2019s method for solving the task does not involve a real calculation of a linear function but an adjustment of values that the model has seen.\n# 5 HOW MIGHT THE MODELS BE LEARNING?\nOlsson et al. (2022) argue that a copying and comparison mechanism (induction head) is at the heart of ICL. They show that induction heads do not exist for 1 layer attention-only models but do for attention-only models with two or more layers. They empirically establish a strong connection between the formation of induction heads in models with greater than 2 attention layers and the model\u2019s ability to ICL. Our experiments with attention-only models showed that multiple attention layers were needed for ICL. Attention-only models with induction heads could ICL linear functions reasonably well, at least in when DF = Dt F ; and in fact the large 2 attention only layer model with 32 AH was more robust than the full transformer model with 1 (attention and MLP layer) and 1 or 2 AH (See Table 2 Appendix B). Olsson et al. (2022) also note that induction head behavior is possible with 1 attention + MLP layer. Our induction head hypothesis is that the induction heads predict a value for f(xn) given a prompt sequence \u20d7x = (x1,1, x1,2(= f(x1)), x2,1, x2,2, ...xn,1, ?) by using a projection from similar sequences or subsequences in the training, \u20d7y = (y1,1, y2,2...yn,1, yn,2), with xi,1 close to yi,1 for some j and xi,2 close to yj,2. Given the effects of prompt length on performance, we know that the whole sequence matters with p2 \u2264p1 for optimal predictions. This is evidence for a pointwise comparison like we are proposing (which is more complicated and potentially more accurate than simply averaging the yn,2 of the three closest yn,1 neighbors of xn,1). Olsson et al. (2022) report that larger models\u2019 induction heads can exploit sequences \u20d7y that are \u201cmore dissimilar\u201d to each other than smaller models can. The fact that the larger models respond well to prompts ordered according to the natural ordering on R suggests that larger models with more attention heads can exploit comparing sequences that converge or diverge from the target sequence \u20d7x as the prompts xi,1 near xn,1 increase or decrease. And they can compare in different ways. For smaller models that did not improve performance with respect to naturally ordered prompts, the hypothesis suggests they are restricted to simpler operations on the sequences they have seen. This\n3This makes sense in terms of Asher et al. (2023)\u2019s characterization of learnability. The concept of a strict monotone increasing or decreasing linear function describes a \u03a00 1 set in the Borel hierarchy which Asher et  (2023) show is not learnable using ordinary LLM assumptions.\nand our observations about boundary values provide further empirical support for the induction head hypothesis.\nand our observations about boundary values provide further empirical support for the induction hea hypothesis.\nGiven boundary values, \u2212B, B, all or the vast majority of the sequences the model has seen have values zi with \u2212B < zi < B. If the target sequence \u20d7x has maximum values \u2212B < xi < B, i.e. \u2212B < Maxvalxi\u20d7x < B, then chances are high that the model will find a weighted set of sequences Y close to the test sequence \u20d7x and compute bounds for xn,2 = f(xn)). Call the sequence \u20d7y generated by a function g = agx + bg a g-sequence. We assume the standard measure over sequences. Given a g-sequence \u20d7y closest to the f-sequence \u20d7x generated by target function f = afx + bf is such that ag = af, then the model will be able to approximate xn,2 by averaging the distances between yi,2 and xj,2 for the closest yi,1 to xj,1 for all xj,1. Now suppose that the closest g-sequences Y are not all such that af = ag. The model must now construct a function h(Y\u20d7x, \u20d7x) that computes a distance d between the values it has seen in Y\u20d7x and the targets \u20d7x for some optimized set Y\u20d7x of sequences close to \u20d7x. If h(Y\u20d7x, \u20d7x)(xk,1) = zk,2 is the k-th member of h(Y\u20d7x, \u20d7x), we optimize h such that |zk,2 \u2212xk,2| is minimized for all k. The model then averages these distances to yield an \u201daverage\u201d h(Y\u20d7x, \u20d7x) to compute z2,n = \u02c6f(x1,n). The larger the set very close \u20d7y \u2208Y\u20d7x, the better the projection and hence the prediction. For prompts outside the boundary values \u2212B, B, the closest \u20d7y are those with values near the boundary (yn,2 \u2248B(\u2212B)). So the model will predict xn,2 \u2248B(\u2212B), if it uses this method of projection. So in sum, we think the model estimates a set Y of closest sequences to the target \u20d7x and computes:\nand \u02c6f(xn) \u2248B(\u2212B), if Maxvalxi\u20d7x < \u2212B \u2212\u03b1L, or Maxvalxi\u20d7x > B + \u03b1L\nOtherwise \u02c6f(xn) takes a random value \u2208[\u2212B, B], \u03b1L > 0 a characterstic model value\nThe observations we have made are clearly compatible with this hypothesis (See Table 6 Appendix F) and the weighted averages are calculable in a 2 layer Attention only model with suitable heads. This also explains why training the model with smaller sequences in a kind of \u201dcurriculum learning\u201d may not be helpful. We trained the same 12L8AH model once with curriculum learning which supposedly helps the model perform better for different types of size prompts and generalize. and the same without curriculum and found that the model without curriculum performs better. This is consistent with our hypothesis, since the model without curriculum looks at batch sizes of up to 41 points in each step, and can therefore see more sequences than the model with curriculum, which looks at batch sizes of up to 41 points. The induction head hypothesis is less precise then linear regression but can approximate it given an appropriate set Y . Our induction head hypothesis predicts that model performance will be sensitive to a choice of training distribution for DF , DI as well as a choice of test distributions and of course to the presence of boundary values.\n# 6 CONCLUSION\nIn this paper we have shown a systematic failure case of decoder-only transformer models of various sizes (up to 9.5 million parameters) and architectures. All models failed to learn robustly the class of linear functions on non-noisy data, a task which is entirely determined by only two points and involves a trivial mathematical operation that has been documented to be by construction learnable by LLMs. However, the models did learn something different that enabled them to approximate linear functions over intervals where their training gave lots of examples. Rather than learning a standard algorithm for the task, these models instead perform a kind of projection from close sequences seen during the training. Our investigations perforce focus on relatively small models. But our study highlights a broader issue with ICL: the gap between what LLMs can learn and what they actually learn. Much larger models also face this limitation. The minimality of our examples and the capacity to easily train the models from scratch is actually a key strength of our study. We hope this contribution will inspire further research into what transformer-based models are actually doing on ICL tasks.\n# REFERENCES\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36:45614\u201345650, 2023. Ekin Aky\u00a8urek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. Nicholas Asher, Swarnadeep Bhar, Akshay Chaturvedi, Julie Hunter, and Soumya Paul. Limits for learning with large language models. In 12th Joint Conference on Lexical and Computational Semantics (*Sem). Association for Computational Linguistics, 2023. Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024. Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. P Kingma Diederik. Adam: A method for stochastic optimization. (No Title), 2014. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. A survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: A study with linear models. arXiv preprint arXiv:2310.17086, 2023. Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022. Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason D Lee. How well can transformers emulate in-context newton\u2019s method? arXiv preprint arXiv:2403.03183, 2024. Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv preprint arXiv:1710.05468, 2017. Jerry Weihong Liu, Jessica Grogan, Owen M Dugan, Simran Arora, Atri Rudra, and Christopher Re. Can transformers solve least squares to high precision? In ICML 2024 Workshop on In-Context Learning. Arvind Mahankali, Tatsunori B Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring generalization in deep learning. Advances in neural information processing systems, 30, 2017. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. Madhur Panwar, Kabir Ahuja, and Navin Goyal. In-context learning through the bayesian prism. arXiv preprint arXiv:2306.04891, 2023.\nJorge P\u00b4erez, Pablo Barcel\u00b4o, and Javier Marinkovic. Attention is turing-complete. Journal of Machine Learning Research, 22(75):1\u201335, 2021. Allan Ravent\u00b4os, Mansheej Paul, Feng Chen, and Surya Ganguli. Pretraining task diversity and the emergence of non-bayesian in-context learning for regression. Advances in Neural Information Processing Systems, 36, 2024. Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability and uniform convergence. The Journal of Machine Learning Research, 11:2635\u20132670, 2010. Silvia Villa, Lorenzo Rosasco, and Tomaso Poggio. On learnability, complexity and stability. In Empirical Inference, pp. 59\u201369. Springer, 2013. Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u02dcao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 35151\u201335174. PMLR, 2023. Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter L Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? arXiv preprint arXiv:2310.08391, 2023. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021. Ruiqi Zhang, Spencer Frei, and Peter L Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):1\u201355, 2024. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. arXiv preprint arXiv:2305.19420, 2023.\n# Appendix A: Training details\nAdditional training information: Like Garg et al. (2022), we use also the Adam optimizer Diederik (2014) , and a learning rate of 10\u22124 for all models. Computational resources: We used 1 GPU Nvidia Volta (V100 - 7,8 Tflops DP) for every training involved in these experiments.\n<div style=\"text-align: center;\">Appendix B: Table of error progression for models trained on N(0, 1) distributions tested on N(0, \u03c3)</div>\nmodels / \u03c3\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1L1AH dembedding=64\n0.1\n0.8\n5.1\n13.1\n26.9\n39.7\n53.0\n84.8\n120.0\n153.2\n1L2AH dembedding=64\n0.1\n0.8\n5.3\n14.4\n29.8\n41.1\n55.0\n93.8\n120.4\n159.2\n1L4AH dembedding=64\n0.0\n0.2\n2.7\n8.7\n19.9\n32.0\n42.8\n64.5\n92.3\n131.2\n2L1AH dembedding=64\n0.0\n0.1\n2.0\n4.9\n13.7\n27.0\n36.1\n64.9\n99.0\n134.0\n2L2AH dembedding=64\n0.0\n0.0\n1.6\n3.2\n9.3\n25.5\n32.0\n61.1\n92.9\n127.8\n2L4AH dembedding=64\n0.0\n0.0\n0.9\n2.6\n7.5\n19.3\n27.3\n51.8\n90.2\n119.4\n3L1AH dembedding=64\n0.0\n0.0\n0.9\n3.0\n8.2\n16.8\n24.4\n48.4\n76.7\n113.2\n3L2AH dembedding=64\n0.0\n0.0\n0.7\n2.3\n6.5\n15.9\n22.5\n43.1\n74.0\n102.5\n3L4AH dembedding=64\n0.0\n0.0\n0.6\n1.9\n5.5\n13.8\n20.4\n42.2\n70.3\n100.4\n6L4AH dembedding=64\n0.0\n0.0\n0.5\n1.6\n4.6\n11.6\n16.8\n33.7\n58.3\n87.9\n12L8AH dembedding=256\n0.0\n0.0\n0.3\n1.1\n2.9\n7.9\n11.9\n28.3\n46.9\n73.5\nREF: y=0\n2.19\n7.05\n19.22\n33.94\n52.23\n73.08\n86.02\n127.43\n165.27\n199.31\nTable 2: Comparison to show the evolution of squared \u03f5 type error depending on the distribution according to which we take the parameters, without taking into account the error of the prediction of the first and second prompts. Dt i = N(0, 1)\nTable 2: Comparison to show the evolution of squared \u03f5 type error depending on the distribution according to which we take the parameters, without taking into account the error of the prediction of the first and second prompts. Dt i = N(0, 1)\nAppendix C: Failure to generalize to longer prompt sequences\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6465/64651126-2cb6-4aa5-a062-7d41da2301d3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 6: Plot of ICL for f(x) = x with DF = DI = Dt I = U(\u22125, 5) for the model 12L8AH; the one on the left is a zoom in on the first 40 points, where we see that models can often learn from 2 points, the second a view of what happens overall, when models are trained on sequences of length 41 prompts.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2425/24257e52-ad18-4003-a7eb-75639e108904.png\" style=\"width: 50%;\"></div>\nAppendix D: Plots for boundary values with U(\u22125, 5)\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/ad78/ad78198e-9132-4d60-bbdb-20e66d1fdb5c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 8: Boundary values: Plots for f(x) = 9.4x for models 3L4AH and 6L4AH, DI = DF = Dt I = Dt F = U(\u22125, 5)</div>\nAppendix E: Example of boundary values for attention only models\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b9e7/b9e7f5a6-7ed2-41b6-99cc-d0721de2a3b1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 9: Boundary values for 2L32ah attention only model, with dembedding = 256 to ICL the function</div>\nmodels / \u03c3\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1AL1AHU\n0.38\n2.29\n9.3\n14.97\n25.25\n37.54\n45.4\n67.0\n95.19\n117.6\n2AL8AHU\n0.1\n0.62\n5.53\n10.59\n18.62\n30.61\n36.97\n57.79\n83.26\n103.58\n3AL4AHU\n0.35\n1.42\n8.17\n15.13\n24.15\n37.99\n45.2\n68.73\n96.37\n118.3\n3AL8AHU\n0.12\n1.16\n5.45\n9.36\n18.22\n28.77\n35.62\n52.44\n78.12\n100.18\n2Al32AHN\n0.06\n0.91\n5.96\n10.43\n18.96\n30.11\n36.77\n55.59\n81.66\n103.17\nREFDt\nF ,Dt\nI : y = 0\n1.52\n4.43\n13.55\n19.94\n30.81\n44.75\n52.71\n76.11\n105.43\n128.52\nTable 3: Comparison showing the evolution of squared errors for models with attention layers only. We give figures for a model with only 1 attention layer/1AH (1AL1AH) two 2-attention layer only models (2AL8AH, 2AL32AH) and two 3 attention layer only model (3AL4AH,3AL8AH). DI = DF = U(\u22121, 1), Dt i = U(\u22121, 1) and Dt F = N(0, \u03c3). All models have embeddings of size 64, except 2Al32AH has size 256.\nmodels / \u03c3\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n1L1AHN dembedding=64\n48.8\n57.62\n73.48\n84.51\n116.63\n129.52\n142.34\n177.69\n191.05\n246.43\n2L8AHN dembedding=64\n2.24\n4.81\n5.8\n7.19\n10.01\n19.04\n30.22\n38.03\n73.32\n118.89\n2L32AHN dembedding=256\n1.17\n2.64\n3.47\n5.01\n7.88\n16.85\n24.1\n40.98\n66.04\n95.03\nREF: y=0\n2.19\n7.05\n19.22\n33.94\n52.23\n73.08\n86.02\n127.43\n165.27\n199.31\nTable 4: Comparison to show the evolution of squared \u03f5 type error depending on the distribution according to which we take the parameters, without taking into account the error of the prediction of the first and second prompts. DF = DI = Dt i = N(0, 1) for models with attention ONLY Appendix F: The model searches for a sequence close to the input sequence.\nTable 4: Comparison to show the evolution of squared \u03f5 type error depending on the distribution according to which we take the parameters, without taking into account the error of the prediction of the first and second prompts. DF = DI = Dt i = N(0, 1) for models with attention ONLY\nAppendix F: The model searches for a sequence close to the input sequence.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/6209/62091c01-9ea2-4619-a999-323add3068e1.png\" style=\"width: 50%;\"></div>\nFigure 10: Plots model 12L8AH, trained on DI = DF = N(0, 1) for f(x) = x for high values of x and f(x) = 10x for low values of x\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of In-Context Learning (ICL) in transformer models, highlighting previous methods that claimed these models could learn linear functions effectively. However, it points out systematic failures in generalization to unseen data, necessitating a deeper investigation into ICL's practical workings.",
        "problem": {
            "definition": "The core problem is the inability of transformer models to robustly learn and generalize the class of linear functions when faced with novel inputs outside their training distribution.",
            "key obstacle": "The main challenge is that existing models fail to generalize their learning beyond the training data, particularly for strictly increasing or decreasing linear functions."
        },
        "idea": {
            "intuition": "Inspired by previous work, the authors explore the limitations of ICL in transformer models, particularly focusing on their failures in learning linear functions from scratch.",
            "opinion": "The proposed idea is that transformer models do not learn linear regression or similar algorithms but instead rely on projections from nearby training sequences.",
            "innovation": "The key innovation lies in demonstrating that models do not implement linear regression as previously suggested, but instead exhibit a different learning mechanism based on proximity in training data."
        },
        "method": {
            "method name": "In-Context Learning of Linear Functions",
            "method abbreviation": "ICL-LF",
            "method definition": "ICL-LF is defined as the process by which transformer models learn to predict outputs for linear functions based on prompts containing examples of input-output pairs provided at inference time.",
            "method description": "The method involves training transformer models to predict the next value in a sequence based on prior examples, without adjusting model parameters.",
            "method steps": "1. Train transformer models from scratch on various distributions. 2. Provide prompts with examples of linear functions. 3. Evaluate model predictions against unseen data.",
            "principle": "The effectiveness of ICL-LF stems from the models' ability to leverage learned patterns from training data, although they struggle with generalization beyond boundary values."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using various training distributions (Gaussian, Bimodal, Uniform) with transformer models of varying sizes and architectures, assessing their performance on unseen test distributions.",
            "evaluation method": "Performance was measured by comparing predicted outputs to actual values, using squared error metrics while excluding the first two predictions in each batch."
        },
        "conclusion": "The study concludes that transformer models, even when trained extensively, fail to learn linear functions robustly. Instead, they rely on projecting from previously seen data points, revealing a significant gap between theoretical capabilities and practical performance in ICL tasks.",
        "discussion": {
            "advantage": "The primary advantage of the proposed approach is its focus on understanding the limitations of ICL in practical scenarios, which is often overlooked in theoretical studies.",
            "limitation": "The main limitation is that the models exhibit poor generalization capabilities beyond the training data, particularly for functions that fall outside the observed range during training.",
            "future work": "Future research should explore alternative architectures or training strategies that could enhance the generalization abilities of transformer models in ICL tasks."
        },
        "other info": [
            {
                "info1": "The models trained on uniform distributions showed improved performance when the training and test distributions matched."
            },
            {
                "info2": {
                    "info2.1": "The study emphasizes the importance of attention layers in achieving effective ICL.",
                    "info2.2": "Boundary values were identified as critical points beyond which model performance deteriorated."
                }
            }
        ]
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "In-Context Learning (ICL) in transformer models addresses the systematic failures in generalization to unseen data, particularly when learning linear functions."
        },
        {
            "section number": "1.2",
            "key information": "The relevance of ICL is highlighted by its practical limitations in transformer models, which struggle to generalize beyond their training data."
        },
        {
            "section number": "3.1",
            "key information": "The paper discusses how transformer models fail to robustly learn and generalize linear functions, emphasizing the challenge of generalization to novel inputs."
        },
        {
            "section number": "3.2",
            "key information": "Theoretical perspectives on ICL suggest that transformer models rely on projections from nearby training sequences rather than implementing linear regression."
        },
        {
            "section number": "4.2",
            "key information": "Challenges in prompt engineering are illustrated by the models' poor generalization capabilities beyond the training data, especially for strictly increasing or decreasing linear functions."
        },
        {
            "section number": "6.1",
            "key information": "The paper identifies model bias in ICL, particularly the inability of transformer models to generalize effectively to unseen data."
        },
        {
            "section number": "6.4",
            "key information": "The study reveals scalability challenges in ICL, as transformer models exhibit poor performance when tested on functions outside the observed range during training."
        }
    ],
    "similarity_score": 0.7128496424875388,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Re-examining learning linear functions in context.json"
}