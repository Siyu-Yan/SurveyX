{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1807.01682",
    "title": "Generating Mandarin and Cantonese F0 Contours with Decision Trees and BLSTMs",
    "abstract": "This paper models the fundamental frequency contours on both Mandarin and Cantonese speech with decision trees and DNNs (deep neural networks). Different kinds of f0 representations and model architectures are tested for decision trees and DNNs. A new model called Additive-BLSTM (additive bidirectional long short term memory) that predicts a base f0 contour and a residual f0 contour with two BLSTMs is proposed. With respect to objective measures of RMSE and correlation, applying tone-dependent trees together with sample normalization and delta feature regularization within decision tree framework performs best. While the new Additive-BLSTM model with delta feature regularization performs even better. Subjective listening tests on both Mandarin and Cantonese comparing Random Forest model (multiple decision trees) and the Additive-BLSTM model were also held and confirmed the advantage of the new model according to the listeners\u2019 preference. Index Terms: f0 modeling, Cantonese, Mandarin, decision tree, BLSTM",
    "bib_name": "yuan2018generatingmandarincantonesef0",
    "md_text": "# Generating Mandarin and Cantonese F0 Contours with Decision Trees and BLSTMs\nWeidong Yuan, Alan W Black\nLanguage Technologies Institute, Carnegie Mellon University, Pittsburgh, USA weidongy@andrew.cmu.edu, awb@cs.cmu.edu\nAbstract\n# Abstract\nThis paper models the fundamental frequency contours on both Mandarin and Cantonese speech with decision trees and DNNs (deep neural networks). Different kinds of f0 representations and model architectures are tested for decision trees and DNNs. A new model called Additive-BLSTM (additive bidirectional long short term memory) that predicts a base f0 contour and a residual f0 contour with two BLSTMs is proposed. With respect to objective measures of RMSE and correlation, applying tone-dependent trees together with sample normalization and delta feature regularization within decision tree framework performs best. While the new Additive-BLSTM model with delta feature regularization performs even better. Subjective listening tests on both Mandarin and Cantonese comparing Random Forest model (multiple decision trees) and the Additive-BLSTM model were also held and confirmed the advantage of the new model according to the listeners\u2019 preference. Index Terms: f0 modeling, Cantonese, Mandarin, decision tree, BLSTM\n[cs.CL]\n# 1. Introduction\nDecision tree models are widely used for modeling and predicting f0 contours. Variant techniques are applied and proved to be useful. In [1][2], Discrete Cosine Transform is introduced as a representation for f0 contours for English and Mandarin respectively. Phrase level and syllable level or even more layers of f0 contours are modeled with different set of features and predicted separately in [1][2][3]. For improving the smoothness of f0 contours across independently predicted segments, dynamic (delta) feature [4] is commonly applied in the models. In addition to decision tree models, deep neural network models have shown their power for modeling and predicting prosody contours in recent years. Different kinds of DNN architectures are already proposed and very good results are achieved. In [5], a hierarchical DNN model is proposed predicting different level of f0 contours with different DNNs and lead to a much better result than an HMM baseline model. Bidirectional LSTM is used to predict the prosody contour in [6] and outperforms the strong baseline DNN model. In [7], a templatebased LSTM is used for solving the problem of failing to construct good contours through entire utterances in conventional approaches and shows a promising result. In this paper, we first explore different f0 representations and decision tree model architectures for modeling two speech tone languages Mandarin and Cantonese and the performances are compared. After that, experiments on different traditional deep neural network architectures for predicting syllable level f0 contours are shown. We also propose a simple AdditiveBLSTM model which explicitly models lexical information using an additional BLSTM leading to the best performance.\n2. Decision tree\n# 2. Decision tree\nTo make the f0 contours predictable for our models, we split the f0 contours according to the syllables in every utterance. Then 10 values in a syllable f0 contour are subsampled to normalize the duration. So in the models, every sample is a vector consisting of 10 subsampled f0 values.\n2.1. Features selection\nWord level Current, previous, next part of speech, word position in utterance; syllable position in current word\nPhrase level The current phrase position in the utterance; phrase number in the utterance; syllable number in phrase; stressed syllables from last, next phrase break; number of accented syllables from last, next phrase break; syllable position in phrase\n# 2.2. Model architecture and f0 representation\nIn this paper, 5 different f0 representations 4 different architectures are explored for the decision tree model.\n# List for different f0 representations\nList for different f0 representations:\n\u2022 OriF0 (original f0 vector): 10 subsampled f0 values as a vector are used to represent the f0 contour for every sample (every syllable). \u2022 DCT: 5 DCT coefficients as a vector are used to represent the f0 contour for every sample.\n ShapeMS(shape, mean and std): we apply z-score normalization on every sample (f0 vector) in the dataset independently. Then every sample will have its own unique shape vector (the values after normalization), mean and std. In our decision tree models, the mean and std of a sample will be predicted together as a vector but is independently predicted with the shape vector. We call this normalization \u201csample normalization\u201d in the paper.\n Cross-Delta (f0 representation vector with cross syllable delta): suppose the tth syllable\u2019s f0 representation vector in an utterance is vt. Cross-Delta for vt is\n(1)\nwhere \u201c[,]\u201d indicates concatenation of vectors. Then [vt, \u2206vt] of each sample is predicted. Note that \u2206vi here is for the regularization, no backward prediction [8] is needed when estimating. After obtaining the prediction [ \u02c6vt, \u2206\u02c6vt], \u2206\u02c6vt will be dropped.  In-Delta (f0 representation vector with syllable internal delta): The delta feature is calculated between the f0 values within a sample. Given a f0 representation vector vt \u2208RD,\nThen\n(2)\nthe same as using Cross-Delta, after making the predic tion [ \u02c6vt, \u2206\u02c6vt], the predicted delta \u2206\u02c6vt will be dropped.\nNote that different f0 representations are not necessary to be mutually exclusive with each other. The same as different model architectures. Since DCT is widely used for modeling and predicting the f0 contours [1][2], tests on Mandarin and Cantonese speech datasets are done for two classes of model separately: model based on the OriF0 (Table 1) and model based on the DCT coefficients vector (Table 2). Some unreasonable combinations of representations and architectures are not shown. In Table 1,2, model (2)(12), model (3) and model (5) show the advantage of ShapeMS (shape, mean and std of sample normalization) representation, In-Delta (syllable internal delta regularization) and ToneDT (tone dependent trees) consistently on both Mandarin and Cantonese speech datasets. Model (8)(11) indicates that DCT coefficients predicted as vector will perform better than predicted separately. However, applying phrase level and syllable level additive model (6)(10) doesn\u2019t show improvement here which is surprising. This may be because the speech datasets used here are based on isolated utterances and all have a more standard prosodic phrasing. Model (7) using ShapeMS, In-Delta and ToneDT performs best. And applying the random forest model [9] (ignore 30% features and 30% predict predictee coefficients, 20 trees) with model (7) will give us a much better result as shown in Table 3.\nTable 1: Statistics of the OriF0 based models\u2019 performance with different f0 representations and architectures on Mandarin and Cantonese speech datasets. \u201cSyl\u201d indicates \u201cSyllable\u201d and \u201cUtt\u201d indicates \u201cUtterance\u201d. True durations are used.\nMandarin\nCantonese\nModel\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\n(1) OriF0\nSinDT\n29.214\n0.780\n33.777\n0.851\n20.990\n0.674\n25.946\n0.756\n(2) ShapeMS\nSinDT\n28.992\n0.797\n33.527\n0.854\n20.923\n0.725\n25.894\n0.758\n(3) In-Delta\nSinDT\n29.094\n0.779\n33.639\n0.853\n20.887\n0.683\n25.829\n0.759\n(4) Cross-Delta\nSinDT\n29.494\n0.775\n34.771\n0.841\n21.235\n0.674\n26.032\n0.754\n(5) OriF0\nToneDT\n29.142\n0.782\n33.683\n0.852\n21.033\n0.679\n26.021\n0.755\n(6) OriF0\nPSLevel\n32.267\n0.768\n36.780\n0.827\n25.938\n0.663\n32.080\n0.642\n(7) ShapeMS\nIn-delta\nToneDT\n28.959\n0.797\n33.513\n0.854\n20.814\n0.725\n25.829\n0.759\nTable 2: Statistics of the DCT based models\u2019 performance with different f0 representations and architectures on Mandarin and Cantonese speech datasets. True durations are used.\nMandarin\nCantonese\nModel\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\n(8) DCT\nSinDT\n29.145\n0.776\n34.546\n0.844\n20.967\n0.680\n26.079\n0.755\n(9) DCT\nToneDT\n29.147\n0.778\n34.540\n0.844\n20.974\n0.682\n26.103\n0.755\n(10) DCT\nPSLevel\n32.221\n0.770\n37.573\n0.819\n25.894\n0.670\n32.644\n0.635\n(11) DCT\nScalarDT\n30.728\n0.763\n35.940\n0.832\n22.770\n0.653\n27.878\n0.724\n(12) ShapeMS\nSinDT\n29.041\n0.793\n34.411\n0.845\n20.938\n0.722\n26.048\n0.757\n(13) Corss-Delta\nSinDT\n29.979\n0.778\n35.370\n0.835\n21.427\n0.662\n26.411\n0.748\nTable 3: Performance of random forest with the best decision tree model\nTable 3: Performance of random forest with the best decision\nMandarin\nCantonese\nSyl level\nrmse\ncorr\nUtt level\nrmse\ncorr\nSyl level\nrmse\ncorr\nUtt level\nrmse\ncorr\n27.717\n0.814\n32.049\n0.868\n20.182\n0.739\n25.057\n0.774\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/d285/d285c484-c187-4547-8f0b-c20baea4c3c3.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 1: Additive-BLSTM Architecture with delta feature regularization</div>\n# 3. Additive-BLSTM model\nIn this section, we investigate the performance of MLP (Multilayer Peceptron), single LSTM (unidirectional LSTM) and single BLSTM which are most commonly used neural network architectures for predicting f0 contours [5][6][7]. We also propose a new model named Additive-BLSTM which gives us the best result.\n# 3.1. Features selection\nThe features used by different DNN (deep neural network) models here include phone level, syllable level, word level and phrase level features (the same features in Section 2.1). In addition, pretrained word-embeddings for Chinese and Cantonese characters [10] are included as word level feature here.\n# 3.2. Additive-BLSTM model\nFigure 1 shows the proposed Additive-BLSTM model with delta feature. We use two BLSTMs to handle two sets of features respectively. The first set of features include phone level, syllable level, phrase level features while the second set includes word level, syllable level features. The intuition is that we use one BLSTM (BLSTM1 in Figure 1) fed with first set of features together with MLP1 to generate base f0 contour \u02c6ybase t . And another BLSTM (BLSTM2 in Figure 1) fed with second set of features together with MLP2 is used to capture lexical information from the word sequence of an utterance and help generate residual f0 contour \u02c6yres t . Lexical information is helpful for modeling f0 contours. For example, if the meaning of a word is important in an utterance, the word will be accented which may make its f0 contour rise or fall more quickly. Note that MLP1 is a 2-hidden-layer MLP using ReLU activation functions and MLP2 is a 2-hidden-layer MLP with Tanh activation functions. After adding up \u02c6ybase t and \u02c6yres t , we get the predicted contour \u02c6yt. Then delta feature (Cross-Delta or InDelta) \u2206\u02c6yt is calculated and used for regularization. During training time, mean squared error loss function is used on [yt, \u2206yt] and [\u02c6yt, \u2206\u02c6yt] (yt is true f0 contour, \u2206yt is true delta feature, \u201c[,]\u201d indicates concatenation of vectors). During estimation stage, \u2206\u02c6yt is dropped.\n# 3.3. Performance comparison\nAn MLP, a single LSTM, and a single BLSTM are trained as baseline models and are compared with our new model with\nrespect to RMSE and correlation as shown in Table 4. For MLP, single LSTM and single BLSTM, features of all the levels are concatenated together as input. As shown in Table 4, the additive architecture can bring a good improvement to the BLSTM model. And the AdditiveBLSTM model with In-Delta performs the best on both Mandarin and Cantonese speech datasets. Figure 2 shows the base f0 contour, residual f0 contour and predicted f0 contour for an selected example synthesized by this best Additive-BLSTM model. In the figure, adding the residual f0 contours on the base f0 contours can make the f0 contours rise and fall in a more natural way and also more similar to the natural contours.\nTable 4: Comparison between the performance of MLP, single LSTM, single BLSTM and Additive-BLSTM. As mentioned in section 2.2, Cross-Delta refers to cross syllable delta feature, In-Delta refers to syllable internal delta feature. True durations are used.\nMandarin\nCantonese\nModel\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\nSyl\nLevel\nrmse\ncorr\nUtt\nLevel\nrmse\ncorr\nMLP\n25.721\n0.803\n30.910\n0.879\n19.644\n0.715\n24.821\n0.781\nSingle LSTM\n24.221\n0.814\n29.233\n0.892\n19.289\n0.715\n24.450\n0.787\nSingle BLSTM\n23.983\n0.818\n28.925\n0.894\n19.224\n0.712\n24.424\n0.789\nAdditive-BLSTM\n23.467\n0.821\n28.354\n0.899\n18.896\n0.723\n24.046\n0.796\nAdditive-BLSTM\nCross-Delta\n23.820\n0.816\n28.797\n0.895\n19.328\n0.704\n24.486\n0.787\nAdditive-BLSTM\nIn-Delta\n23.299\n0.828\n28.266\n0.899\n18.750\n0.733\n23.968\n0.797\n# 4. Experiment\n# 4.1. Dataset\nWe use two corpora in our experiments for testing the performance of our models. One is the CASIA Mandarin Corpus[11] developed for speech synthesis research. 4500 sentences are used for the experiments which includes 76948 Chinese characters (syllables). 3600, 450, 450 sentences are selected randomly as the train data, validation data and test data respectively. Another corpus is CUProsody Cantonese speech dataset. It is a read-speech corpus developed by the DSP and Speech Technology Laboratory of the Chinese University of Hong Kong. It consists of 1,200 newspaper sentences and 1,00 conversational sentences. Only the newspaper sentences are used which include 77164 traditional Chinese characters (syllables). We also split the dataset into 967, 115, 118 sentences as train data, validation data and test data.\n# 4.2. Experiment setup\nFestival speech synthesis system [12] is used for the extraction of the F0 contours and most of the features from CASIA Mandarin Corpus and CUProsody. F0 values in an utterance will be split according to syllables and 10 values are subsampled for every syllable. Wagon CART building program in Edinburgh Speech Tools Library[13] is used for building the decision tree\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2896/2896de76-9d8b-4bd2-ba63-284b9ab4d6b9.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"></div>\n<div style=\"text-align: center;\">Table 5: AB test preference result between two models</div>\nLanguage\nAdditive-\nBLSTM model\nRandom\nforest model\nNo\npreference\nMandarin\n38.5%\n22.0%\n39.5%\nCantonese\n44.5%\n31.0%\n24.5%\nmodels. Besides, Part of speech feature from the raw text is extracted by Stanford CoreNLP toolkit [14]. And FastText pretrained word embeddings [10] are used in the DNN models.\n# 4.3. Subjective listening test\nTwo subjective listening tests were held. The first test named \u201cAB Test\u201d is to compare the sentence pairs synthesized by the best random forest model and the best Additive-BLSTM model respectively. Listeners are asked to select their preference on each sentence pair played in random order. The second test named \u201dtone test\u201d is a dictation test to check whether the models generate the correct tones for the syllables. In this test, listeners listen to the sentences with the same pattern \u201cA teacher wrote A B two words on the blackboard.\u201d in Mandarin or Cantonese. \u201cA\u201d and \u201cB\u201d will be replaced by different words. The listeners are asked to write down what they heard for \u201cA\u201d \u201cB\u201d two words. We selected two-syllable words that are ambiguous with respect to their tone-type. The carrier phrase is selected to not influence the listener with any semantic context on the word choice. The tone test is also held for random forest and Additive-BLSTM on both Cantonese and Mandarin. 10 Mandarin speakers and 10 Cantonese speakers participated in the tests for Mandarin speech and Cantonese speech respectively. 20 sentence pairs in AB Test and 20 sentences in tone test were tested for every listener. Table 5 and Table 6 show the results of two tests. In AB Test, the Additive-BLSTM model is preferred on both Mandarin and Cantonese speech. In tone test, both models have good performance (low error rate) while Additive-BLSTM model is still a little bit better than the random forest model. Interestingly the errors that listeners made in tone test were sometimes phonetic as well as tonal.\n<div style=\"text-align: center;\">Table 6: Tone test tone error rate</div>\nLanguage\nAdditive-\nBLSTM model\nRandom\nforest model\nMandarin\n0.85%\n3.05%\nCantonese\n3.61%\n4.35%\n# 5. Discussion\nObjective evaluation (syllable level, utterance level RMSE and correlation) indicates the advantage of sample normalization, syllable internal delta, tone dependent trees and random forest for decision tree model. However, some techniques like PSLevel model and DCT do not provide improvement on the datasets we use. So more experiments on variable datasets may be needed to explore these techniques comprehensively in the future. For the BLSTM model, our new additive architecture and syllable internal delta regularization provide good improvement compared with a single BLSTM model. Experiments indicate that using an additional BLSTM fed with word level features like word embeddings and part of speech can capture some lexical information which helps improve the prediction result. But further experiments are still needed to find out what kind of lexical information and how much information are captured by the residual contour.\n# 6. Conclusions\nIn this paper, for modeling the f0 contours of Cantonese and Mandarin speech, multiple f0 representations and model architectures are tested for decision tree and good results are achieved. A new simple Additive-BLSTM is also proposed giving a better f0 contours prediction compared with traditional single BLSTM model. All these improvements are consistent on both Cantonese and Mandarin speech languages. In the future, we plan to test our model on more tone languages like Vietnamese, Thai and try to make the model more general for different tone languages.\n# 7. References\n[1] J. Teutenberg, C. Watson, and P. Riddle, \u201cModelling and synthesising f0 contours with the discrete cosine transform,\u201d in Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on. IEEE, 2008, pp. 3973\u20133976. [2] Z. Wu, Y. Qian, F. K. Soong, and B. Zhang, \u201cModeling and generating tone contour with phrase intonation for mandarin chinese speech,\u201d in Chinese Spoken Language Processing, 2008. ISCSLP\u201908. 6th International Symposium on. IEEE, 2008, pp. 1\u20134. [3] X. Sun, \u201cF0 generation for speech synthesis using a multi-tier approach,\u201d in Seventh International Conference on Spoken Language Processing, 2002. [4] T. Yoshimura, K. Tokuda, T. Masuko, T. Kobayashi, and T. Kitamura, \u201cSimultaneous modeling of spectrum, pitch and duration in hmm-based speech synthesis,\u201d in Sixth European Conference on Speech Communication and Technology, 1999. [5] X. Yin, M. Lei, Y. Qian, F. K. Soong, L. He, Z.-H. Ling, and L.-R. Dai, \u201cModeling f0 trajectories in hierarchically structured deep neural networks,\u201d Speech Communication, vol. 76, pp. 82\u2013 92, 2016. [6] R. Fernandez, A. Rendel, B. Ramabhadran, and R. Hoory, \u201cProsody contour prediction with long short-term memory, bidirectional, deep recurrent neural networks.\u201d in Interspeech, 2014, pp. 2268\u20132272. [7] S. Ronanki, G. E. Henter, Z. Wu, and S. King, \u201cA template-based approach for speech synthesis intonation generation using lstms.\u201d in INTERSPEECH, 2016, pp. 2463\u20132467. [8] H. Zen, K. Tokuda, and A. W. Black, \u201cStatistical parametric speech synthesis,\u201d Speech Communication, vol. 51, no. 11, pp. 1039\u20131064, 2009. [9] A. W. Black and P. K. Muthukumar, \u201cRandom forests for statistical speech synthesis,\u201d in Sixteenth Annual Conference of the International Speech Communication Association, 2015. [10] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \u201cEnriching word vectors with subword information,\u201d arXiv preprint arXiv:1607.04606, 2016. [11] J. T. F. L. M. Zhang and H. Jia, \u201cDesign of speech corpus for mandarin text to speech,\u201d in The Blizzard Challenge 2008 workshop, 2008. [12] A. Black, P. Taylor, R. Caley, and R. Clark, \u201cThe festival speech synthesis system,\u201d 1998. [13] P. Taylor, A. Black, and R. Caley, \u201cIntroduction to the edinburgh speech tools, 1999,\u201d Currently available at http://www. cstr. ed. ac. uk/projects/speech tools/manual-1.2. 0. [14] C. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky, \u201cThe stanford corenlp natural language processing toolkit,\u201d in Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, 2014, pp. 55\u201360.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of modeling fundamental frequency (f0) contours in Mandarin and Cantonese speech, highlighting the limitations of previous methods such as decision trees and deep neural networks (DNNs) in achieving optimal results. A new approach is necessary to enhance the accuracy and naturalness of synthesized speech.",
        "problem": {
            "definition": "The problem involves accurately modeling the f0 contours of Mandarin and Cantonese speech, which are crucial for natural speech synthesis.",
            "key obstacle": "Existing methods struggle to effectively capture the complexities of f0 contours, particularly in tonal languages, resulting in less natural speech synthesis."
        },
        "idea": {
            "intuition": "The idea stems from the observation that combining different model architectures and representations can improve the prediction of f0 contours.",
            "opinion": "The proposed solution is the Additive-BLSTM model, which predicts both a base f0 contour and a residual f0 contour using two separate BLSTMs.",
            "innovation": "The key innovation lies in the use of an additive architecture that integrates lexical information through an additional BLSTM, enhancing the model's ability to predict more natural f0 contours."
        },
        "method": {
            "method name": "Additive-BLSTM",
            "method abbreviation": "AB-LSTM",
            "method definition": "The Additive-BLSTM method combines two BLSTM networks to predict base and residual f0 contours, leveraging both phonetic and lexical features.",
            "method description": "This method uses two BLSTMs to generate a combined f0 contour that reflects both the base and residual components.",
            "method steps": "1. Input features are processed through two BLSTMs; 2. Base f0 contour is generated; 3. Residual f0 contour is produced; 4. Both contours are added together to form the final prediction.",
            "principle": "This method is effective because it captures both the phonetic structure and lexical context of speech, leading to more accurate and natural f0 contour predictions."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the CASIA Mandarin Corpus and CUProsody Cantonese dataset, with a total of 4500 Mandarin sentences and 1200 Cantonese sentences used for training and testing.",
            "evaluation method": "Performance was assessed using RMSE and correlation metrics, along with subjective listening tests to evaluate listener preference for synthesized speech."
        },
        "conclusion": "The paper concludes that the Additive-BLSTM model significantly improves the modeling of f0 contours for both Mandarin and Cantonese, outperforming traditional models. Future work will explore its application to other tonal languages.",
        "discussion": {
            "advantage": "The Additive-BLSTM model provides a clear advantage in capturing the nuances of tonal languages, leading to more natural and accurate speech synthesis.",
            "limitation": "The method may face challenges in generalizing to languages with different tonal structures or in cases where lexical information is less relevant.",
            "future work": "Future research will focus on testing the model on additional tonal languages and refining its capabilities to handle diverse speech contexts."
        },
        "other info": {
            "info1": "The Additive-BLSTM model was preferred in subjective listening tests over the Random Forest model.",
            "info2": {
                "info2.1": "The model achieved a low error rate in tone tests, indicating its effectiveness in producing correct tonal representations.",
                "info2.2": "Listeners noted that errors in tone tests were sometimes phonetic as well as tonal."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "This paper addresses the issue of modeling fundamental frequency (f0) contours in Mandarin and Cantonese speech, highlighting the limitations of previous methods such as decision trees and deep neural networks (DNNs) in achieving optimal results."
        },
        {
            "section number": "1.2",
            "key information": "The problem involves accurately modeling the f0 contours of Mandarin and Cantonese speech, which are crucial for natural speech synthesis."
        },
        {
            "section number": "1.3",
            "key information": "The proposed solution is the Additive-BLSTM model, which predicts both a base f0 contour and a residual f0 contour using two separate BLSTMs."
        },
        {
            "section number": "2.1",
            "key information": "The Additive-BLSTM method combines two BLSTM networks to predict base and residual f0 contours, leveraging both phonetic and lexical features."
        },
        {
            "section number": "2.2",
            "key information": "Existing methods struggle to effectively capture the complexities of f0 contours, particularly in tonal languages, resulting in less natural speech synthesis."
        },
        {
            "section number": "3.2",
            "key information": "The Additive-BLSTM model significantly improves the modeling of f0 contours for both Mandarin and Cantonese, outperforming traditional models."
        },
        {
            "section number": "6.4",
            "key information": "The method may face challenges in generalizing to languages with different tonal structures or in cases where lexical information is less relevant."
        },
        {
            "section number": "6.3",
            "key information": "Future research will focus on testing the model on additional tonal languages and refining its capabilities to handle diverse speech contexts."
        }
    ],
    "similarity_score": 0.5613205916388029,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1832_natur/papers/Generating Mandarin and Cantonese F0 Contours with Decision Trees and BLSTMs.json"
}