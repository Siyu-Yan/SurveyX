{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:2312.07476",
    "title": "Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection",
    "abstract": " ABSTRACT\n9 Jan 2024\nIn-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task\u2019s essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task\u2019s essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a novel perspective, providing a deeper insight into the demonstration selection strategy for ICL.\narXiv:2312.07476v2\n# Index Terms\u2014 In-Context Learning, Demonstration Selection, Large Language Models\n# 1. INTRODUCTION\nLarge Language Models (LLMs) [1] display a strong ability to perform In-Context Learning (ICL) [2], i.e. mastering natural language tasks from a small number of in-context demonstrations without any parameter updates [3, 4]. This flexible and efficient paradigm [5] gives LLMs the potential to become general-purpose models [6, 7], i.e. capable of generalizing to most tasks without further fine-tuning [8]. Despite the success of ICL in many NLP scenarios, there remains little understanding of how ICL works [6, 9]. As shown in Fig. 1(a), some previous studies attempted to explore the ICL mechanisms from various perspectives: [6] considered input-label format to be important for ICL; [9, 10]\n\u2217Corresponding author. This work was supported by the Shanghai Mun",
    "bib_name": "fan2024comparabledemonstrationsimportantincontext",
    "md_text": "# COMPARABLE DEMONSTRATIONS ARE IMPORTANT IN IN-CONTEXT LEARNING: A NOVEL PERSPECTIVE ON DEMONSTRATION SELECTION\nCaoyun Fan Jidong Tian Yitian Li Hao He\u2217 Yaohui Jin\u2217\nMoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n# ABSTRACT\n9 Jan 2024\nIn-Context Learning (ICL) is an important paradigm for adapting Large Language Models (LLMs) to downstream tasks through a few demonstrations. Despite the great success of ICL, the limitation of the demonstration number may lead to demonstration bias, i.e. the input-label mapping induced by LLMs misunderstands the task\u2019s essence. Inspired by human experience, we attempt to mitigate such bias through the perspective of the inter-demonstration relationship. Specifically, we construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, in order to highlight the task\u2019s essence and eliminate potential spurious correlations through the inter-demonstration comparison. Through a series of experiments on CDs, we find that (1) demonstration bias does exist in LLMs, and CDs can significantly reduce such bias; (2) CDs exhibit good performance in ICL, especially in out-of-distribution scenarios. In summary, this study explores the ICL mechanisms from a novel perspective, providing a deeper insight into the demonstration selection strategy for ICL.\narXiv:2312.07476v2\n# Index Terms\u2014 In-Context Learning, Demonstration Selection, Large Language Models\n# 1. INTRODUCTION\nLarge Language Models (LLMs) [1] display a strong ability to perform In-Context Learning (ICL) [2], i.e. mastering natural language tasks from a small number of in-context demonstrations without any parameter updates [3, 4]. This flexible and efficient paradigm [5] gives LLMs the potential to become general-purpose models [6, 7], i.e. capable of generalizing to most tasks without further fine-tuning [8]. Despite the success of ICL in many NLP scenarios, there remains little understanding of how ICL works [6, 9]. As shown in Fig. 1(a), some previous studies attempted to explore the ICL mechanisms from various perspectives: [6] considered input-label format to be important for ICL; [9, 10]\n\u2217Corresponding author. This work was supported by the Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), and the Fundamental Research Funds for the Central Universities.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f626/f626a03a-1dc3-4870-b7b7-43f67f71b248.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) Some perspectives on exploring the ICL mechanisms.</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/946a/946ade30-6a9a-4cd3-9c11-38c6a1f96894.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) Analysis of inter-demonstration relationship. Comparable Demonstrations can reduce LLMs from misunderstanding the task\u2019s essence.</div>\n<div style=\"text-align: center;\">Fig. 1. Overview of Comparable Demonstrations in ICL.</div>\nfound that the label space is one of the key drivers in ICL performance; [3, 11] suggested that the demonstration distribution (based on semantic similarity) can affect the information obtained by LLMs in ICL. However, as a potential perspective, the effect of the inter-demonstration relationship in ICL is not widely discussed. In this study, we attempt to explore the ICL mechanisms from the perspective of the inter-demonstration relationship. According to the implementation principles of ICL [12], it requires LLMs to induce from a few demonstrations to the task\u2019s essence, i.e., a specific input-label mapping that satisfies the task. However, due to the limited number of demonstrations, the input-label mapping that conforms to the demonstrations is not unique [4, 13], as shown in the top half of Fig. 1(b). Therefore, LLMs may induce input-label mappings that are different from the task\u2019s essence. In this study, we refer to this phenomenon as demonstration bias. Clearly, demonstration bias is detrimental to ICL, so how to select demonstrations to mitigate such bias is a worthwhile research direction. Based on human experience, a common solution is to consider the inter-demonstration relationship: selecting demonstrations that can be compared with each other, as shown in the bottom half of Fig. 1(b). In this study,\nwe refer to such demonstrations as Comparable Demonstrations (CDs). This solution stems almost from human instinct. For example, when teaching infants to differentiate objects, humans typically select comparable demonstrations (e.g., apples and pears), rather than selecting random or identical demonstrations. Intuitively, this inter-demonstration comparison can highlight the task\u2019s essence while eliminating possible bias and spurious correlations [14, 15]. However, it remains unknown whether the human experience of selecting demonstrations in ICL scenarios can be applied to LLMs. Inspired by [16, 17], we attempt to construct CDs by minimally editing the texts to flip the corresponding labels (in Section 2). Through such manual editing, strong comparisons are created between demonstrations [18], thereby highlighting the task\u2019s essence. To verify the effectiveness of CDs, we conduct extensive experiments: in Section 3, we employ instruction induction (LLMs generate descriptions of the task\u2019s essence based on demonstrations) to intuitively observe the demonstration bias of LLMs, and we confirm that CDs can significantly reduce such bias; in Section 4, we analyze the performance of CDs in ICL scenarios from multiple perspectives, and we verify that CDs can bring performance gains to ICL, especially in the out-of-distribution scenario.\n# 2. METHOD\n# 2.1. Preliminaries of In-Context Learning\nGenerally, In-Context Learning (ICL) can be regarded as a conditional text generation process. LLM (parameterized by \u03b8) performs ICL with K input-label pair demonstrations Ddemo = {x1, y1, x2, y2, . . . , xK, yK}, and LLM combines Ddemo to predict the label of the test example xt. Specifically, this process can be represented as:\n(1)\n\ufffd where T is the generated token length and is task-specific. Here, the role of demonstrations is to help LLM elicit an input-label mapping f : X \u2192Y, x \u2208X, y \u2208Y that is capable of matching the task\u2019s essence.\n# 2.2. Comparable Demonstrations\nAccording to the previous analysis, due to the small number of demonstrations (e.g., K = 4), ICL may suffer from demonstration bias. In this study, we attempt to eliminate such bias through Comparable Demonstrations (CDs). Specifically, the purpose of CDs is to highlight the task\u2019s essence through the inter-demonstration comparison. Inspired by Counterfactually-Augmented Data (CAD) [16, 17], we can construct CDs by minimally editing the texts to flip the corresponding labels1. We consider that this text-level\n1In this study, we employ existing CAD from [16] to construct CDs.\n<div style=\"text-align: center;\">editing can maximize comparisons between demonstrations. Here, we show an example of CDs in sentiment analysis as:</div>\nediting can maximize comparisons between demonstrations. Here, we show an example of CDs in sentiment analysis as:\nOriginal: I like this movie, I never get tired of watching it.\nEdited:\nI hate this movie, I am very tired of watching it. \n!\"#$%$&'\n(')*%$&'\nThis construction method has two benefits: on the one hand, the edited parts in texts are usually considered to involve the essential properties of tasks [19], which helps to highlight the task\u2019s essence; on the other hand, since the majority of texts are unedited, LLMs can spontaneously eliminate potential spurious correlations in the demonstrations [14]. In the fine-tuning paradigm, fine-grained manual editing (similar to CAD), as a data augmentation method, is considered inefficient and costly by the NLP community [20]. However, the ICL paradigm only requires a small number of demonstrations (e.g., \u223c10), while the requirements for data quality are relatively high (to prevent demonstrative bias). Therefore, we consider that this study finds a more meaningful application scenario for fine-grained manual editing, i.e., constructing high-quality CDs for ICL.\n# 3. LLM\u2019S DEMONSTRATION BIAS\nBased on human experience, demonstration bias is evident in ICL, but it is not yet known whether it also exists in LLMs. To explicitly observe demonstration bias in LLMs, we perform instruction induction2: LLMs explicitly generate descriptions of the task\u2019s essence based on a few demonstrations. By analyzing the generated instructions, we can perceive the inputlabel mappings induced by LLMs, and then compare them with the task\u2019s essence. We believe this is the most straightforward method to observe LLM\u2019s demonstration bias. Experimental Setup: In this section, the LLM we analyzed was openAI\u2019s gpt-3.5-turbo, the current state-ofthe-art LLM. The dataset we employed was IMDb [22], a sentiment analysis dataset in the movie domain. The demonstration number was set to K = 4. We tested three strategies for selecting demonstrations: random selection (random), selection based on semantic similarity (nearest), and random selection of CDs (CDs random), and each strategy generated 100 instructions. The text embeddings were obtained via openAI\u2019s text-embedding-ada-002, and we used cosine similarity to measure semantic similarity. Following previous research [21], the prompt we used is:\n2[21] demonstrated that state-of-the-art LLMs (e.g. OpenAI\u2019s InstructGPT) have the ability to implement instruction induction.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/e2ce/e2ce6470-4288-4589-85a1-7715108908e6.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fig. 2. Comparison of instruction quality under three demonstration selection strategies.</div>\nFig. 2. Comparison of instruction quality under three demonstration selection strategies.\nDue to the challenging nature of the automated quality evaluation of instructions, we manually evaluated instructions generated by three demonstration selection strategies, and the evaluation results are shown in Fig. 2. We find that there are significant differences between the instructions induced by LLMs and the task\u2019s essence, which implies that demonstration bias still exists in LLMs. To further analyze the demonstration bias in LLMs, we categorized bad instructions by error types. We summarized the error types of instructions into two categories and selected typical cases of both error types in Table 1: Case 1 highlights the detailed information in demonstrations (review of comedy), which is an over-interpretation of the input-label mapping; Case 2 misinterprets sentiment analysis as text assessment (overall evaluation), which is an over-simplification of the input-label mapping. Both error types are similar to characteristics that humans display in demonstration bias.\n# 3.2. Can CDs Reduce Demonstration Bias?\nAs shown in Fig. 2, the instructions generated by CDs significantly outperform the other two strategies, indicating the effectiveness of CDs in reducing demonstration bias. This result preliminarily indicates that the human experience of selecting demonstrations is likely to be equally effective for LLMs. In addition, it is worth noting that the performance of demonstration selection based on semantic similarity (nearest) is surprisingly poor, and we speculate that similar demonstrations contain too much repetitive information, which hinders LLMs from inducing the correct input-label mapping.\n# 4. PERFORMANCE OF CDS IN ICL\nAccording to the previous analysis in Section 3, it is highly likely that CDs can bring performance gains to ICL, as it has the ability to reduce demonstration bias. Therefore, we evaluate CDs in ICL scenarios and analyze the performance of CDs from multiple perspectives.\nType\nInstruction\nHuman\nFor each input, output whether the sentiment is positive or negative.\nCase 1\nRead the review of comedy and determine if it is positive or negative.\nCase 2\nPlease read the following text and give an overall evaluation.\nTable 1. Two types of bad instruction generated by LLMs. We mark the key parts in red.\nExperimental Setup: In this section, the LLM we analyzed was still openAI\u2019s gpt-3.5-turbo. Since there were existing CADs on sentiment analysis and Natural Language Inference (NLI), we conducted experiments on these two tasks. We conducted evaluations on both In-Distribution (ID) and Out-Of-Distribution (OOD) datasets: for sentiment analysis, ID dataset was IMDb [22], and OOD datasets were Amazon review [23] and Yelp review [24]; for NLI, ID dataset was SNLI [25], and OOD dataset was MNLI (split into matched and mismatched parts) [26]. The demonstration number was set to K = 4, 8, 12. In addition to the three strategies mentioned in Section 3, we added two more: selection based on semantic similarity in each label class (nearest class), and selection based on semantic similarity in CDs (CDs nearest). For each strategy, we sampled 500 examples in each dataset for evaluation. The text embeddings were still obtained via openAI\u2019s text-embedding-ada-002, and we used cosine similarity to measure semantic similarity. For sentiment analysis, the prompt we used is:\nThe sentence is x1, the sentiment is y1 . . . The sentence is\nxK, the sentiment is yK. The sentence is xt, the sentiment is\nwhere {x} refer to texts in sentiment analysis, and y \u2208\n{positive, negative}. For NLI, the prompt we used is:\nThe premise is xp\n1, the hypothesis is xh\n1, the relation is y1 . . .\nThe premise is xp\nK, the hypothesis is xh\nK, the relation is yK.\nThe premise is xp\nt , the hypothesis is xh\nt , the relation is\nwhere {xp} and {xh} refer to premise and hypothesis in NLI, and y \u2208{entailment, neutral, contradiction}. Table 2 & 3 exhibit the experimental results of all demonstration selection strategies in ICL. Next, we analyze these results from three perspectives.\n# 4.1. Dataset Distribution\nDue to the broader application scope of LLMs, we analyze the performance of CDs from the perspective of dataset distribution (ID and OOD scenarios), and our findings are: (1) strategies based on semantic similarity mainly improve ID performance, while strategies based on CDs mainly improve OOD performance; (2) the strategy that combines the advantages of both (CDs nearest) is the most competitive strategy for balancing both ID and OOD scenarios. Our analysis of the performance differences between ID and OOD scenarios is as follows: strategies based on seman-\nMethods\n4-shot\n8-shot\n12-shot\nID\nyelp\nAmazon\nID\nyelp\nAmazon\nID\nyelp\nAmazon\nrandom\n93.4\n93.2\n86.8\n95.6\n91.4\n86.6\n95.6\n91.0\n85.4\nnearest\n95.2 (+1.8%)\n90.4 (-2.8%)\n85.2 (-1.6%)\n95.8 (+0.2%)\n86.8 (-4.6%)\n84.2 (-2.4%)\n96.3 (+0.7%)\n83.8 (-7.2%)\n81.6 (-3.8%)\nnearest class\n95.4 (+2.0%)\n92.0 (-1.2%)\n88.4 (+1.6%)\n96.2 (+0.6%)\n91.2 (-0.2%)\n86.8 (-0.2%)\n96.3 (+0.7%)\n91.0 (+0.0%)\n84.4 (-1.0%)\nCDs random\n93.4 (+0.0%)\n94.0 (+0.8%)\n90.0 (+3.2%)\n95.4 (-0.2%)\n93.4 (+2.0%)\n91.4 (+4.8%)\n95.4 (-0.2%)\n94.4 (+3.4%)\n90.8 (+5.4%)\nCDs nearest\n94.0 (+0.6%)\n93.6 (+0.4%)\n88.8 (+2.0%)\n95.8 (+0.2%)\n94.4 (+3.0%)\n89.4 (+2.8%)\n96.4 (+0.8%)\n93.8 (+2.8%)\n90.2 (+4.8%)\nTable 2. Accuracy of different demonstration selection strategies on sentiment analysis. We consider the random selection (random) as benchmark, those with performance degradation are marked as red; those with performance improvement within 1% are marked as yellow; those with performance improvement above 1% are marked as green. The best performance is bold.\nMethods\n4-shot\n8-shot\n12-shot\nID\nMNLI-m\nMNLI-mm\nID\nMNLI-m\nMNLI-mm\nID\nMNLI-m\nMNLI-mm\nrandom\n74.5\n69.5\n72.8\n76.5\n70.9\n73.9\n74.4\n70.3\n73.7\nnearest\n75.8 (+1.3%)\n69.9 (+0.4%)\n75.1 (+2.3%)\n76.8 (+0.3%)\n70.2 (-0.7%)\n74.1 (+0.2%)\n75.8 (+1.4%)\n71.9 (+1.6%)\n74.9 (+1.2%)\nnearest class\n74.9 (+0.4%)\n71.9 (+2.4%)\n74.5 (+1.7%)\n75.7 (-0.8%)\n73.6 (+2.7%)\n74.5 (+0.6%)\n75.8 (+1.4%)\n72.6 (+2.3%)\n73.5 (-0.2%)\nCDs random\n73.3 (-1.2%)\n73.0 (+3.5%)\n74.7 (+1.9%)\n75.6 (-0.9%)\n73.3 (+2.4%)\n74.0 (+0.1%)\n77.2 (+2.8%)\n72.4 (+2.1%)\n75.5 (+1.8%)\nCDs nearest\n75.2 (+0.7%)\n73.5 (+4.0%)\n75.3 (+2.5%)\n77.1 (+0.6%)\n74.2 (+3.3%)\n76.1 (+2.2%)\n77.3 (+2.9%)\n72.7 (+2.4%)\n74.8 (+1.1%)\ntic similarity would allow LLMs to learn dataset-specific information (e.g., information about movies in IMDb dataset), which cannot be generalized to OOD scenarios; while CDs attempt to help LLMs focus on the task\u2019s essence, which to some extent alleviates the focus on dataset-specific information, thereby helping to improve the performance of LLMs in OOD scenarios. Since the application scenarios of ICL are usually uncertain, we consider that the performance gains of CDs in OOD scenarios is meaningful.\n# 4.2. Task Complexity\nTo clarify the extent of demonstration bias and the applicability of CDs, we analyze the experimental results from task complexity. Generally, the NLP community considers NLI to be a more complex task than sentiment analysis. We find that different strategies show larger performance differences on the simple task (sentiment analysis), sometimes approaching 10%, while their performance is more stable on the complex task (NLI), typically below 3%. This may suggest that demonstration bias is more likely to influence LLMs to induce the task\u2019s essence on simple tasks. Therefore, although the experiments show that CDs are also effective for complex tasks, they are more necessary for simple tasks.\n# 4.3. Demonstration Number\nDemonstration number is often considered to affect the performance of ICL and is closely related to demonstration bias. Therefore, we analyze the performance differences between CDs and other strategies from this aspect.\nThe experimental results indicate that in most cases, the performance of ICL improves as the demonstration number increases, which is in line with our expectation of demonstration bias. In addition, we find that CDs are relatively robust to the demonstration number, while strategies based on semantic similarity are more sensitive. Since the ICL paradigm is extremely sensitive to the demonstration selection, the robustness exhibited by CDs is a significant advantage.\n# 5. CONCLUSION\nIn this study, we explore the mechanisms of ICL from the perspective of the inter-demonstration relationship, and we consider that demonstration bias may exist in LLMs due to the limitation of the demonstration number. Inspired by human experience, we attempt to construct Comparable Demonstrations (CDs) by minimally editing the texts to flip the corresponding labels, to mitigate such potential bias. A series of experiments indicate that CDs bring performance gains to the ICL, especially in the OOD scenario. In the future, we aim to explore ICL in more complex scenarios (e.g. mathematical reasoning, multi-hop inference), and rigorously analyze the mechanisms of ICL from the perspective of the interdemonstration relationship. We consider this study has two main limitations. First, although ICL requires relatively few demonstrations, manual annotation remains expensive, and automatically generating CDs is not currently feasible. Second, CDs only consider oneto-one relationships between demonstrations, without taking into account many-to-many relationships, which clearly does not make full use of the inter-demonstration relationship.\n[1] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus, \u201cEmergent abilities of large language models,\u201d Trans. Mach. Learn. Res., 2022. [2] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu, \u201cHarnessing the power of llms in practice: A survey on chatgpt and beyond,\u201d CoRR, 2023. [3] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen, \u201cWhat makes good in-context examples for gpt-3?,\u201d in ACL, 2022. [4] Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng, Danqi Chen, and He He, \u201cMeasuring inductive biases of in-context learning with underspecified demonstrations,\u201d ACL, 2023. [5] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig, \u201cPre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,\u201d ACM Comput. Surv., 2023. [6] Jane Pan, Tianyu Gao, Howard Chen, and Danqi Chen, \u201cWhat in-context learning \u201dlearns\u201d in-context: Disentangling task recognition and task learning,\u201d ACL, 2023. [7] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He, \u201cCan large language models serve as rational players in game theory? a systematic analysis,\u201d in AAAI, 2024. [8] Caoyun Fan, Jidong Tian, Yitian Li, Wenqing Chen, Hao He, and Yaohui Jin, \u201cChain-of-thought tuning: Masked language models can also think step by step in natural language understanding,\u201d in EMNLP, 2023. [9] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer, \u201cRethinking the role of demonstrations: What makes in-context learning work?,\u201d in EMNLP, 2022. 10] Kang Min Yoo, Junyeob Kim, Hyuhng Joon Kim, Hyunsoo Cho, Hwiyeol Jo, Sang-Woo Lee, Sang-goo Lee, and Taeuk Kim, \u201cGround-truth labels matter: A deeper look into input-label demonstrations,\u201d in EMNLP, 2022. 11] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola, \u201cAutomatic chain of thought prompting in large language models,\u201d CoRR, 2022. 12] Albert Webson and Ellie Pavlick, \u201cDo promptbased models really understand the meaning of their prompts?,\u201d in NAACL, 2022.\n[13] Ruixiang Tang, Dehan Kong, Longtao Huang, and Hui Xue, \u201cLarge language models can be lazy learners: Analyze shortcuts in in-context learning,\u201d CoRR, 2023. [14] Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, and Yaohui Jin, \u201cImproving the out-ofdistribution generalization capability of language models: Counterfactually-augmented data is not enough,\u201d ICASSP, 2023. [15] Caoyun Fan, Wenqing Chen, Jidong Tian, Yitian Li, Hao He, and Yaohui Jin, \u201cUnlock the potential of counterfactually-augmented data in out-of-distribution generalization,\u201d Expert systems with applications, 2024. [16] Divyansh Kaushik, Eduard H. Hovy, and Zachary Chase Lipton, \u201cLearning the difference that makes a difference with counterfactually-augmented data,\u201d ICLR, 2020. [17] Divyansh Kaushik, Amrith Setlur, Eduard H. Hovy, and Zachary Chase Lipton, \u201cExplaining the efficacy of counterfactually augmented data,\u201d in ICLR, 2021. [18] Linyi Yang, Jiazheng Li, P\u2019adraig Cunningham, Yue Zhang, Barry Smyth, and Ruihai Dong, \u201cExploring the efficacy of automatically generated counterfactuals for sentiment analysis,\u201d ACL, 2021. [19] Zhao Wang and Aron Culotta, \u201cRobustness to spurious correlations in text classification via automatically generated counterfactuals,\u201d in AAAI, 2021. [20] Nitish Joshi and He He, \u201cAn investigation of the (in)effectiveness of counterfactually augmented data,\u201d ACL, 2022. [21] Or Honovich, Uri Shaham, Samuel R. Bowman, and Omer Levy, \u201cInstruction induction: From few examples to natural language task descriptions,\u201d CoRR, 2022. [22] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts, \u201cLearning word vectors for sentiment analysis,\u201d in ACL, 2011. [23] Jianmo Ni, Jiacheng Li, and Julian McAuley, \u201cJustifying recommendations using distantly-labeled reviews and fine-grained aspects,\u201d in EMNLP, 2019. [24] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun, \u201cCharacter-level convolutional networks for text classification,\u201d NeurIPS, 2015. [25] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning, \u201cA large annotated corpus for learning natural language inference,\u201d in EMNLP, 2015. [26] Adina Williams, Nikita Nangia, and Samuel R. Bowman, \u201cA broad-coverage challenge corpus for sentence understanding through inference,\u201d in NAACL, 2018.\nIn Table 4, we display more typical bad cases in instruction induction. These cases can be used as a supplement to Table 1.\nType\nInstruction\nCase 1\nPlease write an output for each of the following in-\nputs based on your overall impression of the movie:\npositive, negative, or neutral.\nCase 2\nRead the input and write an output based on your\noverall impression of the movie.\nCase 3\nPlease watch this movie and give me your honest\nopinion about it.\nCase 4\nWatch this foreign film and write your overall impres-\nsion of it.\nCase 5\nProvide an output for each of the given inputs.\nCase 6\nProvide an output based on your opinion of the movie\nor experience described in the input.\nCase 7\nWatch the movie and write a review.\nCase 8\nRead the following statement and determine whether\nthe overall sentiment is positive, negative, or neutral.\nCase 9\nRead the following descriptions of movies and write\n\u2019positive\u2019 or \u2019negative\u2019 as the output based on\nwhether the reviewer liked the movie or not.\nCase 10\nProvide an output based on your experience or opin-\nion of the given input.\nTable 4. More typical bad cases in instruction induction. Key parts are marked in red.\nThese bad cases further confirm that there is also demonstration bias in LLMs. Therefore, we need to study how to eliminate this bias.\n# B. RULES FOR MANUAL EVALUATION\nWe divide the generated instructions into three categories:\n1. Correct and satisfying instruction. This refers to instructions that humans can implement 0-shot reasoning, and can generalize to out-of-distribution datasets. 2. Acceptable instruction with minor imperfections. This refers to the presence of some redundant words. Humans can still implement 0-shot reasoning, but cannot generalize to out-of-distribution datasets. 3. Invalid instruction with significant errors. This refers to the inability of even humans to implement 0-shot reasoning. The instruction completely misunderstands the essence of tasks.\n1. Correct and satisfying instruction. This refers to instructions that humans can implement 0-shot reasoning, and can generalize to out-of-distribution datasets.\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the issue of demonstration bias in In-Context Learning (ICL) for Large Language Models (LLMs), highlighting the limitations of previous methods and the necessity for a new approach to enhance understanding and performance in ICL scenarios.",
        "problem": {
            "definition": "The problem is the existence of demonstration bias in LLMs, where the limited number of input-label demonstrations leads to incorrect input-label mappings that fail to capture the task's essence.",
            "key obstacle": "The main challenge is the non-uniqueness of input-label mappings due to the small number of demonstrations, which can lead to LLMs misunderstanding the task."
        },
        "idea": {
            "intuition": "The idea is inspired by human teaching methods, where comparable demonstrations are used to help learners differentiate between concepts effectively.",
            "opinion": "The proposed idea involves constructing Comparable Demonstrations (CDs) by minimally editing texts to flip labels, allowing for better inter-demonstration comparisons.",
            "innovation": "The key innovation lies in the method of creating CDs, which enhances the selection of demonstrations by focusing on their inter-demonstration relationships, unlike existing methods that do not consider this aspect."
        },
        "method": {
            "method name": "Comparable Demonstrations",
            "method abbreviation": "CDs",
            "method definition": "CDs are constructed by minimally editing existing demonstrations to create strong comparisons that highlight the task's essence.",
            "method description": "CDs involve flipping the labels of demonstrations while keeping the majority of the text unchanged to maximize the effectiveness of comparisons.",
            "method steps": "1. Select original demonstrations. 2. Edit texts to flip corresponding labels. 3. Use the edited texts as Comparable Demonstrations for ICL.",
            "principle": "This method is effective because it reduces demonstration bias by ensuring that the demonstrations used in ICL are more representative of the task's essence, thereby improving the model's understanding."
        },
        "experiments": {
            "evaluation setting": "Experiments were conducted using the IMDb sentiment analysis dataset with OpenAI's gpt-3.5-turbo LLM, comparing the performance of CDs against various demonstration selection strategies in both in-distribution and out-of-distribution contexts.",
            "evaluation method": "The performance of the method was assessed by generating instructions based on demonstrations and comparing the quality of these instructions across different selection strategies."
        },
        "conclusion": "The study concludes that Comparable Demonstrations significantly reduce demonstration bias in LLMs and improve performance in ICL, especially in out-of-distribution scenarios, suggesting a valuable direction for future research.",
        "discussion": {
            "advantage": "The primary advantage of CDs is their ability to mitigate demonstration bias effectively, leading to improved performance in ICL tasks compared to traditional methods.",
            "limitation": "A noted limitation is the manual nature of creating CDs, which can be labor-intensive and not easily scalable, alongside the focus on one-to-one relationships without considering many-to-many relationships.",
            "future work": "Future research will aim to explore ICL in more complex scenarios and develop methods for automatically generating Comparable Demonstrations."
        },
        "other info": {
            "info1": "This work was supported by the Shanghai Municipal Science and Technology Major Project and the Fundamental Research Funds for the Central Universities.",
            "info2": {
                "info2.1": "The authors are affiliated with the MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University.",
                "info2.2": "The study includes references to various previous works that explore different aspects of ICL and LLMs."
            }
        }
    },
    "mount_outline": [
        {
            "section number": "1.1",
            "key information": "The paper addresses the issue of demonstration bias in In-Context Learning (ICL) for Large Language Models (LLMs), emphasizing the limitations of previous methods."
        },
        {
            "section number": "1.2",
            "key information": "Demonstration bias in LLMs leads to incorrect input-label mappings that fail to capture the task's essence, highlighting the relevance of in-context learning."
        },
        {
            "section number": "3.1",
            "key information": "The method of Comparable Demonstrations (CDs) reduces demonstration bias by ensuring that the demonstrations used in ICL are more representative of the task's essence."
        },
        {
            "section number": "3.3",
            "key information": "The paper proposes a method of creating Comparable Demonstrations by minimally editing texts to flip labels, enhancing the selection of demonstrations."
        },
        {
            "section number": "6.1",
            "key information": "The primary advantage of Comparable Demonstrations is their ability to mitigate demonstration bias effectively, improving performance in ICL tasks."
        },
        {
            "section number": "6.2",
            "key information": "A noted limitation of the Comparable Demonstrations method is the manual nature of creating them, which can be labor-intensive and not easily scalable."
        },
        {
            "section number": "7",
            "key information": "The study concludes that Comparable Demonstrations significantly reduce demonstration bias in LLMs and improve performance in ICL, suggesting future research directions."
        }
    ],
    "similarity_score": 0.7509985934536529,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-07-2330_in-co/papers/Comparable Demonstrations are Important in In-Context Learning_ A Novel Perspective on Demonstration Selection.json"
}