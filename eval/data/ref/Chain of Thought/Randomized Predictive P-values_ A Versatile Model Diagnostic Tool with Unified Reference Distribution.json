{
    "from": "arxiv",
    "scholar_id": null,
    "detail_id": "arXiv:1708.08527",
    "title": "Randomized Predictive P-values: A Versatile Model Diagnostic Tool with Unified Reference Distribution",
    "abstract": "Residual analysis is a standard tool for assessing normal regression. However, for a discrete response, the traditional Pearson and deviance residuals cluster on lines and their distributions are far from normality. Graphical and quantitative inspection of these residuals provides little information for model diagnosis. Marshall and Spiegelhalter (2007) defined cross-validatory predictive p-values which are uniformly distributed for a continuous response but not for a discrete response. Randomized predictive p-values (RPP) are uniformly distributed for discrete responses. Normally-transformed RPPs (NRPPs) can be used to diagnose a regression model with a discrete response in the same way as diagnosing normal regression with Pearson residuals. The NRPPs are nearly the same as the randomized quantile residuals (RQR) proposed by Dunn and Smyth (1996) but remain little known by statisticians. This paper provides an exposition of RQR using the RPP perspective. The contributions of this paper include: (1) we give a rigorous proof and illustrative examples of the uniformity of RPPs; (2) we conduct extensive simulation studies and a real data example to demonstrate the normality of NRPPs; (3) we show that the NRPP method is a versatile diagnostic tool for detecting many kinds of model inadequacies.",
    "bib_name": "feng2019randomizedpredictivepvaluesversatile",
    "md_text": "# Randomized Predictive P-values: A Versatile Model Diagnostic Tool with Unified Reference Distribution\nCindy Feng\u2217 School of Public Health, University of Saskatchewan Alireza Sadeghpour\nCindy Feng\u2217\n17 Jan 2019\nCindy Feng\u2217 School of Public Health, University of Saskatchewan Alireza Sadeghpour\nDepartment of Mathematics and Statistics, University of Saskatchewan Longhai Li\u2020 Department of Mathematics and Statistics, University of Saskatchewan January 18, 2019\n# Abstract\nResidual analysis is a standard tool for assessing normal regression. However, for a discrete response, the traditional Pearson and deviance residuals cluster on lines and their distributions are far from normality. Graphical and quantitative inspection of these residuals provides little information for model diagnosis. Marshall and Spiegelhalter (2007) defined cross-validatory predictive p-values which are uniformly distributed for a continuous response but not for a discrete response. Randomized predictive p-values (RPP) are uniformly distributed for discrete responses. Normally-transformed RPPs (NRPPs) can be used to diagnose a regression model with a discrete response in the same way as diagnosing normal regression with Pearson residuals. The NRPPs are nearly the same as the randomized quantile residuals (RQR) proposed by Dunn and Smyth (1996) but remain little known by statisticians. This paper provides an exposition of RQR using the RPP perspective. The contributions of this paper include: (1) we give a rigorous proof and illustrative examples of the uniformity of RPPs; (2) we conduct extensive simulation studies and a real data example to demonstrate the normality of NRPPs; (3) we show that the NRPP method is a versatile diagnostic tool for detecting many kinds of model inadequacies.\nKeywords: p-value; predictive checking; model diagnostics; goodness-of-fit test; non-linearity over-dispersion; zero-inflation.\n\u2217The author gratefully acknowledges NSERC for financial support. \u2020Correspondence should be sent to Longhai Li. The author gratefully acknowledges NSERC and CFI for financial support.\n# 1 Introduction\nModel diagnosis/checking via residual analysis is a standard practice in normal regression modeling based on the theory that Pearson residuals are normally distributed. First, the discrepancy nature between a model and data (e.g., non-linear effects and heavy tails) can be revealed by looking at residual plots. Second, the overall goodness-of-fit (GOF) of the model can be checked graphically and quantitatively by examining the normality of residuals using QQ plots and statistical tests. Third, residuals can identify outliers with the so-called \u201cempirical rules\u201d for normal distribution. However, when a response is discrete, traditional Pearson and deviance residuals cluster on lines corresponding to distinct response values, hence these residuals are far from being normally distributed. As a result, graphical and quantitative examination of these residuals provides little information for diagnosing non-normal models. According to Lin et al. (2002), \u201calthough model misspecification can seriously affect the validity and efficiency of regression analysis, model checking has not become a routine practice, partly because of the lack of suitable tools.\u201d Many alternative residuals have been proposed for specific problems in the literature, see Lin et al. (2002); Arbogast and Lin (2005); Le\u00b4on and Cai (2012); Yuan and Johnson (2012); among others. However, the methods for analyzing these residuals are quite dissimilar to those for Pearson residuals in normal regression. To the best of our knowledge, statistical inferences without serious model checking remain common in today\u2019s statistical practice. In Bayesian statistics, Gelman et al. (1996) proposed to use the posterior predictive distribution of a discrepancy measure, such as \u03c72 statistic to the posterior mean, to check the overall GOF of a model. A posterior predictive p-value is defined as the probability that the replicated discrepancy measure is greater than the observed discrepancy measure. This posterior predictive p-values are not uniformly distributed but are more concentrated around 0.5, because the data is used twice in both training and validating process in machine learning language. Nevertheless, the posterior p-values are still very informative and are widely used to check the GOF of Bayesian models (Gelman, 2013). However, such an overall GOF checking method cannot reveal the discrepancy nature which residual diagnosis can do. Marshall and Spiegelhalter (2003, 2007) defined a cross-validatory (CV)\nP(Y rep i > yi|y\u2212i) + 0.5P(Y rep i = yi|y\u2212i),\nwhere Y rep i is distributed as the CV predictive distribution of yi given the observations except yi. Marshall and Spiegelhalter (2007) shows that when yi is continuous the CV predictive p-values are uniformly distributed on (0, 1). For a discrete response, in order to obtain uniformly distributed predictive p-values, the 0.5 in (1.1) needs to be modified to a random number on (0, 1), that is, the probability of yi is randomly split into the left and right tails. This modification results in randomized predictive p-values (RPPs). Due to the uniformity, the RPPs can be transformed to quantities with any desired distribution with its quantile function. For example, using normal quantile function, RPPs can be transformed to residuals with the normal distribution, which we will call normallytransformed RPP (NRPP). If we do not use cross-validation to eliminate the bias of using the data twice, the NRPPs are the same as the randomized quantile residuals(RQR) introduced by Dunn and Smyth (1996). Unfortunately, the RQR method has not been widely embraced as a standard model diagnostic tool for regression models, although it has been used in a few statistical packages (Benjamin et al., 2003; Rigby and Stasinopoulos, 2005; Ospina and Ferrari, 2012). The lack of awareness and application of RQR is due to two reasons: (1) The definition of RQR is quite different from Pearson residuals; (2) There is a shortage of empirical and theoretical studies for RQR in the literature; Dunn and Smyth (1996) provided neither a proof of the normality nor an investigation of its statistical properties with simulated datasets. This article provides an exposition to RQR using the RPP perspective. In what follows, we will use NRPP instead of RQR because the essential technique in this method is the randomization on predictive p-values, and the fact that p-values of test statistics are uniformly distributed under the null hypothesis is well-known to statisticians. Our contributions are summarized as follows. (1) We provide a rigorous proof of the uniformity of RPP, and illustrative examples for explaining why the randomization is necessary in order to obtain truly uniformly distributed predictive p-values for discrete response variables. (2) We use extensive simulation studies to demonstrate that the NRPPs are normally distributed under the\n(1.1)\ntrue model, and the overall GOF tests by applying Shapiro-Wilk normality test (as opposed to other tests) to the NRPPs are well-calibrated. (3) We show that the NRPP method is a versatile model diagnostic tool for detecting many kinds of mis-specifications due to lack of necessary complexity, such as non-linearity, zero-inflation, and over-dispersion. Identification of these modeling complexities is of great interest in contemporary application areas such as epidemiology, ecology, and bioinformatics (Feng and Dean, 2012; Xu et al., 2015; Brilleman et al., 2016; Zhang et al., 2017). This article is organized as follows. In Section 2, we review the traditional residuals, followed by a discussion of the problems Pearson and deviance residuals. Then, we define the RPPs and NRPPs in Section 3, where we provide illustrative examples for explaining the uniformity of RPPs. In Section 4, simulation studies in three scenarios (non-linearity, zero-inflation, and over-dispersion) are conducted to demonstrate that NRPPs have the normality under the true model and great power in detecting these model complexities. In Section 5, we further demonstrate the advantage of the NRPPs with a health care utilization dataset. Concluding remarks are given in Section 6.\n# 2 Review of Traditional Residuals\n2.1 Common Non-Normal Regression Models\nThe GLM framework generalizes the ordinary linear regression to allow the response variable to follow a non-normal distribution, such as the Poisson and negative binomial. These non-normal distributions belong to a broad family called the exponential family with its probability density function (PDF) or probability mass function (PMF) defined by\nfor some functions a(\u00b7), b(\u00b7) and c(\u00b7). The expected value of the response variable \u00b5i = E(yi|xi) is linked to a set of covariates with a linear function g(\u00b5i) = xi\u03b2. In many contemporary application areas, count data often contains excessive zeros that may not be captured by a conventional Poisson or negative binomial (NB) model; these data are commonly known as zero-inflated data. One popular approach to model such data is a mixture model of degenerate zeros from the non-risk group (i.e., structural zeros)\n(2.1)\nand responses with random zeros or positive values from the at-risk group (Lambert, 1992; Yu et al., 2013). The zero-inflated Poisson (ZIP) model is denoted by ZIP(\u03bbi, pi), with two components \u03bbi and pi: pi is the probability that the ith observation belongs to the non-risk group, and \u03bbi is the mean of counts from at-risk group. The pi and \u03bbi are typically linked to the covariates by logit(pi) = zi\u03b3 and log(\u03bbi) = xi\u03b2, where zi and xi are vectors of covariates. The PMF of ZIP, pzip(yi), is written as pzip i (yi) = pi + (1 \u2212pi)e\u2212\u03bbi for yi = 0 and pzip i (yi) = (1 \u2212pi) e\u2212\u03bbi\u03bbyi i yi! , for yi > 0. With F pois(y; \u03bbi) denoting the CDF of a Poisson distribution with mean parameter \u03bbi, the CDF of ZIP can be written as F zip i (yi) = pi + (1 \u2212pi)F pois(yi; \u03bbi), for yi \u22650; = 0 otherwise. The zero-inflated negative binomial (ZINB) model is similarly defined with Poisson distribution replaced by NB distribution.\n# 2.2 Pearson an Deviance Residuals\nPearson residual is defined as the raw residual scaled by the standard deviation of the response variable, denoted as ri = yi\u2212\u02c6\u00b5i \u221a \ufffdV (yi) where \u02c6\u00b5i is the fitted value for yi and \ufffdV (yi) is the estimated variance for yi. Calculating the Pearson residuals for non-normal models is straightforward once we can compute the mean \u00b5i and V (yi). Let l(y; \u00b5) be the log-likelihood function based on a fitted model. A saturated model is the model which there are as many estimated parameters as data points (Agresti, 2015). By definition, this will lead to a perfect fit and has the highest log-likelihood among all models. For example, for Poisson and negative binomial regression models, l(y, y) yields the highest achievable log-likelihood. Let l (y; \u02dc\u00b5) and l (y; \u02c6\u00b5) denote the log-likelihoods for the saturated and the fitted model, respectively. The deviance is then defined by D = 2 {l (y; \u02dc\u00b5) \u2212l (y; \u02c6\u00b5)} . The deviance residual is defined as the signed square root of the ith term in D, possibly rescaled by a factor that is free of i. For the exponential family, the deviance residual is di = sign(yi \u2212\u02c6\u00b5i) \ufffd 2 \ufffd yi(\u02dc\u03b8i \u2212\u02c6\u03b8i) \u2212b(\u02dc\u03b8i) + b(\u02c6\u03b8i) \ufffd , where \u02dc\u03b8i and \u02c6\u03b8i denote the parameters in the saturated and the fitted model. Finding a saturated model for the deviance residuals may be ambiguous when the model is not in the exponential family. For the ZIP model, Lee et al. (2001) showed that Poisson(yi) is a saturated model for ZIP; hence, the deviance residual for the ZIP model is defined as the signed square root of the ith term in the log likelihood difference between the ZIP and the Poisson(yi) model; see Lee et al. (2001).\nFor normal regression, the Pearson and deviance residuals are identical and have an exact normal distribution under the true model. However, their distributions are often skewed and non-normally distributed for non-normal regression models. The residuals cluster on separated lines according to distinct response values, imposing great challenges for visual inspection. Therefore, Pearson and deviance residuals are difficult to use for graphically diagnosing non-normal regression models. Quantitative assessment of the overall GOF with Pearson and deviance residuals are also challenging. The Pearson \u03c72 statistic is written as X2 = \ufffdn i=1 r2 i . The asymptotic distribution of X2 and D under the true model is often assumed to be \u03c72 n\u2212p, where n is the sample size and p is the number of parameters. However, the use of this asymptotic distribution for both X2 and D lacks theoretical underpinning. To justify a \u03c72 distribution as the asymptotic distribution for X2, the number of squares must be fixed as n \u2192\u221e; this scenario obviously does not occur in X2 because the number of squares approaches infinity as n \u2192\u221e. The general theory (Wilk\u2019s theorem) for the likelihood ratio test (LRT) is often used to justify the \u03c72 n\u2212p as the asymptotic distribution for the deviance D. However, this argument is flawed (Wood, 2006). Wilk\u2019s theorem assumes that the numbers of parameters in two nested models being fixed as n \u2192\u221e. However, the number of parameters in the saturated model increases linearly with n.\n# 3 Randomized Predictive P-value\n3.1 Definition of Randomized Predictive P-values\n# 3.1 Definition of Randomized Predictive P-values\nWe will now define the randomized predictive p-values in technical terms. Let Fi(yi) be the cumulative distribution function (CDF) for a response variable yi given a set of covariates xi in an assumed regression model, and pi(yi) be the corresponding PMF. The randomized predictive p-value (RPP) for an observed yi is defined as\nF \u2217(yi, ui) = P(Y rep i < yi|xi) + ui \u00b7 P(Y rep i = yi|xi) = Fi(yi\u2212) + ui \u00b7 pi(yi) (\nwhere Y rep i represents a random variable with the same distribution as observed yi given xi, Fi(yi\u2212) is the lower limit of Fi at yi (i.e., supy<yi Fi(y)) and ui is a random number from a uniform distribution on (0, 1). A less intuitive expression given by Dunn and Smyth\n(3.1)\n(1996) for the RPP is that F \u2217(yi, ui) is a uniform random number between ai = Fi(yi\u2212) and bi = Fi(yi). We can easily see that they are the same. The RPPs in (3.1) are uniformly distributed on (0, 1) under the true model, which we will prove in Section A. Therefore, we can transform RPPs to quantities with any desired distribution using its quantile function, and assess the transformed RPPs by comparing to this distribution. The normal distribution is well-understood by statisticians with the so-called \u201cempirical rules\u201d. To obtain normally distributed residuals, we can transform RPPs with normal quantile function, resulting in normally-transformed RPP (NRPP):\n# qi = \u03a6\u22121(F \u2217(yi, ui))\nwhere \u03a6\u22121 is the quantile function of the standard normal distribution. When Fi is continuous at yi, the pi(yi) = 0 (note that, pi(yi) is the probability mass function); that is, there is no actual \u201crandomness\u201d in F \u2217(yi, ui). The formula given in (3.1) encompasses this situation. Particularly, one can easily see that for normal regression, the NRPP in (3.2) is the same as Pearson residual. Additionally, the variability in F \u2217(yi, ui) will be smaller when the probability at yi is smaller. This scenario typically occurs when the mean of yi is large. However, when the probability at yi is large, the randomization with ui is necessary to produce uniform p-values. In Marshall and Spiegelhalter (2003, 2007), a predictive p-value with ui = 0.5 is defined for identifying outliers. We will refer this predictive value as \u201cmiddle-point predictive p-values\u201d (MPPs), and refer the corresponding normally-transformed MPPs as NMPPs. NMPP will be compared to NRPP in our examples to see the necessity of the randomization for obtaining uniformly distributed predictive p-values.\n# 3.2 Illustrative Examples\n# 3.2.1 An Example with No Covariate\n3.2.1 An Example with No Covariate We first consider a scenario without covariate. Suppose that the true distribution for yi has the following PMF: p0(yi) = 0.25 for yi = 0 or 2; = 0.5 for yi = 1. For a dataset generated from p0, we expect that a quarter of yi are 0, half of yi are 1, and a quarter of yi are 2. The RPP function F \u2217with p0 as the considered model converts yi = 0 into a uniform random\n(3.2)\nnumber on (0, 0.25), yi = 1 into a uniform random number on (0.25, 0.75) and yi = 2 into a uniform random number on (0.75, 1). Overall, the random numbers converted with F \u2217 are uniformly distributed on (0, 1). To illustrate, we simulate a sample of size 2000 from this distribution and compute F \u2217(yi, ui) based on p0. As depicted in Figure 1a, F \u2217(yi, ui) is uniformly distributed between 0 and 1.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4b49/4b493621-a8df-49bd-906d-cc575fe9ae1e.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/f5bb/f5bb41fd-968a-4b5d-988b-3f836cfc3a4c.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(a) RPPs for the true model</div>\nFigure 1: The RPPs for the true model (left panel) and a wrong model (right panel) for the first example given in Section 3.2.1. For each subfigure, the left plot shows the histogram of 2000 RPPs, and the right plot shows the scatterplots of 100 RPPs against observed yi. Suppose we compute RPPs based on a wrong model with PMF given by: p1(yi) = 0.1 for yi = 0 or 2; = 0.8 for yi = 1. All zeros (around 40% of data) will be scattered uniformly to the interval (0, 0.1), all ones (around 50% data) will be uniformly scattered to the interval (0.1, 0.9), and all two\u2019s (around 25% of data) will be scattered uniformly to the interval (0.9, 1). As such, the distribution of RPPs based on p1 is more dense on both left and right tails than the middle interval (0.1, 0.9); see Figure 1b. The non-uniformity of RPPs indicates that the model p1 is wrong for the data. The fat tails of the histogram of RPPs reveal that the model p1 has shorter tails than the true model (p0). The non-uniformity is indeed caused by the mis-matching of the observed frequencies (0.25,0.5, and 0.25) of RPPs on the three intervals and the theoretical frequencies (0.1,0.8, and 0.1) postulated by the model p1. 3.2.2 An Example with Covariate To further demonstrate the idea of RPP in regression settings, we simulate 1000 observations from a Poisson model with mean \u00b5i given by log(\u00b5i) = \u22121 + 2sin(2xi), and xi \u223cUniform(0, 2\u03c0). Then, we fitted the Poisson regression with the true mean structure\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/2003/2003e689-b85e-4ba9-a827-b61abea56d12.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">(b) RPPs for a wrong model</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1aa0/1aa016c3-f6fd-45dc-a5f6-5738552ff4d1.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/26ed/26ed4043-14c6-4ad6-8678-17be5423ba16.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">MPP, True Model</div>\nFigure 2: An example of the RPPs in comparison with other residuals. Panels in the left column present the residuals or predictive p-values under the true model and panels in the right column present the corresponding values under a wrong model. For the RPPs and MPPs, each black line is a CDF curve of F(k|xi) versus xi associated with a value of k. The colours of points represent values of yi.\nand a wrong Poisson model with mean structure log(\u00b5i) = \u03b20 + \u03b21xi. The CDF of the response variable Yi given xi is denoted by F(k|xi) = P(Yi \u2264k|xi), for k = 0, 1, . . .. Figure 2 shows F(k|xi) as a function of xi, with each black line representing a CDF curve associated with a value k in {0, 1, 2, . . . , }. The distance between two curves, F(yi \u22121|xi) and F(yi|xi), is the \u201ctheoretical\u201d (model-based) probability of observing yi. The F \u2217(yi, ui) for each observed yi is a random point between the CDF lines F(yi \u22121|xi) and F(yi|xi). This random scattering of yi facilitates the comparison of the \u201cobserved\u201d frequency (fraction of points between two curves) and \u201ctheoretical\u201d frequency (distance of two curves). If the \u201cobserved\u201d and \u201ctheoretical\u201d frequencies agree well, the F \u2217(yi, ui) should be uniformly distributed on (0, 1) in each neighbourhood of xi; otherwise, they are not. Under the true model, the top-left plot in Figure 2 depicts that the RPPs are uniformly distributed on (0, 1) given each neighbourhood of xi. By contrast, under the wrong model, the top-right plot of Figure 2 shows that the RPPs given each xi are not uniformly distributed. For example, given xi = 2, the wrong model postulates that P(yi = 1) is about 0.7-0.4=0.3. However, the actual probability of yi = 1 is near 0.1 in the true model, hence, we see very few RPPs in the interval (0.4,0.7) in RRP plot for the wrong model. The mis-matching in \u201cobserved\u201d and \u201ctheoretical\u201d frequencies results in non-uniformity of RPPs under the wrong model. Without randomization, the mid-point predictive p-values (MPP) cluster on separated lines with each associated a distinct value of y. Therefore, we cannot graphically assess the matching of the observed and frequencies, and the MPPs are not uniformly distributed in any neighbourhood of xi when the true model is fitted; indeed they show a clear non-linear trend. Similarly, the Pearson and deviance residuals cluster on lines, hence, are not normally distributed under the true model; they also show a clear non-linear trend under the true model. In summary, due to the clustering on lines, MPPs, Pearson and deviance residuals cannot confirm the good fit by the true model, and hence cannot be used to identify the non-linearity in the wrong model due to lack of a unified reference distribution under the true model for comparison. The randomization in RPP is necessary to produce truly uniform predictive p-values for discrete response under the true model so that the model mis-specification can be revealed by comparing the RPPs to this reference distribution.\n3.3 The Uniformity of RPPs The RPPs are uniformly distributed, and correspondingly the NRPPs are normally distributed, under the true model. First, let us recall the well-known property of p-value for a continuous response variable written as Theorem 3.1. Suppose a continuous random variable Y has the CDF given by F(y), then F(Y ) is uniformly distributed on (0,1).\nTheorem 3.1 leads to the well-known fact that the p-values of a test statistic are uniformly distributed on (0, 1) when the null distribution is true. This uniformity is used to validate the well-calibration of computed p-values. Another equivalent way to express Theorem 3.1 is that: suppose F(y) is a continuous CDF, let F \u22121(u) denote the inverse function defined as F \u22121(u) = inf{x|F(x) > u}, and U is uniformly distributed on (0, 1), then F \u22121(U) is distributed as F(y). That is, we can transform uniform random numbers to random numbers with any desired continuous distribution, such as normal. When the yi is discrete, Theorem 3.1 can be extended to:\nTheorem 3.2. Suppose the true distribution of Yi given Xi = xi has the CDF Fi(yi) an PMF pi(yi), where the subscript i indicates that Fi and pi depends on a covariate xi. Th randomized predictive p-values F \u2217(yi, ui) is defined as Fi(yi\u2212) + ui pi(yi) (3.1). Suppose U is uniformly distributed on (0,1). Then, we have\nF \u2217(Yi, Ui) \u223cUniform((0, 1)), and \u03c6\u22121(F \u2217(Yi, Ui)) \u223cN(0, 1).\nThe proof of Theorem 3.2 can be found in Section A. Next, we make a few remarks to larify the applications of Theorem 3.2.\n. Since the conditional distribution of F \u2217(Yi, Ui) given Xi = xi is uniformly distributed on (0, 1), and this distribution is free of xi, the marginal distribution of F \u2217(Yi, Ui) with xi marginalized away is still uniformly distributed on (0, 1). This justifies that the overall distribution of RPPs is uniform on (0, 1).\n2. In frequentist paradigm, the Fi(yi) is the CDF of the true model with the true parameters that have generated the dataset. In practice, the parameters may be estimated\n(3.3)\nwith the sample data including yi itself. The use of estimated parameters that have learned from yi itself will introduce conservatism in the predictive p-values due to using yi twice. As a result, the predictive p-values may be more concentrated around 0.5 than the uniform distribution on (0,1); correspondingly, the NRPPs tend to be more concentrated around 0 than distributed as N(0, 1). This conservatism is minor when the sample size is much larger than the number of parameters. Our empirical studies (not shown in this paper) also indicated that the conservatism affects less in the following overall GOF test applied to NRPP if we use Shapiro-Wilk normality test compared to other tests, such as Kolmogorov-Smirnov test. For very complex models with a high risk of overfitting, it is necessary to eliminate this conservatism by computing cross-validatory NRPP. In this paper, we focus on discussing the necessity of using \u201crandomization\u201d to obtain truly uniform predictive p-values, hence, we ignore this conservatism by considering relatively simple models. . In Bayesian paradigm, the Fi(yi) is the CDF of the CV predictive distribution of Y rep i given y\u2212i (and covariates x1, . . . , xn) with model parameters \u03b8 marginalized away with respect to the posterior based on y\u2212i, as given below:\nFi(yi) = P(Y rep i \u2264yi|y\u2212i) = \ufffd P(Y rep i \u2264yi|\u03b8)P(\u03b8|y\u2212i)d\u03b8.\nTherefore, the F \u2217(yi, ui) is the cross-validatory randomized predictive p-values (CVRPP Theorem 3.2 is an extension of the theorem proved by Marshall and Spiegelhalter (2007) about the uniformity of CV predictive p-values for continuous response variable under the true model to the uniformity of CVRPP for discrete response variable. In Bayesian sense, the uniformity of CVRPP holds when both of the prior and the likelihood are correctly specified. Therefore, the non-uniformity of CVRPPs may result from mis-specification in either prior or likelihood, or both, and hence, reveals the discrepancies in both prior and likelihood. Last, we note that, although NRPPs without CV is the same as RQRs in simple regression, CVRPPs can be applied to diagnose hierarchical Bayesian models for data with complex correlation structure; see Marshall and Spiegelhalter (2007); Li et al. (2017). Theorem 3.2 provides theoret-\nical foundation for model diagnosis with CVRPPs for models with discrete response. Further discussion of computing CVRPPs for Bayesian models is given in Section 6.\n# 4 Simulation Studies\nIn this section, we investigate via simulation the performance of the NRPPs and compare this approach with the NMPPs, deviance and Pearson residuals. The simulations consist of testing non-linearity in the covariate effect, over-dispersion, and zero-inflation. For each simulation, we first present the performance of NRPP under one simulation scenario. Furthermore, in order to gain more insight of the finite-sample performance, we perform power analysis by setting the sample sizes to n = 20, 50, 100, 200, 400, 600, 800 and 1000 with varying degrees of model complexity. In overall GOF, we tested the specified hypotheses: H0: the model fits the data well versus Ha: the model does not fit the data well. Shapiro-Wilk (SW) test was used for evaluating the normality of residuals. Under each simulation scenario, we randomly generated 500 datasets from a true model for examining the type I error rates and statistical powers using significance level 0.05.\n# 4.1 Detection of Non-linearity\nThe performance of the NRPPs for detecting non-linearity in the covariate effect is evaluated with a count response variable following a NB distribution based on a single dataset. We first simulate the covariate xi \u223cUniform(\u22121.5, 1.5) of size n = 1000. The response variable is then simulated from a NB regression model log(\u00b5i) = \u03b20 + \u03b21x2 i , where \u00b5i is the expected count for the ith subject. A wrong model log(\u00b5i) = \u03b20 + \u03b21xi is considered for fitting. The regression parameters were set as \u03b20 = 0 and \u03b21 = 1 while the reciprocal for the dispersion parameter associated with the NB distribution was set as k = 2. The panels of the first column of Figure 3 display the NRPPs against the covariate under the true and wrong models. Under the true model, NRPPs are randomly scattered without exhibiting any pattern and most being within -3 and 3 as standard normal variates; conversely, under the wrong model, the NRPPs clearly indicate a quadratic trend. The panels of the second column of Figure 3 present the quantile-quantile (QQ) plots of the NRPPs under the true and wrong models. Under the true model, the QQ plot almost perfectly aligns with the diagonal line, whereas under the wrong model, the QQ plot deviates from the diagonal line in both the upper and lower tails. The deviation in the tails is\nminor for this simulation due to the simulated covariate xi being symmetric about 0; larger deviations will be clearly observed if xi is asymmetric about 0. Residual plot QQ plot SW p-values\nminor for this simulation due to the simulated covariate xi being symmetric about 0; larger deviations will be clearly observed if xi is asymmetric about 0.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/315b/315bfca7-0a2d-42eb-8a16-46dbdd75d960.png\" style=\"width: 50%;\"></div>\nFigure 3: Performance of the NRPPs in detecting covariate non-linearity effect of a sample dataset of size n = 1000. The panels in the first row present the NRPPs for the fitted true model: NB model with quadratic covariate effect (i.e., exp(\u03b21x2)). The panels in the second row present the NRPPs for the fitted wrong model: NB model with linearity covariate effect (i.e.,exp(\u03b21x)). The first two columns display the scatter plots and QQ plots of the NRPPs, respectively. The third column presents the histograms of the SW p-values for the NRPPs over 500 simulated datasets from the true model. In order to quantitatively assess the overall GOF, we applied the SW test to evaluate the normality of the NRPPs resulted from fitting the true and wrong models to the 500 datasets. The panels in the third column of Figure 3 present the histograms of 500 SW p-values under the true and wrong models. The SW p-values under the true model are nearly uniform, indicating the well-calibration of this overall GOF test. In contrast, under the wrong model, the SW p-values are highly distributed near 0, implying that the wrong model will be rejected most of the times at a small nominal threshold. Thus, the overall GOF test via the SW test for the NRPPs has a probability of type I error close to 0.05 and great power in detecting the non-linear relationship in the simulation. To further investigate the finite-sample performance of the SW test for the NRPPs, Figure 4 presents the results of the power analysis by generated 500 datasets for each \u03b21 = 0.5, 1, 2 and with varying sample sizes. The probabilities of the type I errors are consistently low at nominal level 0.05 for all scenarios due to the SW p-values for the\n<div style=\"text-align: center;\">Figure 3: Performance of the NRPPs in detecting covariate non-linearity effect of a sample dataset o size n = 1000. The panels in the first row present the NRPPs for the fitted true model: NB model wit quadratic covariate effect (i.e., exp(\u03b21x2)). The panels in the second row present the NRPPs for the fitte wrong model: NB model with linearity covariate effect (i.e.,exp(\u03b21x)). The first two columns display th scatter plots and QQ plots of the NRPPs, respectively. The third column presents the histograms of th SW p-values for the NRPPs over 500 simulated datasets from the true model.</div>\nNRPPs being nearly uniformly distributed. In contrast, the probabilities of type I errors for the SW tests applied to the NMPPs, and deviance and Pearson residuals are significantly above 0.05, as their SW p-values are incorrectly distributed near 0 when the true model is fitted. Thus, these results show that evaluating GOF with the SW test is well-calibrated for the NRPPs, but is unsuitable for the NMPPs, and deviance and Pearson residuals. Figure 4 also show that the power of this GOF test for the NRPPs are reasonably high as long as a large difference between the true and wrong model is present. Although high power results are obtained for the NMPPs, and deviance and Pearson residuals, the substantially high probabilities of type I errors make the GOF tests with these residuals useless.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b4fd/b4fd0704-c229-426e-b5f0-1853ada820bb.png\" style=\"width: 50%;\"></div>\nFigure 4: Comparison of the type I errors and powers of the SW tests for the NRPPs, NMPPs, and deviance and Pearson residuals. Response variable is simulated from the true model at varying sample sizes, and nonlinear covariate effects of \u03b21 = 0.5 ( ), 1 ( ) and 2 ( +). True model: NB model with \u00b5i = exp(\u03b21x2 i ). Wrong model: NB model with \u00b5i = exp(\u03b21xi).\n# 4.2 Detection of Over-dispersion\nAs in the previous section, the same approach is implemented to investigate the performance of the NRPPs in detecting over-dispersion in the data. We first simulate a covariate x \u223c Uniform(\u22121, 2) of size n = 1000. Then, the response variable is simulated from a NB model with log(\u00b5i) = \u03b20 + \u03b21xi. We set the regression parameters as \u03b20 = 1 and \u03b21 = 2 and reciprocal for the dispersion parameter as k = 2. To examine the performance of the various types of residuals in diagnosing over-dispersion, we considered fitting a wrong\nmodel: Poisson model with the same mean function as in the true NB model. The panels in the first column of Figure 5 present the scatter plots of the NRPPs against the covariate under the true and wrong models. Under the true model, the residual plot is mostly bounded between -3 and 3 as standard normal variates without any visible trends; on the other hand, under the wrong model, all the residuals \u201cfan out\u201d from left to right, suggesting the presence of over-dispersion for increasing values of xi. The panels in the second column of Figure 5 provide the QQ plots of NRPP residuals under the true and wrong models. Under the true model, the QQ plot aligns with the diagonal line, whereas under the wrong model, the QQ plot significantly deviates from the diagonal line, exhibiting a non-linear trend. The panels in the third column of Figure 5 present the histograms of 500 SW p-values for testing the normality of the NRPPs under the true and wrong models; the SW p-values under the true model are nearly uniform while the SW p-values under the wrong model are highly distributed around 0. These results demonstrate that the overall GOF test by using the SW test for the NRPPs has a nominal-level probability of type I error and high statistical power for detecting over-dispersion.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/b519/b519ed6c-8dd0-4910-a592-697d8d3e20bd.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 5: Performance of the NRPPs in detecting over-dispersion of a sample dataset of size n = 1000. The panels in the first row present the NRPPs for the true NB model. The panels in the second row present the NRPPs for the wrong Poisson model with the same mean structure as the true model.</div>\nFigure 5: Performance of the NRPPs in detecting over-dispersion of a sample dataset of size n = 1000. The panels in the first row present the NRPPs for the true NB model. The panels in the second row present the NRPPs for the wrong Poisson model with the same mean structure as the true model. In the power analysis, we increased the level of over-dispersion in the data by setting the dispersion parameter as k = 1, 2 and 10. Figure 6 shows that the type I error rates of\npresent the NRPPs for the wrong Poisson model with the same mean structure as the true model. In the power analysis, we increased the level of over-dispersion in the data by setting the dispersion parameter as k = 1, 2 and 10. Figure 6 shows that the type I error rates of\nIn the power analysis, we increased the level of over-dispersion in the data by setting the dispersion parameter as k = 1, 2 and 10. Figure 6 shows that the type I error rates of\nthe SW test for the NRPPs remain at the nominal level 0.05 for all scenarios. In contrast, the type I error rates of the SW tests for the NMPPs, and deviance and Pearson residuals greatly exceed 0.05. Furthermore, the SW tests for the NRPPs are able to maintain high power in all scenarios as long as the sample size is sufficient. NRPP NMPP Deviance Pearson\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5c93/5c9343a4-5afb-4cc3-8315-828b6dd25324.png\" style=\"width: 50%;\"></div>\n# 4.3 Detection of Zero-Inflation\nFinally, we conduct simulations to investigate the performance of the NRPPs in detecting zero-inflation in the data. We first simulate a covariate x \u223cUniform(\u22121, 2) of size n = 1000. Then, the response is simulated from a ZIP model as described in Section 2.1. The expected mean \u03bbi of the Poisson component is set as \u03bbi = exp(\u03b20 + \u03b21xi), with \u03b20 = 1 and \u03b21 = 2, and percentage of excessive zeros pi is fixed at 30% for all observations. A Poisson model with the same expected mean \u03bbi is fitted as a wrong model. The panels in the top row of Figure 7 display the residual plot against the covariate, QQ plot and histogram of 500 SW p-values of the NRPPs under the true model. The results meet the ideal expectations under the true model: Residual plot is mostly bounded between -3 and 3 as standard normal variates without any unusual patterns; QQ plot aligns with the diagonal line, and the histogram of the SW p-values are nearly uniform. The panels in\nthe bottom row of Figure 7 present the corresponding plots of the NRPPs resulted from fitting the wrong model. In this residual plot, a clear separation of the NRPPs is observed from the residuals associated with the zero responses, which may be typical for zero-inflated data. This QQ plot shows observable deviations from the diagonal line with a nonlinear trend present in the lower tail probably due to excessively small residuals associated with the zero responses. This histogram of SW p-values is highly distributed near 0, indicating that the wrong model will be rejected most of the times with a small threshold. Residual plot QQ plot SW p-values\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/5e49/5e49fcc2-22e9-4ac6-87eb-dc851a042290.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Figure 7: Performance of the NRPPs in detecting zero-inflation of a sample dataset of size n = 1000 The panels in the first row present the NRPPs for the fitted true model: ZIP model. The panels in th second row present the NRPPs for the fitted wrong Poisson model.</div>\nIn the power analysis, we set the probability of generating excessive zeros, pi, as 0.1, 0.3 and 0.5 for various sample sizes. Figure 8 shows that the type I error rates of the NRPPs remain at the nominal level 0.05 for all scenarios. In contrast, the type I error rates of the NMPPs, and deviance and Pearson residuals greatly exceed the 0.05 threshold. In all scenarios, the NRPPs demonstrate high power even in the presence of small sample sizes.\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/4773/4773aa87-22bf-4053-adff-259bd68dedcb.png\" style=\"width: 50%;\"></div>\n# 5 Application\nIn this section, we apply the NRPP approach to examine the GOF of four non-normal regression models fitted to the National Medical Expenditure Survey (NMES) (Deb and Trivedi, 1997), a large dataset on 4406 individuals surveying the demand of health care amongst the elderly in the United States. The response variable considered in this study is the number of emergency department (ED) visits. The covariates considered include demographic characteristics (e.g., age, race, sex, marital status, education and region), socioeconomic variables (e.g., family income, employment status, supplementary private insurance status and public insurance status) and health measures (e.g., self-perceived health, the number of chronic conditions and a measure of disability status). Over 81% of the patient-year records were zero, implying that the majority of patients did not make any ED visits during the year of the study. The number of nonzero visits ranged from 1 to 12, with only 5% having more than one visit in the study year. Due to the high evidence of over-dispersion and/or excessive zero counts in this dataset, we consider fitting Poisson, NB, ZIP and ZINB regression models (described in Section 2.1). Using backward elimination with a 5% statistical significance level, all final models included the\nfollowing covariates: The number of chronic conditions, self-perceived health (excellent vs. poor; average vs. poor), limited activities of daily living (yes vs. no) and the number of years of education. In addition to those covariates, black race was significantly associated with increased ED use for the Poisson and ZIP models, but not for the NB and ZINB models. Table 1 contains the regression results of these models. It is observed that the standard errors of the estimated regression coefficients for the NB and ZINB models are all larger relative to Poisson and ZIP models, indicating that the choice of model distribution has a significant impact on assessing covariate effects. This discrepancy highlights the importance of examining the model GOF; that is, the validity of statistical inferences depend on the correctness of a fitted model. Table 1: Estimated regression coefficients for the Poisson, NB, ZIP and ZINB models fitted to the National Medical Expenditure Survey. Standard errors are given in parentheses.\n<div style=\"text-align: center;\">Table 1: Estimated regression coefficients for the Poisson, NB, ZIP and ZINB models fitte to the National Medical Expenditure Survey. Standard errors are given in parentheses.</div>\nVariables\nPoisson\nNB\nZIP\nZINB\nBlack vs. others\n0.188(0.085)*\n\u2212\n0.300(0.097)*\n\u2212\nChronic conditions\n0.221(0.020)**\n0.217(0.026)**\n0.216(0.023)**\n0.217(0.027)**\nSelf-perceived health\nExcellent vs. poor\n-1.093(0.190)**\n-1.089(0.216)**\n-1.028(0.207)**\n-1.089(0.216)**\nAverage vs. poor\n-0.505(0.074)**\n-0.478(0.100)**\n-0.451(0.088)**\n-0.478(0.101)**\nLimited daily activities\n0.453(0.070)**\n0.464(0.087)**\n0.426(0.077)**\n0.464(0.087)**\nYears of education\n-0.017(0.008)*\n-0.023(0.010)*\n-0.019(0.009)*\n-0.023(0.100)*\na\naSignificance at the 5% and 1% level is indicated with \u2217and \u2217\u2217, respectively.\nTo compare the competing models, the Akaike Information Criterion (AIC) is used in which smaller values indicate models with better out-of-sample prediction. The AIC scores for the Poisson, NB, ZIP, and ZINB are 5648, 5352, 5418 and 5354, respectively; this suggests that the NB and ZINB models provide an almost equivalent fit to the data and are superior to their counterpart Poisson models. Although AIC can be used to compare the GOF of competing models, it cannot check the goodness-of-fit of the models for assessing the need for additional complexity and validating the distribution assumption of the response variable. Model diagnosis with residuals is therefore imperative to address these concerns. The panels in the first column of Figure 9 present the scatter plots of the NRPPs versus the fitted values for each model. It is evident that the NB and ZINB models fit the dataset fairly well with NRPPs ranging mostly between -3 and 3, and no specific pattern present. In contrast, for the Poisson and ZIP models, there are many NRPPs greater than 3 and\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/66dd/66ddee66-2b86-48da-94ee-c116ee11a465.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/1ea2/1ea239eb-50f3-4be6-a4af-c4199c4ca556.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ZIP ZIP</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/fee5/fee53ba6-ada6-49c8-8e37-123459de799b.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/90d9/90d990ef-7a90-451d-a5cf-c291674ddbfa.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/a338/a3386cca-be5b-4034-b56d-0125e577bcbf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">NB NB</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/97f6/97f6b3da-e3da-4a7b-92a2-49c14796bc81.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">ZINB ZINB</div>\n<div style=\"text-align: center;\">ZINB</div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c68b/c68b112f-4bc2-47fc-80fd-435378ed72bf.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\"><img src=\"https://public-pdf-extract-kit.oss-cn-shanghai.aliyuncs.com/c939/c9394d7e-dd2a-4087-87ab-37705b3e8895.png\" style=\"width: 50%;\"></div>\n<div style=\"text-align: center;\">Fitted values</div>\nFigure 9: NRPPs for the Poisson, NB, ZIP and ZINB models fitted to the National Medical Expenditur Survey. The panels in the first two columns present the scatter plots and QQ plots of the NRPPs versus th fitted values, respectively. The third column presents the histograms of the p-values of the SW normality test for 1000 replicated NRPPs.\n<div style=\"text-align: center;\">Figure 9: NRPPs for the Poisson, NB, ZIP and ZINB models fitted to the National Medical Expenditu Survey. The panels in the first two columns present the scatter plots and QQ plots of the NRPPs versus th fitted values, respectively. The third column presents the histograms of the p-values of the SW normali test for 1000 replicated NRPPs.</div>\n<div style=\"text-align: center;\">ZINB</div>\nsome of them are as large as 6. The QQ plots of the NRPPs, as presented in the panels of the second column of Figure 9, deviate evidently away from the straight line in the upward end. The residual plots and QQ plots both indicate that the tails of Poisson and ZIP are too light for the dataset and distributions with heavier tails to accommodate over-dispersed large counts are necessary for the dataset. The residual and QQ plots of NB and ZINB models show that these two models with heavier right tails appear adequate for the dataset. In addition, we quantitatively check the GOF of the four models for this dataset through applying SW normality test to their NRPPs. One concern in using the NRPP method is the fluctuation in the residuals introduced by the randomization for producing continuously distributed residuals. To address this randomization, we replicated 1000 realizations of the NRPPs based on the same dataset. The panels in the third column of Figure 9 display the histograms of 1000 replicated p-values of the SW tests. The randomization introduced little variation for the SW p-values of the NRPPs for the fitted Poisson and ZIP models as almost all their p-values were close to 0, confirming the inadequacies of both Poisson models. Conversely, the SW p-values of the NRPPs for the fitted NB and ZINB models varied between 0 and 1 with about 96% of the p-values being above 0.05, confirming the adequacies of both NB and ZINB models. Hence, randomization does not compromise the statistical power of the NRPPs in this application. Nevertheless, when a model fits a dataset well, there are some fluctuations for the SW p-values with a roughly 5% chance that the p-value will be below 0.05. This leads to our recommendation that a large number of replicated realizations of the NRPPs should be produced to ensure that the observed discrepancies are not made by the randomization in producing the NRPPs. Although this offers a solution to alleviate the impact of the randomness in the NRPPs, it may be still desired to have a \u201cnon-random\u201d overall GOF test p-value for NRPPs. It is interesting to investigate the distribution of the mean or other summaries of replicated SW test p-values under the true model; this distribution may not be uniform.\n# Conclusion and Future Work\nDiagnosing regression models is extremely important in statistical inference. Pearson and deviance residuals and their corresponding \u03c72 tests are commonly used in practice. However, the use of these tools in non-normal regression often lacks justification because both\nPearson and deviance residuals are typically not normally distributed, and their \u03c72 tests are not well-calibrated under the true model. We have demonstrated that the NRPPs are normally distributed under the true models and the overall GOF test by checking the normality of NRPPs with Shapiro-Wilk (SW) tests is well-calibrated. We have also demonstrated the versatility and power of NRPPs in detecting a variety of model complexities, including non-linearity, zero-inflation, and over-dispersion. We have also conducted simulations to investigate the behaviours of the NRPPs with datasets generated from a logistic regression model with quadratic covariate effect. Results are not included but available upon request. The residual plots of the Pearson and deviance residuals against the covariate cluster on two separated curves according to the two possible response values 0 and 1. In contrast, the NRPPs under the true model are randomly scattered without a special pattern, whereas the residual plots of NRPPs under a wrong model assuming to have a linear covariate effect exhibit a clear quadratic pattern. The residual plot of the NRPPs is, therefore, an excellent graphical tool for detecting the nonlinearity in the logistic regression model. Nevertheless, the power of the overall GOF test by merely testing the normality of NRPPs of the logistic regression is not very high. Development of more specialized tests based on NRPPs or RPPs for detecting various model mis-specifications for logistic regression retains an interesting topic. In many clinical and public health research, correlated data (i.e., longitudinal, spatial or multilevel data) involving both structural and stochastic features are often collected due to measuring unobserved heterogeneity between clusters after conditioning on the covariates. Bayesian hierarchical model with random effects (latent variables) are typically adopted to model the complex dependence structure in these types of data. The randomization as used in RPPs is also necessary to produce truly uniformly distributed predictive p-values for diagnosing complex Bayesian hierarchical models. The randomization technique will result in modifying the CV mid-point predictive p-values defined in Marshall and Spiegelhalter (2003, 2007); Li et al. (2017) to CV randomized predictive p-values (CVRPP). Computing actual CVRPPs is time-consuming because Markov chain samplings are required to repeat for each CV posterior distribution in which an observation is excluded as a test case. There have been many approximating methods (Marshall and Spiegelhalter, 2003, 2007;\nLi et al., 2017; Vehtari et al., 2017) proposed to compute CV quantities with the Markov chain samples from the posterior distribution based on the full dataset, avoiding the actual LOOCV being implemented. It is interesting to investigate the performance of CVRPPs computed with these approximating methods in complex Bayesian hierarchical models.\nchain samples from the posterior distribution based on the full dataset, avoiding the actual LOOCV being implemented. It is interesting to investigate the performance of CVRPPs computed with these approximating methods in complex Bayesian hierarchical models. References Agresti, A. (2015). Foundations of linear and generalized linear models. John Wiley & Sons. Arbogast, P. G. and D. Y. Lin (2005, January). Model-checking techniques for stratified case-control studies. Stat Med 24(2), 229\u2013247. Benjamin, M. A., R. A. Rigby, and M. D. Stasinopoulos (2003). Generalized autoregressive moving average models. Journal of the American Statistical association 98(461), 214\u2013 223. Brilleman, S. L., M. J. Crowther, M. T. May, M. Gompels, and K. R. Abrams (2016). Joint longitudinal hurdle and time-to-event models: an application related to viral load and duration of the first treatment regimen in patients with HIV initiating therapy. Statistics in Medicine 35(20), 3583\u20133594. Deb, P. and P. Trivedi (1997). Demand for medical care by the elderly: A finite mixture approach. Journal of Applied Econometrics 12, 313\u2013336. Dunn, P. K. and G. K. Smyth (1996). Randomized quantile residuals. Journal of Computational and Graphical Statistics 5(3), 236\u2013244. Feng, C. and C. Dean (2012). Joint analysis of multivariate spatial count and zero-heavy count outcomes using common spatial factor models. Environmetrics 23(6), 493\u2013508. Gelman, A. (2013). Two simple examples for understanding posterior p-values whose distributions are far from uniform. Electron. J. Statist. 7, 2595\u20132602. Gelman, A., X. Meng, and H. Stern (1996). Posterior predictive assessment of model fitness via realized discrepancies. Statistica Sinica 6, 733\u2013760. Lambert, D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics 34(1), 1\u201314. Lee, A. H., K. Wang, and K. K. W. Yau (2001). Analysis of zero-inflated Poisson data incorporating extent of exposure. Biometrical Journal 43(8), 963.\n# References\nLe\u00b4on, L. F. and T. Cai (2012, April). Model Checking Techniques for Assessing Functional Form Specifications in Censored Linear Regression Models. Stat Sin 22(2), 509\u2013530. Li, L., C. X. Feng, and S. Qiu (2017, January). Estimating cross-validatory predictive p-values with integrated importance sampling for disease mapping models. Statist. Med. 36(14), 2220\u20132236. Lin, D. Y., L. J. Wei, and Z. Ying (2002). Model-checking techniques based on cumulative residuals. Biometrics 58(1), 1\u201312. Marshall, E. C. and D. J. Spiegelhalter (2003). Approximate cross-validatory predictive checks in disease mapping models. Statistics in Medicine 22, 1649\u20131660. Marshall, E. C. and D. J. Spiegelhalter (2007). Identifying outliers in Bayesian hierarchical models: a simulation-based approach. Bayesian Analysis 2(2), 409\u2013444. Ospina, R. and S. L. Ferrari (2012). A general class of zero-or-one inflated beta regression models. Computational Statistics & Data Analysis 56(6), 1609\u20131623. Rigby, R. A. and M. D. Stasinopoulos (2005). Generalized additive models for location, scale and shape. Journal of the Royal Statistical Society: Series C (Applied Statistics) 54(3), 507\u2013554. Vehtari, A., A. Gelman, and J. Gabry (2017, September). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Stat Comput 27(5), 1413\u20131432. Wood, S. (2006). Generalized Additive Models: An Introduction with R. CRC Press. Xu, L., A. D. Paterson, W. Turpin, and W. Xu (2015). Assessment and selection of competing models for zero-inflated microbiome data. PLOS ONE 10(7), e0129606. Yu, Q., R. Chen, W. Tang, H. He, R. Gallop, P. Crits-Christoph, J. Hu, and X. Tu (2013). Distribution-free models for longitudinal count responses with overdispersion and structural zeros. Statistics in Medicine 32(14), 2390\u20132405. Yuan, Y. and V. E. Johnson (2012). Goodness-of-Fit Diagnostics for Bayesian Hierarchical Models. Biometrics 68(1), 156\u2013164. Zhang, X., H. Mallick, Z. Tang, L. Zhang, X. Cui, A. K. Benson, and N. Yi (2017). Negative binomial mixed models for analyzing microbiome count data. BMC Bioinformatics 18(4), 1\u201310.\n# A Proof of the Uniformity of RPPs (Theorem 3.2)\nA Proof of the Uniformity of RPPs (Theorem 3.2) Proof: First, we note that the normality of \u03c6\u22121(F \u2217(Yi, Ui)) can be derived from the uniformity of RPPs based on Theorem 3.1, and hence, it suffices to prove the uniformity of RPPs. Suppose all the possible values (with positive mass) for Yi given Xi = xi are k(1), k(2), . . .. Let P(Yi = k(j)) = p(j), and F (j) = (Fi(k(j)\u2212), Fi(k(j))); note that \u03bb(F (j)) = p(j) where \u03bb(B) denotes the ordinary length (ie, Lebesgue measure) of B. We note that \u222a\u221e j=1F (j) = (0, 1)\\{Fi(k(1), Fi(k(2), . . .} and the collection of sets {F (j)|j = 1, 2, . . .} are mutually exclusive. Conditional on Yi = k(j), F \u2217(Yi, Ui) is uniformly distributed on F (j) because Ui is uniformly distributed on (0, 1). Therefore, for any interval (or Borel set) B \u2286(0, 1), P(F \u2217(Yi, Ui) \u2208B|Yi = k(j)) = \u03bb(F (j)\u2229B) p(j) . By the law of total probability, we have P(F \u2217(Yi, Ui) \u2208B) = \u221e \ufffd j=1 P(F \u2217(Yi, Ui) \u2208B|Yi = k(j)) \u00d7 P(Yi = k(j)) = \u221e \ufffd j=1 \u03bb(F (j) \u2229B) p(j) \u00d7 p(j) = \u221e \ufffd j=1 \u03bb(F (j) \u2229B) = \u03bb(\u222a\u221e j=1F (j) \u2229B) = \u03bb(B)\nA Proof of the Uniformity of RPPs (Theorem 3.2) Proof: First, we note that the normality of \u03c6\u22121(F \u2217(Yi, Ui)) can be derived from the uniformity of RPPs based on Theorem 3.1, and hence, it suffices to prove the uniformity of RPPs. Suppose all the possible values (with positive mass) for Yi given Xi = xi are k(1), k(2), . . .. Let P(Yi = k(j)) = p(j), and F (j) = (Fi(k(j)\u2212), Fi(k(j))); note that \u03bb(F (j)) = p(j) where \u03bb(B) denotes the ordinary length (ie, Lebesgue measure) of B. We note that \u222a\u221e j=1F (j) = (0, 1)\\{Fi(k(1), Fi(k(2), . . .} and the collection of sets {F (j)|j = 1, 2, . . .} are mutually exclusive. Conditional on Yi = k(j), F \u2217(Yi, Ui) is uniformly distributed on F (j) because Ui is uniformly distributed on (0, 1). Therefore, for any interval (or Borel set) B \u2286(0, 1), P(F \u2217(Yi, Ui) \u2208B|Yi = k(j)) = \u03bb(F (j)\u2229B) p(j) . By the law of total probability, we have\n",
    "paper_type": "method",
    "attri": {
        "background": "This paper addresses the challenges of model diagnostics in regression analysis for discrete responses, emphasizing the limitations of traditional Pearson and deviance residuals which are not normally distributed in such cases. It highlights the need for improved methods for model checking and validation.",
        "problem": {
            "definition": "The problem is the inadequacy of traditional residual analysis methods, such as Pearson and deviance residuals, when applied to discrete response models, leading to poor model diagnostics and potential mis-specification.",
            "key obstacle": "The core obstacle is that existing residuals cluster on lines corresponding to distinct response values, making it difficult to visually assess model fit and identify discrepancies between the model and the data."
        },
        "idea": {
            "intuition": "The idea stems from the observation that traditional predictive p-values are not uniformly distributed for discrete responses, necessitating a new approach that ensures uniformity to facilitate better model diagnostics.",
            "opinion": "The proposed idea is to introduce randomized predictive p-values (RPPs) which can be transformed into normally-distributed quantities, thereby providing a robust diagnostic tool for discrete response models.",
            "innovation": "The key innovation lies in the randomization of predictive p-values to achieve uniform distribution, enabling the use of normally-transformed RPPs (NRPPs) for effective model diagnostics, unlike previous methods which did not adequately address the challenges posed by discrete data."
        },
        "method": {
            "method name": "Randomized Predictive P-values",
            "method abbreviation": "RPP",
            "method definition": "RPPs are defined as the cumulative distribution function for a response variable, adjusted by a random uniform variable to ensure uniformity across discrete responses.",
            "method description": "The method involves defining predictive p-values that are uniformly distributed for discrete responses, facilitating the transformation into normally-distributed residuals for analysis.",
            "method steps": [
                "Define the cumulative distribution function for the response variable.",
                "Introduce a random uniform variable to adjust the predictive p-values.",
                "Transform the RPPs into normally-distributed quantities using the normal quantile function."
            ],
            "principle": "The effectiveness of the RPP method lies in its ability to produce uniformly distributed predictive p-values under the true model, which can then be transformed to assess model fit through normality tests."
        },
        "experiments": {
            "evaluation setting": "The evaluation involved extensive simulations and a real data application to demonstrate the normality of NRPPs and their utility in detecting model inadequacies across various scenarios, including non-linearity, over-dispersion, and zero-inflation.",
            "evaluation method": "The performance of NRPPs was assessed through Shapiro-Wilk normality tests and comparisons against traditional methods like Pearson and deviance residuals across multiple datasets."
        },
        "conclusion": "The study concludes that NRPPs are a powerful and versatile tool for diagnosing regression models with discrete responses, successfully addressing the shortcomings of traditional residual methods and providing reliable insights into model fit and specification.",
        "discussion": {
            "advantage": "The primary advantage of NRPPs is their ability to provide normally distributed residuals for discrete response models, allowing for effective model diagnostics and better identification of model mis-specifications.",
            "limitation": "A limitation of the method is the potential conservatism introduced by using estimated parameters from the same data, which may affect the uniformity of predictive p-values.",
            "future work": "Future research should focus on developing specialized tests based on NRPPs for other types of regression models and exploring the application of cross-validatory randomized predictive p-values in complex Bayesian hierarchical models."
        },
        "other info": {
            "acknowledgments": "The authors acknowledge financial support from NSERC and CFI.",
            "keywords": [
                "p-value",
                "predictive checking",
                "model diagnostics",
                "goodness-of-fit test",
                "non-linearity",
                "over-dispersion",
                "zero-inflation"
            ]
        }
    },
    "mount_outline": [
        {
            "section number": "2.1",
            "key information": "The problem is the inadequacy of traditional residual analysis methods, such as Pearson and deviance residuals, when applied to discrete response models, leading to poor model diagnostics and potential mis-specification."
        },
        {
            "section number": "2.3",
            "key information": "The key innovation lies in the randomization of predictive p-values to achieve uniform distribution, enabling the use of normally-transformed RPPs (NRPPs) for effective model diagnostics, unlike previous methods which did not adequately address the challenges posed by discrete data."
        },
        {
            "section number": "3.3",
            "key information": "The effectiveness of the RPP method lies in its ability to produce uniformly distributed predictive p-values under the true model, which can then be transformed to assess model fit through normality tests."
        },
        {
            "section number": "5.3",
            "key information": "Future research should focus on developing specialized tests based on NRPPs for other types of regression models and exploring the application of cross-validatory randomized predictive p-values in complex Bayesian hierarchical models."
        },
        {
            "section number": "8",
            "key information": "The study concludes that NRPPs are a powerful and versatile tool for diagnosing regression models with discrete responses, successfully addressing the shortcomings of traditional residual methods and providing reliable insights into model fit and specification."
        }
    ],
    "similarity_score": 0.5107713682872591,
    "image": null,
    "path": "/home/dany/codes/autosurvey/outputs/2025-01-11-1142_cogni/papers/Randomized Predictive P-values_ A Versatile Model Diagnostic Tool with Unified Reference Distribution.json"
}